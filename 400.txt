{
    "questions": [
        {
            "id": "d2a2b0c6ff844d848e94160c84bae1fc",
            "questionNumber": 301,
            "type": "single",
            "content": "Question #301<p>A company is building an application that will run on an AWS Lambda function. Hundreds of customers will use the application. The company wants to give each customer a quota of requests for a specific time period. The quotas must match customer usage patterns. Some customers must receive a higher quota for a shorter time period. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. For each customer, configure an API Gateway usage plan that includes an appropriate request quota. Create an API key from the usage plan for each user that the customer needs."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon API Gateway HTTP API with a proxy integration to invoke the Lambda function. For each customer configure an API Gateway usage plan that includes an appropriate request quota Configure route-level throttling for each usage plan. Create an API Key from the usage plan for each user that the customer needs."
                },
                {
                    "label": "C",
                    "content": "Create a Lambda function alias for each customer. Include a concurrency limit with an appropriate request quota. Create a Lambda function URL for each function alias. Share the Lambda function URL for each alias with the relevant customer."
                },
                {
                    "label": "D",
                    "content": "Create an Application Load Balancer (ALB) in a VPC. Configure the Lambda function as a target for the ALB. Configure an AWS WAF web ACL for the ALB. For each customer configure a rule-based rule that includes an appropriate request quota."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer because it allows for the creation of individual usage plans for each customer with specific request quotas through Amazon API Gateway. This approach provides a scalable and flexible way to manage customer-specific quotas for a Lambda function.</p><p>The requirements are: &nbsp;</p><p>1. Quota per customer – Each customer should have a specific request quota for a given time period. &nbsp;</p><p>2. Flexible quotas – Some customers need higher quotas for shorter periods, meaning quotas should be customizable per customer. &nbsp;</p><p>3. Hundreds of customers – The solution must scale efficiently. &nbsp;</p><p>Why Option A is correct? &nbsp;</p><p>- API Gateway Usage Plans + API Keys allow you to define request quotas per customer. &nbsp;</p><p>- Each usage plan can specify: &nbsp;</p><p> &nbsp;- Throttling limits (requests per second) &nbsp;</p><p> &nbsp;- Quota limits (total requests per day/week/month) &nbsp;</p><p>- API Keys are assigned to customers, enforcing their respective usage plan. &nbsp;</p><p>- REST API with proxy integration is a valid way to invoke Lambda while managing quotas. &nbsp;</p><p>Why other options are incorrect? &nbsp;</p><p>- B: HTTP API does not support usage plans or API keys (only REST API does). &nbsp;</p><p>- C: Lambda concurrency limits are not per-customer quotas; they limit concurrent executions, not request counts over time. &nbsp;</p><p>- D: AWS WAF rate-based rules are not per-customer quotas; they apply globally and lack the granularity of API Gateway usage plans. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Create an Amazon API Gateway REST API with a proxy integration to invoke the Lambda function. For each customer, configure an API Gateway usage plan that includes an appropriate request quota. Create an API key from the usage plan for each user that the customer needs.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "de1aba3e25c94cae98e4db76bea50570",
            "questionNumber": 302,
            "type": "single",
            "content": "<p>Question #302</p><p>A company is planning to migrate its on-premises VMware cluster of 120 VMs to AWS. The VMs have many different operating systems and many custom software packages installed. The company also has an on-premises NFS server that is 10 TB in size. The company has set up a 10 Gbps AWS Direct Connect connection to AWS for the migration. <br><br>Which solution will complete the migration to AWS in the LEAST amount of time?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Export the on-premises VMs and copy them to an Amazon S3 bucket. Use VM Import/Export to create AMIs from the VM images that are stored in Amazon S3. Order an AWS Snowball Edge device. Copy the NFS server data to the device. Restore the NFS server data to an Amazon EC2 instance that has NFS configured."
                },
                {
                    "label": "B",
                    "content": "Configure AWS Application Migration Service with a connection to the VMware cluster. Create a replication job for the VMs. Create an Amazon Elastic File System (Amazon EFS) file system. Configure AWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection."
                },
                {
                    "label": "C",
                    "content": "Recreate the VMs on AWS as Amazon EC2 instances. Install all the required software packages. Create an Amazon FSx for Lustre file system. Configure AWS DataSync to copy the NFS server data to the FSx for Lustre file system over the Direct Connect connection."
                },
                {
                    "label": "D",
                    "content": "Order two AWS Snowball Edge devices. Copy the VMs and the NFS server data to the devices. Run VM Import/Export after the data from the devices is loaded to an Amazon S3 bucket. Create an Amazon Elastic File System (Amazon EFS) file system. Copy the NFS server data from Amazon S3 to the EFS file system."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer because it provides a fast migration strategy by using AWS Application Migration Service for the VMware cluster and AWS DataSync for the NFS server data over the 10 Gbps Direct Connect connection. This approach minimizes the migration time without the need for physical transportation of data or recreating VMs and software packages on AWS.</p><p>The requirements are: &nbsp;</p><p>1. Migrate 120 VMs (with diverse OS and custom software) quickly. &nbsp;</p><p>2. Migrate a 10 TB NFS server efficiently. &nbsp;</p><p>3. Use the existing 10 Gbps Direct Connect connection for the migration. &nbsp;</p><p> Why Option B is the Best? &nbsp;</p><p>- AWS Application Migration Service (MGN) is the fastest way to lift-and-shift VMware VMs to AWS with minimal downtime. &nbsp;</p><p> &nbsp;- It replicates VMs directly from VMware to AWS over Direct Connect. &nbsp;</p><p> &nbsp;- No manual reinstallation of software is needed. &nbsp;</p><p>- EFS + DataSync is efficient for migrating the NFS server: &nbsp;</p><p> &nbsp;- DataSync optimizes transfer over Direct Connect, speeding up the 10 TB migration. &nbsp;</p><p> &nbsp;- EFS is a managed NFS-compatible service, eliminating the need to manage an EC2-based NFS server. &nbsp;</p><p> Why Other Options Are Worse? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- VM Import/Export + Snowball is slow (manual exports, Snowball shipping delays). &nbsp;</p><p> &nbsp;- Snowball is unnecessary since Direct Connect is already available. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Manually recreating VMs is time-consuming (installing software again). &nbsp;</p><p> &nbsp;- FSx for Lustre is overkill (optimized for HPC, not general NFS workloads). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Snowball is unnecessary (Direct Connect is faster for 10 TB). &nbsp;</p><p> &nbsp;- Manual VM Import/Export + EFS copy adds extra steps, increasing migration time. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ B. Configure AWS Application Migration Service with a connection to the VMware cluster. Create a replication job for the VMs. Create an Amazon Elastic File System (Amazon EFS) file system. Configure AWS DataSync to copy the NFS server data to the EFS file system over the Direct Connect connection.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "24a1a6566e3a43ec8756224ee136d369",
            "questionNumber": 303,
            "type": "single",
            "content": "<p>Question #303</p><p>An online survey company runs its application in the AWS Cloud. The application is distributed and consists of microservices that run in an automatically scaled Amazon Elastic Container Service (Amazon ECS) cluster. The ECS cluster is a target for an Application Load Balancer (ALB). The ALB is a custom origin for an Amazon CloudFront distribution. <br><br>The company has a survey that contains sensitive data. The sensitive data must be encrypted when it moves through the application. The application's data-handling microservice is the only microservice that should be able to decrypt the data. <br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a field-level encryption profile and a configuration. Associate the KMS key and the configuration with the CloudFront cache behavior."
                },
                {
                    "label": "B",
                    "content": "Create an RSA key pair that is dedicated to the data-handling microservice. Upload the public key to the CloudFront distribution. Create a field-level encryption profile and a configuration. Add the configuration to the CloudFront cache behavior."
                },
                {
                    "label": "C",
                    "content": "Create a symmetric AWS Key Management Service (AWS KMS) key that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the KMS key to encrypt the sensitive data."
                },
                {
                    "label": "D",
                    "content": "Create an RSA key pair that is dedicated to the data-handling microservice. Create a Lambda@Edge function. Program the function to use the private key of the RSA key pair to encrypt the sensitive data."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer because it involves using an RSA key pair for field-level encryption with Amazon CloudFront, which allows the data-handling microservice to have the corresponding private key for decryption. This ensures that only the specific microservice can decrypt the sensitive data as required.</p><p>The requirements are: &nbsp;</p><p>1. Encrypt sensitive survey data in transit through the application. &nbsp;</p><p>2. Only the data-handling microservice should decrypt it (no other microservices). &nbsp;</p><p>3. The architecture involves CloudFront → ALB → ECS microservices. &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- CloudFront Field-Level Encryption (FLE) is the best solution for encrypting specific fields (e.g., survey responses) before they reach the ALB/ECS. &nbsp;</p><p> &nbsp;- RSA Key Pair ensures only the data-handling microservice (with the private key) can decrypt the data. &nbsp;</p><p> &nbsp;- Public key is uploaded to CloudFront, which encrypts sensitive fields before caching/forwarding requests. &nbsp;</p><p> &nbsp;- Private key remains with the data-handling microservice, ensuring no other component can decrypt the data. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- AWS KMS symmetric keys are not suitable for CloudFront FLE (FLE requires asymmetric keys). &nbsp;</p><p> &nbsp;- KMS keys would allow any authorized service to decrypt, violating the \"only data-handling microservice\" requirement. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Lambda@Edge + KMS symmetric key does not ensure end-to-end encryption (data is decrypted at CloudFront, then re-encrypted). &nbsp;</p><p> &nbsp;- KMS symmetric keys do not restrict decryption to a single microservice. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Lambda@Edge + RSA private key encryption is backwards (private keys should decrypt, not encrypt). &nbsp;</p><p> &nbsp;- CloudFront FLE is a managed solution and more secure than custom Lambda@Edge encryption. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "98fcbae2e9aa4ec49548e764fff63b22",
            "questionNumber": 304,
            "type": "single",
            "content": "<p>Question #304</p><p>A solutions architect is determining the DNS strategy for an existing VPC. The VPC is provisioned to use the 10.24.34.0/24 CIDR block. The VPC also uses Amazon Route 53 Resolver for DNS. New requirements mandate that DNS queries must use private hosted zones. Additionally, instances that have public IP addresses must receive corresponding public hostnames. <br><br>Which solution will meet these requirements to ensure that the domain names are correctly resolved within the VPC?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a private hosted zone. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=10.24.34.2."
                },
                {
                    "label": "B",
                    "content": "Create a private hosted zone Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute and the enableDnsHostnames attribute for the VPC. Create a new VPC DHCP options set, and configure domain-name-servers=AmazonProvidedDNS. Associate the new DHCP options set with the VPC."
                },
                {
                    "label": "C",
                    "content": "Deactivate the enableDnsSupport attribute for the VPC. Activate the enableDnsHostnames attribute for the VPC. Create a new VPC DHCP options set, and configure domain-name-servers=10.24.34.2. Associate the new DHCP options set with the VPC."
                },
                {
                    "label": "D",
                    "content": "Create a private hosted zone. Associate the private hosted zone with the VPC. Activate the enableDnsSupport attribute for the VPC. Deactivate the enableDnsHostnames attribute for the VPC. Update the VPC DHCP options set to include domain-name-servers=AmazonProvidedDNS."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer because it involves creating a private hosted zone and associating it with the VPC while also enabling both DNS support and DNS hostnames attributes. Configuring the DHCP options set to use AmazonProvidedDNS ensures that the domain names are correctly resolved for both private and public instances within the VPC.</p><p>The requirements are: &nbsp;</p><p>1. DNS queries must use private hosted zones (for internal domain resolution). &nbsp;</p><p>2. Instances with public IPs must receive public hostnames (for public DNS resolution). &nbsp;</p><p>3. The VPC already uses Amazon Route 53 Resolver for DNS. &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- Private hosted zone associated with the VPC → Ensures internal DNS queries resolve via Route 53. &nbsp;</p><p>- enableDnsSupport = true → Allows DNS resolution within the VPC (required for private hosted zones). &nbsp;</p><p>- enableDnsHostnames = true → Ensures instances with public IPs get public DNS hostnames. &nbsp;</p><p>- DHCP options set with `domain-name-servers=AmazonProvidedDNS` → Forces instances to use AWS's DNS (Route 53 Resolver). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Uses a custom DNS server (10.24.34.2) instead of `AmazonProvidedDNS`, breaking integration with Route 53 Resolver. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Disables `enableDnsSupport`, which prevents private hosted zone resolution. &nbsp;</p><p> &nbsp;- Uses a custom DNS server (10.24.34.2) instead of AWS DNS. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Disables `enableDnsHostnames`, preventing public DNS hostnames for instances with public IPs. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ B. Create a private hosted zone. Associate the private hosted zone with the VPC. Activate the `enableDnsSupport` attribute and the `enableDnsHostnames` attribute for the VPC. Create a new VPC DHCP options set, and configure `domain-name-servers=AmazonProvidedDNS`. Associate the new DHCP options set with the VPC.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "040667ef15fd4a8e908aee8a87317825",
            "questionNumber": 305,
            "type": "single",
            "content": "<p>Question #305</p><p>A data analytics company has an Amazon Redshift cluster that consists of several reserved nodes. The cluster is experiencing unexpected bursts of usage because a team of employees is compiling a deep audit analysis report. The queries to generate the report are complex read queries and are CPU intensive. <br><br>Business requirements dictate that the cluster must be able to service read and write queries at all times. A solutions architect must devise a solution that accommodates the bursts of usage. <br><br>Which solution meets these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Provision an Amazon EMR cluster Offload the complex data processing tasks."
                },
                {
                    "label": "B",
                    "content": "Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using a classic resize operation when the cluster&rsquo;s CPU metrics in Amazon CloudWatch reach 80%."
                },
                {
                    "label": "C",
                    "content": "Deploy an AWS Lambda function to add capacity to the Amazon Redshift cluster by using an elastic resize operation when the cluster&rsquo;s CPU metrics in Amazon CloudWatch reach 80%."
                },
                {
                    "label": "D",
                    "content": "Turn on the Concurrency Scaling feature for the Amazon Redshift cluster."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Turn on the Concurrency Scaling feature for the Amazon Redshift cluster.</p><p>This solution meets the requirements most cost-effectively for the following reasons:</p><p>Designed for burst workloads: Concurrency Scaling is specifically designed to handle unexpected bursts of usage, which is exactly what the company is experiencing with the deep audit analysis report.</p><p>Optimized for read queries: Concurrency Scaling is particularly effective for read-heavy workloads, and the scenario specifically mentions that the queries are \"complex read queries and are CPU intensive.\"</p><p>Maintains service continuity: When enabled, Concurrency Scaling automatically adds additional cluster capacity when needed, ensuring that both read and write queries can be serviced at all times, which is a stated business requirement.</p><p>Cost-effectiveness:</p><p>Amazon Redshift provides up to one hour of free Concurrency Scaling credits per day, which may be sufficient for the burst usage described.</p><p>You only pay for what you use beyond the free credits, and the scaling clusters are automatically turned off when not needed.</p><p>There are no upfront costs or commitments required.</p><p>Simplicity: Turning on Concurrency Scaling is a simple configuration change that doesn't require building and maintaining additional infrastructure or monitoring systems.</p><p>Option C (using Lambda to add capacity through elastic resize operations) is less optimal because:</p><p>Elastic resize operations take time to complete and may cause a brief interruption in service.</p><p>The solution would require building and maintaining a Lambda function and CloudWatch alarms.</p><p>Once resized, the cluster would remain at the larger size (and higher cost) until another resize operation is performed, even if the burst workload has completed.</p><p>There would be a delay between reaching the CPU threshold and completing the resize operation, during which performance could be degraded.</p><p>Concurrency Scaling provides a more elegant, purpose-built solution for this exact scenario - handling burst workloads while maintaining service levels for all queries, with minimal operational overhead and cost-effective pricing.</p><p>Sources</p><p>Concurrency scaling - Amazon Redshift （https://docs.aws.amazon.com/redshift/latest/dg/concurrency-scaling.html）</p><p>Scale read and write workloads with Amazon Redshift | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/scale-read-and-write-workloads-with-amazon-redshift/）</p><p>Amazon Redshift Concurrency Scaling - Amazon Web Services （https://aws.amazon.com/cn/redshift/features/concurrency-scaling/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b8abda73866246a2ae5e4b90db85dc21",
            "questionNumber": 306,
            "type": "multiple",
            "content": "<p>Question #306</p><p>A research center is migrating to the AWS Cloud and has moved its on-premises 1 PB object storage to an Amazon S3 bucket. One hundred scientists are using this object storage to store their work-related documents. Each scientist has a personal folder on the object store. All the scientists are members of a single IAM user group. </p><p><br></p><p>The research center's compliance officer is worried that scientists will be able to access each other's work. The research center has a strict obligation to report on which scientist accesses which documents. The team that is responsible for these reports has little AWS experience and wants a ready-to-use solution that minimizes operational overhead. </p><p><br></p><p>Which combination of actions should a solutions architect take to meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an identity policy that grants the user read and write access. Add a condition that specifies that the S3 paths must be prefixed with $(aws:username). Apply the policy on the scientists&rsquo; IAM user group."
                },
                {
                    "label": "B",
                    "content": "Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket. Store the trail output in another S3 bucket. Use Amazon Athena to query the logs and generate reports."
                },
                {
                    "label": "C",
                    "content": "Enable S3 server access logging. Configure another S3 bucket as the target for log delivery. Use Amazon Athena to query the logs and generate reports."
                },
                {
                    "label": "D",
                    "content": "Create an S3 bucket policy that grants read and write access to users in the scientists&rsquo; IAM user group."
                },
                {
                    "label": "E",
                    "content": "Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket and write the events to Amazon CloudWatch. Use the Amazon Athena CloudWatch connector to query the logs and generate reports."
                }
            ],
            "correctAnswer": "AB",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. Prevent scientists from accessing each other’s folders (data isolation). &nbsp;</p><p>2. Track and report who accessed which documents (audit compliance). &nbsp;</p><p>3. Minimize operational overhead (simple, ready-to-use solution). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- IAM policy with `${aws:username}` prefix ensures each scientist can only access their own folder. &nbsp;</p><p>- Applied at the IAM group level, reducing management overhead. &nbsp;</p><p>- No need for complex S3 bucket policies—access control is handled via IAM. &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- CloudTrail logs all S3 object-level API calls, including `GetObject`, `PutObject`, etc. &nbsp;</p><p>- Stored in another S3 bucket for security. &nbsp;</p><p>- Amazon Athena can query logs directly (no need for manual processing). &nbsp;</p><p>- Fully managed solution (minimal operational effort). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- S3 server access logs lack user identity (only show IP addresses), making compliance reporting difficult. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- A bucket policy alone cannot enforce per-user folder access (would require complex conditions). &nbsp;</p><p>- E: &nbsp;</p><p> &nbsp;- CloudWatch is unnecessary—CloudTrail logs can be queried directly via Athena without extra steps. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Create an identity policy that grants the user read and write access. Add a condition that specifies that the S3 paths must be prefixed with `${aws:username}`. Apply the policy on the scientists’ IAM user group. &nbsp;</p><p>✅ B. Configure a trail with AWS CloudTrail to capture all object-level events in the S3 bucket. Store the trail output in another S3 bucket. Use Amazon Athena to query the logs and generate reports.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ef0eaf3fea8c46ea93698c9206163ed4",
            "questionNumber": 307,
            "type": "single",
            "content": "<p>Question #307</p><p>A company uses AWS Organizations to manage a multi-account structure. The company has hundreds of AWS accounts and expects the number of accounts to increase. The company is building a new application that uses Docker images. The company will push the Docker images to Amazon Elastic Container Registry (Amazon ECR). Only accounts that are within the company’s organization should have access to the images. </p><p><br></p><p>The company has a CI/CD process that runs frequently. The company wants to retain all the tagged images. However, the company wants to retain only the five most recent untagged images. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a private repository in Amazon ECR. Create a permissions policy for the repository that allows only required ECR operations. Include a condition to allow the ECR operations if the value of the aws:PrincipalOrgID condition key is equal to the ID of the company&rsquo;s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five."
                },
                {
                    "label": "B",
                    "content": "Create a public repository in Amazon ECR. Create an IAM role in the ECR account. Set permissions so that any account can assume the role if the value of the aws:PrincipalOrgID condition key is equal to the ID of the company&rsquo;s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five."
                },
                {
                    "label": "C",
                    "content": "Create a private repository in Amazon ECR. Create a permissions policy for the repository that includes only required ECR operations. Include a condition to allow the ECR operations for all account IDs in the organization. Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five."
                },
                {
                    "label": "D",
                    "content": "Create a public repository in Amazon ECR. Configure Amazon ECR to use an interface VPC endpoint with an endpoint policy that includes the required permissions for images that the company needs to pull. Include a condition to allow the ECR operations for all account IDs in the company&rsquo;s organization. Schedule a daily Amazon EventBridge rule to invoke an AWS Lambda function that deletes all untagged images over the count of five."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer because it sets up a private repository in Amazon ECR with a permissions policy that restricts access to only those accounts within the organization, ensuring security. Additionally, the lifecycle rule helps manage untagged images efficiently, reducing operational overhead.</p><p>The requirements are: &nbsp;</p><p>1. Restrict ECR access to accounts within the organization (security). &nbsp;</p><p>2. Retain all tagged images (compliance/rollback needs). &nbsp;</p><p>3. Keep only the 5 most recent untagged images (storage optimization). &nbsp;</p><p>4. Minimize operational overhead (automated, no manual cleanup). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- Private ECR repository ensures only authorized users can access images. &nbsp;</p><p>- Repository policy with `aws:PrincipalOrgID` restricts access to organization members. &nbsp;</p><p>- Lifecycle rule automatically deletes untagged images beyond the 5 most recent (no manual intervention). &nbsp;</p><p>- Least operational overhead (fully managed by AWS, no Lambda/EventBridge needed). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Public repositories are insecure (anyone can pull images, violating isolation). &nbsp;</p><p> &nbsp;- IAM role assumption adds unnecessary complexity. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Manual cleanup via Lambda/EventBridge increases operational overhead (lifecycle rules are simpler). &nbsp;</p><p> &nbsp;- Hardcoding account IDs is not scalable (vs. `aws:PrincipalOrgID`). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Public repositories are insecure (violates access control requirements). &nbsp;</p><p> &nbsp;- VPC endpoints are irrelevant (access control should be policy-based, not network-based). &nbsp;</p><p> &nbsp;- Lambda/EventBridge cleanup is more complex than lifecycle rules. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Create a private repository in Amazon ECR. Create a permissions policy for the repository that allows only required ECR operations. Include a condition to allow the ECR operations if the value of the `aws:PrincipalOrgID` condition key is equal to the ID of the company’s organization. Add a lifecycle rule to the ECR repository that deletes all untagged images over the count of five.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1564c638b4e547cea04f077458a26554",
            "questionNumber": 308,
            "type": "single",
            "content": "<p>Question #308</p><p>A solutions architect is reviewing a company's process for taking snapshots of Amazon RDS DB instances. The company takes automatic snapshots every day and retains the snapshots for 7 days. </p><p><br></p><p>The solutions architect needs to recommend a solution that takes snapshots every 6 hours and retains the snapshots for 30 days. The company uses AWS Organizations to manage all of its AWS accounts. The company needs a consolidated view of the health of the RDS snapshots. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Turn on the cross-account management feature in AWS Backup. Create a backup plan that specifies the frequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags. Use AWS Backup to monitor the status of the backups."
                },
                {
                    "label": "B",
                    "content": "Turn on the cross-account management feature in Amazon RDS. Create a snapshot global policy that specifies the frequency and retention requirements. Use the RDS console in the management account to monitor the status of the backups."
                },
                {
                    "label": "C",
                    "content": "Turn on the cross-account management feature in AWS CloudFormation. From the management account, deploy a CloudFormation stack set that contains a backup plan from AWS Backup that specifies the frequency and retention requirements. Create an AWS Lambda function in the management account to monitor the status of the backups. Create an Amazon EventBridge rule in each account to run the Lambda function on a schedule."
                },
                {
                    "label": "D",
                    "content": "Configure AWS Backup in each account. Create an Amazon Data Lifecycle Manager lifecycle policy that specifies the frequency and retention requirements. Specify the DB instances as the target resource. Use the Amazon Data Lifecycle Manager console in each member account to monitor the status of the backups."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer because it utilizes AWS Backup's cross-account management feature to create and apply a backup plan with the desired frequency and retention settings. This approach allows for a consolidated view of the backup status across all accounts with minimal operational overhead.</p><p>The requirements are: &nbsp;</p><p>1. Take RDS snapshots every 6 hours (higher frequency than daily). &nbsp;</p><p>2. Retain snapshots for 30 days (longer retention than 7 days). &nbsp;</p><p>3. Consolidated view of backup health across all accounts (using AWS Organizations). &nbsp;</p><p>4. Minimize operational overhead (automated, no manual scripting). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- AWS Backup is the managed service designed for centralized backup management. &nbsp;</p><p>- Cross-account management allows backups to be managed from a central account (aligned with AWS Organizations). &nbsp;</p><p>- Backup plan easily configures 6-hour snapshots + 30-day retention. &nbsp;</p><p>- Tag-based application ensures only relevant RDS instances are included. &nbsp;</p><p>- AWS Backup Dashboard provides a consolidated view of backup status across accounts. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Amazon RDS does not natively support cross-account snapshot policies (AWS Backup is required for centralized management). &nbsp;</p><p> &nbsp;- No built-in way to enforce 6-hour snapshots (only manual or AWS Backup can do this). &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Overly complex (CloudFormation stack sets + Lambda + EventBridge introduce unnecessary overhead). &nbsp;</p><p> &nbsp;- AWS Backup already provides monitoring—no need for custom Lambda functions. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Amazon Data Lifecycle Manager (DLM) is for EBS snapshots, not RDS (AWS Backup is the correct service for RDS). &nbsp;</p><p> &nbsp;- Requires manual setup in each account, violating the \"consolidated view\" requirement. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Turn on the cross-account management feature in AWS Backup. Create a backup plan that specifies the frequency and retention requirements. Add a tag to the DB instances. Apply the backup plan by using tags. Use AWS Backup to monitor the status of the backups.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3f9f28110fc94b05b6b361d0462658ee",
            "questionNumber": 309,
            "type": "multiple",
            "content": "<p>Question #309</p><p>A company is using AWS Organizations with a multi-account architecture. The company's current security configuration for the account architecture includes SCPs, resource-based policies, identity-based policies, trust policies, and session policies. A solutions architect needs to allow an IAM user in Account A to assume a role in Account B. Which combination of steps must the solutions architect take to meet this requirement? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the SCP for Account A to allow the action."
                },
                {
                    "label": "B",
                    "content": "Configure the resource-based policies to allow the action."
                },
                {
                    "label": "C",
                    "content": "Configure the identity-based policy on the user in Account A to allow the action."
                },
                {
                    "label": "D",
                    "content": "Configure the identity-based policy on the user in Account B to allow the action."
                },
                {
                    "label": "E",
                    "content": "Configure the trust policy on the target role in Account B to allow the action."
                },
                {
                    "label": "F",
                    "content": "Configure the session policy to allow the action and to be passed programmatically by the GetSessionToken API operation."
                }
            ],
            "correctAnswer": "ACE",
            "explanation": "<p>Based on the information provided, the correct combination of steps that the solutions architect must take to allow an IAM user in Account A to assume a role in Account B are:</p><p>A. Configure the SCP for Account A to allow the action. </p><p>C. Configure the identity-based policy on the user in Account A to allow the action. </p><p>E. Configure the trust policy on the target role in Account B to allow the action.</p><p>Here's why these three steps are necessary:</p><p>Configure the SCP for Account A to allow the action (A):</p><p>Service Control Policies (SCPs) in AWS Organizations set the maximum permissions boundary for accounts.</p><p>If there are restrictive SCPs in place, they could block the sts:AssumeRole action even if other policies allow it.</p><p>The SCP must not deny the ability for users in Account A to perform the sts:AssumeRole action.</p><p>Configure the identity-based policy on the user in Account A to allow the action (C):</p><p>The IAM user in Account A needs explicit permission to assume the specific role in Account B.</p><p>This is done by attaching an identity-based policy to the user (or to a group the user belongs to) that includes the sts:AssumeRole permission for the specific role ARN in Account B.</p><p>Without this permission, the user cannot initiate the role assumption process.</p><p>Configure the trust policy on the target role in Account B to allow the action (E):</p><p>The role in Account B must trust the IAM user from Account A to assume it.</p><p>This is configured in the role's trust policy, which specifies which principals (in this case, the user from Account A) are allowed to assume the role.</p><p>Without this trust relationship, the role will reject assumption attempts even if the user has permission to try.</p><p>The other options are not required:</p><p>B. Configure the resource-based policies to allow the action - While a trust policy is a type of resource-based policy, this option is referring to other resource-based policies, which are not necessary for cross-account role assumption.</p><p>D. Configure the identity-based policy on the user in Account B to allow the action - This is incorrect because there is no user in Account B involved in this scenario. The requirement is for a user in Account A to assume a role in Account B.</p><p>In summary, cross-account role assumption requires coordination between policies in both accounts: the permission to assume the role (in Account A) and the trust relationship allowing that assumption (in Account B), all within the boundaries set by any applicable SCPs.</p><p>Sources</p><p>How to allow an IAM user in Account A to assume a role in Account B in an AWS organization | AWS re:Post （https://repost.aws/questions/QU0DuguIVGSAO8tK80hP_E7w/how-to-allow-an-iam-user-in-account-a-to-assume-a-role-in-account-b-in-an-aws-organization）</p><p>Four ways to grant cross-account access in AWS | AWS Security Blog （https://aws.amazon.com/blogs/security/four-ways-to-grant-cross-account-access-in-aws/）</p><p>Effectively implementing resource controls policies in a multi-account environment | AWS Security Blog （https://aws.amazon.com/blogs/security/effectively-implementing-resource-controls-policies-in-a-multi-account-environment/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "92f226fc36c347f3bcaad5c22e4dcecb",
            "questionNumber": 310,
            "type": "single",
            "content": "<p>Question #310</p><p>A company wants to use Amazon S3 to back up its on-premises file storage solution. The company’s on-premises file storage solution supports NFS, and the company wants its new solution to support NFS. The company wants to archive the backup files after 5 days. If the company needs archived files for disaster recovery, the company is willing to wait a few days for the retrieval of those files. </p><p><br></p><p>Which solution meets these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days."
                },
                {
                    "label": "B",
                    "content": "Deploy an AWS Storage Gateway volume gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the volume gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days."
                },
                {
                    "label": "C",
                    "content": "Deploy an AWS Storage Gateway tape gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the tape gateway. Create an S3 Lifecycle rule to move the files to S3 Standard-Infrequent Access (S3 Standard-IA) after 5 days."
                },
                {
                    "label": "D",
                    "content": "Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer because it uses AWS Storage Gateway's file gateway, which supports NFS, and the S3 Lifecycle rule to move files to S3 Glacier Deep Archive after 5 days. This option is cost-effective for long-term archiving and retrieval that can tolerate a few days of delay, as it provides the lowest storage cost for archiving data that is infrequently accessed.</p><p>The requirements are: &nbsp;</p><p>1. Support NFS (since the on-premises file storage uses NFS). &nbsp;</p><p>2. Archive backups after 5 days (with a willingness to wait days for retrieval if needed). &nbsp;</p><p>3. Cost-effective solution (prioritizing lower storage costs for archived data). &nbsp;</p><p> Why Option D is Correct? &nbsp;</p><p>- Storage Gateway File Gateway provides NFS support, matching the on-premises file storage solution. &nbsp;</p><p>- S3 Glacier Deep Archive is the most cost-effective storage class for long-term backups that are rarely accessed (retrieval times of 12+ hours are acceptable per the requirement). &nbsp;</p><p>- Lifecycle rule automates the transition from S3 Standard to Glacier Deep Archive after 5 days. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Uses S3 Standard-IA, which is not cost-effective for long-term archiving (higher cost than Glacier Deep Archive). &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Volume Gateway is for block storage (iSCSI), not file storage (NFS)—does not meet the NFS requirement. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Tape Gateway is for backup to virtual tapes, not file storage (NFS)—does not meet the NFS requirement. &nbsp;</p><p> &nbsp;- S3 Standard-IA is again not cost-effective for archiving. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ D. Deploy an AWS Storage Gateway file gateway that is associated with an S3 bucket. Move the files from the on-premises file storage solution to the file gateway. Create an S3 Lifecycle rule to move the files to S3 Glacier Deep Archive after 5 days.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2f5e69af80fb4721a96fe7ffaff32386",
            "questionNumber": 311,
            "type": "single",
            "content": "<p>Question #311</p><p>A company runs its application on Amazon EC2 instances and AWS Lambda functions. The EC2 instances experience a continuous and stable load. The Lambda functions experience a varied and unpredictable load. The application includes a caching layer that uses an Amazon MemoryDB for Redis cluster. <br><br>A solutions architect must recommend a solution to minimize the company's overall monthly costs. <br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Purchase an EC2 instance Savings Plan to cover the EC2 instances. Purchase a Compute Savings Plan for Lambda to cover the minimum expected consumption of the Lambda functions. Purchase reserved nodes to cover the MemoryDB cache nodes."
                },
                {
                    "label": "B",
                    "content": "Purchase a Compute Savings Plan to cover the EC2 instances. Purchase Lambda reserved concurrency to cover the expected Lambda usage. Purchase reserved nodes to cover the MemoryDB cache nodes."
                },
                {
                    "label": "C",
                    "content": "Purchase a Compute Savings Plan to cover the entire expected cost of the EC2 instances, Lambda functions, and MemoryDB cache nodes."
                },
                {
                    "label": "D",
                    "content": "Purchase a Compute Savings Plan to cover the EC2 instances and the MemoryDB cache nodes. Purchase Lambda reserved concurrency to cover the expected Lambda usage."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Purchase an EC2 instance Savings Plan to cover the EC2 instances. Purchase a Compute Savings Plan for Lambda to cover the minimum expected consumption of the Lambda functions. Purchase reserved nodes to cover the MemoryDB cache nodes.</p><p>This solution will minimize the company's overall monthly costs for the following reasons:</p><p>EC2 Instance Savings Plan for EC2 instances with continuous and stable load:</p><p>EC2 Instance Savings Plans provide the deepest discounts (up to 72%) for EC2 instances</p><p>Since the EC2 instances experience a \"continuous and stable load,\" this makes them perfect candidates for EC2 Instance Savings Plans</p><p>These plans are instance-specific and ideal for predictable workloads</p><p>Compute Savings Plan for Lambda functions with varied and unpredictable load:</p><p>Compute Savings Plans offer flexibility across compute services including Lambda</p><p>For the \"varied and unpredictable\" Lambda workload, a Compute Savings Plan is ideal because:</p><p>It only commits to the minimum expected consumption level</p><p>Any usage above the commitment is charged at on-demand rates</p><p>It provides flexibility without over-committing to capacity that might not be used</p><p>Reserved nodes for MemoryDB cache:</p><p>MemoryDB reserved nodes provide significant discounts for long-term commitments</p><p>Since caching layers typically have stable requirements, reserved nodes are appropriate</p><p>Option B is less cost-effective because:</p><p>Using a Compute Savings Plan for EC2 instances would provide less savings than an EC2 Instance Savings Plan for the stable EC2 workload.</p><p>Lambda reserved concurrency is not a cost-saving mechanism but rather a capacity reservation feature:</p><p>It guarantees that a specific number of execution environments will be available</p><p>It doesn't provide any discount on Lambda execution costs</p><p>For unpredictable workloads, this could lead to paying for reserved capacity that isn't used</p><p>The combination in option A aligns each service with the most appropriate cost-saving mechanism based on its usage pattern:</p><p>Stable workloads (EC2 and MemoryDB) with service-specific commitments for maximum savings</p><p>Variable workloads (Lambda) with flexible commitments that cover only the minimum expected usage</p><p>This approach provides the optimal balance of cost savings and flexibility for the described workload characteristics.</p><p>Sources</p><p>Decide on Savings Plans or Reserved Instances for EC2 instances | AWS re:Post （https://ap-southeast-2.console.aws.amazon.com/console/home?nc2=h_ct&region=ap-southeast-2&src=header-signin#）</p><p>My boss asking me on cost optimization in AWS | AWS re:Post （https://repost.aws/questions/QU3MV37BYRR6ePI6WmUgOW0g/my-boss-asking-me-on-cost-optimization-in-aws）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "81ba7a7c9a074cd7a96f2fe99159ee48",
            "questionNumber": 312,
            "type": "single",
            "content": "<p>Question #312</p><p>A company is launching a new online game on Amazon EC2 instances. The game must be available globally. The company plans to run the game in three AWS Regions us-east-1, eu-west-1, and ap-southeast-1. The game's leaderboards, player inventory, and event status must be available across Regions. <br><br>A solutions architect must design a solution that will give any Region the ability to scale to handle the load of all Regions. Additionally, users must automatically connect to the Region that provides the least latency. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an EC2 Spot Fleet. Attach the Spot Fleet to a Network Load Balancer (NLB) in each Region. Create an AWS Global Accelerator IP address that points to the NLB. Create an Amazon Route 53 latency-based routing entry for the Global Accelerator IP address. Save the game metadata to an Amazon RDS for MySQL DB instance in each Region. Set up a read replica in the other Regions."
                },
                {
                    "label": "B",
                    "content": "Create an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses geoproximity routing and points to the NLB in that Region. Save the game metadata to MySQL databases on EC2 instances in each Region. Set up replication between the database EC2 instances in each Region."
                },
                {
                    "label": "C",
                    "content": "Create an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses latency-based routing and points to the NLB in that Region. Save the game metadata to an Amazon DynamoDB global table."
                },
                {
                    "label": "D",
                    "content": "Use EC2 Global View. Deploy the EC2 instances to each Region. Attach the instances to a Network Load Balancer (NLB). Deploy a DNS server on an EC2 instance in each Region. Set up custom logic on each DNS server to redirect the user to the Region that provides the lowest latency. Save the game metadata to an Amazon Aurora global database."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct answer because it leverages Amazon DynamoDB global tables to maintain game metadata across Regions, which simplifies the data management and replication process. Additionally, using Amazon Route 53 latency-based routing ensures that users connect to the Region with the least latency, and attaching the Auto Scaling group to an NLB in each Region allows for scaling to handle variable loads with minimal operational overhead.</p><p>The requirements are: &nbsp;</p><p>1. Global availability (game must run in multiple Regions). &nbsp;</p><p>2. Low-latency connections (users must automatically connect to the nearest Region). &nbsp;</p><p>3. Cross-Region data consistency (leaderboards, inventory, and event status must sync globally). &nbsp;</p><p>4. Scalability (any Region should handle the load of all Regions). &nbsp;</p><p>5. Minimal operational overhead (fully managed services preferred). &nbsp;</p><p> Why Option C is Correct? &nbsp;</p><p>- Auto Scaling + NLB ensures scalability within each Region. &nbsp;</p><p>- Route 53 latency-based routing automatically directs users to the closest Region (lowest latency). &nbsp;</p><p>- DynamoDB Global Tables provide multi-Region replication with low-latency reads/writes, meeting the cross-Region data requirement. &nbsp;</p><p>- Fully managed services (no manual replication or DNS logic required). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Global Accelerator + RDS read replicas adds unnecessary complexity (Global Accelerator is for fixed endpoints, not dynamic scaling). &nbsp;</p><p> &nbsp;- RDS cross-Region replication has higher latency than DynamoDB Global Tables. &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Geoproximity routing is less precise than latency-based routing. &nbsp;</p><p> &nbsp;- Manual MySQL replication increases operational overhead (vs. DynamoDB Global Tables). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- EC2 Global View is for monitoring, not load balancing. &nbsp;</p><p> &nbsp;- Custom DNS logic violates the \"least operational overhead\" requirement. &nbsp;</p><p> &nbsp;- Aurora Global Database is overkill (DynamoDB is better for leaderboards/inventory). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ C. Create an Auto Scaling group for the EC2 instances. Attach the Auto Scaling group to a Network Load Balancer (NLB) in each Region. For each Region, create an Amazon Route 53 entry that uses latency-based routing and points to the NLB in that Region. Save the game metadata to an Amazon DynamoDB global table.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5c0445361b374e1bbd788b3776c89058",
            "questionNumber": 313,
            "type": "multiple",
            "content": "<p>Question #313</p><p>A company is deploying a third-party firewall appliance solution from AWS Marketplace to monitor and protect traffic that leaves the company's AWS environments. The company wants to deploy this appliance into a shared services VPC and route all outbound internet-bound traffic through the appliances. </p><p><br></p><p>A solutions architect needs to recommend a deployment method that prioritizes reliability and minimizes failover time between firewall appliances within a single AWS Region. The company has set up routing from the shared services VPC to other VPCs. </p><p><br></p><p>Which steps should the solutions architect recommend to meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy two firewall appliances into the shared services VPC, each in a separate Availability Zone."
                },
                {
                    "label": "B",
                    "content": "Create a new Network Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Network Load Balancer. Add each of the firewall appliance instances to the target group."
                },
                {
                    "label": "C",
                    "content": "Create a new Gateway Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Gateway Load Balancer. Add each of the firewall appliance instances to the target group."
                },
                {
                    "label": "D",
                    "content": "Create a VPC interface endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs."
                },
                {
                    "label": "E",
                    "content": "Deploy two firewall appliances into the shared services VPC, each in the same Availability Zone."
                },
                {
                    "label": "F",
                    "content": "Create a VPC Gateway Load Balancer endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs."
                }
            ],
            "correctAnswer": "ACF",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. High reliability (minimized failover time for firewall appliances). &nbsp;</p><p>2. Centralized outbound traffic inspection (all internet-bound traffic must pass through the firewall). &nbsp;</p><p>3. Deployment in a shared services VPC with routing from other VPCs. &nbsp;</p><p> Why These Options Are Correct? &nbsp;</p><p>- A. Deploy two firewall appliances in separate AZs → Ensures high availability (if one AZ fails, the other remains operational). &nbsp;</p><p>- C. Use a Gateway Load Balancer (GWLB) → GWLB is designed for third-party virtual appliances (like firewalls) and provides scalability + seamless failover. &nbsp;</p><p>- F. Create a VPC GWLB endpoint + update route tables → Directs all traffic through the firewall appliances (GWLB endpoint acts as the next hop). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- B. Network Load Balancer (NLB) → NLB is not optimized for firewall appliances (GWLB is purpose-built for this). &nbsp;</p><p>- D. VPC interface endpoint → Used for AWS services (e.g., S3, DynamoDB), not for routing traffic through firewalls. &nbsp;</p><p>- E. Deploy both firewalls in the same AZ → Single point of failure (violates reliability requirements). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Deploy two firewall appliances into the shared services VPC, each in a separate Availability Zone. &nbsp;</p><p>✅ C. Create a new Gateway Load Balancer in the shared services VPC. Create a new target group, and attach it to the new Gateway Load Balancer. Add each of the firewall appliance instances to the target group. &nbsp;</p><p>✅ F. Create a VPC Gateway Load Balancer endpoint. Add a route to the route table in the shared services VPC. Designate the new endpoint as the next hop for traffic that enters the shared services VPC from other VPCs.</p><p>参考：https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/getting-started.html，<br>https://docs.aws.amazon.com/vpc/latest/privatelink/create-gateway-load-balancer-endpoint-service.html<br>https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-aws-gateway-load-balancer-supported-architecture-patterns/</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "09033d1ef37c454f84e62f8c22e41d86",
            "questionNumber": 314,
            "type": "multiple",
            "content": "<p>Question #314</p><p>A solutions architect needs to migrate an on-premises legacy application to AWS. The application runs on two servers behind a load balancer. The application requires a license file that is associated with the MAC address of the server's network adapter. It takes the software vendor 12 hours to send new license files. The application also uses configuration files with a static IP address to access a database server; host names are not supported. Given these requirements, which combination of steps should be taken to implement highly available architecture for the application servers in AWS? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a pool of Elastic Network Interfaces (ENIs). Request license files from the vendor for the pool, and store the license files in Amazon S3. Create a bootstrap automation script to download a license file and attach the corresponding ENI to an Amazon EC2 instance."
                },
                {
                    "label": "B",
                    "content": "Create a pool of ENIs. Request license files from the vendor for the pool, store the license files on an Amazon EC2 instance. Create an AMI from the instance and use this AMI for all future EC2 instances."
                },
                {
                    "label": "C",
                    "content": "Create a bootstrap automation script to request a new license file from the vendor. When the response is received, apply the license file to an Amazon EC2 instance."
                },
                {
                    "label": "D",
                    "content": "Edit the bootstrap automation script to read the database server IP address from the AWS Systems Manager Parameter Store, and inject the value into the local configuration files."
                },
                {
                    "label": "E",
                    "content": "Edit an Amazon EC2 instance to include the database server IP address in the configuration files and recreate the AMI to use for all future EC2 instances."
                }
            ],
            "correctAnswer": "AD",
            "explanation": "<p>To implement a highly available architecture while addressing the licensing and configuration requirements, the architect should create a pool of ENIs with associated license files and store them in Amazon S3 (Option A), and use a bootstrap script to attach the correct ENI to the EC2 instance. Additionally, the database server IP address should be dynamically injected into the configuration files using the AWS Systems Manager Parameter Store (Option D), eliminating the need for manual updates or AMI recreation.</p><p>The requirements are: &nbsp;</p><p>1. Legacy application requires MAC-bound licensing (must persist across instance replacements). &nbsp;</p><p>2. Configuration files use static IPs (must be dynamically injected since hostnames are not supported). &nbsp;</p><p>3. High availability (must survive instance failures without manual intervention). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- ENI Pool with Pre-Licensed MACs: &nbsp;</p><p> &nbsp;- Elastic Network Interfaces (ENIs) retain MAC addresses even when detached. &nbsp;</p><p> &nbsp;- Pre-request licenses for the ENIs and store them in S3 (scalable, durable storage). &nbsp;</p><p> &nbsp;- Bootstrap script automatically attaches the correct ENI and applies the license. &nbsp;</p><p>- Ensures HA: If an instance fails, a new one can attach the same ENI (preserving licensing). &nbsp;</p><p> Why Option D is Correct? &nbsp;</p><p>- AWS Systems Manager (SSM) Parameter Store for dynamic IP injection: &nbsp;</p><p> &nbsp;- Store the database IP in Parameter Store (managed, secure). &nbsp;</p><p> &nbsp;- Bootstrap script fetches the IP and updates config files at launch. &nbsp;</p><p>- Eliminates hardcoded IPs in AMIs, enabling flexibility and easier updates. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Storing licenses on an EC2 instance (not S3) is not scalable or durable. &nbsp;</p><p> &nbsp;- AMI-based licensing is inflexible (requires rebuilding AMIs for new licenses). &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Requesting new licenses on-demand violates the 12-hour vendor delay (breaks HA during failover). &nbsp;</p><p>- E: &nbsp;</p><p> &nbsp;- Hardcoding IPs in AMIs is inflexible (IP changes require AMI rebuilds). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Create a pool of ENIs. Request license files from the vendor for the pool, and store the license files in Amazon S3. Create a bootstrap automation script to download a license file and attach the corresponding ENI to an Amazon EC2 instance. &nbsp;</p><p>✅ D. Edit the bootstrap automation script to read the database server IP address from the AWS Systems Manager Parameter Store, and inject the value into the local configuration files.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f856779ba344414c863910fc096d818e",
            "questionNumber": 315,
            "type": "single",
            "content": "<p>Question #315</p><p>A company runs its sales reporting application in an AWS Region in the United States. The application uses an Amazon API Gateway Regional API and AWS Lambda functions to generate on-demand reports from data in an Amazon RDS for MySQL database. The frontend of the application is hosted on Amazon S3 and is accessed by users through an Amazon CloudFront distribution. The company is using Amazon Route 53 as the DNS service for the domain. Route 53 is configured with a simple routing policy to route traffic to the API Gateway API. </p><p><br></p><p>In the next 6 months, the company plans to expand operations to Europe. More than 90% of the database traffic is read-only traffic. The company has already deployed an API Gateway API and Lambda functions in the new Region. A solutions architect must design a solution that minimizes latency for users who download reports. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use an AWS Database Migration Service (AWS DMS) task with full load to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to latency-based routing to connect to the API Gateway API."
                },
                {
                    "label": "B",
                    "content": "Use an AWS Database Migration Service (AWS DMS) task with full load plus change data capture (CDC) to replicate the primary database in the original Region to the database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API."
                },
                {
                    "label": "C",
                    "content": "Configure a cross-Region read replica for the RDS database in the new Region. Change the Route 53 record to latency-based routing to connect to the API Gateway API."
                },
                {
                    "label": "D",
                    "content": "Configure a cross-Region read replica for the RDS database in the new Region. Change the Route 53 record to geolocation routing to connect to the API Gateway API."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C createsa cross-Region read replica of the RDS database in the new Region, which allows users to accessand  download reports from a database that is closer to their location. </p><p>The read-only traffic makes this solution suitable,as it does not require any write operations on the secondary database. </p><p>Configuring latency-based routing on Route 53 directs users to the closest available API Gateway API based on their &nbsp;geolocation, which minimizes latency for report downloads</p><p>The requirements are: &nbsp;</p><p>1. Minimize latency for European users accessing reports. &nbsp;</p><p>2. 90% of database traffic is read-only, making read replicas ideal. &nbsp;</p><p>3. API Gateway and Lambda are already deployed in Europe—only database synchronization is needed. &nbsp;</p><p> Why Option C is Correct? &nbsp;</p><p>- Cross-Region RDS Read Replica in Europe: &nbsp;</p><p> &nbsp;- Synchronizes data from the US primary database with low latency. &nbsp;</p><p> &nbsp;- Handles read traffic locally, reducing transatlantic database queries. &nbsp;</p><p>- Route 53 Latency-Based Routing: &nbsp;</p><p> &nbsp;- Automatically directs users to the closest API Gateway endpoint (US or Europe). &nbsp;</p><p> &nbsp;- Better performance than geolocation routing (which is based on location, not actual latency). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A & B: &nbsp;</p><p> &nbsp;- AWS DMS is overkill—RDS natively supports cross-Region read replicas (simpler, no ETL setup). &nbsp;</p><p> &nbsp;- Geolocation routing (B) is less precise than latency-based routing (C). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Geolocation routing is static (based on user location), while latency-based routing dynamically selects the fastest path. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ C. Configure a cross-Region read replica for the RDS database in the new Region. Change the Route 53 record to latency-based routing to connect to the API Gateway API.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3d0dee68910e481296098d8030a79e3c",
            "questionNumber": 316,
            "type": "single",
            "content": "<p>Question #316</p><p>A software company needs to create short-lived test environments to test pull requests as part of its development process. Each test environment consists of a single Amazon EC2 instance that is in an Auto Scaling group. </p><p><br></p><p>The test environments must be able to communicate with a central server to report test results. The central server is located in an on-premises data center. A solutions architect must implement a solution so that the company can create and delete test environments without any manual intervention. The company has created a transit gateway with a VPN attachment to the on-premises network. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS CloudFormation template that contains a transit gateway attachment and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets to deploy a new stack for each VPC in the account. Deploy a new VPC for each test environment."
                },
                {
                    "label": "B",
                    "content": "Create a single VPC for the test environments. Include a transit gateway attachment and related routing configurations. Use AWS CloudFormation to deploy all test environments into the VPC."
                },
                {
                    "label": "C",
                    "content": "Create a new OU in AWS Organizations for testing. Create an AWS CloudFormation template that contains a VPC, necessary networking resources, a transit gateway attachment, and related routing configurations. Create a CloudFormation stack set that includes this template. Use CloudFormation StackSets for deployments into each account under the testing OU. Create a new account for each test environment."
                },
                {
                    "label": "D",
                    "content": "Convert the test environment EC2 instances into Docker images. Use AWS CloudFormation to configure an Amazon Elastic Kubernetes Service (Amazon EKS) cluster in a new VPC, create a transit gateway attachment, and create related routing configurations. Use Kubernetes to manage the deployment and lifecycle of the test environments."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer because it simplifies the setup by using a single VPC for all test environments, which reduces the complexity and operational overhead. By including a transit gateway attachment and related routing configurations within the VPC, test environments can communicate with the central server without the need for creating new VPCs or managing multiple accounts.</p><p>The requirements are: &nbsp;</p><p>1. Automated creation/deletion of test environments (no manual intervention). &nbsp;</p><p>2. Each test environment is a single EC2 instance in an Auto Scaling group. &nbsp;</p><p>3. Test environments must communicate with an on-premises central server (via transit gateway VPN). &nbsp;</p><p>4. Minimize operational overhead (simple, scalable solution). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- Single VPC for all test environments reduces complexity (no need for multiple VPCs or accounts). &nbsp;</p><p>- Transit gateway attachment in the VPC enables on-premises connectivity once (shared by all test environments). &nbsp;</p><p>- CloudFormation automates deployment of EC2 instances (Auto Scaling) and networking. &nbsp;</p><p>- Least operational overhead: No need for StackSets, multi-account setups, or container orchestration. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Deploying a new VPC per test environment is overly complex (transit gateway attachments and routing must be repeated). &nbsp;</p><p> &nbsp;- StackSets add unnecessary management overhead (single VPC is simpler). &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Creating new accounts per test environment is excessive (violates \"least operational overhead\"). &nbsp;</p><p> &nbsp;- StackSets + multi-account OU introduces unnecessary complexity. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Containerizing EC2 instances + EKS is overkill for single-instance test environments. &nbsp;</p><p> &nbsp;- Adds Kubernetes management overhead when simple Auto Scaling suffices. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ B. Create a single VPC for the test environments. Include a transit gateway attachment and related routing configurations. Use AWS CloudFormation to deploy all test environments into the VPC.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "315274204d7f4416b737dc34474e0761",
            "questionNumber": 317,
            "type": "multiple",
            "content": "<p>Question #317</p><p>A company is deploying a new API to AWS. The API uses Amazon API Gateway with a Regional API endpoint and an AWS Lambda function for hosting. The API retrieves data from an external vendor API, stores data in an Amazon DynamoDB global table, and retrieves data from the DynamoDB global table. The API key for the vendor's API is stored in AWS Secrets Manager and is encrypted with a customer managed key in AWS Key Management Service (AWS KMS). The company has deployed its own API into a single AWS Region. A solutions architect needs to change the API components of the company’s API to ensure that the components can run across multiple Regions in an active-active configuration. Which combination of changes will meet this requirement with the LEAST operational overhead? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy the API to multiple Regions. Configure Amazon Route 53 with custom domain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy."
                },
                {
                    "label": "B",
                    "content": "Create a new KMS multi-Region customer managed key. Create a new KMS customer managed replica key in each in-scope Region."
                },
                {
                    "label": "C",
                    "content": "Replicate the existing Secrets Manager secret to other Regions. For each in-scope Region&#39;s replicated secret, select the appropriate KMS key."
                },
                {
                    "label": "D",
                    "content": "Create a new AWS managed KMS key in each in-scope Region. Convert an existing key to a multi-Region key. Use the multi-Region key in other Regions."
                },
                {
                    "label": "E",
                    "content": "Create a new Secrets Manager secret in each in-scope Region. Copy the secret value from the existing Region to the new secret in each in-scope Region."
                },
                {
                    "label": "F",
                    "content": "Modify the deployment process for the Lambda function to repeat the deployment across in-scope Regions. Turn on the multi-Region option for the existing API. Select the Lambda function that is deployed in each Region as the backend for the multi-Region API."
                }
            ],
            "correctAnswer": "ABC",
            "explanation": "<p>To deploy the API across multiple Regions with an active-active configuration and minimal operational overhead, the architect should deploy the API to multiple Regions (Option A), create a new KMS multi-Region key (Option B), and replicate the Secrets Manager secret to other Regions (Option C). This approach ensures that the encryption keys and secrets are available in each Region without the need to manage multiple secrets or keys.</p><p>The requirements are: &nbsp;</p><p>1. Deploy the API in an active-active configuration across multiple Regions (high availability). &nbsp;</p><p>2. Minimize operational overhead (use managed services where possible). &nbsp;</p><p>3. Ensure cross-Region data consistency (DynamoDB global table is already set up). &nbsp;</p><p> Why Option A is Correct? &nbsp;</p><p>- Route 53 with multivalue answer routing distributes traffic across Regional API endpoints, enabling active-active failover. &nbsp;</p><p>- Custom domain names ensure a consistent endpoint for users. &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- KMS multi-Region keys allow Secrets Manager to decrypt secrets in other Regions without re-encrypting. &nbsp;</p><p>- Replica keys sync automatically, reducing manual key management. &nbsp;</p><p> Why Option C is Correct? &nbsp;</p><p>- Secrets Manager cross-Region replication ensures the vendor API key is available in all Regions. &nbsp;</p><p>- Automatically uses the correct KMS key per Region (aligned with Option B). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- AWS-managed KMS keys cannot be converted to multi-Region keys (only customer-managed keys support this). &nbsp;</p><p>- E: &nbsp;</p><p> &nbsp;- Manually copying secrets increases overhead (Secrets Manager replication is automated). &nbsp;</p><p>- F: &nbsp;</p><p> &nbsp;- API Gateway \"multi-Region\" option does not exist—this is a distractor (Route 53 handles routing). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Deploy the API to multiple Regions. Configure Amazon Route 53 with custom domain names that route traffic to each Regional API endpoint. Implement a Route 53 multivalue answer routing policy. &nbsp;</p><p>✅ B. Create a new KMS multi-Region customer managed key. Create a new KMS customer managed replica key in each in-scope Region. &nbsp;</p><p>✅ C. Replicate the existing Secrets Manager secret to other Regions. For each in-scope Region's replicated secret, select the appropriate KMS key.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6b04f815ee904e1ebab0000711259b9b",
            "questionNumber": 318,
            "type": "single",
            "content": "<p>Question #318</p><p>An online retail company hosts its stateful web-based application and MySQL database in an on-premises data center on a single server. The company wants to increase its customer base by conducting more marketing campaigns and promotions. In preparation, the company wants to migrate its application and database to AWS to increase the reliability of its architecture. </p><p><br></p><p>Which solution should provide the HIGHEST level of reliability?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the database to an Amazon RDS MySQL Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon Neptune."
                },
                {
                    "label": "B",
                    "content": "Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis replication group."
                },
                {
                    "label": "C",
                    "content": "Migrate the database to Amazon DocumentDB (with MongoDB compatibility). Deploy the application in an Auto Scaling group on Amazon EC2 instances behind a Network Load Balancer. Store sessions in Amazon Kinesis Data Firehose."
                },
                {
                    "label": "D",
                    "content": "Migrate the database to an Amazon RDS MariaDB Multi-AZ DB instance. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in Amazon ElastiCache for Memcached."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Migrate the database to Amazon Aurora MySQL. Deploy the application in an Auto Scaling group on Amazon EC2 instances behind an Application Load Balancer. Store sessions in an Amazon ElastiCache for Redis replication group.</p><p>This solution provides the highest level of reliability for the following reasons:</p><p>Database Layer: Amazon Aurora MySQL offers superior reliability compared to standard RDS MySQL Multi-AZ:</p><p>Aurora's storage layer is distributed across multiple Availability Zones by default</p><p>It provides faster automated failover (typically under 30 seconds)</p><p>Aurora has a self-healing storage system that automatically repairs disk failures</p><p>It can have up to 15 read replicas for higher availability and read scaling</p><p>Application Layer: The Auto Scaling group with EC2 instances behind an Application Load Balancer provides:</p><p>Automatic replacement of unhealthy instances</p><p>Distribution of traffic across multiple Availability Zones</p><p>Ability to scale out during high-demand periods (like marketing campaigns and promotions)</p><p>Fault tolerance against instance and Availability Zone failures</p><p>Session Management: Amazon ElastiCache for Redis replication group is purpose-built for session storage:</p><p>Redis is widely used and optimized for session caching</p><p>A replication group provides automatic failover capabilities</p><p>It offers multi-AZ deployment for high availability</p><p>Redis provides in-memory performance with persistence options</p><p>Integration and Ecosystem: This architecture uses services that are designed to work together seamlessly:</p><p>Aurora MySQL is compatible with MySQL, making migration straightforward</p><p>ElastiCache for Redis has extensive integration with common web frameworks for session handling</p><p>The entire stack can be deployed across multiple Availability Zones for maximum reliability</p><p>Option A is less reliable because:</p><p>Standard RDS MySQL Multi-AZ, while reliable, doesn't offer the same level of durability and availability as Aurora</p><p>Amazon Neptune is a graph database service that isn't optimized for session storage (it's designed for highly connected data)</p><p>Using Neptune for session storage would be an unusual architecture choice that could introduce unnecessary complexity</p><p>The architecture in option B represents AWS best practices for highly available web applications with stateful requirements. It provides redundancy at every layer (database, application, and session storage) and can handle the increased load from marketing campaigns and promotions while maintaining high reliability.</p><p>Sources</p><p>Data tier (Amazon Aurora and Amazon ElastiCache) - Best Practices for WordPress on AWS （https://docs.aws.amazon.com/whitepapers/latest/best-practices-wordpress/data-tier-amazon-aurora-and-amazon-elasticache.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ea6c17b9406840049e1c201deaf68001",
            "questionNumber": 319,
            "type": "single",
            "content": "<p>Question #319</p><p>A company’s solutions architect needs to provide secure Remote Desktop connectivity to users for Amazon EC2 Windows instances that are hosted in a VPC. The solution must integrate centralized user management with the company's on-premises Active Directory. Connectivity to the VPC is through the internet. The company has hardware that can be used to establish an AWS Site-to-Site VPN connection. <br><br>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy an EC2 instance as a bastion host in the VPC. Ensure that the EC2 instance is joined to the domain. Use the bastion host to access the target instances through RDP."
                },
                {
                    "label": "B",
                    "content": "Configure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory by using the AWS Directory Service for Microsoft Active Directory AD Connector. Configure permission sets against user groups for access to AWS Systems Manager. Use Systems Manager Fleet Manager to access the target instances through RDP."
                },
                {
                    "label": "C",
                    "content": "Implement a VPN between the on-premises environment and the target VPC. Ensure that the target instances are joined to the on-premises Active Directory domain over the VPN connection. Configure RDP access through the VPN. Connect from the company&rsquo;s network to the target instances."
                },
                {
                    "label": "D",
                    "content": "Deploy a managed Active Directory by using AWS Directory Service for Microsoft Active Directory. Establish a trust with the on-premises Active Directory. Deploy a Remote Desktop Gateway on AWS by using an AWS Quick Start. Ensure that the Remote Desktop Gateway is joined to the domain. Use the Remote Desktop Gateway to access the target instances through RDP."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. Secure RDP access to EC2 Windows instances (over the internet). &nbsp;</p><p>2. Centralized user management via on-premises Active Directory. &nbsp;</p><p>3. Cost-effective solution (avoid unnecessary infrastructure). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- AWS IAM Identity Center (SSO) + AD Connector integrates with on-premises AD without deploying a full managed AD. &nbsp;</p><p>- Systems Manager Session Manager (via Fleet Manager) provides secure RDP without exposing instances to the internet. &nbsp;</p><p>- No bastion host/VPN needed (reduces cost and complexity). &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Bastion host adds cost/management overhead (requires public IP, security hardening). &nbsp;</p><p> &nbsp;- AWS Managed AD is expensive if only used for RDP access. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- VPN is not cost-effective for individual user RDP access (better for site-to-site connectivity). &nbsp;</p><p> &nbsp;- Exposes instances to on-premises network risks. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Remote Desktop Gateway is overkill (adds unnecessary EC2/licensing costs). &nbsp;</p><p> &nbsp;- AWS Managed AD is expensive for this use case. &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ B. Configure AWS IAM Identity Center (AWS Single Sign-On) to integrate with the on-premises Active Directory by using the AWS Directory Service for Microsoft Active Directory AD Connector. Configure permission sets against user groups for access to AWS Systems Manager. Use Systems Manager Fleet Manager to access the target instances through RDP.</p><p>https://aws.amazon.com/blogs/mt/console-based-access-to-windows-instances-using-aws-systems-manager-fleet-manager/</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "53996820046949b0baac6684d2954103",
            "questionNumber": 320,
            "type": "single",
            "content": "<p>Question #320</p><p>A company's compliance audit reveals that some Amazon Elastic Block Store (Amazon EBS) volumes that were created in an AWS account were not encrypted. A solutions architect must implement a solution to encrypt all new EBS volumes at rest. <br><br>Which solution will meet this requirement with the LEAST effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon EventBridge rule to detect the creation of unencrypted EBS volumes. Invoke an AWS Lambda function to delete noncompliant volumes."
                },
                {
                    "label": "B",
                    "content": "Use AWS Audit Manager with data encryption."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Config rule to detect the creation of a new EBS volume. Encrypt the volume by using AWS Systems Manager Automation."
                },
                {
                    "label": "D",
                    "content": "Turn on EBS encryption by default in all AWS Regions."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Turn on EBS encryption by default in all AWS Regions.</p><p>This solution will meet the requirement to encrypt all new EBS volumes at rest with the least effort for the following reasons:</p><p>Built-in functionality: Enabling EBS encryption by default is a native AWS feature that doesn't require any custom code, additional services, or complex configurations.</p><p>Simple implementation: It's a one-time setting change per region that can be quickly enabled through the EC2 console, AWS CLI, or API.</p><p>Proactive approach: Once enabled, all new EBS volumes created in the region will be automatically encrypted at creation time, regardless of how they are created (console, CLI, API, or through services that create EBS volumes).</p><p>No ongoing maintenance: Unlike the other options, this solution doesn't require monitoring, updating code, or managing additional AWS services.</p><p>Comprehensive coverage: This setting applies to all EBS volumes created in the region, ensuring complete compliance with the encryption requirement.</p><p>The other options require significantly more effort:</p><p>A. Creating an EventBridge rule with Lambda would require writing code, setting up IAM permissions, testing the function, and monitoring its operation. It's also reactive rather than preventive, as it would delete volumes after they're created unencrypted.</p><p>B. AWS Audit Manager is primarily for compliance reporting and doesn't directly enforce encryption. It would help identify unencrypted volumes but wouldn't prevent their creation.</p><p>C. Creating an AWS Config rule with Systems Manager Automation would require setting up multiple services, creating automation documents, and ongoing maintenance. It's more complex and resource-intensive than simply enabling encryption by default.</p><p>Turning on EBS encryption by default is the most straightforward and effective solution to ensure all new EBS volumes are encrypted at rest, making it the option that requires the least effort to implement.</p><p>Sources</p><p>SEC08-BP02 Enforce encryption at rest - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/sec_protect_data_rest_encrypt.html）</p><p>Data at Rest Encryption with Amazon EBS - AWS Key Management Service Best Practices （https://docs.aws.amazon.com/whitepapers/latest/kms-best-practices/data-at-rest-encryption-with-amazon-ebs.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6ceaaf4748e3409088e76a848f1a9e87",
            "questionNumber": 321,
            "type": "single",
            "content": "<p>Question #321</p><p>A research company is running daily simulations in the AWS Cloud to meet high demand. The simulations run on several hundred Amazon EC2 instances that are based on Amazon Linux 2. Occasionally, a simulation gets stuck and requires a cloud operations engineer to solve the problem by connecting to an EC2 instance through SSH. </p><p><br></p><p>Company policy states that no EC2 instance can use the same SSH key and that all connections must be logged in AWS CloudTrail. </p><p><br></p><p>How can a solutions architect meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Launch new EC2 instances, and generate an individual SSH key for each instance. Store the SSH key in AWS Secrets Manager. Create a new IAM policy, and attach it to the engineers&rsquo; IAM role with an Allow statement for the GetSecretValue action. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Systems Manager document to run commands on EC2 instances to set a new unique SSH key. Create a new IAM policy, and attach it to the engineers&rsquo; IAM role with an Allow statement to run Systems Manager documents. Instruct the engineers to run the document to set an SSH key and to connect through any SSH client."
                },
                {
                    "label": "C",
                    "content": "Launch new EC2 instances without setting up any SSH key for the instances. Set up EC2 Instance Connect on each instance. Create a new IAM policy, and attach it to the engineers&rsquo; IAM role with an Allow statement for the SendSSHPublicKey action. Instruct the engineers to connect to the instance by using a browser-based SSH client from the EC2 console."
                },
                {
                    "label": "D",
                    "content": "Set up AWS Secrets Manager to store the EC2 SSH key. Create a new AWS Lambda function to create a new SSH key and to call AWS Systems Manager Session Manager to set the SSH key on the EC2 instance. Configure Secrets Manager to use the Lambda function for automatic rotation once daily. Instruct the engineers to fetch the SSH key from Secrets Manager when they connect through any SSH client."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct answer because it uses EC2 Instance Connect, which allows engineers to connect to their instances via SSH without the need to manage SSH keys. Instance Connect generates a new SSH key pair for each connection, and all connections are logged in CloudTrail, meeting both company policy requirements with minimal effort.</p><p>The requirements are: &nbsp;</p><p>1. No EC2 instance can use the same SSH key (unique keys per instance). &nbsp;</p><p>2. All SSH connections must be logged in CloudTrail (auditability). &nbsp;</p><p>3. Engineers must be able to SSH into instances for troubleshooting. &nbsp;</p><p> Why Option C is Correct? &nbsp;</p><p>- EC2 Instance Connect automatically generates ephemeral SSH keys (unique per session). &nbsp;</p><p>- No long-term SSH keys stored (complies with company policy). &nbsp;</p><p>- CloudTrail logs all `SendSSHPublicKey` API calls (meets audit requirements). &nbsp;</p><p>- Browser-based SSH client from the EC2 console simplifies access for engineers. &nbsp;</p><p> Why Other Options Are Incorrect? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- Storing SSH keys in Secrets Manager violates the \"no same key\" policy (keys must be unique per instance). &nbsp;</p><p> &nbsp;- Manual key management is error-prone and unscalable for hundreds of instances. &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Systems Manager documents cannot enforce unique SSH keys per instance (manual setup required). &nbsp;</p><p> &nbsp;- No CloudTrail logging for SSH connections (only logs Systems Manager API calls). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Daily key rotation is insufficient (keys must be unique per instance, not just rotated). &nbsp;</p><p> &nbsp;- Overly complex (Lambda + Secrets Manager + Session Manager for SSH is unnecessary). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ C. Launch new EC2 instances without setting up any SSH key for the instances. Set up EC2 Instance Connect on each instance. Create a new IAM policy, and attach it to the engineers’ IAM role with an Allow statement for the `SendSSHPublicKey` action. Instruct the engineers to connect to the instance by using a browser-based SSH client from the EC2 console.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3c7fd08e03574d3d8a0e12cdd74cc4e2",
            "questionNumber": 322,
            "type": "single",
            "content": "<p>Question #322</p><p>A company is migrating mobile banking applications to run on Amazon EC2 instances in a VPC. Backend service applications run in an on-premises data center. The data center has an AWS Direct Connect connection into AWS. The applications that run in the VPC need to resolve DNS requests to an on-premises Active Directory domain that runs in the data center. </p><p><br></p><p>Which solution will meet these requirements with the LEAST administrative overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Provision a set of EC2 instances across two Availability Zones in the VPC as caching DNS servers to resolve DNS queries from the application servers within the VPC."
                },
                {
                    "label": "B",
                    "content": "Provision an Amazon Route 53 private hosted zone. Configure NS records that point to on-premises DNS servers."
                },
                {
                    "label": "C",
                    "content": "Create DNS endpoints by using Amazon Route 53 Resolver. Add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC."
                },
                {
                    "label": "D",
                    "content": "Provision a new Active Directory domain controller in the VPC with a bidirectional trust between this new domain and the on-premises Active Directory domain."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create DNS endpoints by using Amazon Route 53 Resolver. Add conditional forwarding rules to resolve DNS namespaces between the on-premises data center and the VPC.</p><p>This solution will meet the requirements with the least administrative overhead for the following reasons:</p><p>Fully managed service: Route 53 Resolver is a fully managed AWS service that eliminates the need to provision, configure, and maintain your own DNS servers. AWS handles the underlying infrastructure, scaling, and high availability.</p><p>Purpose-built for hybrid DNS resolution: Route 53 Resolver with conditional forwarding rules is specifically designed to handle DNS resolution between AWS and on-premises environments, making it ideal for this banking application scenario.</p><p>Simple configuration: Setting up Route 53 Resolver endpoints and forwarding rules is straightforward through the AWS Console, CLI, or API. Once configured, it requires minimal ongoing management.</p><p>High availability by default: Route 53 Resolver endpoints are automatically deployed across multiple Availability Zones for high availability without additional configuration.</p><p>Integration with Direct Connect: Route 53 Resolver works seamlessly with Direct Connect connections, utilizing the existing network path to forward DNS queries to on-premises DNS servers.</p><p>The other options would require significantly more administrative overhead:</p><p>A. Provisioning EC2 instances as caching DNS servers would require:</p><p>Launching and maintaining multiple EC2 instances across AZs</p><p>Installing and configuring DNS server software</p><p>Managing security patches and updates</p><p>Implementing monitoring and failover mechanisms</p><p>Scaling the solution as needed</p><p>B. Using a Route 53 private hosted zone with NS records pointing to on-premises DNS servers has limitations:</p><p>NS records in private hosted zones don't function the same way as in public zones</p><p>This approach doesn't properly handle recursive DNS resolution for Active Directory</p><p>It would require additional configuration and potentially custom DNS solutions</p><p>Route 53 Resolver with conditional forwarding rules provides a clean, managed solution that's specifically designed for hybrid DNS resolution scenarios like this one, making it the option with the least administrative overhead.</p><p>Sources</p><p>Set up DNS resolution for hybrid networks in a single-account AWS environment - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/set-up-dns-resolution-for-hybrid-networks-in-a-single-account-aws-environment.html）</p><p>Active Directory Domain Services integration with Amazon Route 53 | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/active-directory-domain-services-integration-with-amazon-route-53/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "248c7f45405a4bda86ec37091c905749",
            "questionNumber": 323,
            "type": "single",
            "content": "<p>Question #323</p><p>A company processes environmental data. The company has set up sensors to provide a continuous stream of data from different areas in a city. The data is available in JSON format. <br><br>The company wants to use an AWS solution to send the data to a database that does not require fixed schemas for storage. The data must be sent in real time. <br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Amazon Kinesis Data Firehose to send the data to Amazon Redshift."
                },
                {
                    "label": "B",
                    "content": "Use Amazon Kinesis Data Streams to send the data to Amazon DynamoDB."
                },
                {
                    "label": "C",
                    "content": "Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) to send the data to Amazon Aurora."
                },
                {
                    "label": "D",
                    "content": "Use Amazon Kinesis Data Firehose to send the data to Amazon Keyspaces (for Apache Cassandra)."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use Amazon Kinesis Data Streams to send the data to Amazon DynamoDB.</p><p>This solution will meet the requirements for the following reasons:</p><p>Real-time data processing: Amazon Kinesis Data Streams is designed to collect and process large streams of data records in real time. It can handle the continuous stream of sensor data and make it available for processing within milliseconds of being received.</p><p>Schema-less database storage: Amazon DynamoDB is a NoSQL database service that does not require fixed schemas. This makes it ideal for storing JSON data from sensors that might have varying structures or attributes over time. Each item in DynamoDB can have a different set of attributes, allowing for flexible data modeling.</p><p>JSON format support: DynamoDB natively supports JSON data, making it straightforward to store the sensor data in its original format without transformation.</p><p>Scalability: Both Kinesis Data Streams and DynamoDB are fully managed services that can scale to handle the high volume of data from multiple sensors across different areas in the city.</p><p>Let's examine why the other options don't fully meet the requirements:</p><p>A. Kinesis Data Firehose to Amazon Redshift - Amazon Redshift is a columnar database that requires a predefined schema. While Firehose provides real-time data delivery, Redshift doesn't meet the requirement for schema-less storage.</p><p>C. Amazon MSK to Amazon Aurora - While Amazon MSK provides real-time streaming capabilities, Amazon Aurora (whether PostgreSQL or MySQL compatible) requires defined schemas for tables, which doesn't meet the schema-less requirement.</p><p>D. Kinesis Data Firehose to Amazon Keyspaces - While Amazon Keyspaces is a NoSQL database that could potentially handle schema-less data, it's optimized for Apache Cassandra workloads and typically requires more schema definition than DynamoDB. Additionally, Keyspaces is less commonly used for this type of IoT sensor data compared to DynamoDB.</p><p>The combination of Kinesis Data Streams and DynamoDB is a well-established pattern for real-time processing of schema-less data in AWS, making it the most appropriate solution for the company's environmental data processing requirements.</p><p>Sources</p><p>Serverless Stream-Based Processing for Real-Time Insights | AWS Architecture Blog （https://aws.amazon.com/cn/blogs/architecture/serverless-stream-based-processing-for-real-time-insights/）</p><p>Amazon Redshift data ingestion options | AWS Big Data Blog（https://aws.amazon.com/cn/blogs/big-data/amazon-redshift-data-ingestion-options/） </p><p>Working with streaming data on AWS - Build Modern Data Streaming Architectures on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/working-with-streaming-data-on-aws.html）</p><p>Architecture patterns to optimize Amazon Redshift performance at scale | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/architecture-patterns-to-optimize-amazon-redshift-performance-at-scale/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1a8d905880764d3c8574897f9bbcce97",
            "questionNumber": 324,
            "type": "single",
            "content": "<p>Question #324</p><p>A company is migrating a legacy application from an on-premises data center to AWS. The application uses MongoDB as a key-value database. According to the company's technical guidelines, all Amazon EC2 instances must be hosted in a private subnet without an internet connection. In addition, all connectivity between applications and databases must be encrypted. The database must be able to scale based on demand. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the instance endpoint to connect to Amazon DocumentDB."
                },
                {
                    "label": "B",
                    "content": "Create new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables."
                },
                {
                    "label": "C",
                    "content": "Create new Amazon DynamoDB tables for the application with on-demand capacity. Use an interface VPC endpoint for DynamoDB to connect to the DynamoDB tables."
                },
                {
                    "label": "D",
                    "content": "Create new Amazon DocumentDB (with MongoDB compatibility) tables for the application with Provisioned IOPS volumes. Use the cluster endpoint to connect to Amazon DocumentDB."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create new Amazon DynamoDB tables for the application with on-demand capacity. Use a gateway VPC endpoint for DynamoDB to connect to the DynamoDB tables.</p><p>This solution meets all the requirements for the following reasons:</p><p>Private subnet requirement: The gateway VPC endpoint for DynamoDB allows EC2 instances in private subnets to communicate with DynamoDB without requiring internet access. This endpoint creates a private connection between your VPC and the DynamoDB service, ensuring that traffic doesn't traverse the public internet.</p><p>Encrypted connectivity: DynamoDB automatically encrypts all traffic in transit using HTTPS/TLS, meeting the requirement that all connectivity between applications and databases must be encrypted.</p><p>Scalability based on demand: DynamoDB with on-demand capacity automatically scales up and down based on the actual traffic patterns of your application. There's no need to predict capacity in advance, and the database will scale instantly to accommodate workload changes, perfectly meeting the requirement to scale based on demand.</p><p>Key-value database compatibility: Since the legacy application uses MongoDB as a key-value database, DynamoDB is well-suited as it's designed specifically as a key-value and document database service.</p><p>Option A (using Amazon DocumentDB with instance endpoints) doesn't fully meet the requirements because:</p><p>While DocumentDB is MongoDB-compatible, the instance endpoint approach doesn't provide the same level of automatic scaling as DynamoDB with on-demand capacity. With DocumentDB, you would need to manually add or remove instances to scale, or set up Auto Scaling, which is more complex than DynamoDB's on-demand scaling.</p><p>DocumentDB instance endpoints would require additional networking configuration to ensure private connectivity from the EC2 instances in private subnets, whereas the gateway VPC endpoint for DynamoDB is a more straightforward solution for private connectivity.</p><p>Provisioned IOPS volumes provide consistent performance but don't automatically scale capacity based on demand without manual intervention.</p><p>DynamoDB with on-demand capacity and a gateway VPC endpoint provides a fully managed, automatically scaling solution that meets all the security, connectivity, and scalability requirements specified in the scenario.</p><p>Sources</p><p>Migrating to Amazon DocumentDB - Getting Started with Amazon DocumentDB (with MongoDB Compatibility) (https://docs.aws.amazon.com/documentdb/latest/developerguide/get-started-guide.html)</p><p>Amazon DocumentDB (with MongoDB compatibility) - Choosing an AWS NoSQL Database （https://docs.aws.amazon.com/whitepapers/latest/choosing-an-aws-nosql-database/amazon-documentdb.html）</p><p>Hosting MongoDB in AWS | AWS re:Post （https://repost.aws/questions/QUXa08K95KROy1vpNJJQithA/hosting-mongodb-in-aws）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d509702d12c64333b57ecb6b4ce143a0",
            "questionNumber": 325,
            "type": "single",
            "content": "<p>Question #325</p><p>A company is running an application on Amazon EC2 instances in the AWS Cloud. The application is using a MongoDB database with a replica set as its data tier. The MongoDB database is installed on systems in the company’s on-premises data center and is accessible through an AWS Direct Connect connection to the data center environment.</p><p><br></p><p> A solutions architect must migrate the on-premises MongoDB database to Amazon DocumentDB (with MongoDB compatibility). </p><p><br></p><p>Which strategy should the solutions architect choose to perform this migration?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a fleet of EC2 instances. Install MongoDB Community Edition on the EC2 instances, and create a database. Configure continuous synchronous replication with the database that is running in the on-premises data center."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Database Migration Service (AWS DMS) replication instance. Create a source endpoint for the on-premises MongoDB database by using change data capture (CDC). Create a target endpoint for the Amazon DocumentDB database. Create and run a DMS migration task."
                },
                {
                    "label": "C",
                    "content": "Create a data migration pipeline by using AWS Data Pipeline. Define data nodes for the on-premises MongoDB database and the Amazon DocumentDB database. Create a scheduled task to run the data pipeline."
                },
                {
                    "label": "D",
                    "content": "Create a source endpoint for the on-premises MongoDB database by using AWS Glue crawlers. Configure continuous asynchronous replication between the MongoDB database and the Amazon DocumentDB database."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. AWS Database Migration Service (AWS DMS) is designed for migrating databases to AWS with minimal downtime. Using CDC with AWS DMS allows for continuous data replication from the source MongoDB database to the target Amazon DocumentDB, which is compatible with MongoDB.</p><p>AWS Database Migration Service (AWS DMS) is the recommended service for migrating databases to AWS, including migrations from MongoDB to Amazon DocumentDB (with MongoDB compatibility). Here’s why: &nbsp;</p><p>- AWS DMS supports Change Data Capture (CDC), which allows continuous replication of changes from the source (on-premises MongoDB) to the target (Amazon DocumentDB) with minimal downtime. &nbsp;</p><p>- It handles schema conversion (if needed) and ensures data consistency. &nbsp;</p><p>- The steps involve: &nbsp;</p><p> &nbsp;1. Creating a DMS replication instance. &nbsp;</p><p> &nbsp;2. Defining a source endpoint for the on-premises MongoDB. &nbsp;</p><p> &nbsp;3. Defining a target endpoint for Amazon DocumentDB. &nbsp;</p><p> &nbsp;4. Creating and running a migration task (initial load + CDC for ongoing changes). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Running MongoDB on EC2 with synchronous replication is complex, costly, and not a managed solution like Amazon DocumentDB. &nbsp;</p><p>- C: AWS Data Pipeline is not designed for real-time or CDC-based database migrations; it’s better for batch data processing. &nbsp;</p><p>- D: AWS Glue crawlers are for metadata discovery (not database migration), and asynchronous replication is not a standard feature for this use case. &nbsp;</p><p>Thus, B is the correct and most efficient approach. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6cdd3bd4a5304f288c3a156d6051ec97",
            "questionNumber": 326,
            "type": "single",
            "content": "<p>Question #326</p><p>A company is rearchitecting its applications to run on AWS. The company’s infrastructure includes multiple Amazon EC2 instances. The company's development team needs different levels of access. The company wants to implement a policy that requires all Windows EC2 instances to be joined to an Active Directory domain on AWS. The company also wants to implement enhanced security processes such as multifactor authentication (MFA). The company wants to use managed AWS services wherever possible. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Directory Service Simple AD implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Directory Service Simple AD implementation. Launch an Amazon Workspace. Connect to and use the Workspace for domain security configuration tasks."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS Directory Service for Microsoft Active Directory implementation. Launch an EC2 instance. Connect to and use the EC2 instance for domain security configuration tasks.</p><p>This solution meets the company's requirements for the following reasons:</p><p>AWS Directory Service for Microsoft Active Directory (AWS Managed Microsoft AD) is a fully managed implementation of Active Directory on AWS, which satisfies the requirement to use managed AWS services wherever possible.</p><p>AWS Managed Microsoft AD allows Windows EC2 instances to be joined to an Active Directory domain, meeting the policy requirement that all Windows EC2 instances must be joined to an Active Directory domain on AWS.</p><p>AWS Managed Microsoft AD supports multi-factor authentication (MFA), addressing the enhanced security requirement.</p><p>Using an EC2 instance for domain security configuration tasks provides the necessary administrative capabilities to:</p><p>Configure different levels of access for the development team</p><p>Manage Group Policy Objects (GPOs) to enforce security policies</p><p>Set up and manage MFA for users</p><p>Configure and manage domain joining for Windows EC2 instances</p><p>While option A (using Amazon WorkSpaces) might seem similar, it's less appropriate for this specific scenario because:</p><p>Amazon WorkSpaces is primarily a managed desktop service, not a tool for domain administration. While it can be used to access administrative tools, it's an additional service layer that isn't necessary for the stated requirements.</p><p>The scenario specifically mentions managing EC2 instances and implementing domain-joining policies. Working directly with an EC2 instance provides more direct control over these domain management tasks.</p><p>The company needs to implement different levels of access for the development team, which is more efficiently managed through direct domain administration on an EC2 instance rather than through WorkSpaces.</p><p>Using an EC2 instance for domain administration is more cost-effective than provisioning WorkSpaces when the primary goal is domain management rather than providing virtual desktops to users.</p><p>In summary, option B provides the most direct and appropriate solution for implementing and managing an Active Directory domain on AWS with the required security features, while still leveraging managed AWS services (AWS Directory Service for Microsoft Active Directory).</p><p>Sources</p><p>Directory services options in AWS - Active Directory Domain Services on AWS （https://docs.aws.amazon.com/whitepapers/latest/active-directory-domain-services/directory-services-options-in-aws.html）</p><p>Connect on-prem AD to VPC, is it a AD connector required? | AWS re:Post （https://repost.aws/questions/QUNPG4paiiRJWzFobuUtbyTQ/connect-on-prem-ad-to-vpc-is-it-a-ad-connector-required）</p><p>The Role of the AWS AD Connector with Amazon WorkSpaces - Best Practices for Deploying WorkSpaces （https://docs.aws.amazon.com/whitepapers/latest/best-practices-deploying-amazon-workspaces/ad-connector-role-with-workspaces.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5e176a78f8d541b189121c8df1105f6f",
            "questionNumber": 327,
            "type": "single",
            "content": "<p>Question #327</p><p>A company wants to migrate its on-premises application to AWS. The database for the application stores structured product data and temporary user session data. The company needs to decouple the product data from the user session data. The company also needs to implement replication in another AWS Region for disaster recovery.<br><br>Which solution will meet these requirements with the HIGHEST performance?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon RDS DB instance with separate schemas to host the product data and the user session data. Configure a read replica for the DB instance in another Region."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create a global datastore in Amazon ElastiCache for Memcached to host the user session data."
                },
                {
                    "label": "C",
                    "content": "Create two Amazon DynamoDB global tables. Use one global table to host the product data. Use the other global table to host the user session data. Use DynamoDB Accelerator (DAX) for caching."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon RDS DB instance to host the product data. Configure a read replica for the DB instance in another Region. Create an Amazon DynamoDB global table to host the user session data."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>C&D争议比较大，建议选择C，解释如下：</p><p>To migrate the on-premises application to AWS while decoupling product data from user session data and implementing cross-region replication for disaster recovery, the solution that will meet these requirements with the highest performance is:</p><p><br></p><p>C. Create two Amazon DynamoDB global tables. Use one global table to host the product data. Use the other global table to host the user session data. Use DynamoDB Accelerator (DAX) for caching.</p><p><br></p><p>Here's a breakdown of the solution and implementation steps:</p><p>1. Set up Amazon DynamoDB Global Tables</p><p> &nbsp; a. Create two separate DynamoDB global tables: one for product data and another for user session data.</p><p> &nbsp; b. Configure these tables in your primary region and the disaster recovery region.</p><p> &nbsp; c. Enable global table replication to ensure data consistency across regions.</p><p>2. Migrate Product Data</p><p> &nbsp; a. Use AWS Database Migration Service (DMS) to migrate the structured product data from the on-premises database to the DynamoDB global table for products.</p><p> &nbsp; b. Configure and run the migration task, ensuring data integrity during the transfer.</p><p>3. Migrate User Session Data</p><p> &nbsp; a. Set up the DynamoDB global table for user session data.</p><p> &nbsp; b. Modify the application to write new session data to this table instead of the on-premises database.</p><p> &nbsp; c. Migrate existing session data if necessary, considering the temporary nature of this data.</p><p>4. Implement DynamoDB Accelerator (DAX)</p><p> &nbsp; a. Set up DAX clusters in both the primary and disaster recovery regions.</p><p> &nbsp; b. Configure the application to use DAX for caching, which will significantly improve read performance for frequently accessed items.</p><p>5. Update Application Configuration</p><p> &nbsp; a. Modify the application to connect to the new DynamoDB tables instead of the on-premises database.</p><p> &nbsp; b. Implement proper error handling and retry logic for database operations.</p><p>6. Test and Validate</p><p> &nbsp; a. Perform thorough testing of the application with the new database setup.</p><p> &nbsp; b. Verify that data replication is working correctly between regions.</p><p> &nbsp; c. Test the disaster recovery scenario by simulating a failover to the secondary region.</p><p>7. Monitor and Optimize</p><p> &nbsp; a. Set up Amazon CloudWatch alarms to monitor DynamoDB and DAX performance metrics.</p><p> &nbsp; b. Use AWS X-Ray for tracing and identifying performance bottlenecks in the application.</p><p>This solution offers several advantages:</p><p><br></p><p>- High performance: DynamoDB provides single-digit millisecond latency, and DAX further improves read performance.</p><p>- Scalability: DynamoDB automatically scales to handle varying workloads.</p><p>- Built-in disaster recovery: Global tables provide automatic, multi-region replication.</p><p>- Separation of concerns: Product data and session data are stored in separate tables, allowing for independent scaling and management.</p><p>Sources</p><p>[1] [Migration strategy for relational databases - AWS Prescriptive Guidance] (https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-database-migration/welcome.html)</p><p>[3] [Migrating databases to their Amazon RDS equivalents with AWS DMS - AWS Database Migration Service] (https://docs.aws.amazon.com/dms/latest/userguide/data-migrations.html)</p><p>[6] [Choosing an AWS database service - Choosing an AWS database service] (https://docs.aws.amazon.com/decision-guides/latest/databases-on-aws-how-to-choose/databases-on-aws-how-to-choose.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "e1093c90958646a89ec8c420ebfc5233",
            "questionNumber": 328,
            "type": "single",
            "content": "<p>Question #328</p><p>A company orchestrates a multi-account structure on AWS by using AWS Control Tower. The company is using AWS Organizations, AWS Config, and AWS Trusted Advisor. The company has a specific OU for development accounts that developers use to experiment on AWS. The company has hundreds of developers, and each developer has an individual development account. </p><p><br></p><p>The company wants to optimize costs in these development accounts. Amazon EC2 instances and Amazon RDS instances in these accounts must be burstable. The company wants to disallow the use of other services that are not relevant. </p><p><br></p><p>What should a solutions architect recommend to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a custom SCP in AWS Organizations to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the SCP to the development OU."
                },
                {
                    "label": "B",
                    "content": "Create a custom detective control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU."
                },
                {
                    "label": "C",
                    "content": "Create a custom preventive control (guardrail) in AWS Control Tower. Configure the control (guardrail) to allow the deployment of only burstable instances and to disallow services that are not relevant. Apply the control (guardrail) to the development OU."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Config rule in the AWS Control Tower account. Configure the AWS Config rule to allow the deployment of only burstable instances and to disallow services that are not relevant. Deploy the AWS Config rule to the development OU by using AWS CloudFormation StackSets."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer. A Service Control Policy (SCP) in AWS Organizations can enforce compliance across multiple accounts, including restricting the deployment of resources to burstable types and disallowing the use of irrelevant services. Applying the SCP to the development OU ensures that all developer accounts follow the company's cost optimization guidelines.</p><p>To enforce cost optimization and restrict services in development accounts, AWS Organizations Service Control Policies (SCPs) are the most effective solution. Here’s why: &nbsp;</p><p>- SCPs are preventive controls that define what AWS services, resources, or actions are allowed or denied across accounts in an OU. &nbsp;</p><p>- The requirement is to: &nbsp;</p><p> &nbsp;- Allow only burstable EC2 (e.g., T-series) and RDS (e.g., T-series) instances. &nbsp;</p><p> &nbsp;- Disallow non-relevant services (e.g., expensive instance types or prohibited services). &nbsp;</p><p>- Applying an SCP to the development OU ensures all accounts under it comply with these restrictions. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B & C: AWS Control Tower guardrails are predefined compliance rules (either preventive or detective), but they do not support custom restrictions like allowing only burstable instances or blocking specific services. &nbsp;</p><p>- D: AWS Config rules are detective, not preventive—they check compliance after resources are created but cannot block actions proactively. &nbsp;</p><p> Key Takeaway: &nbsp;</p><p>SCPs (Option A) are the correct way to enforce cost controls at the OU level. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "da9059fbec4c4236960ed20c389c27d0",
            "questionNumber": 329,
            "type": "single",
            "content": "<p>Question #329</p><p>A financial services company runs a complex, multi-tier application on Amazon EC2 instances and AWS Lambda functions. The application stores temporary data in Amazon S3. The S3 objects are valid for only 45 minutes and are deleted after 24 hours. </p><p><br></p><p>The company deploys each version of the application by launching an AWS CloudFormation stack. The stack creates all resources that are required to run the application. When the company deploys and validates a new application version.</p><p><br></p><p>The company deletes the CloudFormation stack of the old version. The company recently tried to delete the CloudFormation stack of an old application version, but the operation failed. An analysis shows that CloudFormation failed to delete an existing S3 bucket. A solutions architectneeds to resolve this issue without making major changes to the application's architecture. <br><br>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Implement a Lambda function that deletes all files from a given S3 bucket. Integrate this Lambda function as a custom resource into the CloudFormation stack. Ensure that the custom resource has a DependsOn attribute that points to the S3 bucket&#39;s resource."
                },
                {
                    "label": "B",
                    "content": "Modify the CloudFormation template to provision an Amazon Elastic File System (Amazon EFS) file system to store the temporary files there instead of in Amazon S3. Configure the Lambda functions to run in the same VPC as the file system. Mount the file system to the EC2 instances and Lambda functions."
                },
                {
                    "label": "C",
                    "content": "Modify the CloudFormation stack to create an S3 Lifecycle rule that expires all objects 45 minutes after creation. Add a DependsOn attribute that points to the S3 bucket&rsquo;s resource."
                },
                {
                    "label": "D",
                    "content": "Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer. By implementing a Lambda function that empties the S3 bucket before deletion, the architect can ensure that the S3 bucket gets deleted as part of the CloudFormation stack deletion process. The DependsOn attribute ensures the order of operations, allowing the bucket to be emptied before attempting to delete it.</p><p>The issue is that CloudFormation fails to delete an S3 bucket because it contains objects, and CloudFormation cannot delete non-empty buckets by default. The solution must: &nbsp;</p><p>1. Ensure the bucket is empty before deletion (without major architectural changes). &nbsp;</p><p>2. Integrate seamlessly with CloudFormation stack deletion. &nbsp;</p><p>Option A achieves this by: &nbsp;</p><p>- Using a Lambda-backed custom resource to programmatically delete all objects in the bucket. &nbsp;</p><p>- Adding a `DependsOn` attribute to ensure the Lambda runs before CloudFormation attempts to delete the bucket. &nbsp;</p><p>- This is a minimal change and follows CloudFormation best practices for cleanup. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: Switching to EFS is a major architectural change (against the requirements). &nbsp;</p><p>- C: A lifecycle rule does not help—it only deletes objects after 45 minutes, but CloudFormation deletion is immediate. &nbsp;</p><p>- D: `DeletionPolicy: Delete` does not work for non-empty S3 buckets (default behavior already tries to delete the bucket). &nbsp;</p><p> Key Takeaway: &nbsp;</p><p>A custom resource (Lambda) to empty the bucket before deletion is the cleanest solution. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3001f60b45104e1c97af31cec81672c5",
            "questionNumber": 330,
            "type": "single",
            "content": "<p>Question #330</p><p>A company has developed a mobile game. The backend for the game runs on several virtual machines located in an on-premises data center. The business logic is exposed using a REST API with multiple functions. Player session data is stored in central file storage. Backend services use different API keys for throttling and to distinguish between live and test traffic. <br><br>The load on the game backend varies throughout the day. During peak hours, the server capacity is not sufficient. There are also latency issues when fetching player session data. Management has asked a solutions architect to present a cloud architecture that can handle the game’s varying load and provide low-latency data access. The API model should not be changed. <br><br>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Implement the REST API using a Network Load Balancer (NLB). Run the business logic on an Amazon EC2 instance behind the NLB. Store player session data in Amazon Aurora Serverless."
                },
                {
                    "label": "B",
                    "content": "Implement the REST API using an Application Load Balancer (ALB). Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity."
                },
                {
                    "label": "C",
                    "content": "Implement the REST API using Amazon API Gateway. Run the business logic in AWS Lambda. Store player session data in Amazon DynamoDB with on-demand capacity."
                },
                {
                    "label": "D",
                    "content": "Implement the REST API using AWS AppSync. Run the business logic in AWS Lambda. Store player session data in Amazon Aurora Serverless."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct answer. Using Amazon API Gateway in combination with AWS Lambda allows for a serverless architecture that can scale automatically to handle varying loads. DynamoDB with on-demand capacity provides low-latency access to player session data and scales well with the game's needs.</p><p>The requirements are: &nbsp;</p><p>1. Handle variable load (scaling during peak hours). &nbsp;</p><p>2. Low-latency data access (for player session data). &nbsp;</p><p>3. No changes to the API model (REST API must remain the same). &nbsp;</p><p>Option C meets all these requirements: &nbsp;</p><p>- Amazon API Gateway provides a fully managed REST API solution with automatic scaling and throttling (supports API keys). &nbsp;</p><p>- AWS Lambda runs the business logic, scaling automatically with demand. &nbsp;</p><p>- Amazon DynamoDB (on-demand) offers low-latency access to session data and scales seamlessly. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- EC2 + NLB does not auto-scale (still requires manual capacity management). &nbsp;</p><p> &nbsp;- Aurora Serverless is not optimized for low-latency session storage (better for structured data, not transient session data). &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- ALB + Lambda works, but API Gateway is better suited for REST APIs (built-in features like API keys, caching, and throttling). &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- AWS AppSync is for GraphQL, not REST (violates the \"no API changes\" requirement). &nbsp;</p><p> Key Takeaway: &nbsp;</p><p>API Gateway + Lambda + DynamoDB (on-demand) is the best combination for scalable, low-latency REST APIs with variable load. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a03f9343ea55426eb916c4048f6e31ea",
            "questionNumber": 331,
            "type": "single",
            "content": "<p>Question #331</p><p>A company is migrating an application to the AWS Cloud. The application runs in an on-premises data center and writes thousands of images into a mounted NFS file system each night. After the company migrates the application, the company will host the application on an Amazon EC2 instance with a mounted Amazon Elastic File System (Amazon EFS) file system. </p><p><br></p><p>The company has established an AWS Direct Connect connection to AWS. Before the migration cutover, a solutions architect must build a process that will replicate the newly created on-premises images to the EFS file system. </p><p><br></p><p>What is the MOST operationally efficient way to replicate the images?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure a periodic process to run the aws s3 sync command from the on-premises file system to Amazon S3. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system."
                },
                {
                    "label": "B",
                    "content": "Deploy an AWS Storage Gateway file gateway with an NFS mount point. Mount the file gateway file system on the on-premises server. Configure a process to periodically copy the images to the mount point."
                },
                {
                    "label": "C",
                    "content": "Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an S3 bucket by using a public VIF. Configure an AWS Lambda function to process event notifications from Amazon S3 and copy the images from Amazon S3 to the EFS file system."
                },
                {
                    "label": "D",
                    "content": "Deploy an AWS DataSync agent to an on-premises server that has access to the NFS file system. Send data over the Direct Connect connection to an AWS PrivateLink interface VPC endpoint for Amazon EFS by using a private VIF. Configure a DataSync scheduled task to send the images to the EFS file system every 24 hours."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. Using AWS DataSync with a scheduled task to directly replicate data to Amazon EFS over a Direct Connect connection is the most operationally efficient solution. It leverages the Direct Connect for a secure and fast connection and automates the replication process without the need for additional Lambda functions or S3 intermediation.</p><p>The requirement is to efficiently replicate thousands of images from an on-premises NFS file system to Amazon EFS using AWS Direct Connect. The most operationally efficient solution should: &nbsp;</p><p>1. Leverage AWS DataSync (optimized for high-performance, scheduled data transfers). &nbsp;</p><p>2. Use Direct Connect (private VIF) for secure, low-latency transfer. &nbsp;</p><p>3. Avoid unnecessary steps (e.g., intermediate S3 storage + Lambda). &nbsp;</p><p>Option D meets all these criteria: &nbsp;</p><p>- AWS DataSync agent is installed on-premises to access the NFS file system. &nbsp;</p><p>- Direct Connect (private VIF) ensures secure, high-throughput transfer. &nbsp;</p><p>- PrivateLink for EFS allows direct, private connectivity to EFS. &nbsp;</p><p>- Scheduled task automates nightly syncs without manual intervention. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: &nbsp;</p><p> &nbsp;- S3 + Lambda adds complexity (extra steps, slower than direct EFS sync). &nbsp;</p><p> &nbsp;- S3 sync is not optimized for large-scale file transfers compared to DataSync. &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Storage Gateway (File Gateway) is not ideal for nightly bulk syncs (better for continuous access, not batch transfers). &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- S3 as an intermediate step is unnecessary (DataSync can write directly to EFS). &nbsp;</p><p> &nbsp;- Public VIF is less secure than private VIF for internal data transfers. &nbsp;</p><p> Key Takeaway: &nbsp;</p><p>AWS DataSync + Direct Connect (private VIF) + EFS PrivateLink is the most efficient and secure way to automate nightly image replication. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "30fcaaf443314a15bcf4020d2db6a590",
            "questionNumber": 332,
            "type": "single",
            "content": "<p>Question #332</p><p>A company recently migrated a web application from an on-premises data center to the AWS Cloud. The web application infrastructure consists of an Amazon CloudFront distribution that routes to an Application Load Balancer (ALB), with Amazon Elastic Container Service (Amazon ECS) to process requests. A recent security audit revealed that the web application is accessible by using both CloudFront and ALB endpoints. However, the company requires that the web application must be accessible only by using the CloudFront endpoint. </p><p><br></p><p>Which solution will meet this requirement with the LEAST amount of effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new security group and attach it to the CloudFront distribution. Update the ALB security group ingress to allow access only from the CloudFront security group."
                },
                {
                    "label": "B",
                    "content": "Update ALB security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing CloudFront managed prefix list."
                },
                {
                    "label": "C",
                    "content": "Create a com.amazonaws.region.elasticloadbalancing VPC interface endpoint for Elastic Load Balancing. Update the ALB scheme from internet-facing to internal."
                },
                {
                    "label": "D",
                    "content": "Extract CloudFront IPs from the AWS provided ip-ranges.json document. Update ALB security group ingress to allow access only from CloudFront IPs."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Update ALB security group ingress to allow access only from the com.amazonaws.global.cloudfront.origin-facing CloudFront managed prefix list.</p><p>This solution will meet the requirement with the least amount of effort for the following reasons:</p><p>AWS-managed solution: The CloudFront managed prefix list (com.amazonaws.global.cloudfront.origin-facing) is maintained by AWS and contains all the IP addresses that CloudFront uses to route requests to origins. This means you don't need to manually track or update IP ranges.</p><p>Simple implementation: Updating the ALB security group to reference this managed prefix list requires just a single configuration change in the security group's inbound rules.</p><p>Automatic updates: When AWS adds or changes CloudFront IP ranges, the managed prefix list is automatically updated, ensuring your security configuration remains effective without any manual intervention.</p><p>No additional resources: This approach doesn't require creating any new AWS resources, just modifying an existing security group.</p><p>Option A (creating a new security group for CloudFront) is not feasible because:</p><p>CloudFront is a global service and doesn't run within your VPC, so you cannot attach a security group to a CloudFront distribution.</p><p>CloudFront uses a wide range of IP addresses that change over time, making it impractical to manually maintain a list of these IPs in a security group.</p><p>This approach would require ongoing maintenance to keep the IP ranges up to date, increasing operational overhead.</p><p>By using the AWS-managed prefix list in the ALB security group, you ensure that only requests coming through CloudFront can reach your ALB, effectively restricting direct access to the ALB endpoint. This meets the security requirement that the web application must be accessible only through the CloudFront endpoint, and it does so with minimal effort and maintenance.</p><p>Sources</p><p>ALB security group with cloudFront origin-facing prefix list ingress setting blocks target group health check for eks fargate | AWS re:Post （https://repost.aws/questions/QUHLS6hJytSXOTIIUVL9UFnQ/alb-security-group-with-cloudfront-origin-facing-prefix-list-ingress-setting-blocks-target-group-health-check-for-eks-fargate）</p><p>Limit access to your origins using the AWS-managed prefix list for Amazon CloudFront | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/limit-access-to-your-origins-using-the-aws-managed-prefix-list-for-amazon-cloudfront/）</p><p>Restrict access to Application Load Balancers - Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/restrict-access-to-load-balancer.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "81dc0df32a67430b91b45df41ffd0df3",
            "questionNumber": 333,
            "type": "single",
            "content": "Question #333<p>A company hosts a community forum site using an Application Load Balancer (ALB) and a Docker application hosted in an Amazon ECS cluster. The site data is stored in Amazon RDS for MySQL and the container image is stored in ECR. The company needs to provide their customers with a disaster recovery SLA with an RTO of no more than 24 hours and RPO of no more than 8 hours. <br><br>Which of the following solutions is the MOST cost-effective way to meet the requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS CloudFormation to deploy identical ALB, EC2, ECS and RDS resources in two regions. Schedule RDS snapshots every 8 hours. Use RDS multi-region replication to update the secondary region&#39;s copy of the database. In the event of a failure, restore from the latest snapshot, and use an Amazon Route 53 DNS failover policy to automatically redirect customers to the ALB in the secondary region."
                },
                {
                    "label": "B",
                    "content": "Store the Docker image in ECR in two regions. Schedule RDS snapshots every 8 hours with snapshots copied to the secondary region. In the event of a failure, use AWS CloudFormation to deploy the ALB, EC2, ECS and RDS resources in the secondary region, restore from the latest snapshot, and update the DNS record to point to the ALB in the secondary region."
                },
                {
                    "label": "C",
                    "content": "Use AWS CloudFormation to deploy identical ALB, EC2, ECS, and RDS resources in a secondary region. Schedule hourly RDS MySQL backups to Amazon S3 and use cross-region replication to replicate data to a bucket in the secondary region. In the event of a failure, import the latest Docker image to Amazon ECR in the secondary region, deploy to the EC2 instance, restore the latest MySQL backup, and update the DNS record to point to the ALB in the secondary region."
                },
                {
                    "label": "D",
                    "content": "Deploy a pilot light environment in a secondary region with an ALB and a minimal resource EC2 deployment for Docker in an AWS Auto Scaling group with a scaling policy to increase instance size and number of nodes. Create a cross-region read replica of the RDS data. In the event of a failure, promote the replica to primary, and update the DNS record to point to the ALB in the secondary region."
                }
            ],
            "correctAnswer": "B",
            "explanation": "Option B is the correct answer. This solution involves storing the Docker image in ECR across two regions and taking RDS snapshots every 8 hours, which aligns with the RPO requirement. In the event of a failure, the company can deploy necessary resources in the secondary region using AWS CloudFormation and restore from the latest RDS snapshot, which meets the RTO requirement. This approach is cost-effective as it does not maintain a hot standby environment continuously.",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "e187f286e5c54871a806a19350359b22",
            "questionNumber": 334,
            "type": "single",
            "content": "<p>Question #334</p><p>A company is migrating its infrastructure to the AWS Cloud. The company must comply with a variety of regulatory standards for different projects. The company needs a multi-account environment. </p><p><br></p><p>A solutions architect needs to prepare the baseline infrastructure. The solution must provide a consistent baseline of management and security, but it must allow flexibility for different compliance requirements within various AWS accounts. The solution also needs to integrate with the existing on-premises Active Directory Federation Services (AD FS) server. </p><p><br></p><p>Which solution meets these requirements with the LEAST amount of operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an organization in AWS Organizations. Create a single SCP for least privilege access across all accounts. Create a single OU for all accounts. Configure an IAM identity provider for federation with the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with conformance packs for all accounts."
                },
                {
                    "label": "B",
                    "content": "Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Add OUs as necessary. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server."
                },
                {
                    "label": "C",
                    "content": "Create an organization in AWS Organizations. Create SCPs for least privilege access. Create an OU structure, and use it to group AWS accounts. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server. Configure a central logging account with a defined process for log generating services to send log events to the central account. Enable AWS Config in the central account with aggregators and conformance packs."
                },
                {
                    "label": "D",
                    "content": "Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Configure an IAM identity provider for federation with the on-premises AD FS server."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The best solution that meets the requirements with the LEAST operational overhead is: &nbsp;</p><p> Option B &nbsp;</p><p>Create an organization in AWS Organizations. Enable AWS Control Tower on the organization. Review included controls (guardrails) for SCPs. Check AWS Config for areas that require additions. Add OUs as necessary. Connect AWS IAM Identity Center (AWS Single Sign-On) to the on-premises AD FS server. &nbsp;</p><p> Why Option B? &nbsp;</p><p>1. AWS Control Tower – Provides a pre-configured baseline for security, compliance, and governance across multiple accounts with automated guardrails (SCPs) and centralized logging. This reduces manual setup and operational overhead. &nbsp;</p><p>2. IAM Identity Center (AWS SSO) – Simplifies federation with on-premises AD FS, enabling seamless single sign-on (SSO) for users. &nbsp;</p><p>3. Organizational Units (OUs) – Allows grouping accounts by compliance requirements while maintaining a consistent baseline. &nbsp;</p><p>4. Least Operational Overhead – Control Tower automates many best practices (SCPs, logging, identity management), reducing manual configurations. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option A: Requires manual setup of SCPs, logging, and AWS Config, increasing operational overhead. &nbsp;</p><p>- Option C: Involves manual SCP and AWS Config management, which is less efficient than Control Tower’s automation. &nbsp;</p><p>- Option D: Uses IAM Identity Provider instead of IAM Identity Center (AWS SSO), which is less streamlined for AD FS integration. &nbsp;</p><p> Conclusion &nbsp;</p><p>Option B is the best choice because it leverages AWS Control Tower for automated governance and IAM Identity Center (AWS SSO) for seamless AD FS integration, minimizing operational effort while ensuring compliance flexibility. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "608fb8610d4e43be92add75e80b3c713",
            "questionNumber": 335,
            "type": "multiple",
            "content": "<p>Question #335</p><p>An online magazine will launch its latest edition this month. This edition will be the first to be distributed globally. The magazine's dynamic website currently uses an Application Load Balancer (ALB) in front of the web tier, a fleet of Amazon EC2 instances for web and application servers, and Amazon Aurora MySQL. Portions of the website include static content and almost all traffic is read-only. </p><p><br></p><p>The magazine is expecting a significant spike in internet traffic when the new edition is launched. Optimal performance is a top priority for the week following the launch. </p><p><br></p><p>Which combination of steps should a solutions architect take to reduce system response times for a global audience? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use logical cross-Region replication to replicate the Aurora MySQL database to a secondary Region. Replace the web servers with Amazon S3. Deploy S3 buckets in cross-Region replication mode."
                },
                {
                    "label": "B",
                    "content": "Ensure the web and application tiers are each in Auto Scaling groups. Introduce an AWS Direct Connect connection. Deploy the web and application tiers in Regions across the world."
                },
                {
                    "label": "C",
                    "content": "Migrate the database from Amazon Aurora to Amazon RDS for MySQL. Ensure all three of the application tiers &ndash; web, application, and database &ndash; are in private subnets."
                },
                {
                    "label": "D",
                    "content": "Use an Aurora global database for physical cross-Region replication. Use Amazon S3 with cross-Region replication for static content and resources. Deploy the web and application tiers in Regions across the world."
                },
                {
                    "label": "E",
                    "content": "Introduce Amazon Route 53 with latency-based routing and Amazon CloudFront distributions. Ensure the web and application tiers are each in Auto Scaling groups."
                }
            ],
            "correctAnswer": "DE",
            "explanation": "<p>Option D is correct because using an Aurora global database provides cross-Region replication with low latency, which is suitable for read-only traffic. Option E is also correct as Amazon CloudFront can cache static content close to users worldwide, and Route 53 with latency-based routing can direct users to the nearest web tier, reducing response times. </p><p>Option D &nbsp;</p><p>Use an Aurora global database for physical cross-Region replication. Use Amazon S3 with cross-Region replication for static content and resources. Deploy the web and application tiers in Regions across the world. &nbsp;</p><p> Option E &nbsp;</p><p>Introduce Amazon Route 53 with latency-based routing and Amazon CloudFront distributions. Ensure the web and application tiers are each in Auto Scaling groups. &nbsp;</p><p> Why Options D & E? &nbsp;</p><p> Option D (Global Database & Multi-Region Deployment) &nbsp;</p><p>- Aurora Global Database – Provides low-latency reads in multiple regions with physical replication (faster than logical replication). &nbsp;</p><p>- S3 Cross-Region Replication (CRR) – Ensures static content is available globally with minimal latency. &nbsp;</p><p>- Multi-Region Deployment – Places web and app tiers closer to users, reducing response times. &nbsp;</p><p> Option E (CDN & Auto Scaling) &nbsp;</p><p>- Amazon CloudFront – Caches static and dynamic content at edge locations, reducing latency for global users. &nbsp;</p><p>- Route 53 (Latency-Based Routing) – Directs users to the nearest available endpoint for the best performance. &nbsp;</p><p>- Auto Scaling – Ensures the web and app tiers scale dynamically to handle traffic spikes. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option A: Replacing web servers with S3 is impractical (S3 is for static content, not dynamic web apps). &nbsp;</p><p>- Option B: Direct Connect is unnecessary for global performance (it’s for private network connectivity, not latency reduction). &nbsp;</p><p>- Option C: Migrating to RDS MySQL is a downgrade (Aurora is better for scalability and global replication). &nbsp;</p><p> Conclusion &nbsp;</p><p>D + E is the best combination because: &nbsp;</p><p>✔ Aurora Global Database + S3 CRR ensures low-latency data access worldwide. &nbsp;</p><p>✔ CloudFront + Route 53 optimizes content delivery and routing. &nbsp;</p><p>✔ Auto Scaling maintains performance during traffic spikes. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "0baf1dd989a247fb9760530bff6fc5da",
            "questionNumber": 336,
            "type": "single",
            "content": "<p>Question #336</p><p>An online gaming company needs to optimize the cost of its workloads on AWS. The company uses a dedicated account to host the production environment for its online gaming application and an analytics application. <br><br>Amazon EC2 instances host the gaming application and must always be available. The EC2 instances run all year. The analytics application uses data that is stored in Amazon S3. The analytics application can be interrupted and resumed without issue. <br><br>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use On-Demand Instances for the analytics application."
                },
                {
                    "label": "B",
                    "content": "Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use Spot Instances for the analytics application."
                },
                {
                    "label": "C",
                    "content": "Use Spot Instances for the online gaming application and the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount."
                },
                {
                    "label": "D",
                    "content": "Use On-Demand Instances for the online gaming application. Use Spot Instances for the analytics application. Set up a catalog in AWS Service Catalog to provision services at a discount."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Purchase an EC2 Instance Savings Plan for the online gaming application instances. Use Spot Instances for the analytics application.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>For the online gaming application:</p><p>The gaming application must always be available and runs continuously throughout the year</p><p>EC2 Instance Savings Plan is ideal for this workload as it provides significant discounts (up to 72% compared to On-Demand pricing) in exchange for a commitment to a consistent amount of compute usage</p><p>This type of Savings Plan is instance-specific, allowing the company to commit to the exact instance types needed for the gaming application</p><p>Since the application runs all year, the company can maximize the utilization of their Savings Plan commitment</p><p>For the analytics application:</p><p>The key characteristic of this application is that it \"can be interrupted and resumed without issue\"</p><p>Spot Instances are perfect for this use case as they offer the deepest discounts (up to 90% off On-Demand prices) but can be interrupted with short notice</p><p>Since the analytics application is designed to handle interruptions, it can take advantage of Spot Instance pricing without compromising functionality</p><p>The data for the analytics application is stored in Amazon S3, so even if instances are terminated, no data is lost</p><p>Option A (using On-Demand Instances for the analytics application) would be significantly more expensive than using Spot Instances. On-Demand Instances provide flexibility without upfront payment or long-term commitment, but they come at a premium price compared to Spot Instances. Since the analytics application can handle interruptions, there's no operational benefit to paying the higher On-Demand prices.</p><p>The combination of EC2 Instance Savings Plan for the always-on gaming application and Spot Instances for the interruptible analytics application provides the optimal balance of reliability and cost-effectiveness. This approach ensures the gaming application remains continuously available while minimizing costs for the analytics workload that can tolerate interruptions.</p><p>Sources</p><p>Select the best pricing model - Cost Optimization Pillar （https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/select-the-best-pricing-model.html）</p><p>Cost-effective resources - Games Industry Lens（https://docs.aws.amazon.com/wellarchitected/latest/games-industry-lens/games-cost-cost-eff-resc.html） </p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "0ec34022d19c4545b8b2c3b82bdd5f7b",
            "questionNumber": 337,
            "type": "multiple",
            "content": "<p>Question #337</p><p>A company runs applications in hundreds of production AWS accounts. The company uses AWS Organizations with all features enabled and has a centralized backup operation that uses AWS Backup. The company is concerned about ransomware attacks. </p><p><br></p><p>To address this concern, the company has created a new policy that all backups must be resilient to breaches of privileged-user credentials in any production account. </p><p><br></p><p>Which combination of steps will meet this new requirement? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Implement cross-account backup with AWS Backup vaults in designated non-production accounts."
                },
                {
                    "label": "B",
                    "content": "Add an SCP that restricts the modification of AWS Backup vaults."
                },
                {
                    "label": "C",
                    "content": "Implement AWS Backup Vault Lock in compliance mode."
                },
                {
                    "label": "D",
                    "content": "Implement least privilege access for the IAM service role that is assigned to AWS Backup."
                },
                {
                    "label": "E",
                    "content": "Configure the backup frequency, lifecycle, and retention period to ensure that at least one backup always exists in the cold tier."
                }
            ],
            "correctAnswer": "ABC",
            "explanation": "<p>Based on the information provided, the correct combination of steps to meet the company's requirement for making backups resilient to breaches of privileged-user credentials in production accounts is:</p><p>A. Implement cross-account backup with AWS Backup vaults in designated non-production accounts. B. Add an SCP that restricts the modification of AWS Backup vaults. C. Implement AWS Backup Vault Lock in compliance mode.</p><p>Here's why these three steps together provide the most effective protection against ransomware attacks and privileged credential breaches:</p><p>Cross-account backup with AWS Backup vaults in designated non-production accounts (A):</p><p>This creates a security boundary between production accounts and backup storage</p><p>Even if privileged credentials in a production account are compromised, the attacker won't have direct access to the backup vaults</p><p>This implements the security principle of isolation, keeping backups separate from the systems they protect</p><p>Adding an SCP that restricts the modification of AWS Backup vaults (B):</p><p>Service Control Policies operate at the AWS Organizations level, providing a higher level of protection</p><p>SCPs can prevent even privileged users (including account administrators) from modifying or deleting backup vaults</p><p>This adds an organization-wide governance layer that can't be bypassed by individual account credentials</p><p>Implementing AWS Backup Vault Lock in compliance mode (C):</p><p>Vault Lock in compliance mode creates immutable backups that cannot be deleted by anyone, including root users</p><p>Once enabled, the lock cannot be removed until the retention period expires</p><p>This provides the strongest protection against ransomware by ensuring backups cannot be tampered with, even by users with administrative privileges</p><p>The option \"Implement least privilege access for the IAM service role that is assigned to AWS Backup\" is a good security practice but doesn't specifically address the requirement of making backups resilient to breaches of privileged-user credentials. If privileged credentials are compromised, the attacker could modify these IAM roles regardless of how they were initially configured.</p><p>The combination of cross-account backups, SCPs, and Vault Lock creates multiple layers of protection that work together to ensure backups remain secure and available for recovery even if privileged credentials in production accounts are compromised. This defense-in-depth approach is essential for protecting against sophisticated ransomware attacks that specifically target backup systems.</p><p>Sources</p><p>Regular backups - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/essential-eight-maturity/regular-backups.html）</p><p>FSISEC11: How are you protecting against ransomware? - Financial Services Industry Lens （https://docs.aws.amazon.com/wellarchitected/latest/financial-services-industry-lens/fsisec11.html）</p><p>AWS Data protection | AWS re:Post （https://repost.aws/questions/QUF8ud3QyOQqyoobUm6zmFPw/aws-data-protection）</p><p>Immutability of AWS Backup | AWS re:Post （https://repost.aws/questions/QUySKXzqgnTRuuTf7BRqTZkw/immutability-of-aws-backup）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "815a643391264df2bbc3c614e9ab4291",
            "questionNumber": 338,
            "type": "multiple",
            "content": "<p>Question #338</p><p>A company needs to aggregate Amazon CloudWatch logs from its AWS accounts into one central logging account. The collected logs must remain in the AWS Region of creation. The central logging account will then process the logs, normalize the logs into standard output format, and stream the output logs to a security tool for more processing. </p><p><br></p><p>A solutions architect must design a solution that can handle a large volume of logging data that needs to be ingested. Less logging will occur outside normal business hours than during normal business hours. The logging solution must scale with the anticipated load. The solutions architect has decided to use an AWS Control Tower design to handle the multi-account logging process. </p><p><br></p><p>Which combination of steps should the solutions architect take to meet the requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a destination Amazon Kinesis data stream in the central logging account."
                },
                {
                    "label": "B",
                    "content": "Create a destination Amazon Simple Queue Service (Amazon SQS) queue in the central logging account."
                },
                {
                    "label": "C",
                    "content": "Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Kinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a subscription filter for each log group to send data to the Kinesis data stream."
                },
                {
                    "label": "D",
                    "content": "Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Amazon Simple Queue Service (Amazon SQS) queue. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a single subscription filter for all log groups to send data to the SQS queue."
                },
                {
                    "label": "E",
                    "content": "Create an AWS Lambda function. Program the Lambda function to normalize the logs in the central logging account and to write the logs to the security tool."
                },
                {
                    "label": "F",
                    "content": "Create an AWS Lambda function. Program the Lambda function to normalize the logs in the member accounts and to write the logs to the security tool."
                }
            ],
            "correctAnswer": "ACE",
            "explanation": "<p>The correct combination of steps to meet the requirements is: &nbsp;</p><p> Option A &nbsp;</p><p>✅ Create a destination Amazon Kinesis data stream in the central logging account. &nbsp;</p><p>- Kinesis Data Streams is ideal for high-volume, real-time log ingestion and can scale automatically with varying loads (business hours vs. off-hours). &nbsp;</p><p> Option C &nbsp;</p><p>✅ Create an IAM role that grants Amazon CloudWatch Logs the permission to add data to the Kinesis data stream. Create a trust policy. Specify the trust policy in the IAM role. In each member account, create a subscription filter for each log group to send data to the Kinesis data stream. &nbsp;</p><p>- IAM Role & Trust Policy: Ensures secure cross-account log streaming. &nbsp;</p><p>- Subscription Filters: Allow per-log-group forwarding to Kinesis (required for regional log retention). &nbsp;</p><p> Option E &nbsp;</p><p>✅ Create an AWS Lambda function. Program the Lambda function to normalize the logs in the central logging account and to write the logs to the security tool. &nbsp;</p><p>- Lambda can process, normalize, and forward logs to the security tool efficiently. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option B (SQS): Not suitable for high-volume log streaming (Kinesis is better for real-time, scalable ingestion). &nbsp;</p><p>- Option D (SQS + Single Subscription Filter): SQS is not optimal for logs, and a single subscription filter cannot handle per-log-group forwarding. &nbsp;</p><p>- Option F (Lambda in Member Accounts): Violates the requirement to process logs in the central account. &nbsp;</p><p> Summary of Correct Steps &nbsp;</p><p>1. Kinesis Data Stream (A) → Handles scalable log ingestion. &nbsp;</p><p>2. IAM Role + Subscription Filters (C) → Securely forwards logs from member accounts. &nbsp;</p><p>3. Lambda in Central Account (E) → Normalizes and forwards logs to the security tool. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3cc8f79791b14f51b73e290f9cf04021",
            "questionNumber": 339,
            "type": "multiple",
            "content": "<p>Question #339</p><p>A company is migrating a legacy application from an on-premises data center to AWS. The application consists of a single application server and a Microsoft SQL Server database server. Each server is deployed on a VMware VM that consumes 500 TB of data across multiple attached volumes. </p><p><br></p><p>The company has established a 10 Gbps AWS Direct Connect connection from the closest AWS Region to its on-premises data center. The Direct Connect connection is not currently in use by other services. </p><p><br></p><p>Which combination of steps should a solutions architect take to migrate the application with the LEAST amount of downtime? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use anAWS Server Migration Service (AWS SMS) replication job to migrate the database server VM to AWS."
                },
                {
                    "label": "B",
                    "content": "Use VM Import/Export to import the application server VM."
                },
                {
                    "label": "C",
                    "content": "Export the VM images to an AWS Snowball Edge Storage Optimized device."
                },
                {
                    "label": "D",
                    "content": "Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS."
                },
                {
                    "label": "E",
                    "content": "Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance."
                }
            ],
            "correctAnswer": "DE",
            "explanation": "<p>Based on the information provided, the correct combination of steps to migrate the application with the least amount of downtime is:</p><p>D. Use an AWS Server Migration Service (AWS SMS) replication job to migrate the application server VM to AWS. E. Use an AWS Database Migration Service (AWS DMS) replication instance to migrate the database to an Amazon RDS DB instance.</p><p>Here's why this combination provides the least downtime:</p><p>AWS Server Migration Service (AWS SMS) for the application server:</p><p>AWS SMS is designed specifically for migrating VMware VMs to AWS</p><p>It supports incremental replication, which means it can transfer the initial data and then only sync the changes</p><p>This approach minimizes downtime because the application can continue running during the replication process</p><p>SMS automatically handles the conversion of the VM to an Amazon EC2 instance, preserving the operating system, applications, and configurations</p><p>AWS Database Migration Service (AWS DMS) for the database:</p><p>AWS DMS is purpose-built for database migrations with minimal downtime</p><p>It supports Microsoft SQL Server as a source and Amazon RDS as a target</p><p>DMS can perform continuous data replication using Change Data Capture (CDC), which means the source database can remain operational during migration</p><p>It captures ongoing changes to the source database and applies them to the target RDS instance, keeping both in sync</p><p>The combination of these services leverages the 10 Gbps Direct Connect connection efficiently by:</p><p>Allowing both migrations to occur simultaneously</p><p>Supporting incremental and continuous replication to minimize the final cutover time</p><p>Enabling testing of the migrated environment before the final switch</p><p>The migration process would typically follow these steps:</p><p>Set up AWS SMS to replicate the application server VM to AWS</p><p>Configure AWS DMS to perform the initial load of the SQL Server database to Amazon RDS</p><p>Enable ongoing replication in DMS to capture changes during the migration</p><p>Test the migrated application with the new RDS database</p><p>Schedule a brief maintenance window for the final cutover</p><p>Redirect traffic to the new AWS environment</p><p>This approach allows the company to migrate both the application server and database with minimal disruption to their operations, making it the optimal choice for achieving the least amount of downtime during the migration.</p><p>Sources</p><p>AWS Migration and Transfer category icon Migration and transfer - Overview of Amazon Web Services （https://docs.aws.amazon.com/whitepapers/latest/aws-overview/migration-services.html）</p><p>Tools for heterogeneous database migrations - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/heterogeneous-migration-tools.html）</p><p>Migrate legacy database | AWS re:Post （https://repost.aws/questions/QUTC4STLNPTNeCYjbQMSErcA/migrate-legacy-database）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1dc1c9f955c84bee94de361c094a655c",
            "questionNumber": 340,
            "type": "multiple",
            "content": "<p>Question #340</p><p>A company operates a fleet of servers on premises and operates a fleet of Amazon EC2 instances in its organization in AWS Organizations. The company's AWS accounts contain hundreds of VPCs. The company wants to connect its AWS accounts to its on-premises network. AWS Site-to-Site VPN connections are already established to a single AWS account. The company wants to control which VPCs can communicate with other VPCs. </p><p><br></p><p>Which combination of steps will achieve this level of control with the LEAST operational effort? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM)."
                },
                {
                    "label": "B",
                    "content": "Configure attachments to all VPCs and VPNs."
                },
                {
                    "label": "C",
                    "content": "Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables."
                },
                {
                    "label": "D",
                    "content": "Configure VPC peering between the VPCs."
                },
                {
                    "label": "E",
                    "content": "Configure attachments between the VPCs and VPNs."
                },
                {
                    "label": "F",
                    "content": "Setup route tables on the VPCs and VPNs."
                }
            ],
            "correctAnswer": "ABC",
            "explanation": "<p>Based on the information provided, the correct combination of steps to achieve the required level of control with the least operational effort is:</p><p>A. Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM). B. Configure attachments to all VPCs and VPNs. C. Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables.</p><p>Here's why this combination is the most effective solution:</p><p>Create a transit gateway in an AWS account. Share the transit gateway across accounts by using AWS Resource Access Manager (AWS RAM):</p><p>Transit Gateway acts as a network transit hub that connects VPCs and on-premises networks</p><p>Using AWS RAM to share the transit gateway across accounts eliminates the need to create and manage separate transit gateways in each account</p><p>This centralized approach significantly reduces operational overhead compared to managing hundreds of individual connections</p><p>The transit gateway can be managed from a single account while being accessible to all accounts in the organization</p><p>Configure attachments to all VPCs and VPNs:</p><p>Transit Gateway attachments connect the transit gateway to VPCs and the existing Site-to-Site VPN connections</p><p>This creates a hub-and-spoke network architecture where all traffic flows through the transit gateway</p><p>Each VPC only needs a single attachment to the transit gateway, rather than multiple connections to other VPCs</p><p>The existing VPN connections can be attached to the transit gateway, leveraging the already established connections to the on-premises network</p><p>Setup transit gateway route tables. Associate the VPCs and VPNs with the route tables:</p><p>Transit Gateway route tables provide granular control over which VPCs can communicate with each other</p><p>By creating multiple route tables and associating specific VPCs and VPNs with them, the company can implement network segmentation</p><p>This allows for precise control of traffic flows between VPCs and between VPCs and the on-premises network</p><p>Route propagation can be configured to automatically update routes when new resources are added</p><p>This combination provides a scalable, centrally managed solution that can handle hundreds of VPCs across multiple accounts while maintaining fine-grained control over network traffic. It leverages the existing VPN connections and provides a framework that can easily accommodate future growth with minimal operational overhead.</p><p>Sources</p><p>Transit gateway attachment configuration | AWS re:Post （https://repost.aws/questions/QUvX0QyO3pRQWkBQspwcgVKg/transit-gateway-attachment-configuration）</p><p>Enable communication from VPC A to on-premise through VPC B | AWS re:Post （https://repost.aws/questions/QUousDSAqlR4-KscOJXcEA_w/enable-communication-from-vpc-a-to-on-premise-through-vpc-b）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "71ce224ed1c544389c68cd1922cfde5b",
            "questionNumber": 341,
            "type": "single",
            "content": "<p>Question #341</p><p>A company needs to optimize the cost of its application on AWS. The application uses AWS Lambda functions and Amazon Elastic Container Service (Amazon ECS) containers that run on AWS Fargate. The application is write-heavy and stores data in an Amazon Aurora MySQL database. <br><br>The load on the application is not consistent. The application experiences long periods of no usage, followed by sudden and significant increases and decreases in traffic. The database runs on a memory optimized DB instance that cannot handle the load. <br><br>A solutions architect must design a solution that can scale to handle the changes in traffic.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Add additional read replicas to the database. Purchase Instance Savings Plans and RDS Reserved Instances."
                },
                {
                    "label": "B",
                    "content": "Migrate the database to an Aurora DB cluster that has multiple writer instances. Purchase Instance Savings Plans."
                },
                {
                    "label": "C",
                    "content": "Migrate the database to an Aurora global database. Purchase Compute Savings Plans and RDS Reserved instances."
                },
                {
                    "label": "D",
                    "content": "Migrate the database to Aurora Serverless v1. Purchase Compute Savings Plans."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. Aurora Serverless v1 is designed to automatically scale with the application's load, making it a cost-effective choice for workloads with unpredictable traffic. Compute Savings Plans can provide additional cost savings for compute resources.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6e4ec199c02e480c86f6f06188ff94e3",
            "questionNumber": 342,
            "type": "single",
            "content": "<p>Question #342</p><p>A company migrated an application to the AWS Cloud. The application runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). <br><br>Application data is stored in a MySQL database that runs on an additional EC2 instance. The application's use of the database is read-heavy. <br><br>The application loads static content from Amazon Elastic Block Store (Amazon EBS) volumes that are attached to each EC2 instance. The static content is updated frequently and must be copied to each EBS volume. <br><br>The load on the application changes throughout the day. During peak hours, the application cannot handle all the incoming requests. Trace data shows that the database cannot handle the read load during peak hours.</p><p><br></p><p>Which solution will improve the reliability of the application?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the application to a set of AWS Lambda functions. Set the Lambda functions as targets for the ALB. Create a new single EBS volume for the static content. Configure the Lambda functions to read from the new EBS volume. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster."
                },
                {
                    "label": "B",
                    "content": "Migrate the application to a set of AWS Step Functions state machines. Set the state machines as targets for the ALB. Create an Amazon Elastic File System (Amazon EFS) file system for the static content. Configure the state machines to read from the EFS file system. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance."
                },
                {
                    "label": "C",
                    "content": "Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create a new single EBS volume for the static content. Mount the new EBS volume on the ECS cluster. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to an Amazon RDS for MySQL Multi-AZ DB cluster."
                },
                {
                    "label": "D",
                    "content": "Containerize the application. Migrate the application to an Amazon Elastic Container Service (Amazon ECS) cluster. Use the AWS Fargate launch type for the tasks that host the application. Create an Amazon Elastic File System (Amazon EFS) file system for the static content. Mount the EFS file system to each container. Configure AWS Application Auto Scaling on the ECS cluster. Set the ECS service as a target for the ALB. Migrate the database to Amazon Aurora MySQL Serverless v2 with a reader DB instance."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The correct answer is D. Here's the detailed reasoning:</p><p> Key Issues in the Current Setup:</p><p>1. Database Read Bottleneck: The MySQL database cannot handle read-heavy workloads during peak hours.</p><p>2. Static Content Management: Static content is stored on individual EBS volumes, which must be manually copied to each EC2 instance, causing inefficiency.</p><p>3. Scalability: The application cannot handle peak traffic due to fixed EC2 capacity.</p><p> Solution Analysis:</p><p>- Option A: </p><p> &nbsp;- Migrating to Lambda is not ideal for applications requiring persistent storage (static content on EBS). </p><p> &nbsp;- A single EBS volume cannot be shared across Lambda functions (they are stateless).</p><p> &nbsp;- RDS Multi-AZ improves availability but does not solve the read scalability issue.</p><p>- Option B: </p><p> &nbsp;- AWS Step Functions is not suitable for hosting a web application (it’s an orchestration service, not a compute service). </p><p> &nbsp;- While EFS solves the static content issue and Aurora Serverless v2 with a reader helps with read scaling, the compute solution is incorrect.</p><p>- Option C: </p><p> &nbsp;- Containerizing with ECS Fargate is a good approach for scalability. </p><p> &nbsp;- However, using a single EBS volume is problematic because EBS cannot be mounted to multiple tasks simultaneously. </p><p> &nbsp;- RDS Multi-AZ does not address read scaling.</p><p>- Option D (Correct Answer):</p><p> &nbsp;- Containerization with ECS Fargate: Enables automatic scaling of compute resources.</p><p> &nbsp;- Amazon EFS for Static Content: Provides a shared file system accessible by all containers, eliminating the need to manually sync content.</p><p> &nbsp;- Aurora MySQL Serverless v2 with a Reader: </p><p> &nbsp; &nbsp;- Aurora Serverless v2 automatically scales database capacity.</p><p> &nbsp; &nbsp;- A reader instance offloads read traffic, solving the read-heavy bottleneck.</p><p> &nbsp;- Application Auto Scaling: Dynamically adjusts the number of ECS tasks based on demand.</p><p> Why Option D is Best:</p><p>- Solves database read scalability with Aurora Serverless v2 + reader.</p><p>- Solves static content management with EFS (shared across containers).</p><p>- Solves compute scalability with ECS Fargate + Auto Scaling.</p><p>- Maintains high availability and reliability.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "66d35abc872a4e74882889d2e3067479",
            "questionNumber": 343,
            "type": "single",
            "content": "<p>Question #343</p><p>A solutions architect wants to make sure that only AWS users or roles with suitable permissions can access a new Amazon API Gateway endpoint. The solutions architect wants an end-to-end view of each request to analyze the latency of the request and create service maps.</p><p><br></p><p>How can the solutions architect design the API Gateway access control and perform request inspections?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway."
                },
                {
                    "label": "B",
                    "content": "For the API Gateway resource, set CORS to enabled and only return the company&#39;s domain in Access-Control-Allow-Origin headers. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Use Amazon CloudWatch to trace and analyze user requests to API Gateway."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Lambda function as the custom authorizer, ask the API client to pass the key and secret when making the call, and then use Lambda to validate the key/secret pair against the IAM system. Use AWS X-Ray to trace and analyze user requests to API Gateway."
                },
                {
                    "label": "D",
                    "content": "Create a client certificate for API Gateway. Distribute the certificate to the AWS users and roles that need to access the endpoint. Enable the API caller to pass the client certificate when accessing the endpoint. Use Amazon CloudWatch to trace and analyze user requests to API Gateway."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the requirements to ensure only AWS users or roles with suitable permissions can access an API Gateway endpoint and to have an end-to-end view of each request for latency analysis and service mapping, the answer is:</p><p>A. For the API Gateway method, set the authorization to AWS_IAM. Then, give the IAM user or role execute-api:Invoke permission on the REST API resource. Enable the API caller to sign requests with AWS Signature when accessing the endpoint. Use AWS X-Ray to trace and analyze user requests to API Gateway.</p><p>This is the correct solution because:</p><p>Setting the API Gateway method authorization to AWS_IAM ensures that only authenticated AWS users or roles can access the API. This directly addresses the requirement that \"only AWS users or roles with suitable permissions can access the new API Gateway endpoint.\"</p><p>Granting the execute-api:Invoke permission to specific IAM users or roles provides fine-grained access control, allowing you to determine exactly who can invoke the API.</p><p>AWS Signature (Sigv4) signing of requests provides a secure authentication mechanism that verifies the identity of the requester and ensures the integrity of the request.</p><p>AWS X-Ray provides end-to-end tracing capabilities that allow you to:</p><p>Track requests as they flow through your application</p><p>Analyze latency at each step of the request processing</p><p>Create service maps that visualize the relationships between API Gateway and other services</p><p>Identify bottlenecks and troubleshoot performance issues</p><p>Option B is incorrect because:</p><p>CORS (Cross-Origin Resource Sharing) is primarily designed to control which domains can access your API from a web browser. While it's useful for browser-based security, it doesn't provide the IAM-based authentication and authorization required in the scenario.</p><p>Simply returning the company's domain in Access-Control-Allow-Origin headers doesn't restrict API access to specific AWS users or roles.</p><p>Amazon CloudWatch alone doesn't provide the comprehensive end-to-end tracing and service mapping capabilities that X-Ray does. While CloudWatch offers metrics and logs for API Gateway, it doesn't create the service maps needed to visualize request flows across services.</p><p>The combination of AWS_IAM authorization and X-Ray tracing in Option A provides both the security controls and the observability features required by the solutions architect.</p><p>Sources</p><p>CloudWatch integration with X-Ray - AWS X-Ray （https://docs.aws.amazon.com/xray/latest/devguide/xray-services-cloudwatch.html）</p><p>Security design principles - Security Overview of Amazon API Gateway（https://docs.aws.amazon.com/whitepapers/latest/security-overview-amazon-api-gateway/security-design-principles.html） </p><p>Securing Your AWS Infrastructure: A Zero Trust Approach | AWS re:Post （https://repost.aws/articles/ARDWfm5wKhROyIzu8gghrHKw/securing-your-aws-infrastructure-a-zero-trust-approach）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c49951699b5949859640b76b0cc1dacd",
            "questionNumber": 344,
            "type": "single",
            "content": "<p>Question #344</p><p>A company is using AWS CodePipeline for the CI/CD of an application to an Amazon EC2 Auto Scaling group. All AWS resources are defined in AWS CloudFormation templates. The application artifacts are stored in an Amazon S3 bucket and deployed to the Auto Scaling group using instance user data scripts. As the application has become more complex, recent resource changes in the CloudFormation templates have caused unplanned downtime.</p><p><br></p><p>How should a solutions architect improve the CI/CD pipeline to reduce the likelihood that changes in the templates will cause downtime?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Adapt the deployment scripts to detect and report CloudFormation error conditions when performing deployments. Write test plans for a testing team to run in a non-production environment before approving the change for production."
                },
                {
                    "label": "B",
                    "content": "Implement automated testing using AWS CodeBuild in a test environment. Use CloudFormation change sets to evaluate changes before deployment. Use AWS CodeDeploy to leverage blue/green deployment patterns to allow evaluations and the ability to revert changes, if needed."
                },
                {
                    "label": "C",
                    "content": "Use plugins for the integrated development environment (IDE) to check the templates for errors, and use the AWS CLI to validate that the templates are correct. Adapt the deployment code to check for error conditions andgenerate notifications on errors. Deploy to a test environment and run a manual test plan before approving the change for production."
                },
                {
                    "label": "D",
                    "content": "Use AWS CodeDeploy and a blue/green deployment pattern with CloudFormation to replace the user data deployment scripts. Have the operators log in to running instances and go through a manual test plan to verify the application is running as expected."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. Here's the detailed reasoning:</p><p> Key Issues in the Current Setup:</p><p>1. Unplanned Downtime: Recent CloudFormation changes caused outages, indicating insufficient testing and risk mitigation.</p><p>2. Manual Deployment via User Data Scripts: Reliance on instance user data scripts is error-prone and lacks proper deployment controls.</p><p>3. Lack of Automated Testing & Safe Deployment Strategies: No structured way to validate changes before production.</p><p> Solution Analysis:</p><p>- Option A: </p><p> &nbsp;- While error detection and manual testing help, they are reactive and slow (manual testing introduces delays). </p><p> &nbsp;- Does not address the root cause (lack of automated validation and safe deployment mechanisms).</p><p>- Option B (Correct Answer):</p><p> &nbsp;- Automated Testing (CodeBuild): Ensures changes are validated in a test environment before production.</p><p> &nbsp;- CloudFormation Change Sets: Allows previewing changes before applying them, reducing unintended impacts.</p><p> &nbsp;- CodeDeploy with Blue/Green: </p><p> &nbsp; &nbsp;- Eliminates dependency on user data scripts.</p><p> &nbsp; &nbsp;- Enables zero-downtime deployments by shifting traffic only after validation.</p><p> &nbsp; &nbsp;- Provides a rollback mechanism if issues arise.</p><p>- Option C: </p><p> &nbsp;- IDE plugins and CLI validation are helpful but static checks only (they don’t test runtime behavior). </p><p> &nbsp;- Manual testing in a staging environment is better than nothing but not scalable compared to automation.</p><p>- Option D: </p><p> &nbsp;- Blue/Green via CodeDeploy is good, but manual testing is inefficient and error-prone. </p><p> &nbsp;- Logging into instances for verification is not a best practice in cloud-native CI/CD.</p><p> Why Option B is Best:</p><p>- Prevents Downtime by validating changes before production (change sets + automated testing).</p><p>- Eliminates Risky User Data Scripts by using CodeDeploy’s controlled deployment strategies.</p><p>- Enables Fast Rollback with blue/green deployments if issues occur.</p><p>- Follows AWS Best Practices for CI/CD (automation, testing, and safe deployments).</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "08f41e773240422ebd8fb36272259d44",
            "questionNumber": 345,
            "type": "single",
            "content": "<p>Question #345</p><p>A North American company with headquarters on the East Coast is deploying a new web application running on Amazon EC2 in the us-east-1 Region. The application should dynamically scale to meet user demand and maintain resiliency. Additionally, the application must have disaster recovery capabilities in an active-passive configuration with the us-west-1 Region.</p><p><br></p><p>Which steps should a solutions architect take after creating a VPC in the us-east-1 Region?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs in each Region as part of an Auto Scaling group spanning both VPCs and served by the ALB."
                },
                {
                    "label": "B",
                    "content": "Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create an Amazon Route 53 record set with a failover routing policy and health checks enabled to provide high availability across both Regions."
                },
                {
                    "label": "C",
                    "content": "Create a VPC in the us-west-1 Region. Use inter-Region VPC peering to connect both VPCs. Deploy an Application Load Balancer (ALB) that spans both VPCs. Deploy EC2 instances across multiple Availability Zones as part of an Auto Scaling group in each VPC served by the ALB. Create an Amazon Route 53 record that points to the ALB."
                },
                {
                    "label": "D",
                    "content": "Deploy an Application Load Balancer (ALB) spanning multiple Availability Zones (AZs) to the VPC in the us-east-1 Region. Deploy EC2 instances across multiple AZs as part of an Auto Scaling group served by the ALB. Deploy the same solution to the us-west-1 Region. Create separate Amazon Route 53 records in each Region that point to the ALB in the Region. Use Route 53 health checks to provide high availability across both Regions."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p>The question requires a solution that provides: &nbsp;</p><p>1. Dynamic scaling and resiliency in the primary Region (us-east-1). &nbsp;</p><p>2. Disaster recovery (DR) in an active-passive configuration with us-west-1. &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Deploying an ALB across multiple AZs in us-east-1 for high availability. &nbsp;</p><p>- Using Auto Scaling groups to dynamically scale EC2 instances. &nbsp;</p><p>- Replicating the infrastructure in us-west-1 (passive standby). &nbsp;</p><p>- Using Route 53 with failover routing and health checks to automatically redirect traffic to the secondary Region if the primary fails. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: Incorrect because an ALB cannot span multiple Regions, and inter-Region VPC peering is not required for an active-passive setup. &nbsp;</p><p>- C: Incorrect because an ALB cannot span multiple VPCs/Regions, and Route 53 failover routing is missing. &nbsp;</p><p>- D: Incorrect because it suggests using separate Route 53 records without a failover policy, which does not ensure automatic failover. &nbsp;</p><p>Best Practice: &nbsp;</p><p>- Active-Passive DR requires Route 53 failover routing with health checks. &nbsp;</p><p>- ALB and Auto Scaling ensure scalability and resiliency within a Region. &nbsp;</p><p>- Inter-Region VPC peering is unnecessary for this scenario since the secondary Region is passive. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a3315ad3085547e995afd77455ea3c7b",
            "questionNumber": 346,
            "type": "single",
            "content": "<p>Question #346</p><p>A company has a legacy application that runs on multiple .NET Framework components. The components share the same Microsoft SQL Server database and communicate with each other asynchronously by using Microsoft Message Queuing (MSMQ). <br><br>The company is starting a migration to containerized .NET Core components and wants to refactor the application to run on AWS. The .NET Core components require complex orchestration. The company must have full control over networking and host configuration. The application's database model is strongly relational.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Host the INET Core components on AWS App Runner. Host the database on Amazon RDS for SQL Server. Use Amazon EventBridge for asynchronous messaging."
                },
                {
                    "label": "B",
                    "content": "Host the .NET Core components on Amazon Elastic Container Service (Amazon ECS) with the AWS Fargate launch type. Host the database on Amazon DynamoDB. Use Amazon Simple Notification Service (Amazon SNS) for asynchronous messaging."
                },
                {
                    "label": "C",
                    "content": "Host the .NET Core components on AWS Elastic Beanstalk. Host the database on Amazon Aurora PostgreSQL Serverless v2. Use Amazon Managed Streaming for Apache Kafka (Amazon MSK) for asynchronous messaging."
                },
                {
                    "label": "D",
                    "content": "Host the .NET Core components on Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type. Host the database on Amazon Aurora MySQL Serverless v2. Use Amazon Simple Queue Service (Amazon SQS) for asynchronous messaging."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. Amazon ECS with the EC2 launch type provides full control over networking and host configuration, which is a requirement mentioned. Amazon Aurora MySQL Serverless v2 supports strongly relational database models, and Amazon SQS can be used for asynchronous messaging, replacing MSMQ.The question outlines the following requirements: &nbsp;</p><p>1. Containerized .NET Core components with complex orchestration. &nbsp;</p><p>2. Full control over networking and host configuration (implies EC2 launch type is preferred over serverless). &nbsp;</p><p>3. Strongly relational database model (requires a relational database, not NoSQL). &nbsp;</p><p>4. Asynchronous messaging (replacing MSMQ). &nbsp;</p><p>Option D meets all these requirements: &nbsp;</p><p>- Amazon ECS with EC2 launch type → Provides full control over networking and host configuration (unlike Fargate). &nbsp;</p><p>- Amazon Aurora MySQL Serverless v2 → A fully managed relational database compatible with SQL Server migrations. &nbsp;</p><p>- Amazon SQS → A managed queue service that replaces MSMQ for asynchronous messaging. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: AWS App Runner is serverless and doesn’t allow full control over networking/host configuration. Amazon EventBridge is for event-driven architectures, not direct MSMQ replacement. &nbsp;</p><p>- B: DynamoDB is NoSQL (not relational). Fargate removes host-level control. SNS is pub/sub, not a direct MSMQ replacement. &nbsp;</p><p>- C: Elastic Beanstalk is PaaS (less control than ECS). Aurora PostgreSQL is fine, but Amazon MSK is overkill for simple async messaging (SQS is better). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- ECS with EC2 launch type is ideal when fine-grained control over infrastructure is needed. &nbsp;</p><p>- Aurora MySQL/PostgreSQL is a good replacement for SQL Server in migrations. &nbsp;</p><p>- SQS is the AWS equivalent of MSMQ for decoupled messaging. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ac2989a7126242758bfd83fe86da983c",
            "questionNumber": 347,
            "type": "single",
            "content": "<p>Question #347</p><p>A solutions architect has launched multiple Amazon EC2 instances in a placement group within a single Availability Zone. Because of additional load on the system, the solutions architect attempts to add new instances to the placement group. However, the solutions architect receives an insufficient capacity error.</p><p><br></p><p>What should the solutions architect do to troubleshoot this issue?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use a spread placement group. Set a minimum of eight instances for each Availability Zone."
                },
                {
                    "label": "B",
                    "content": "Stop and start all the instances in the placement group. Try the launch again."
                },
                {
                    "label": "C",
                    "content": "Create a new placement group. Merge the new placement group with the original placement group."
                },
                {
                    "label": "D",
                    "content": "Launch the additional instances as Dedicated Hosts in the placement groups."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the scenario where a solutions architect is experiencing an insufficient capacity error when trying to add new EC2 instances to an existing placement group, the correct answer is:</p><p>B. Stop and start all the instances in the placement group. Try the launch again.</p><p>Here's why this is the correct approach:</p><p>When you create a placement group and launch instances in it, AWS attempts to place all instances close together on the underlying hardware to optimize network performance. However, as time passes and you try to add more instances, AWS might not have enough capacity available in that specific hardware section to accommodate the new instances while maintaining the placement group's requirements.</p><p>Stopping and starting all existing instances in the placement group forces AWS to re-evaluate the placement of these instances. When you restart them, AWS will attempt to find a section of hardware that can accommodate all your existing instances plus the new ones you want to add. This essentially gives AWS a chance to reorganize the placement group on hardware that has enough capacity for all your instances.</p><p>The other options are incorrect for the following reasons:</p><p>A. Use a spread placement group. Set a minimum of eight instances for each Availability Zone.</p><p>This doesn't address the current issue with the existing placement group</p><p>Changing to a spread placement group would require creating a new group and migrating instances</p><p>Spread placement groups are designed for distributing instances across separate hardware (reducing correlated failures), not for solving capacity issues</p><p>There's no setting for \"minimum instances\" in placement groups</p><p>C. Create a new placement group. Merge the new placement group with the original placement group.</p><p>AWS doesn't support merging placement groups</p><p>This approach wouldn't solve the underlying capacity issue</p><p>D. Launch the additional instances as Dedicated Hosts in the placement groups.</p><p>Dedicated Hosts cannot be used with placement groups</p><p>Placement groups and Dedicated Hosts are separate features with different purposes</p><p>The stop-and-start approach is a documented AWS best practice for addressing insufficient capacity errors in placement groups, as it gives AWS the opportunity to reallocate resources in a way that can accommodate all your instances together.</p><p>Sources</p><p>Placement strategies for your placement groups - Amazon Elastic Compute Cloud（https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-strategies.html） </p><p>Why is my Auto Scaling group not launching new instances when the load increases? | AWS re:Post （https://repost.aws/questions/QUQsrSasV_Tyyqw_lB8W9Lfw/why-is-my-auto-scaling-group-not-launching-new-instances-when-the-load-increases）</p><p>Troubleshoot Amazon EC2 instance launch issues - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html）</p><p>Placement Groups | AWS re:Post （https://repost.aws/questions/QUV59mDpbkTNO3ZEGu1oRwmQ/placement-groups）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c3337ad4b703482ebad711ee42b237f6",
            "questionNumber": 348,
            "type": "multiple",
            "content": "<p>Question #348</p><p>A company has used infrastructure as code (IaC) to provision a set of two Amazon EC2 instances. The instances have remained the same for several years. The company's business has grown rapidly in the past few months. In response, the company’s operations team has implemented an Auto Scaling group to manage the sudden increases in traffic. Company policy requires a monthly installation of security updates on all operating systems that are running.</p><p><br></p><p>The most recent security update required a reboot. As a result, the Auto Scaling group terminated the instances and replaced them with new, unpatched instances.</p><p><br></p><p>Which combination of steps should a solutions architect recommend to avoid a recurrence of this issue? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Modify the Auto Scaling group by setting the Update policy to target the oldest launch configuration for replacement."
                },
                {
                    "label": "B",
                    "content": "Create a new Auto Scaling group before the next patch maintenance. During the maintenance window, patch both groups and reboot the instances."
                },
                {
                    "label": "C",
                    "content": "Create an Elastic Load Balancer in front of the Auto Scaling group. Configure monitoring to ensure that target group health checks return healthy after the Auto Scaling group replaces the terminated instances."
                },
                {
                    "label": "D",
                    "content": "Create automation scripts to patch an AMI, update the launch configuration, and invoke an Auto Scaling instance refresh."
                },
                {
                    "label": "E",
                    "content": "Create an Elastic Load Balancer in front of the Auto Scaling group. Configure termination protection on the instances."
                }
            ],
            "correctAnswer": "AD",
            "explanation": "<p>The core issue is that Auto Scaling replaces manually patched instances with unpatched instances from the original launch configuration/launch template. To prevent this, we need to: &nbsp;</p><p>1. Ensure new instances launch with the latest patches (Option D). &nbsp;</p><p> &nbsp; - Automate AMI patching → Create a golden AMI with security updates. &nbsp;</p><p> &nbsp; - Update the launch configuration/template → Point to the new AMI. &nbsp;</p><p> &nbsp; - Trigger an Auto Scaling instance refresh → Gracefully replace old instances with patched ones. &nbsp;</p><p>2. Prioritize replacement of outdated instances (Option A). &nbsp;</p><p> &nbsp; - Auto Scaling can be configured to first terminate instances using the oldest launch template (ensuring newer, patched versions persist). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B (Dual ASGs): Overly complex for this use case; instance refresh (Option D) is simpler. &nbsp;</p><p>- C (ELB + Health Checks): Doesn’t solve the patching issue—only ensures healthy instances. &nbsp;</p><p>- E (Termination Protection): Breaks Auto Scaling by preventing instance replacement. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Instance Refresh is the AWS-recommended way to roll out updates to Auto Scaling groups. &nbsp;</p><p>- AMI lifecycle management ensures new instances are pre-patched. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f4993c849bf6402db622803118bde324",
            "questionNumber": 349,
            "type": "single",
            "content": "<p>Question #349</p><p>A team of data scientists is using Amazon SageMaker instances and SageMaker APIs to train machine learning (ML) models. The SageMaker instances are deployed in a VPC that does not have access to or from the internet. Datasets for ML model training are stored in an Amazon S3 bucket. Interface VPC endpoints provide access to Amazon S3 and the SageMaker APIs.</p><p><br></p><p>Occasionally, the data scientists require access to the Python Package Index (PyPI) repository to update Python packages that they use as part of their workflow. A solutions architect must provide access to the PyPI repository while ensuring that the SageMaker instances remain isolated from the internet.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS CodeCommit repository for each package that the data scientists need to access. Configure code synchronization between the PyPI repository and the CodeCommit repository. Create a VPC endpoint for CodeCommit."
                },
                {
                    "label": "B",
                    "content": "Create a NAT gateway in the VPC. Configure VPC routes to allow access to the internet with a network ACL that allows access to only the PyPI repository endpoint."
                },
                {
                    "label": "C",
                    "content": "Create a NAT instance in the VPC. Configure VPC routes to allow access to the internet. Configure SageMaker notebook instance firewall rules that allow access to only the PyPI repository endpoint."
                },
                {
                    "label": "D",
                    "content": "Create an AWS CodeArtifact domain and repository. Add an external connection for public:pypi to the CodeArtifact repository. Configure the Python client to use the CodeArtifact repository. Create a VPC endpoint for CodeArtifact."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. SageMaker instances must remain isolated from the internet (no direct internet access). &nbsp;</p><p>2. Data scientists need access to PyPI (Python Package Index) for package updates. &nbsp;</p><p>3. Current setup uses VPC endpoints for S3 and SageMaker APIs (private connectivity). &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Using AWS CodeArtifact as a private PyPI proxy (no internet access needed). &nbsp;</p><p>- Configuring an external connection to public PyPI (so CodeArtifact can fetch packages). &nbsp;</p><p>- Using a VPC endpoint for CodeArtifact (keeps traffic within AWS, no internet exposure). &nbsp;</p><p>- Configuring the Python client to use CodeArtifact instead of PyPI (seamless integration). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: AWS CodeCommit is for Git repositories, not PyPI package hosting (does not solve the package dependency issue). &nbsp;</p><p>- B & C: Both use NAT gateway/NAT instance, which allows internet access (violates the requirement to keep SageMaker instances isolated). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- CodeArtifact is the AWS-managed solution for private package repositories. &nbsp;</p><p>- VPC endpoints ensure secure, private AWS service access without internet exposure. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b4748625d17a40dba6ade0721e5cdca7",
            "questionNumber": 350,
            "type": "single",
            "content": "<p>Question #350</p><p>A solutions architect works for a government agency that has strict disaster recovery requirements. All Amazon Elastic Block Store (Amazon EBS) snapshots are required to be saved in at least two additional AWS Regions. The agency also is required to maintain the lowest possible operational overhead. <br><br>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to copy the EBS snapshots to the additional Regions."
                },
                {
                    "label": "B",
                    "content": "Use Amazon EventBridge to schedule an AWS Lambda function to copy the EBS snapshots to the additional Regions."
                },
                {
                    "label": "C",
                    "content": "Setup AWS Backup to create the EBS snapshots. Configure Amazon S3 Cross-Region Replication to copy the EBS snapshots to the additional Regions."
                },
                {
                    "label": "D",
                    "content": "Schedule Amazon EC2 Image Builder to run once daily to create an AMI and copy the AMI to the additional Regions."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the government agency's requirements to save EBS snapshots in at least two additional AWS Regions with the lowest possible operational overhead, the correct answer is:</p><p>A. Configure a policy in Amazon Data Lifecycle Manager (Amazon DLM) to run once daily to copy the EBS snapshots to the additional Regions.</p><p>Here's why this is the best solution:</p><p>Amazon Data Lifecycle Manager (DLM) is specifically designed to automate the creation, retention, and deletion of EBS snapshots. It has built-in functionality to copy snapshots across regions, which directly addresses the requirement to save snapshots in multiple regions.</p><p>DLM provides the lowest operational overhead among the options because:</p><p>It's a fully managed service that requires minimal setup and maintenance</p><p>It uses policies that can be configured once and then run automatically</p><p>It doesn't require any custom code or additional infrastructure</p><p>It handles scheduling, copying, and retention management automatically</p><p>DLM policies can be configured to run daily, which meets the implied frequency requirement, and can specify multiple destination regions for the copies.</p><p>The other options have significant drawbacks:</p><p>B. Using Amazon EventBridge with AWS Lambda would require:</p><p>Writing and maintaining custom code for the Lambda function</p><p>Setting up and managing EventBridge rules</p><p>Handling error cases and retries This creates much higher operational overhead compared to DLM.</p><p>C. AWS Backup with S3 Cross-Region Replication is incorrect because:</p><p>EBS snapshots are not stored in S3 buckets in a way that can be directly managed by S3 Cross-Region Replication</p><p>S3 Cross-Region Replication applies to S3 objects, not EBS snapshots</p><p>This would require additional custom processes to work, increasing operational overhead</p><p>D. EC2 Image Builder is designed for creating and managing AMIs (Amazon Machine Images), not for managing EBS snapshots directly. Using it would:</p><p>Create unnecessary AMIs when only snapshots are required</p><p>Involve more complex processes than needed</p><p>Result in higher costs and operational overhead</p><p>Amazon Data Lifecycle Manager provides the most direct, purpose-built solution for automatically copying EBS snapshots to multiple regions with minimal operational overhead, making it the best choice for meeting the government agency's requirements.</p><p>Sources</p><p>Amazon DLM for snapshot automation | AWS re:Post （https://repost.aws/questions/QU7IIT89-2TVeEXmsR4z9lxQ/amazon-dlm-for-snapshot-automation）</p><p>Creating EBS volume backups with AMIs and EBS snapshots - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/new-ebs-volume-backups.html）</p><p>Resilience in Amazon EC2 - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/disaster-recovery-resiliency.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5d490b1a47fc4dcfae622f45f3923083",
            "questionNumber": 351,
            "type": "single",
            "content": "<p>Question #351</p><p>A company has a project that is launching Amazon EC2 instances that are larger than required. The project's account cannot be part of the company's organization in AWS Organizations due to policy restrictions to keep this activity outside of corporate IT. The company wants to allow only the launch of t3.small EC2 instances by developers in the project's account. These EC2 instances must be restricted to the us-east-2 Region. </p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new developer account. Move all EC2 instances, users, and assets into us-east-2. Add the account to the company&#39;s organization in AWS Organizations. Enforce a tagging policy that denotes Region affinity."
                },
                {
                    "label": "B",
                    "content": "Create an SCP that denies the launch of all EC2 instances except t3.small EC2 instances in us-east-2. Attach the SCP to the project&#39;s account."
                },
                {
                    "label": "C",
                    "content": "Create and purchase a t3.small EC2 Reserved Instance for each developer in us-east-2. Assign each developer a specific EC2 instance with their name as the tag."
                },
                {
                    "label": "D",
                    "content": "Create an IAM policy than allows the launch of only t3.small EC2 instances in us-east-2. Attach the policy to the roles and groups that the developers use in the project&#39;s account."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the requirement to allow only the launch of t3.small EC2 instances in the us-east-2 Region for developers in a project account that cannot be part of the company's AWS Organizations, the correct answer is:</p><p>D. Create an IAM policy than allows the launch of only t3.small EC2 instances in us-east-2. Attach the policy to the roles and groups that the developers use in the project's account.</p><p>This is the correct solution because:</p><p>IAM policies provide precise control over what actions users can perform in AWS. By creating a policy that specifically allows only t3.small instances in us-east-2, you can enforce the restriction at the permission level.</p><p>The policy can be attached to IAM roles and groups that developers use, ensuring consistent application of the restriction across all developer accounts.</p><p>IAM policies are preventative controls that block unauthorized actions before they occur, rather than detective controls that identify violations after the fact.</p><p>This approach doesn't require any additional cost or resource provisioning - it's purely a permission-based solution.</p><p>Option C (Creating and purchasing t3.small EC2 Reserved Instances for each developer) is incorrect for several reasons:</p><p>Reserved Instances are primarily a billing construct for cost savings, not a security control. Purchasing Reserved Instances doesn't prevent users from launching other instance types.</p><p>Assigning specific EC2 instances to developers with name tags doesn't restrict them from launching additional instances of any size or in any region.</p><p>This approach would require ongoing management overhead to purchase and assign Reserved Instances for each developer.</p><p>It would incur unnecessary costs by requiring the company to purchase Reserved Instances for all developers, even if they're not always running instances.</p><p>The IAM policy approach directly addresses the requirement to restrict instance types and regions with minimal operational overhead and no additional cost. It's a preventative control that enforces the restriction at the permission level, making it the most effective solution for this scenario.</p><p>Sources</p><p>Example policies to control access to the Amazon EC2 console - Amazon Elastic Compute Cloud </p><p>Amazon EC2: Allows full EC2 access within a specific Region, programmatically and in the console - AWS Identity and Access Management </p><p>AWS global condition context keys - AWS Identity and Access Management </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f0bee05820774522962dfb3e10ddcfff",
            "questionNumber": 352,
            "type": "single",
            "content": "<p>Question #352</p><p>A scientific company needs to process text and image data from an Amazon S3 bucket. The data is collected from several radar stations during a live, time-critical phase of a deep space mission. The radar stations upload the data to the source S3 bucket. The data is prefixed by radar station identification number. <br><br>The company created a destination S3 bucket in a second account. Data must be copied from the source S3 bucket to the destination S3 bucket to meet a compliance objective. This replication occurs through the use of an S3 replication rule to cover all objects in the source S3 bucket. <br><br>One specific radar station is identified as having the most accurate data. Data replication at this radar station must be monitored for completion within 30 minutes after the radar station uploads the objects to the source S3 bucket.</p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Setup an AWS DataSync agent to replicate the prefixed data from the source S3 bucket to the destination S3 bucket. Select to use all available bandwidth on the task, and monitor the task to ensure that it is in the TRANSFERRING status. Create an Amazon EventBridge rule to initiate an alert if this status changes."
                },
                {
                    "label": "B",
                    "content": "In the second account, create another S3 bucket to receive data from the radar station with the most accurate data. Set up a new replication rule for this new S3 bucket to separate the replication from the other radar stations. Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold."
                },
                {
                    "label": "C",
                    "content": "Enable Amazon S3 Transfer Acceleration on the source S3 bucket, and configure the radar station with the most accurate data to use the new endpoint. Monitor the S3 destination bucket&#39;s TotalRequestLatency metric. Create an Amazon EventBridge rule to initiate an alert if this status changes."
                },
                {
                    "label": "D",
                    "content": "Create a new S3 replication rule on the source S3 bucket that filters for the keys that use the prefix of the radar station with the most accurate data. Enable S3 Replication Time Control (S3 RTC). Monitor the maximum replication time to the destination. Create an Amazon EventBridge rule to initiate an alert when the time exceeds the desired threshold."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. By creating a specific S3 replication rule that filters for the prefix of the radar station with the most accurate data and enabling S3 Replication Time Control, the architect can ensure that the data is replicated within the required 30-minute window. Setting up an Amazon EventBridge rule to monitor and alert on the replication time exceeds the threshold provides the necessary monitoring for compliance.</p><p>The requirements are: &nbsp;</p><p>1. Replicate data from a source S3 bucket to a destination bucket in another account (already configured). &nbsp;</p><p>2. Ensure data from one specific radar station (identified by prefix) replicates within 30 minutes. &nbsp;</p><p>3. Monitor replication time and trigger alerts if the SLA is breached. &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Creating a separate replication rule for the high-priority radar station (filtering by prefix). &nbsp;</p><p>- Enabling S3 Replication Time Control (RTC) → Guarantees replication within 15 minutes (well under the 30-minute requirement). &nbsp;</p><p>- Monitoring replication time and triggering EventBridge alerts if delays occur. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: AWS DataSync is unnecessary for S3-to-S3 replication (native S3 replication is more efficient). Monitoring `TRANSFERRING` status doesn’t ensure timing compliance. &nbsp;</p><p>- B: Creating a separate bucket adds complexity without solving the monitoring requirement. &nbsp;</p><p>- C: S3 Transfer Acceleration improves upload speed to S3 but doesn’t help with replication timing. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- S3 Replication Time Control (RTC) is the only way to guarantee replication within a defined timeframe (99.99% of objects within 15 minutes). &nbsp;</p><p>- Prefix-based filtering allows granular control over replication rules. &nbsp;</p><p>- EventBridge alerts can track replication metrics (e.g., `MaximumReplicationTime`). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "bc1d9dd07caf46c6b26be8ffad7a73ec",
            "questionNumber": 353,
            "type": "multiple",
            "content": "<p>Question #353</p><p>A company wants to migrate its on-premises data center to the AWS Cloud. This includes thousands of virtualized Linux and Microsoft Windows servers, SAN storage, Java and PHP applications with MySQL, and Oracle databases. There are many dependent services hosted either in the same data center or externally. The technical documentation is incomplete and outdated. A solutions architect needs to understand the current environment and estimate the cloud resource costs after the migration. </p><p><br></p><p>Which tools or services should the solutions architect use to plan the cloud migration? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "AWS Application Discovery Service"
                },
                {
                    "label": "B",
                    "content": "AWS SMS"
                },
                {
                    "label": "C",
                    "content": "AWS X-Ray"
                },
                {
                    "label": "D",
                    "content": "AWS Cloud Adoption Readiness Tool (CART)"
                },
                {
                    "label": "E",
                    "content": "Amazon Inspector"
                },
                {
                    "label": "F",
                    "content": "AWS Migration Hub"
                }
            ],
            "correctAnswer": "ADF",
            "explanation": "<p>Based on the scenario where a company needs to migrate thousands of virtualized servers, applications, and databases from an on-premises data center to AWS with incomplete documentation, the three most appropriate tools or services for planning the cloud migration are:</p><p>A. AWS Application Discovery Service D. AWS Cloud Adoption Readiness Tool (CART) F. AWS Migration Hub</p><p>Here's why these three options are the correct choices:</p><p>AWS Application Discovery Service (Option A):</p><p>This service automatically discovers on-premises servers, applications, and their dependencies</p><p>It collects detailed information about server configurations, utilization metrics, and network connections</p><p>It's specifically designed to help understand the current environment when documentation is incomplete or outdated</p><p>The collected data helps in right-sizing AWS resources and estimating costs based on actual usage patterns</p><p>This addresses the requirement to \"understand the current environment\" despite incomplete documentation</p><p>AWS Cloud Adoption Readiness Tool (CART) (Option D):</p><p>CART helps assess an organization's readiness for cloud adoption across business, technical, and financial dimensions</p><p>It provides a framework for identifying gaps in the migration planning process</p><p>It helps estimate cloud resource costs by analyzing the financial aspects of cloud adoption</p><p>This addresses the requirement to \"estimate the cloud resource costs after the migration\"</p><p>AWS Migration Hub (Option F):</p><p>Migration Hub serves as a central console for planning and tracking application migrations</p><p>It integrates with Application Discovery Service to organize discovered data into applications</p><p>It provides a unified view of migration status across multiple applications and servers</p><p>It helps plan migration waves and strategies based on dependencies identified during discovery</p><p>This addresses the need to coordinate a complex migration with \"many dependent services\"</p><p>The other options are not as suitable for this specific scenario:</p><p>B. AWS SMS (Server Migration Service) - While useful for the actual migration of virtual machines, it's not primarily a discovery or cost estimation tool.</p><p>C. AWS X-Ray - This is a distributed tracing service for applications already running on AWS, not a migration planning tool.</p><p>E. Amazon Inspector - This is a security assessment service for applications deployed on AWS, not a tool for understanding on-premises environments or estimating migration costs.</p><p>The combination of Application Discovery Service, Cloud Adoption Readiness Tool, and Migration Hub provides a comprehensive approach to understanding the current environment, planning the migration strategy, and estimating cloud resource costs, which are the key requirements specified in the scenario.</p><p>Sources</p><p>Discovery, planning, and recommendation migration tools - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-tools/discovery.html）</p><p>Tools for migrating to the AWS Cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-tools/introduction.html）</p><p>Windows migration process - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-microsoft-workloads-aws/overview-windows-migration-process.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "901bb383e49a4b0ca207d0018c0b75b3",
            "questionNumber": 354,
            "type": "single",
            "content": "<p>Question #354</p><p>A solutions architect is reviewing an application's resilience before launch. The application runs on an Amazon EC2 instance that is deployed in a private subnet of a VPC. The EC2 instance is provisioned by an Auto Scaling group that has a minimum capacity of 1 and a maximum capacity of 1. The application stores data on an Amazon RDS for MySQL DB instance. The VPC has subnets configured in three Availability Zones and is configured with a single NAT gateway.</p><p><br></p><p>The solutions architect needs to recommend a solution to ensure that the application will operate across multiple Availability Zones.</p><p><br></p><p>Which solution will meet this requirement?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to a Multi-AZ configuration. Configure the Auto Scaling group to launch the instances across Availability Zones. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3."
                },
                {
                    "label": "B",
                    "content": "Replace the NAT gateway with a virtual private gateway. Replace the RDS for MySQL DB instance with an Amazon Aurora MySQL DB cluster. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Set the minimum capacity and maximum capacity of the Auto Scaling group to 3."
                },
                {
                    "label": "C",
                    "content": "Replace the NAT gateway with a NAT instance. Migrate the RDS for MySQL DB instance to an RDS for PostgreSQL DB instance. Launch a new EC2 instance in the other Availability Zones."
                },
                {
                    "label": "D",
                    "content": "Deploy an additional NAT gateway in the other Availability Zones. Update the route tables with appropriate routes. Modify the RDS for MySQL DB instance to turn on automatic backups and retain the backups for 7 days. Configure the Auto Scaling group to launch instances across all subnets in the VPC. Keep the minimum capacity and the maximum capacity of the Auto Scaling group at 1."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer. Deploying an additional NAT gateway in other Availability Zones and updating the route tables ensures that the application can access the internet from those zones. Modifying the RDS for MySQL DB instance to a Multi-AZ configuration provides database resilience across zones. Configuring the Auto Scaling group to launch instances across Availability Zones and setting the minimum and maximum capacity to 3 ensures that the application can continue to operate even if one Availability Zone is down.</p><p>The requirement is to ensure the application operates across multiple Availability Zones (AZs) for high availability. The current setup has: &nbsp;</p><p>- Single EC2 instance (Auto Scaling min/max = 1) in one private subnet. &nbsp;</p><p>- Single NAT gateway (a single point of failure). &nbsp;</p><p>- Single RDS MySQL instance (not Multi-AZ). &nbsp;</p><p>Option A addresses all these issues by: &nbsp;</p><p>1. Adding NAT gateways in other AZs → Eliminates the NAT gateway bottleneck. &nbsp;</p><p>2. Converting RDS MySQL to Multi-AZ → Automatic failover to a standby in another AZ. &nbsp;</p><p>3. Configuring Auto Scaling to span AZs (min/max = 3) → Ensures instances run in multiple AZs. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: &nbsp;</p><p> &nbsp;- Virtual private gateway is for VPN/Direct Connect (not needed here). &nbsp;</p><p> &nbsp;- Aurora MySQL is good but doesn’t solve the NAT gateway issue. &nbsp;</p><p> &nbsp;- Auto Scaling min/max = 3 is correct, but the rest is unnecessary. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- NAT instance is less reliable than NAT gateway. &nbsp;</p><p> &nbsp;- PostgreSQL migration is irrelevant to resilience. &nbsp;</p><p>- D: &nbsp;</p><p> &nbsp;- Auto Scaling min/max = 1 means only one instance runs (no multi-AZ redundancy). &nbsp;</p><p> &nbsp;- Backups don’t provide high availability (only disaster recovery). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Multi-AZ RDS ensures database failover. &nbsp;</p><p>- Multi-AZ NAT gateways prevent network bottlenecks. &nbsp;</p><p>- Auto Scaling across AZs ensures compute redundancy. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6c157b40fe9d4fa1949a9b3015b530b8",
            "questionNumber": 355,
            "type": "single",
            "content": "<p>Question #355</p><p>A company is planning to migrate its on-premises transaction-processing application to AWS. The application runs inside Docker containers that are hosted on VMs in the company's data center. The Docker containers have shared storage where the application records transaction data. <br><br>The transactions are time sensitive. The volume of transactions inside the application is unpredictable. The company must implement a low latency storage solution that will automatically scale throughput to meet increased demand. The company cannot develop the application further and cannot continue to administer the Docker hosting environment.</p><p><br></p><p>How should the company migrate the application to AWS to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the containers that run the application to Amazon Elastic Kubernetes Service (Amazon EKS). Use Amazon S3 to store the transaction data that the containers share."
                },
                {
                    "label": "B",
                    "content": "Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic File System (Amazon EFS) file system. Create a Fargate task definition. Add a volume to the task definition to point to the EFS file system."
                },
                {
                    "label": "C",
                    "content": "Migrate the containers that run the application to AWS Fargate for Amazon Elastic Container Service (Amazon ECS). Create an Amazon Elastic Block Store (Amazon EBS) volume. Create a Fargate task definition. Attach the EBS volume to each running task."
                },
                {
                    "label": "D",
                    "content": "Launch Amazon EC2 instances. Install Docker on the EC2 instances. Migrate the containers to the EC2 instances. Create an Amazon Elastic File System (Amazon EFS) file system. Add a mount point to the EC2 instances for the EFS file system."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. Migrating the application to AWS Fargate for Amazon ECS allows the company to run containers without managing the underlying infrastructure. Using Amazon EFS provides a shared file system that can be accessed by multiple tasks running the application, and it can automatically scale to meet the throughput demands. Fargate's serverless model aligns with the company's requirement to avoid further development and administration of the Docker environment.</p><p>The requirements are: &nbsp;</p><p>1. Migrate Docker containers without managing the hosting environment → AWS Fargate (serverless containers). &nbsp;</p><p>2. Shared storage for transaction data → Amazon EFS (scalable, low-latency shared file storage). &nbsp;</p><p>3. Automatic throughput scaling → EFS automatically scales IOPS based on demand. &nbsp;</p><p>4. No application changes → EFS provides a POSIX-compliant file system (compatible with existing storage). &nbsp;</p><p>Option B meets all these requirements by: &nbsp;</p><p>- Using Fargate for ECS (no Docker host management). &nbsp;</p><p>- Configuring EFS as shared storage (attached via task definition). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Amazon S3 is not a file system (cannot be mounted directly by containers). &nbsp;</p><p>- C: EBS volumes cannot be shared across multiple containers (no shared storage). &nbsp;</p><p>- D: EC2 instances require Docker management (violates the \"no administration\" requirement). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Fargate + EFS is the standard for serverless containers with shared storage. &nbsp;</p><p>- EFS scales automatically and provides low-latency access for transactional workloads. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8535ad044f6341a08c43427d76d94575",
            "questionNumber": 356,
            "type": "single",
            "content": "<p>Question #356</p><p>A company is planning to migrate to the AWS Cloud. The company hosts many applications on Windows servers and Linux servers. Some of the servers are physical, and some of the servers are virtual. The company uses several types of databases in its on-premises environment. The company does not have an accurate inventory of its on-premises servers and applications. <br><br>The company wants to rightsize its resources during migration. A solutions architect needs to obtain information about the network connections and the application relationships. The solutions architect must assess the company’s current environment and develop a migration plan.</p><p><br></p><p>Which solution will provide the solutions architect with the required information to develop the migration plan?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Migration Evaluator to request an evaluation of the environment from AWS. Use the AWS Application Discovery Service Agentless Collector to import the details into a Migration Evaluator Quick Insights report."
                },
                {
                    "label": "B",
                    "content": "Use AWS Migration Hub and install the AWS Application Discovery Agent on the servers. Deploy the Migration Hub Strategy Recommendations application data collector. Generate a report by using Migration Hub Strategy Recommendations."
                },
                {
                    "label": "C",
                    "content": "Use AWS Migration Hub and run the AWS Application Discovery Service Agentless Collector on the servers. Group the servers and databases by using AWS Application Migration Service. Generate a report by using Migration Hub Strategy Recommendations."
                },
                {
                    "label": "D",
                    "content": "Use the AWS Migration Hub import tool to load the details of the company&rsquo;son-premises environment. Generate a report by using Migration Hub Strategy Recommendations."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. Installing the AWS Application Discovery Agent on the servers and using AWS Migration Hub will allow the architect to collect detailed information about the on-premises environment, including network connections and application relationships. This data can then be used to generate a report with Migration Hub Strategy Recommendations, which will assist in developing an informed migration plan.</p><p>The company needs to: &nbsp;</p><p>1. Discover and inventory on-premises servers (physical/virtual, Windows/Linux). &nbsp;</p><p>2. Analyze application dependencies (network connections, relationships). &nbsp;</p><p>3. Rightsize resources for migration planning. &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using AWS Migration Hub (central tracking for migration). &nbsp;</p><p>- Installing AWS Application Discovery Agents (collects detailed server configs, performance data, and network dependencies). &nbsp;</p><p>- Deploying Migration Hub Strategy Recommendations (analyzes data to suggest optimal AWS sizing and migration strategies). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Migration Evaluator (formerly TSO Logic) provides high-level cost estimates but lacks dependency mapping. The Agentless Collector only works for VMware (not physical servers). &nbsp;</p><p>- C: The Agentless Collector is limited to VMware (cannot inventory physical servers or non-VMware environments). &nbsp;</p><p>- D: The import tool requires manual data entry (no automated discovery). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Application Discovery Service (ADS) with agents is the most comprehensive way to map dependencies for heterogeneous environments (physical/virtual, Windows/Linux). &nbsp;</p><p>- Migration Hub Strategy Recommendations uses ADS data to suggest rightsizing (e.g., EC2 instance types, RDS options). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7db8b20adc9a44219172283375b38fc1",
            "questionNumber": 357,
            "type": "single",
            "content": "<p>Question #357</p><p>A financial services company sells its software-as-a-service (SaaS) platform for application compliance to large global banks. The SaaS platform runs on AWS and uses multiple AWS accounts that are managed in an organization in AWS Organizations. The SaaS platform uses many AWS resources globally. <br><br>For regulatory compliance, all API calls to AWS resources must be audited, tracked for changes, and stored in a durable and secure data store.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new AWS CloudTrail trail. Use an existing Amazon S3 bucket in the organization&#39;s management account to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Create a new AWS CloudTrail trail in each member account of the organization. Create new Amazon S3 buckets to store the logs. Deploy the trail to all AWS Regions. Enable MFA delete and encryption on the S3 buckets."
                },
                {
                    "label": "C",
                    "content": "Create a new AWS CloudTrail trail in the organization&#39;s management account. Create a new Amazon S3 bucket with versioning turned on to store the logs. Deploy the trail for all accounts in the organization. Enable MFA delete and encryption on the S3 bucket."
                },
                {
                    "label": "D",
                    "content": "Create a new AWS CloudTrail trail in the organization&#39;s management account. Create a new Amazon S3 bucket to store the logs. Configure Amazon Simple Notification Service (Amazon SNS) to send log-file delivery notifications to an external management system that will track the logs. Enable MFA delete and encryption on the S3 bucket."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct answer. By creating a new AWS CloudTrail trail in the organization's management account and configuring it to apply to all accounts in the organization, the solution provides a centralized auditing mechanism with minimal operational overhead. Using a single S3 bucket with versioning ensures that logs are durable and can't be accidentally deleted, and MFA delete and encryption provide additional security.</p><p>The requirements are: &nbsp;</p><p>1. Audit and track all API calls across multiple AWS accounts in an organization. &nbsp;</p><p>2. Store logs durably and securely (with encryption and MFA delete protection). &nbsp;</p><p>3. Minimize operational overhead (centralized management). &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Creating a single CloudTrail trail in the management account (applies to all member accounts via AWS Organizations). &nbsp;</p><p>- Storing logs in a centralized S3 bucket (with versioning for durability). &nbsp;</p><p>- Enabling MFA delete and encryption for security. &nbsp;</p><p>- Deploying the trail to all accounts automatically (no per-account setup). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Uses an existing S3 bucket, which may not have versioning enabled (a compliance risk). &nbsp;</p><p>- B: Per-account trails and buckets increase operational overhead (violates the \"LEAST operational overhead\" requirement). &nbsp;</p><p>- D: SNS notifications add unnecessary complexity (tracking changes is already handled by CloudTrail + S3). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Organization-wide CloudTrail trails reduce management complexity. &nbsp;</p><p>- S3 versioning + MFA delete ensures log integrity and prevents tampering. &nbsp;</p><p>- KMS encryption meets regulatory requirements for data security. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "df4ace549f9a4986b6d9d7bbad181ca2",
            "questionNumber": 358,
            "type": "single",
            "content": "<p>Question #358</p><p>A company is deploying a distributed in-memory database on a fleet of Amazon EC2 instances. The fleet consists of a primary node and eight worker nodes. The primary node is responsible for monitoring cluster health, accepting user requests, distributing user requests to worker nodes, and sending an aggregate response back to a client. Worker nodes communicate with each other to replicate data partitions. <br><br>The company requires the lowest possible networking latency to achieve maximum performance.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Launch memory optimized EC2 instances in a partition placement group."
                },
                {
                    "label": "B",
                    "content": "Launch compute optimized EC2 instances in a partition placement group."
                },
                {
                    "label": "C",
                    "content": "Launch memory optimized EC2 instances in a cluster placement group."
                },
                {
                    "label": "D",
                    "content": "Launch compute optimized EC2 instances in a spread placement group."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirement for a distributed in-memory database with the lowest possible networking latency, the correct answer is:</p><p>C. Launch memory optimized EC2 instances in a cluster placement group.</p><p>Here's why this is the best solution:</p><p>Memory Optimized Instances:</p><p>A distributed in-memory database, as the name suggests, primarily relies on memory for data storage and processing</p><p>Memory optimized instances (like the R-series) are specifically designed with high memory-to-CPU ratios to support memory-intensive applications</p><p>These instances provide the necessary RAM capacity for storing the database entirely in memory, which is essential for optimal performance</p><p>Cluster Placement Group:</p><p>Cluster placement groups are specifically designed to provide the lowest possible network latency and highest packet-per-second performance between instances</p><p>They pack instances close together within a single Availability Zone on high-bandwidth, low-latency hardware</p><p>This placement strategy is ideal for applications that benefit from low-latency node-to-node communication, such as distributed databases where nodes need to communicate frequently for data replication</p><p>The other options are less suitable for the following reasons:</p><p>A. Memory optimized EC2 instances in a partition placement group:</p><p>While memory optimized instances are appropriate, partition placement groups are designed to spread instances across distinct hardware partitions</p><p>Partition groups help reduce the impact of hardware failures but don't provide the same level of network performance as cluster placement groups</p><p>This would not deliver the \"lowest possible networking latency\" required</p><p>B. Compute optimized EC2 instances in a partition placement group:</p><p>Compute optimized instances prioritize CPU performance over memory, which is not ideal for an in-memory database</p><p>Partition placement groups, as mentioned above, don't provide the lowest possible network latency</p><p>D. Compute optimized EC2 instances in a spread placement group:</p><p>Compute optimized instances are not optimal for memory-intensive workloads</p><p>Spread placement groups place instances on distinct hardware to maximize availability, but this comes at the cost of higher network latency between instances</p><p>This option would provide the highest network latency among all the choices</p><p>The combination of memory optimized instances (for the in-memory database requirements) and a cluster placement group (for the lowest possible networking latency) makes option C the optimal solution for achieving maximum performance in this distributed database scenario.</p><p>Sources</p><p>PERF04-BP06 Choose your workload's location based on network requirements - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/perf_networking_choose_workload_location_network_requirements.html）</p><p>Placement strategies for your placement groups - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-strategies.html）</p><p>Improve network latency for Linux based EC2 instances - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ena-improve-network-latency-linux.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7baa191bed9941c0ac1a36e843b3365f",
            "questionNumber": 359,
            "type": "single",
            "content": "<p>Question #359</p><p>A company maintains information on premises in approximately 1 million .csv files that are hosted on a VM. The data initially is 10 TB in size and grows at a rate of 1 TB each week. The company needs to automate backups of the data to the AWS Cloud. <br><br>Backups of the data must occur daily. The company needs a solution that applies custom filters to back up only a subset of the data that is located in designated source directories. The company has set up an AWS Direct Connect connection.</p><p><br></p><p>Which solution will meet the backup requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use the Amazon S3 CopyObject API operation with multipart upload to copy the existing data to Amazon S3. Use the CopyObject API operation to replicate new data to Amazon S3 daily."
                },
                {
                    "label": "B",
                    "content": "Create a backup plan in AWS Backup to back up the data to Amazon S3. Schedule the backup plan to run daily."
                },
                {
                    "label": "C",
                    "content": "Install the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily."
                },
                {
                    "label": "D",
                    "content": "Use an AWS Snowball Edge device for the initial backup. Use AWS DataSync for incremental backups to Amazon S3 daily."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirement to automate backups of approximately 1 million CSV files (10 TB initially, growing at 1 TB weekly) from on-premises to AWS Cloud with custom filters for specific directories, the solution with the least operational overhead is:</p><p>C. Install the AWS DataSync agent as a VM that runs on the on-premises hypervisor. Configure a DataSync task to replicate the data to Amazon S3 daily.</p><p>Here's why this is the best solution:</p><p>AWS DataSync is specifically designed for transferring large amounts of data between on-premises environments and AWS Cloud. It's purpose-built for this exact use case.</p><p>DataSync provides built-in filtering capabilities that allow you to include or exclude specific directories, which directly addresses the requirement to \"apply custom filters to back up only a subset of the data that is located in designated source directories.\"</p><p>DataSync can efficiently utilize the existing Direct Connect connection, optimizing transfer speeds and providing secure data transfer.</p><p>The DataSync agent runs as a VM on your existing hypervisor, making deployment straightforward with minimal infrastructure changes.</p><p>DataSync tasks can be easily scheduled to run daily, meeting the requirement for daily backups.</p><p>DataSync handles the complexity of transferring millions of small files efficiently, which is important given the 1 million CSV files in this scenario.</p><p>DataSync provides detailed monitoring, logging, and verification of transferred data, reducing the operational overhead of manually checking if backups were successful.</p><p>Option B (AWS Backup) is not the optimal choice because:</p><p>AWS Backup is primarily designed for backing up AWS resources, not for transferring data from on-premises to AWS.</p><p>AWS Backup would require additional components like AWS Storage Gateway to handle on-premises data, increasing the operational overhead.</p><p>AWS Backup has more limited filtering capabilities compared to DataSync, making it harder to back up only specific directories.</p><p>For large-scale data transfer from on-premises to AWS, AWS Backup would be less efficient than DataSync, which is specifically optimized for this purpose.</p><p>AWS DataSync provides the most streamlined, purpose-built solution for this specific use case with the least operational overhead, making option C the correct answer.</p><p>Sources</p><p>AWS DataSync for Backup solution | AWS re:Post （https://repost.aws/questions/QUK80Pmh1YRj60N53GDtl7UA/aws-datasync-for-backup-solution）</p><p>REL09-BP01 Identify and back up all data that needs to be backed up, or reproduce the data from sources - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_backing_up_data_identified_backups_data.html）</p><p>Choosing what AWS DataSync transfers - AWS DataSync （https://docs.aws.amazon.com/datasync/latest/userguide/task-options.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "90f03026ee1e48f88bc0b3cf11c8ca0b",
            "questionNumber": 360,
            "type": "single",
            "content": "<p>Question #360</p><p>A financial services company has an asset management product that thousands of customers use around the world. The customers provide feedback about the product through surveys. The company is building a new analytical solution that runs on Amazon EMR to analyze the data from these surveys. The following user personas need to access the analytical solution to perform different actions:</p><p><br></p><p>- Administrator: Provisions the EMR cluster for the analytics team based on the team’s requirements</p><p>- Data engineer: Runs ETL scripts to process, transform, and enrich the datasets</p><p>- Data analyst: Runs SQL and Hive queries on the data</p><p><br></p><p>A solutions architect must ensure that all the user personas have least privilege access to only the resources that they need. The user personas must be able to launch only applications that are approved and authorized. The solution also must ensure tagging for all resources that the user personas create.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create IAM roles for each user persona. Attach identity-based policies to define which actions the user who assumes the role can perform. Create an AWS Config rule to check for noncompliant resources. Configure the rule to notify the administrator to remediate the noncompliant resources."
                },
                {
                    "label": "B",
                    "content": "Setup Kerberos-based authentication for EMR clusters upon launch. Specify a Kerberos security configuration along with cluster-specific Kerberos options."
                },
                {
                    "label": "C",
                    "content": "Use AWS Service Catalog to control the Amazon EMR versions available for deployment, the cluster configuration, and the permissions for each user persona."
                },
                {
                    "label": "D",
                    "content": "Launch the EMR cluster by using AWS CloudFormation, Attach resource-based policies to the EMR cluster during cluster creation. Create an AWS Config rule to check for noncompliant clusters and noncompliant Amazon S3 buckets. Configure the rule to notify the administrator to remediate the noncompliant resources."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct answer. AWS Service Catalog allows for the creation of a catalog of approved resources and applications that users can launch. It enables the definition of specific permissions for eachuser persona, ensuring that they have least privilege access and can only launch authorized applications. It also supports resource tagging, which helps in tracking the resources created by each user.</p><p>The requirements are: &nbsp;</p><p>1. Least privilege access for each user persona (Administrator, Data Engineer, Data Analyst). &nbsp;</p><p>2. Restrict launch to approved/authorized applications (controlled EMR versions/configurations). &nbsp;</p><p>3. Enforce tagging for all created resources. &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Using AWS Service Catalog to: &nbsp;</p><p> &nbsp;- Define approved EMR configurations (versions, cluster setups). &nbsp;</p><p> &nbsp;- Assign IAM permissions per persona (e.g., Data Analysts can only run queries, not modify clusters). &nbsp;</p><p> &nbsp;- Enforce tagging policies (via Service Catalog TagOptions). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: IAM roles + AWS Config lacks centralized control over approved EMR configurations. AWS Config only detects noncompliance (doesn’t prevent it). &nbsp;</p><p>- B: Kerberos authentication secures access but doesn’t address least privilege or approved application launches. &nbsp;</p><p>- D: CloudFormation + AWS Config enforces resource policies but doesn’t simplify user access control like Service Catalog. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Service Catalog is the standard for pre-approved, governed AWS resource deployments. &nbsp;</p><p>- Tag enforcement ensures compliance and cost tracking. &nbsp;</p><p>- Least privilege is achieved by mapping IAM permissions to Service Catalog portfolios. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "23b15a75a1ab45eeaa64c133b80e5cf9",
            "questionNumber": 361,
            "type": "single",
            "content": "<p>Question #361</p><p>A software as a service (SaaS) company uses AWS to host a service that is powered by AWS PrivateLink. The service consists of proprietary software that runs on three Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in private subnets in multiple Availability Zones in the eu-west-2 Region. All the company's customers are in eu-west-2. <br><br>However, the company now acquires a new customer in the us-east-1 Region. The company creates a new VPC and new subnets in us-east-1. The company establishes inter-Region VPC peering between the VPCs in the two Regions.</p><p><br></p><p>The company wants to give the new customer access to the SaaS service, but the company does not want to immediately deploy new EC2 resources in us-east-1.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure a PrivateLink endpoint service in us-east-1 to use the existing NLB that is in eu-west-2. Grant specific AWS accounts access to connect to the SaaS service."
                },
                {
                    "label": "B",
                    "content": "Create an NLB in us-east-1. Create an IP target group that uses the IP addresses of the company&#39;s instances in eu-west-2 that host the SaaS service. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service."
                },
                {
                    "label": "C",
                    "content": "Create an Application Load Balancer (ALB) in front of the EC2 instances in eu-west-2. Create an NLB in us-east-1. Associate the NLB that is in us-east-1 with an ALB target group that uses the ALB that is in eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service."
                },
                {
                    "label": "D",
                    "content": "Use AWS Resource Access Manager (AWS RAM) to share the EC2 instances that are in eu-west-2. In us-east-1, create an NLB and an instance target group that includes the shared EC2 instances from eu-west-2. Configure a PrivateLink endpoint service that uses the NLB that is in us-east-1. Grant specific AWS accounts access to connect to the SaaS service."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. By creating a new NLB in us-east-1 and using the IP addresses of the instances in eu-west-2 as the target group, the company can establish a PrivateLink endpoint service that allows the new customer to access the SaaS service without deploying new EC2 resources in us-east-1. This solution leverages inter-Region VPC peering to maintain the connection between the regions.</p><p>The requirements are: &nbsp;</p><p>1. Provide access to the SaaS service (hosted in eu-west-2) to a new customer in us-east-1. &nbsp;</p><p>2. Avoid deploying new EC2 instances in us-east-1 (must reuse existing eu-west-2 instances). &nbsp;</p><p>3. Use AWS PrivateLink (secure, private connectivity). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Creating an NLB in us-east-1 (required for PrivateLink endpoint services). &nbsp;</p><p>- Configuring an IP target group pointing to the private IPs of the eu-west-2 instances (traffic routes via inter-Region VPC peering). &nbsp;</p><p>- Setting up a PrivateLink endpoint service in us-east-1 using the new NLB (customer connects securely via PrivateLink). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: PrivateLink endpoint services cannot reference NLBs in another Region (must be in the same Region). &nbsp;</p><p>- C: ALB cannot be used with PrivateLink (only NLBs are supported). &nbsp;</p><p>- D: AWS RAM cannot share EC2 instances across Regions (only within the same Region). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Inter-Region VPC peering enables private connectivity across Regions. &nbsp;</p><p>- PrivateLink + NLB provides secure, scalable access without exposing resources to the public internet. &nbsp;</p><p>- IP target groups allow cross-Region routing (unlike instance/ALB target groups). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7ffc4798af494020a5c48828d0153b34",
            "questionNumber": 362,
            "type": "single",
            "content": "<p>Question #362</p><p>A company needs to monitor a growing number of Amazon S3 buckets across two AWS Regions. The company also needs to track the percentage of objects that are encrypted in Amazon S3. The company needs a dashboard to display this information for internal compliance teams.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new S3 Storage Lens dashboard in each Region to track bucket and encryption metrics. Aggregate data from both Region dashboards into a single dashboard in Amazon QuickSight for the compliance teams."
                },
                {
                    "label": "B",
                    "content": "Deploy an AWS Lambda function in each Region to list the number of buckets and the encryption status of objects. Store this data in Amazon S3. Use Amazon Athena queries to display the data on a custom dashboard in Amazon QuickSight for the compliance teams."
                },
                {
                    "label": "C",
                    "content": "Use the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give the compliance teams access to the dashboard directly in the S3 console."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon EventBridge rule to detect AWS CloudTrail events for S3 object creation. Configure the rule to invoke an AWS Lambda function to record encryption metrics in Amazon DynamoDB. Use Amazon QuickSight to display the metrics in a dashboard for the compliance teams."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirement to monitor S3 buckets across two AWS Regions and track the percentage of encrypted objects with the least operational overhead, the correct answer is:</p><p>C. Use the S3 Storage Lens default dashboard to track bucket and encryption metrics. Give the compliance teams access to the dashboard directly in the S3 console.</p><p>Here's why this is the best solution with the least operational overhead:</p><p>S3 Storage Lens default dashboard is specifically designed to provide organization-wide visibility into object storage usage and activity across all AWS Regions from a single console.</p><p>The default dashboard automatically includes metrics for tracking bucket counts, storage usage, and encryption status across all regions without any additional configuration.</p><p>S3 Storage Lens provides built-in encryption metrics that show the percentage of objects that are encrypted, directly addressing the company's requirement.</p><p>Using the default dashboard eliminates the need to create and maintain separate dashboards for each region, significantly reducing operational overhead.</p><p>Compliance teams can be given direct access to the S3 Storage Lens dashboard through IAM permissions, allowing them to view the necessary metrics without requiring additional tools or data aggregation.</p><p>Option A (Creating new S3 Storage Lens dashboards in each Region and aggregating in QuickSight) would involve much higher operational overhead because:</p><p>It requires creating and maintaining multiple S3 Storage Lens dashboards (one for each region).</p><p>It requires setting up and configuring Amazon QuickSight as an additional service.</p><p>It necessitates building data pipelines to aggregate data from multiple dashboards into QuickSight.</p><p>It would require ongoing maintenance of both the individual dashboards and the QuickSight dashboard.</p><p>It adds complexity with no additional benefit, since the default dashboard already provides cross-region visibility.</p><p>The S3 Storage Lens default dashboard is specifically designed to address multi-region monitoring use cases like this one, making it the solution with the least operational overhead while still meeting all the requirements.</p><p>Sources</p><p>Assessing your storage activity and usage with Amazon S3 Storage Lens - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens.html）</p><p>AWS S3 Advanced metrics and recommendations | AWS re:Post （https://repost.aws/questions/QUP1Werf6jRd2WfjVI32chMg/aws-s3-advanced-metrics-and-recommendations）</p><p>Viewing S3 Storage Lens metrics on the dashboards - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens_view_metrics_dashboard.html）</p><p>Cutting-Edge Innovation, Yet No Simple Way to See Your S3 Storage Size? | AWS re:Post （https://repost.aws/questions/QUw4nuXCbTSN6D-jdt90c1uQ/cutting-edge-innovation-yet-no-simple-way-to-see-your-s3-storage-size）</p><p>Amazon S3 Features – Storage Lens – AWS （https://aws.amazon.com/s3/storage-lens/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c4be4af432d84e5d81cfa606e68e0262",
            "questionNumber": 363,
            "type": "single",
            "content": "<p>Question #363</p><p>A company’s CISO has asked a solutions architect to re-engineer the company's current CI/CD practices to make sure patch deployments to its application can happen as quickly as possible with minimal downtime if vulnerabilities are discovered. The company must also be able to quickly roll back a change in case of errors.</p><p><br></p><p>The web application is deployed in a fleet of Amazon EC2 instances behind an Application Load Balancer. The company is currently using GitHub to host the application source code, and has configured an AWS CodeBuild project to build the application. The company also intends to use AWS CodePipeline to trigger builds from GitHub commits using the existing CodeBuild project.</p><p><br></p><p>What CI/CD configuration meets all of the requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for in-place deployment. Monitor the newly deployed code, and, if there are any issues, push another code update."
                },
                {
                    "label": "B",
                    "content": "Configure CodePipeline with a deploy stage using AWS CodeDeploy configured for blue/green deployments. Monitor the newly deployed code, and, if there are any issues, trigger a manual rollback using CodeDeploy."
                },
                {
                    "label": "C",
                    "content": "Configure CodePipeline with a deploy stage using AWS CloudFormation to create a pipeline for test and production stacks. Monitor the newly deployed code, and, if there are any issues, push another code update."
                },
                {
                    "label": "D",
                    "content": "Configure the CodePipeline with a deploy stage using AWS OpsWorks and in-place deployments. Monitor the newly deployed code, and, if there are any issues, push another code update."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. Configuring AWS CodePipeline with AWS CodeDeploy for blue/green deployments allows for minimal downtime and quick rollbacks. This approach maintains two production environments, with one serving as a standby for the other. If issues are detected in the new deployment, the company can quickly switch back to the previous version, ensuring high availability and fault tolerance.</p><p>The requirements are: &nbsp;</p><p>1. Fast patch deployments with minimal downtime → Blue/Green deployment (avoids downtime by shifting traffic to new instances). &nbsp;</p><p>2. Quick rollback capability → CodeDeploy’s built-in rollback feature (reverts to the last known good version). &nbsp;</p><p>3. CI/CD pipeline integration → CodePipeline + CodeDeploy (supports GitHub, CodeBuild, and automated deployments). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using CodeDeploy with Blue/Green deployments (minimizes downtime and enables instant rollback). &nbsp;</p><p>- Monitoring + manual rollback (if issues arise, revert with a single action). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: In-place deployments cause downtime and lack automated rollback. &nbsp;</p><p>- C: CloudFormation alone doesn’t handle Blue/Green deployments for EC2 fleets (CodeDeploy is better suited). &nbsp;</p><p>- D: OpsWorks in-place deployments also cause downtime and lack native rollback. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Blue/Green deployments are ideal for zero-downtime patching. &nbsp;</p><p>- CodeDeploy rollback ensures quick recovery from failed deployments. &nbsp;</p><p>- CodePipeline orchestrates the workflow (GitHub → CodeBuild → CodeDeploy). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b68a748d71f440958e7815d8ed93121c",
            "questionNumber": 364,
            "type": "single",
            "content": "Question #364<p>A company is managing many AWS accounts by using an organization in AWS Organizations. Different business units in the company run applications on Amazon EC2 instances. All the EC2 instances must have a BusinessUnit tag so that the company can track the cost for each business unit.</p><p><br></p><p>A recent audit revealed that some instances were missing this tag. The company manually added the missing tag to the instances.</p><p><br></p><p>What should a solutions architect do to enforce the tagging requirement in the future?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned off. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the root of the organization."
                },
                {
                    "label": "B",
                    "content": "Enable tag policies in the organization. Create a tag policy for the BusinessUnit tag. Ensure that compliance with tag key capitalization is turned on. Implement the tag policy for the ec2:instance resource type. Attach the tag policy to the organization&#39;s management account."
                },
                {
                    "label": "C",
                    "content": "Create an SCP and attach the SCP to the root of the organization. Include the following statement in the SCP:<br><img src=\"https://up.zaixiankaoshi.com/5240831/question/7b8a2faa7e0882339cad8102536ba102.png\" >"
                },
                {
                    "label": "D",
                    "content": "Create an SCP and attach the SCP to the organization&rsquo;s management account. Include the following statement in the SCP:<br><img src=\"https://up.zaixiankaoshi.com/5240831/question/8cfc6056a858598d67b3d016e9891fb1.png\" >"
                }
            ],
            "correctAnswer": "C",
            "explanation": "Answer is C. <p>To enforce the tagging requirement for EC2 instances across multiple AWS accounts in an organization, a solutions architect should implement the following measures:</p><p><br></p><p>1. Create a tag policy:</p><p>   - Use AWS Organizations to create a tag policy that defines the required \"BusinessUnit\" tag for EC2 instances.</p><p>   - Specify the tag key, allowed values, and enforcement rules in the policy.</p><p>2. Attach the tag policy:</p><p>   - Attach the created tag policy to the root of the organization or to specific Organizational Units (OUs) that contain the accounts running EC2 instances.</p><p>3. Enable tag policy enforcement:</p><p>   - In the tag policy, enable enforcement for EC2 instances to prevent the creation or modification of instances without the required tag.</p><p>4. Implement proactive controls:</p><p>   - Use Service Control Policies (SCPs) to deny the creation of EC2 instances without the required tag.</p><p>   - This adds an extra layer of protection at the organization level.</p><p>5. Set up compliance monitoring:</p><p>   - Use AWS Config to create rules that check for compliance with the tagging policy.</p><p>   - Configure AWS Config to send notifications when non-compliant resources are detected.</p><p>6. Automate tag addition:</p><p>   - Implement AWS Lambda functions triggered by CloudTrail events to automatically add the required tag if it's missing when an instance is created or modified.</p><p>7. Educate and communicate:</p><p>   - Inform all teams about the tagging requirements and provide clear guidelines on how to properly tag resources.</p><p>8. Regular audits:</p><p>   - Schedule periodic reviews of resource tagging to ensure ongoing compliance.</p><p>9. Consider using AWS Resource Groups Tagging API:</p><p>   - This can help in managing tags across multiple accounts and services more efficiently.</p><p><br></p><p>By implementing these measures, the company can ensure that all EC2 instances have the required \"BusinessUnit\" tag, enabling accurate cost tracking for each business unit. Remember to test these policies in a non-production environment before applying them organization-wide, and always follow the principle of least privilege when setting up permissions and policies.</p><p>Sources</p><p>[1] [Best practices for managing organizational units (OUs) with AWS Organizations - AWS Organizations] (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_ous_best_practices.html)</p><p>[3] [Best practices for a multi-account environment - AWS Organizations] (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_best-practices.html)</p><p>[6] [Implementing a tagging strategy for detailed cost and usage data - AWS Prescriptive Guidance] (https://docs.aws.amazon.com/prescriptive-guidance/latest/cost-allocation-tagging/introduction.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a43f2a7190134b7aacf6282a1bbde289",
            "questionNumber": 365,
            "type": "single",
            "content": "<p>Question #365</p><p>A company is running a workload that consists of thousands of Amazon EC2 instances. The workload is running in a VPC that contains several public subnets and private subnets. The public subnets have a route for 0.0.0.0/0 to an existing internet gateway. The private subnets have a route for 0.0.0.0/0 to an existing NAT gateway.</p><p><br></p><p>A solutions architect needs to migrate the entire fleet of EC2 instances to use IPv6. The EC2 instances that are in private subnets must not be accessible from the public internet.</p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Update all the VPC route tables, and add a route for ::/0 to the internet gateway."
                },
                {
                    "label": "B",
                    "content": "Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Update the VPC route tables for all private subnets, and add a route for ::/0 to the NAT gateway."
                },
                {
                    "label": "C",
                    "content": "Update the existing VPC, and associate an Amazon-provided IPv6 CIDR block with the VPC and all subnets. Create an egress-only internet gateway. Update the VPC route tables for all private subnets, and add a route for ::/0 to the egress-only internet gateway."
                },
                {
                    "label": "D",
                    "content": "Update the existing VPC, and associate a custom IPv6 CIDR block with the VPC and all subnets. Create a new NAT gateway, and enable IPv6 support. Update the VPC route tables for all private subnets, and add a route for ::/0 to the IPv6-enabled NAT gateway."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct answer. By associating an Amazon-provided IPv6 CIDR block with the VPC and subnets, and creating an egress-only internet gateway, the architect can ensure that the instances in private subnets use IPv6 for outbound traffic without being accessible from the public internet. An egress-only internet gateway allows outbound traffic from a VPC to the internet, but does not allow inbound traffic.</p><p>The requirements are: &nbsp;</p><p>1. Migrate EC2 instances to IPv6 while maintaining current networking behavior. &nbsp;</p><p>2. Private subnets must not be publicly accessible (IPv6 equivalent of NAT for IPv4). &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Associating an Amazon-provided IPv6 CIDR block (ensures globally unique addresses). &nbsp;</p><p>- Creating an egress-only internet gateway (EIGW) → Allows outbound IPv6 traffic but blocks inbound traffic (like NAT for IPv4). &nbsp;</p><p>- Adding `::/0` routes in private subnets to the EIGW (ensures private instances can access the internet but remain unreachable from it). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Routes `::/0` to an internet gateway (IGW), making private instances publicly accessible (violates security requirements). &nbsp;</p><p>- B: NAT gateways don’t support IPv6 (they’re IPv4-only). &nbsp;</p><p>- D: NAT gateways cannot be IPv6-enabled (AWS doesn’t support IPv6 NAT). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Egress-only internet gateway (EIGW) is the IPv6 equivalent of NAT for private subnets. &nbsp;</p><p>- Amazon-provided IPv6 CIDR blocks simplify address allocation (custom blocks are unnecessary). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "098fb5246d594a90a1ba94bb49fb3335",
            "questionNumber": 366,
            "type": "single",
            "content": "<p>Question #366</p><p>A company is using Amazon API Gateway to deploy a private REST API that will provide access to sensitive data. The API must be accessible only from an application that is deployed in a VPC. The company deploys the API successfully. However, the API is not accessible from an Amazon EC2 instance that is deployed in the VPC.</p><p><br></p><p>Which solution will provide connectivity between the EC2 instance and the API?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows apigateway:* actions. Disable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC. Use the VPC endpoint&#39;s DNS name to access the API."
                },
                {
                    "label": "B",
                    "content": "Create an interface VPC endpoint for API Gateway. Attach an endpoint policy that allows the execute-api:Invoke action. Enable private DNS naming for the VPC endpoint. Configure an API resource policy that allows access from the VPC endpoint. Use the API endpoint&rsquo;s DNS names to access the API."
                },
                {
                    "label": "C",
                    "content": "Create a Network Load Balancer (NLB) and a VPC link. Configure private integration between API Gateway and the NLB. Use the API endpoint&rsquo;s DNS names to access the API."
                },
                {
                    "label": "D",
                    "content": "Create an Application Load Balancer (ALB) and a VPC Link. Configure private integration between API Gateway and the ALB. Use the ALB endpoint&rsquo;s DNS name to access the API."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. Creating an interface VPC endpoint for API Gateway with private DNS naming enabled allows the EC2 instance within the VPC to communicate with the API Gateway using the VPC endpoint's DNS name. Attaching an endpoint policy that allows the execute-api:Invoke action and configuring an API resource policy to allow access from the VPC endpoint ensures secure and private connectivity.</p><p>The requirements are: &nbsp;</p><p>1. Private REST API (accessible only from within a VPC). &nbsp;</p><p>2. API must be reachable from an EC2 instance in the VPC. &nbsp;</p><p>3. Current issue: API is not accessible from the EC2 instance. &nbsp;</p><p>Option B resolves this by: &nbsp;</p><p>- Creating an interface VPC endpoint for API Gateway (enables private connectivity from the VPC). &nbsp;</p><p>- Attaching an endpoint policy allowing `execute-api:Invoke` (grants API access permissions). &nbsp;</p><p>- Enabling private DNS naming (simplifies API calls by using standard API Gateway DNS names). &nbsp;</p><p>- Configuring an API resource policy to restrict access to the VPC endpoint (ensures security). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Disabling private DNS naming forces manual DNS configuration (adds unnecessary complexity). &nbsp;</p><p>- C: NLB + VPC Link is for integrating private backend services (not needed here; the issue is API Gateway access). &nbsp;</p><p>- D: ALB + VPC Link is also for backend integration (irrelevant for this use case). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Interface VPC endpoints provide secure, private access to AWS services (like API Gateway). &nbsp;</p><p>- Private DNS naming avoids manual endpoint URL management. &nbsp;</p><p>- API resource policies enforce least-privilege access control. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ff59c07a8dd245af8c75c3427070ab0c",
            "questionNumber": 367,
            "type": "single",
            "content": "<p>Question #367</p><p>A large payroll company recently merged with a small staffing company. The unified company now has multiple business units, each with its own existing AWS account. A solutions architect must ensure that the company can centrally manage the billing and access policies for all the AWS accounts. The solutions architect configures AWS Organizations by sending an invitation to all member accounts of the company from a centralized management account.</p><p><br></p><p>What should the solutions architect do next to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create the OrganizationAccountAccess IAM group in each member account. Include the necessary IAM roles for each administrator."
                },
                {
                    "label": "B",
                    "content": "Create the OrganizationAccountAccessPolicy IAM policy in each member account. Connect the member accounts to the management account by using cross-account access."
                },
                {
                    "label": "C",
                    "content": "Create the OrganizationAccountAccessRole IAM role in each member account. Grant permission to the management account to assume the IAM role."
                },
                {
                    "label": "D",
                    "content": "Create the OrganizationAccountAccessRole IAM role in the management account. Attach the AdministratorAccess AWS managed policy to the IAM role. Assign the IAM role to the administrators in each member account."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the scenario where a solutions architect needs to centrally manage billing and access policies for multiple AWS accounts after configuring AWS Organizations and sending invitations to member accounts, the correct next step is:</p><p>C. Create the OrganizationAccountAccessRole IAM role in each member account. Grant permission to the management account to assume the IAM role.</p><p>Here's why this is the correct answer:</p><p>The OrganizationAccountAccessRole is the standard IAM role that AWS Organizations uses to enable cross-account access from the management account to member accounts. This is a best practice recommended by AWS for organizations.</p><p>By creating this role in each member account and granting permission to the management account to assume it, administrators in the management account can access and manage resources in the member accounts. This enables centralized management of all accounts.</p><p>This approach follows the principle of role-based access control, which is more secure than sharing credentials or creating users in each account.</p><p>The OrganizationAccountAccessRole typically has administrative permissions in the member account, allowing the management account to perform necessary management tasks, including access policy management.</p><p>For billing management, AWS Organizations automatically enables consolidated billing once member accounts join the organization, allowing the management account to centrally manage billing for all accounts.</p><p>The other options are incorrect for the following reasons:</p><p>A. Creating an OrganizationAccountAccess IAM group in each member account is not the correct approach. IAM groups are collections of IAM users within a single account and don't facilitate cross-account access. Groups cannot be assumed like roles can, making them unsuitable for cross-account management.</p><p>B. There is no standard \"OrganizationAccountAccessPolicy\" in AWS Organizations. While custom policies can be created, they need to be attached to roles or users to be effective. Simply creating a policy without attaching it to a role that can be assumed by the management account wouldn't enable cross-account access.</p><p>Creating the OrganizationAccountAccessRole in each member account is the standard and recommended approach for enabling centralized management of AWS accounts within an organization, making option C the correct answer.</p><p>Sources</p><p>Managing account invitations with AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_invites.html）</p><p>AWS Identity and Access Management and AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-iam.html）</p><p>Manage member accounts - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/transitioning-to-multiple-aws-accounts/manage-member-accounts.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1f9602eccd094921910b7a2eebde4ce3",
            "questionNumber": 368,
            "type": "single",
            "content": "<p>Question #368</p><p>A company has application services that have been containerized and deployed on multiple Amazon EC2 instances with public IPs. An Apache Kafka cluster has been deployed to the EC2 instances. A PostgreSQL database has been migrated to Amazon RDS for PostgreSQL. The company expects a significant increase of orders on its platform when a new version of its flagship product is released.</p><p><br></p><p>What changes to the current architecture will reduce operational overhead and support the product release?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an EC2 Auto Scaling group behind an Application Load Balancer. Create additional read replicas for the DB instance. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3."
                },
                {
                    "label": "B",
                    "content": "Create an EC2 Auto Scaling group behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create Amazon Kinesis data streams and configure the application services to use the data streams. Store and serve static content directly from Amazon S3."
                },
                {
                    "label": "C",
                    "content": "Deploy the application on a Kubernetes cluster created on the EC2 instances behind an Application Load Balancer. Deploy the DB instance in Multi-AZ mode and enable storage auto scaling. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution."
                },
                {
                    "label": "D",
                    "content": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS) with AWS Fargate and enable auto scaling behind an Application Load Balancer. Create additional read replicas for the DB instance. Create an Amazon Managed Streaming for Apache Kafka cluster and configure the application services to use the cluster. Store static content in Amazon S3 behind an Amazon CloudFront distribution."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. Deploying the application on Amazon EKS with AWS Fargate allows the company to run containers without managing the underlying infrastructure, which reduces operational overhead. Using an Application Load Balancer for auto scaling and Multi-AZ deployment with storage auto scaling for the RDS instance ensures high availability and scalability. Creating an Amazon Managed Streaming for Apache Kafka cluster provides a scalable and managed streaming platform for Kafka. Storing static content in Amazon S3 and serving it through Amazon CloudFront reduces latency and improves the content delivery experience for users.</p><p>The requirements are: &nbsp;</p><p>1. Reduce operational overhead (automate scaling, minimize management). &nbsp;</p><p>2. Handle a surge in orders (scalable architecture). &nbsp;</p><p>3. Containerized services (needs orchestration). &nbsp;</p><p>4. Replace self-managed Kafka (reduce complexity). &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Using Amazon EKS with Fargate → Serverless Kubernetes (no EC2 management, auto-scaling). &nbsp;</p><p>- Multi-AZ RDS + read replicas → Scalable, highly available database. &nbsp;</p><p>- Amazon MSK → Fully managed Kafka (replaces self-managed clusters). &nbsp;</p><p>- S3 + CloudFront → Scalable static content delivery. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & B: &nbsp;</p><p> &nbsp;- EC2 Auto Scaling requires manual capacity management (higher overhead than EKS/Fargate). &nbsp;</p><p> &nbsp;- Self-managed Kafka remains operational burden. &nbsp;</p><p>- C: &nbsp;</p><p> &nbsp;- Self-managed Kubernetes on EC2 still requires node management (vs. Fargate’s serverless approach). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- EKS + Fargate eliminates node management. &nbsp;</p><p>- Amazon MSK removes Kafka operational overhead. &nbsp;</p><p>- CloudFront optimizes static content delivery globally. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "383bf43bbaed408199c7329bc0b6e53b",
            "questionNumber": 369,
            "type": "multiple",
            "content": "<p>Question #369</p><p>A company hosts a VPN in an on-premises data center. Employees currently connect to the VPN to access files in their Windows home directories. Recently, there has been a large growth in the number of employees who work remotely. As a result, bandwidth usage for connections into the data center has begun to reach 100% during business hours.</p><p><br></p><p>The company must design a solution on AWS that will support the growth of the company's remote workforce, reduce the bandwidth usage for connections into the data center, and reduce operational overhead.</p><p><br></p><p>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Storage Gateway Volume Gateway. Mount a volume from the Volume Gateway to the on-premises file server."
                },
                {
                    "label": "B",
                    "content": "Migrate the home directories to Amazon FSx for Windows File Server."
                },
                {
                    "label": "C",
                    "content": "Migrate the home directories to Amazon FSx for Lustre."
                },
                {
                    "label": "D",
                    "content": "Migrate remote users to AWS Client VPN."
                },
                {
                    "label": "E",
                    "content": "Create an AWS Direct Connect connection from the on-premises data center to AWS "
                }
            ],
            "correctAnswer": "BD",
            "explanation": "<p>Based on the scenario where a company needs to support a growing remote workforce, reduce bandwidth usage for connections to their on-premises data center, and reduce operational overhead, the two steps that would meet these requirements with the least operational overhead are:</p><p>B. Migrate the home directories to Amazon FSx for Windows File Server. D. Migrate remote users to AWS Client VPN.</p><p>Here's why these two options are the best combination:</p><p>Amazon FSx for Windows File Server (Option B):</p><p>It's a fully managed native Windows file system that's specifically designed for Windows workloads</p><p>It supports all the Windows file system features needed for home directories, including SMB protocol, Active Directory integration, and Windows ACLs</p><p>As a managed service, it has minimal operational overhead compared to self-managed alternatives</p><p>Moving the home directories to FSx would eliminate the need for remote users to access the on-premises data center for file access, directly addressing the bandwidth issue</p><p>It provides high availability and automatic backups, further reducing operational overhead</p><p>AWS Client VPN (Option D):</p><p>This is a managed client-based VPN service that would replace the on-premises VPN</p><p>It can scale to support the growing number of remote employees</p><p>It would direct remote user traffic to AWS instead of the on-premises data center, reducing bandwidth usage</p><p>As a managed service, it eliminates the operational overhead of maintaining an on-premises VPN solution</p><p>It can be configured to provide secure access to both the FSx file shares and any remaining on-premises resources</p><p>The other options are less suitable:</p><p>A. Creating an AWS Storage Gateway Volume Gateway would still require maintaining an on-premises file server and would not fully address the bandwidth issue, as data would still need to be synchronized between AWS and the on-premises environment. This would result in higher operational overhead compared to a fully managed solution.</p><p>C. Amazon FSx for Lustre is designed for high-performance computing workloads, not for Windows home directories. It lacks native support for Windows-specific features like Active Directory integration and would require additional components to work with Windows environments, increasing operational overhead.</p><p>The combination of migrating home directories to FSx for Windows File Server and moving remote users to AWS Client VPN provides a comprehensive solution that addresses all the requirements with minimal operational overhead. Users would connect to AWS Client VPN and access their home directories on FSx, bypassing the on-premises data center entirely for file access.</p><p>Sources</p><p>AWS Storage Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/storage-main-storage-gateway.html）</p><p>Choose the right SMB file storage - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/storage-fsx-smb.html）</p><p>PERF03-BP01 Understand storage characteristics and requirements - AWS Well-Architected Framework (2022-03-31) （https://docs.aws.amazon.com/wellarchitected/2022-03-31/framework/perf_right_storage_solution_understand_char.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6bb4880bcd114de8823d871d4806f176",
            "questionNumber": 370,
            "type": "multiple",
            "content": "<p>Question #370</p><p>A company has multiple AWS accounts. The company recently had a security audit that revealed many unencrypted Amazon Elastic Block Store (Amazon EBS) volumes attached to Amazon EC2 instances.</p><p><br></p><p>A solutions architect must encrypt the unencrypted volumes and ensure that unencrypted volumes will be detected automatically in the future. Additionally, the company wants a solution that can centrally manage multiple AWS accounts with a focus on compliance and security.</p><p><br></p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs."
                },
                {
                    "label": "B",
                    "content": "Use the AWS CLI to list all the unencrypted volumes in all the AWS accounts. Run a script to encrypt all the unencrypted volumes in place."
                },
                {
                    "label": "C",
                    "content": "Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume."
                },
                {
                    "label": "D",
                    "content": "Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the mandatory controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs."
                },
                {
                    "label": "E",
                    "content": "Turn on AWS CloudTrail. Configure an Amazon EventBridge rule to detect and automatically encrypt unencrypted volumes."
                }
            ],
            "correctAnswer": "AC",
            "explanation": "<p>Based on the requirement to encrypt unencrypted EBS volumes, detect unencrypted volumes automatically in the future, and centrally manage multiple AWS accounts with a focus on compliance and security, the correct combination of steps is:</p><p>A. Create an organization in AWS Organizations. Set up AWS Control Tower, and turn on the strongly recommended controls (guardrails). Join all accounts to the organization. Categorize the AWS accounts into OUs.</p><p>C. Create a snapshot of each unencrypted volume. Create a new encrypted volume from the unencrypted snapshot. Detach the existing volume, and replace it with the encrypted volume.</p><p>Here's why these two options work best together:</p><p>Option A addresses the requirements for centralized management and future detection:</p><p>AWS Organizations provides a way to centrally manage multiple AWS accounts</p><p>AWS Control Tower sets up a well-architected multi-account environment with built-in governance</p><p>The strongly recommended guardrails include controls that enforce EBS volume encryption</p><p>Organizing accounts into Organizational Units (OUs) allows for applying different policies based on account types</p><p>This setup will automatically detect and prevent the creation of unencrypted volumes in the future</p><p>This solution focuses on compliance and security through centralized management</p><p>Option C addresses the requirement to encrypt the currently unencrypted volumes:</p><p>This is the standard AWS-recommended process for encrypting existing unencrypted EBS volumes</p><p>Creating a snapshot, then creating an encrypted volume from that snapshot, and finally replacing the original volume is the only way to encrypt an existing EBS volume</p><p>This process preserves all the data while applying encryption</p><p>While this process requires some operational effort, it's necessary to address the existing unencrypted volumes</p><p>Together, these options provide a complete solution that:</p><p>Encrypts all existing unencrypted volumes (Option C)</p><p>Prevents the creation of new unencrypted volumes (Option A)</p><p>Provides centralized management of multiple AWS accounts (Option A)</p><p>Focuses on compliance and security through guardrails and organizational structure (Option A)</p><p>The combination of these two approaches addresses both the immediate need to encrypt existing volumes and the long-term requirement to maintain compliance and security across multiple AWS accounts.</p><p>Sources</p><p>Automatically encrypt existing and new Amazon EBS volumes - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automatically-encrypt-existing-and-new-amazon-ebs-volumes.html）</p><p>SEC08-BP02 Enforce encryption at rest - Security Pillar （https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/sec_protect_data_rest_encrypt.html）</p><p>Use encryption with EBS-backed AMIs - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "94807acad6634a2b856c79a4c31c2d66",
            "questionNumber": 371,
            "type": "single",
            "content": "<p>Question #371</p><p>A company hosts an intranet web application on Amazon EC2 instances behind an Application Load Balancer (ALB). Currently, users authenticate to the application against an internal user database.</p><p><br></p><p>The company needs to authenticate users to the application by using an existing AWS Directory Service for Microsoft Active Directory directory. All users with accounts in the directory must have access to the application.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new app client in the directory. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule. Configure the listener rule with the appropriate issuer, client ID and secret, and endpoint details for the Active Directory service. Configure the new app client with the callback URL that the ALB provides."
                },
                {
                    "label": "B",
                    "content": "Configure an Amazon Cognito user pool. Configure the user pool with a federated identity provider (IdP) that has metadata from the directory. Create an app client. Associate the app client with the user pool. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule. Configure the listener rule to use the user pool and app client."
                },
                {
                    "label": "C",
                    "content": "Add the directory as a new IAM identity provider (IdP). Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Configure the new role as the default authenticated user role for the IdP. Create a listener rule for the ALB. Specify the authenticate-oidc action for the listener rule."
                },
                {
                    "label": "D",
                    "content": "Enable AWS IAM Identity Center (AWS Single Sign-On). Configure the directory as an external identity provider (IdP) that uses SAML. Use the automatic provisioning method. Create a new IAM role that has an entity type of SAML 2.0 federation. Configure a role policy that allows access to the ALB. Attach the new role to all groups. Create a listener rule for the ALB. Specify the authenticate-cognito action for the listener rule."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. Authenticate users via AWS Directory Service (Microsoft AD). &nbsp;</p><p>2. Allow all directory users access to the web app. &nbsp;</p><p>3. Integrate with ALB for seamless authentication. &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using Amazon Cognito as an intermediary: &nbsp;</p><p> &nbsp;- Configure a user pool with a federated identity provider (IdP) linked to the existing Active Directory. &nbsp;</p><p> &nbsp;- Set up an app client for the web application. &nbsp;</p><p>- ALB Listener Rule: &nbsp;</p><p> &nbsp;- Use the `authenticate-cognito` action to redirect users to Cognito for authentication via Active Directory. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: ALB’s `authenticate-oidc` requires manual OIDC configuration (complex and error-prone vs. Cognito’s managed integration). &nbsp;</p><p>- C: IAM SAML federation is not natively supported by ALB for authentication (ALB requires OIDC/Cognito). &nbsp;</p><p>- D: AWS IAM Identity Center (SSO) is for centralized access to AWS accounts/apps, not for ALB-integrated web app authentication. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Cognito + ALB `authenticate-cognito` is the standard for AD-integrated web app authentication. &nbsp;</p><p>- Federated identities simplify user management (sync with existing AD users). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "be74e9fdef94403db7016751e8a51e46",
            "questionNumber": 372,
            "type": "single",
            "content": "<p>Question #372</p><p>A company has a website that serves many visitors. The company deploys a backend service for the website in a primary AWS Region and a disaster recovery (DR) Region.</p><p><br></p><p>A single Amazon CloudFront distribution is deployed for the website. The company creates an Amazon Route 53 record set with health checks and a failover routing policy for the primary Region’s backend service. The company configures the Route 53 record set as an origin for the CloudFront distribution. The company configures another record set that points to the backend service's endpoint in the DR Region as a secondary failover record type. The TTL for both record sets is 60 seconds.</p><p><br></p><p>Currently, failover takes more than 1 minute. A solutions architect must design a solution that will provide the fastest failover time.</p><p><br></p><p>Which solution will achieve this goal?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy an additional CloudFront distribution. Create a new Route 53 failover record set with health checks for both CloudFront distributions."
                },
                {
                    "label": "B",
                    "content": "Set the TTL to 4 seconds for the existing Route 53 record sets that are used for the backend service in each Region."
                },
                {
                    "label": "C",
                    "content": "Create new record sets for the backend services by using a latency routing policy. Use the record sets as an origin in the CloudFront distribution."
                },
                {
                    "label": "D",
                    "content": "Create a CloudFront origin group that includes two origins, one for each backend service Region. Configure origin failover as a cache behavior for the CloudFront distribution."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The requirement is to minimize failover time for the website’s backend service (currently &gt;1 minute due to Route 53 TTL and health check delays). &nbsp;</p><p>Option D solves this by: &nbsp;</p><p>- Using CloudFront Origin Groups: &nbsp;</p><p> &nbsp;- Configure two origins (primary and DR Region backends). &nbsp;</p><p> &nbsp;- Enable origin failover in CloudFront (automatically switches to the secondary origin if the primary fails). &nbsp;</p><p> &nbsp;- Failover happens in seconds (bypasses Route 53 TTL/health check delays). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Dual CloudFront distributions + Route 53 failover still relies on Route 53 health checks (TTL delays persist). &nbsp;</p><p>- B: 4-second TTL is impractical (violates AWS’s minimum TTL of 10 seconds for alias records). &nbsp;</p><p>- C: Latency routing doesn’t address failover (it routes based on latency, not health). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- CloudFront Origin Failover provides near-instant failover (vs. Route 53’s dependency on TTL/health checks). &nbsp;</p><p>- Eliminates dependency on DNS propagation (which causes delays in Route 53-based failover). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2d246acc2dbd4fdf99dee0d25e7454eb",
            "questionNumber": 373,
            "type": "multiple",
            "content": "<p>Question #373</p><p>A company is using multiple AWS accounts and has multiple DevOps teams running production and non-production workloads in these accounts. The company would like to centrally-restrict access to some of the AWS services that the DevOps teams do not use. The company decided to use AWS Organizations and successfully invited all AWS accounts into the Organization. They would like to allow access to services that are currently in-use and deny a few specific services. Also, they would like to administer multiple accounts together as a single unit.</p><p><br></p><p>What combination of steps should the solutions architect take to satisfy these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use a Deny list strategy."
                },
                {
                    "label": "B",
                    "content": "Review the Access Advisor in AWS IAM to determine services recently used."
                },
                {
                    "label": "C",
                    "content": "Review the AWS Trusted Advisor report to determine services recently used."
                },
                {
                    "label": "D",
                    "content": "Remove the default FullAWSAccess SCP."
                },
                {
                    "label": "E",
                    "content": "Define organizational units (OUs) and place the member accounts in the OUs."
                },
                {
                    "label": "F",
                    "content": "Remove the default DenyAWSAccess SCP."
                }
            ],
            "correctAnswer": "ABE",
            "explanation": "<p>Based on the requirement to centrally restrict access to some AWS services across multiple accounts while allowing access to services currently in use, the correct combination of steps is:</p><p>A. Use a Deny list strategy. B. Review the Access Advisor in AWS IAM to determine services recently used. E. Define organizational units (OUs) and place the member accounts in the OUs.</p><p>Here's why these three options are the correct choices:</p><p>Use a Deny list strategy (Option A):</p><p>A Deny list strategy is the most appropriate approach when you want to allow most services but restrict access to specific ones.</p><p>This aligns with the company's requirement to \"allow access to services that are currently in-use and deny a few specific services.\"</p><p>With a Deny list strategy, you keep the default FullAWSAccess SCP in place and create additional SCPs that explicitly deny access to specific services.</p><p>This approach requires less maintenance as new AWS services are automatically allowed unless explicitly denied.</p><p>Review the Access Advisor in AWS IAM to determine services recently used (Option B):</p><p>IAM Access Advisor provides insights into which AWS services have been accessed by IAM entities (users, groups, and roles) and when they were last used.</p><p>This tool is perfect for identifying which services are currently in use across the organization, which is necessary to determine which services should remain accessible.</p><p>Access Advisor provides historical data (typically for the last 365 days), allowing the company to make informed decisions about which services to restrict.</p><p>Define organizational units (OUs) and place the member accounts in the OUs (Option E):</p><p>Organizing accounts into OUs allows the company to \"administer multiple accounts together as a single unit\" as required.</p><p>OUs provide a hierarchical structure for applying SCPs to groups of accounts with similar requirements.</p><p>This approach simplifies policy management and allows for more granular control over which restrictions apply to which groups of accounts.</p><p>OUs can be structured to align with the company's organizational structure (e.g., separating production and non-production workloads).</p><p>The other options are not appropriate for the given requirements:</p><p>C. Review the AWS Trusted Advisor report to determine services recently used - While Trusted Advisor provides useful insights, it's not specifically designed to show which services are being used across accounts. IAM Access Advisor is the more appropriate tool for this purpose.</p><p>D. Remove the default FullAWSAccess SCP - This would be appropriate for an Allow list strategy, not a Deny list strategy. Removing this SCP would deny access to all services by default, which contradicts the requirement to \"allow access to services that are currently in-use.\"</p><p>The combination of using a Deny list strategy, reviewing IAM Access Advisor to determine which services are in use, and organizing accounts into OUs provides the most effective approach to meet the company's requirements for centrally restricting access to unused AWS services while maintaining access to necessary services.</p><p>Sources</p><p>Restrict access using AWS Organizations service control policies - AWS Account Management （https://docs.aws.amazon.com/accounts/latest/reference/using-orgs-example-scps.html）</p><p>Using AWS Organizations for security - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/organizations-security.html）</p><p>Community | Manage Security and Costs Across the Enterprise With AWS Organizations （https://community.aws/content/2iCmFQ2kf2EWfMXQUXZh6BJIyM7/practical-cloud-guide-manage-security-and-costs-across-the-enterprise-with-aws-organizations）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5affae832d5b430dbd018bb845b944f4",
            "questionNumber": 374,
            "type": "single",
            "content": "<p>Question #374</p><p>A live-events company is designing a scaling solution for its ticket application on AWS. The application has high peaks of utilization during sale events. Each sale event is a one-time event that is scheduled. The application runs on Amazon EC2 instances that are in an Auto Scaling group. The application uses PostgreSQL for the database layer.</p><p><br></p><p>The company needs a scaling solution to maximize availability during the sale events.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Serverless v2 Multi-AZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine."
                },
                {
                    "label": "B",
                    "content": "Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL Multi-AZ DB instance with automatically scaling read replicas. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger read replica before a sale event. Fail over to the larger read replica. Create another EventBridge rule that invokes another Lambda function to scale down the read replica after the sale event."
                },
                {
                    "label": "C",
                    "content": "Use a predictive scaling policy for the EC2 instances. Host the database on an Amazon RDS for PostgreSQL MultiAZ DB instance with automatically scaling read replicas. Create an AWS Step Functions state machine to run parallel AWS Lambda functions to pre-warm the database before a sale event. Create an Amazon EventBridge rule to invoke the state machine."
                },
                {
                    "label": "D",
                    "content": "Use a scheduled scaling policy for the EC2 instances. Host the database on an Amazon Aurora PostgreSQL Multi-AZ DB cluster. Create an Amazon EventBridge rule that invokes an AWS Lambda function to create a larger Aurora Replica before a sale event. Fail over to the larger Aurora Replica. Create another EventBridge rule that invokes another Lambda function to scale down the Aurora Replica after the sale event."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. Using a scheduled scaling policy for EC2 instances allows for the pre-planning of resources for known high-traffic events. Amazon Aurora's ability to handle high transaction rates and its support for scaling individual replicas makes it a good choice for the database layer. The use of AWS Lambda and Amazon EventBridge for automating the scaling of Aurora Replicas before and after the sale events ensures that the database can handle the load while keeping costs in check during off-peak times.</p><p>The requirements are: &nbsp;</p><p>1. Handle high, predictable traffic peaks (one-time scheduled sale events). &nbsp;</p><p>2. Maximize availability for both compute (EC2) and database (PostgreSQL) layers. &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Scheduled scaling for EC2: Proactively scales out before the event (predictive scaling is less precise for one-time events). &nbsp;</p><p>- Aurora PostgreSQL Multi-AZ: &nbsp;</p><p> &nbsp;- Scaling read replicas: Lambda automates replica scaling before/after the event. &nbsp;</p><p> &nbsp;- Failover to larger replicas: Ensures database performance during peak loads. &nbsp;</p><p>- EventBridge rules: Trigger Lambda functions to manage scaling dynamically. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & C: Predictive scaling is better for recurring patterns (not one-time events). Pre-warming databases is unnecessary with Aurora’s auto-scaling. &nbsp;</p><p>- B: RDS PostgreSQL read replicas require manual promotion (Aurora’s failover is faster and more reliable). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Scheduled scaling is ideal for known traffic spikes (e.g., ticket sales). &nbsp;</p><p>- Aurora Multi-AZ + read replicas provides high availability and scalable performance. &nbsp;</p><p>- EventBridge + Lambda automates scaling workflows. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "fd78be1fc6f1436e8d8a8dd2320bfe17",
            "questionNumber": 375,
            "type": "multiple",
            "content": "<p>Question #375</p><p>A company runs an intranet application on premises. The company wants to configure a cloud backup of the application. The company has selected AWS Elastic Disaster Recovery for this solution. The company requires that replication traffic does not travel through the public internet. The application also must not be accessible from the internet. The company does not want this solution to consume all available network bandwidth because other applications require bandwidth.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway."
                },
                {
                    "label": "B",
                    "content": "Create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Site-to-Site VPN connection between the on-premises network and the target AWS network."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network."
                },
                {
                    "label": "E",
                    "content": "During configuration of the replication servers, select the option to use private IP addresses for data replication."
                },
                {
                    "label": "F",
                    "content": "During configuration of the launch settings for the target servers, select the option to ensure that the Recovery instance&rsquo;s private IP address matches the source server&#39;s private IP address."
                }
            ],
            "correctAnswer": "ADE",
            "explanation": "<p>Based on the requirement to configure AWS Elastic Disaster Recovery with private connectivity that doesn't use the public internet, keeps the application inaccessible from the internet, and manages bandwidth consumption, the correct combination of steps is:</p><p>A. Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway. </p><p><br></p><p>D. Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network. </p><p><br></p><p>E. During configuration of the replication servers, select the option to use private IP addresses for data replication.</p><p>Here's why these three options are the correct choices:</p><p>Option A: Create a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway.</p><p>The VPC provides the network infrastructure in AWS to host the disaster recovery environment</p><p>Private subnets ensure the application is not accessible from the internet</p><p>Multiple subnets across availability zones provide high availability</p><p>The virtual private gateway is necessary to terminate the Direct Connect connection</p><p>NAT gateways allow resources in private subnets to access AWS services while maintaining security</p><p>Option D: Create an AWS Direct Connect connection and a Direct Connect gateway between the on-premises network and the target AWS network.</p><p>Direct Connect provides a dedicated private connection between on-premises and AWS</p><p>This ensures that \"replication traffic does not travel through the public internet\" as required</p><p>Direct Connect offers consistent network performance and bandwidth</p><p>The Direct Connect gateway allows connectivity between on-premises networks and multiple VPCs</p><p>Option E: During configuration of the replication servers, select the option to use private IP addresses for data replication.</p><p>This setting ensures that all replication traffic uses private IP addressing</p><p>It prevents any replication traffic from traversing the public internet</p><p>This configuration works in conjunction with Direct Connect to maintain private connectivity</p><p>These three steps together create a comprehensive solution that:</p><p>Establishes private connectivity between on-premises and AWS (Direct Connect)</p><p>Creates the necessary AWS network infrastructure (VPC with private subnets)</p><p>Configures the replication to use private addressing (private IP option)</p><p>Maintains security by keeping resources in private subnets</p><p>Allows for bandwidth management through Direct Connect's dedicated connection</p><p>The combination of these three options satisfies all the requirements: private connectivity, application security, and bandwidth management for the AWS Elastic Disaster Recovery solution.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "29d02a54fed547caafb8ca79115fdfb7",
            "questionNumber": 376,
            "type": "single",
            "content": "<p>Question #376</p><p>A company that provides image storage services wants to deploy a customer-facing solution to AWS. Millions of individual customers will use the solution. The solution will receive batches of large image files, resize the files, and store the files in an Amazon S3 bucket for up to 6 months.<br><br>The solution must handle significant variance in demand. The solution must also be reliable at enterprise scale and have the ability to rerun processing jobs in the event of failure.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Step Functions to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months."
                },
                {
                    "label": "B",
                    "content": "Use Amazon EventBridge to process the S3 event that occurs when a user uploads an image. Run an AWS Lambda function that resizes the image in place and replaces the original file in the S3 bucket. Create an S3 Lifecycle expiration policy to expire all stored images after 6 months."
                },
                {
                    "label": "C",
                    "content": "Use S3 Event Notifications to invoke an AWS Lambda function when a user stores an image. Use the Lambda function to resize the image in place and to store the original file in the S3 bucket. Create an S3 Lifecycle policy to move all stored images to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months."
                },
                {
                    "label": "D",
                    "content": "Use Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs when a user stores an image. Run an AWS Lambda function that resizes the image and stores the resized file in an S3 bucket that uses S3 Standard-Infrequent Access (S3 Standard-IA). Create an S3 Lifecycle policy to move all stored images to S3 Glacier Deep Archive after 6 months."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. Handle variable demand (scale automatically). &nbsp;</p><p>2. Enterprise-scale reliability (retry failed jobs). &nbsp;</p><p>3. Cost-effective storage (images stored for 6 months). &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Using SQS + Lambda: &nbsp;</p><p> &nbsp;- SQS queues decouple uploads from processing, enabling retries and buffering during demand spikes. &nbsp;</p><p> &nbsp;- Lambda auto-scales to process resizing jobs. &nbsp;</p><p>- Cost-optimized storage: &nbsp;</p><p> &nbsp;- S3 Standard-IA for frequently accessed resized images. &nbsp;</p><p> &nbsp;- S3 Glacier Deep Archive after 6 months (lowest cost for archival). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & B: Step Functions/EventBridge + Lambda lack built-in retry mechanisms (SQS is more reliable for job reprocessing). &nbsp;</p><p>- C: In-place resizing risks data loss if Lambda fails; no archival tier for cost savings. &nbsp;</p><p> Best Practices: &nbsp;</p><p>- SQS + Lambda is ideal for scalable, fault-tolerant batch processing. &nbsp;</p><p>- S3 Lifecycle policies optimize storage costs automatically. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7e349c0775744186bce5e2f35ad46eae",
            "questionNumber": 377,
            "type": "single",
            "content": "<p>Question #377</p><p>A company has an organization in AWS Organizations that includes a separate AWS account for each of the company’s departments. Application teams from different departments develop and deploy solutions independently.</p><p><br></p><p>The company wants to reduce compute costs and manage costs appropriately across departments. The company also wants to improve visibility into billing for individual departments. The company does not want to lose operational flexibility when the company selects compute resources.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Budgets for each department. Use Tag Editor to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans."
                },
                {
                    "label": "B",
                    "content": "Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use SCPs to apply tags to appropriate resources. Purchase EC2 Instance Savings Plans."
                },
                {
                    "label": "C",
                    "content": "Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use Tag Editor to apply tags to appropriate resources. Purchase Compute Savings Plans."
                },
                {
                    "label": "D",
                    "content": "Use AWS Budgets for each department. Use SCPs to apply tags to appropriate resources. Purchase Compute Savings Plans."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirement to reduce compute costs, manage costs across departments, improve billing visibility, and maintain operational flexibility, the correct answer is:</p><p>C. Configure AWS Organizations to use consolidated billing. Implement a tagging strategy that identifies departments. Use Tag Editor to apply tags to appropriate resources. Purchase Compute Savings Plans.</p><p>Here's why this is the best solution:</p><p>Consolidated Billing with AWS Organizations:</p><p>This allows the company to receive a single bill for all department accounts</p><p>Enables volume discounts across all accounts, reducing overall costs</p><p>Provides a centralized view of costs across all departments</p><p>Allows Savings Plans benefits to be shared across all accounts in the organization</p><p>Tagging Strategy that Identifies Departments:</p><p>Tags allow resources to be associated with specific departments</p><p>Enables detailed cost allocation and reporting by department</p><p>Provides visibility into which departments are using which resources</p><p>Supports chargeback or showback models for internal cost management</p><p>Using Tag Editor to Apply Tags:</p><p>Tag Editor is a flexible tool that allows for bulk tagging of resources</p><p>It doesn't restrict operational flexibility, unlike SCPs which are restrictive policies</p><p>Allows application teams to maintain their independence while still implementing tags</p><p>Supports the company's requirement not to \"lose operational flexibility\"</p><p>Compute Savings Plans:</p><p>Provide significant discounts (up to 66%) compared to On-Demand pricing</p><p>Offer maximum flexibility across EC2, Lambda, and Fargate services</p><p>Allow departments to change instance types, sizes, and even compute services while still receiving discounts</p><p>Support the requirement for operational flexibility while reducing costs</p><p>Option B is not optimal because:</p><p>It uses SCPs to apply tags, which are restrictive policies that could limit operational flexibility</p><p>EC2 Instance Savings Plans are less flexible than Compute Savings Plans as they're limited to specific EC2 instance families in specific regions</p><p>This approach would restrict the application teams' ability to choose the most appropriate compute resources for their needs</p><p>The solution in option C provides the best balance of cost savings, billing visibility, and operational flexibility, making it the most suitable choice for the company's requirements.</p><p>Sources</p><p>Decide on Savings Plans or Reserved Instances for EC2 instances | AWS re:Post （https://repost.aws/knowledge-center/ec2-savings-plan-reserved-instances）</p><p>Understanding how Savings Plans apply to your usage - Savings Plans （https://docs.aws.amazon.com/savingsplans/latest/userguide/sp-applying.html）</p><p>Optimize spending for Windows on Amazon EC2 - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/savings-plans.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5e318f74f6494bbbac2b15a217aa194b",
            "questionNumber": 378,
            "type": "single",
            "content": "<p>Question #378</p><p>A company has a web application that securely uploads pictures and videos to an Amazon S3 bucket. The company requires that only authenticated users are allowed to post content. The application generates a presigned URL that is used to upload objects through a browser interface. Most users are reporting slow upload times for objects larger than 100 MB.</p><p><br></p><p>What can a solutions architect do to improve the performance of these uploads while ensuring only authenticated users are allowed to post content?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up an Amazon API Gateway with an edge-optimized API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using a COGNITO_USER_POOLS authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects."
                },
                {
                    "label": "B",
                    "content": "Set up an Amazon API Gateway with a regional API endpoint that has a resource as an S3 service proxy. Configure the PUT method for this resource to expose the S3 PutObject operation. Secure the API Gateway using an AWS Lambda authorizer. Have the browser interface use API Gateway instead of the presigned URL to upload objects."
                },
                {
                    "label": "C",
                    "content": "Enable an S3 Transfer Acceleration endpoint on the S3 bucket. Use the endpoint when generating the presigned URL. Have the browser interface upload the objects to this URL using the S3 multipart upload API."
                },
                {
                    "label": "D",
                    "content": "Configure an Amazon CloudFront distribution for the destination S3 bucket. Enable PUT and POST methods for the CloudFront cache behavior. Update the CloudFront origin to use an origin access identity (OAI). Give the OAI user 3: PutObject permissions in the bucket policy. Have the browser interface upload objects using the CloudFront distribution."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. Improve upload performance for large files (&gt;100 MB). &nbsp;</p><p>2. Ensure only authenticated users can upload (presigned URLs already enforce this). &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Enabling S3 Transfer Acceleration → Uses CloudFront’s edge network for faster uploads globally. &nbsp;</p><p>- Using multipart upload API → Parallelizes large file uploads (improves speed and reliability). &nbsp;</p><p>- Retaining presigned URLs → Maintains authentication without added complexity. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & B: API Gateway as an S3 proxy adds latency and complexity (no performance gain over direct S3 uploads). &nbsp;</p><p>- D: CloudFront with PUT/POST is not optimized for uploads (Transfer Acceleration is purpose-built for S3). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- S3 Transfer Acceleration leverages CloudFront’s backbone for faster uploads. &nbsp;</p><p>- Multipart uploads split large files into parts (improves throughput). &nbsp;</p><p>- Presigned URLs remain the simplest way to authenticate uploads. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c2900b0f4ccf4b2495e245f606a8405e",
            "questionNumber": 379,
            "type": "multiple",
            "content": "<p>Question #379</p><p>A large company is migrating its entire IT portfolio to AWS. Each business unit in the company has a standalone AWS account that supports both development and test environments. New accounts to support production workloads will be needed soon.</p><p><br></p><p>The finance department requires a centralized method for payment but must maintain visibility into each group's spending to allocate costs. The security team requires a centralized mechanism to control IAM usage in all the company’s accounts.</p><p><br></p><p>What combination of the following options meets the company’s needs with the LEAST effort? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use a collection of parameterized AWS CloudFormation templates defining common IAM permissions that are launched into each account. Require all new and existing accounts to launch the appropriate stacks to enforce the least privilege model."
                },
                {
                    "label": "B",
                    "content": "Use AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Invite the existing accounts to join the organization and create new accounts using Organizations."
                },
                {
                    "label": "C",
                    "content": "Require each business unit to use its own AWS accounts. Tag each AWS account appropriately and enable Cost Explorer to administer chargebacks."
                },
                {
                    "label": "D",
                    "content": "Enable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts."
                },
                {
                    "label": "E",
                    "content": "Consolidate all of the company&#39;s AWS accounts into a single AWS account. Use tags for billing purposes and the IAM&rsquo;s Access Advisor feature to enforce the least privilege model.&nbsp;"
                }
            ],
            "correctAnswer": "BD",
            "explanation": "<p>Based on the requirements for centralizing payment methods while maintaining visibility into each group's spending, and implementing centralized control over IAM usage across all accounts with the least effort, the correct combination is:</p><p>B. Use AWS Organizations to create a new organization from a chosen payer account and define an organizational unit hierarchy. Invite the existing accounts to join the organization and create new accounts using Organizations.</p><p>D. Enable all features of AWS Organizations and establish appropriate service control policies that filter IAM permissions for sub-accounts.</p><p>Here's why these two options are the best solution:</p><p>Option B addresses the finance department's requirements:</p><p>Creates a centralized billing structure through AWS Organizations</p><p>The payer account becomes the management account that handles all billing</p><p>Existing standalone accounts can be invited to join the organization</p><p>New production accounts can be created directly within the organization</p><p>Organizational unit hierarchy allows for logical grouping of accounts by business unit</p><p>Consolidated billing provides visibility into each group's spending for cost allocation</p><p>This approach requires minimal effort as it leverages existing accounts</p><p>Option D addresses the security team's requirements:</p><p>Enabling all features of AWS Organizations (not just consolidated billing) is necessary to use service control policies</p><p>Service control policies (SCPs) provide a centralized mechanism to control IAM usage across all accounts</p><p>SCPs act as permission guardrails that limit maximum permissions available in member accounts</p><p>This approach allows the security team to enforce consistent IAM policies across all accounts</p><p>SCPs can be applied at the organizational unit level, making it easy to manage permissions for groups of accounts</p><p>Together, these options provide:</p><p>Centralized billing with cost visibility (finance requirement)</p><p>Centralized IAM control (security requirement)</p><p>Support for both existing and new accounts (migration requirement)</p><p>Minimal effort implementation by leveraging AWS's purpose-built solution</p><p>This combination represents the most straightforward and least-effort approach to meeting both the finance department's need for centralized billing with visibility and the security team's requirement for centralized IAM control across all accounts.</p><p>Sources</p><p>University of British Columbia Cloud Innovation Centre: Governing an innovation hub using AWS management services | AWS Public Sector Blog （https://aws.amazon.com/cn/blogs/publicsector/university-of-british-columbia-cloud-innovation-centre-governing-an-innovation-hub-using-aws-management-services/）</p><p>When to use AWS Organizations - AWS Account Management （https://docs.aws.amazon.com/accounts/latest/reference/using-orgs.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "05d6913f4c164b409c04aeb05d53cb82",
            "questionNumber": 380,
            "type": "single",
            "content": "<p>Question #380</p><p>A company has a solution that analyzes weather data from thousands of weather stations. The weather stations send the data over an Amazon API Gateway REST API that has an AWS Lambda function integration. The Lambda function calls a third-party service for data pre-processing. The third-party service gets overloaded and fails the pre-processing, causing a loss of data.</p><p><br></p><p>A solutions architect must improve the resiliency of the solution. The solutions architect must ensure that no data is lost and that data can be processed later if failures occur.</p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the queue as the dead-letter queue for the API."
                },
                {
                    "label": "B",
                    "content": "Set up two Amazon Simple Queue Service (Amazon SQS) queues: a primary queue and a secondary queue. Configure the secondary queue as the dead-letter queue for the primary queue. Update the API to use a new integration to the primary queue. Configure the Lambda function as the invocation target for the primary queue."
                },
                {
                    "label": "C",
                    "content": "Create two Amazon EventBridge event buses: a primary event bus and a secondary event bus. Update the API to use a new integration to the primary event bus. Configure an EventBridge rule to react to all events on the primary event bus. Specify the Lambda function as the target of the rule. Configure the secondary event bus as the failure destination for the Lambda function."
                },
                {
                    "label": "D",
                    "content": "Create a custom Amazon EventBridge event bus. Configure the event bus as the failure destination for the Lambda function."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. By setting up two SQS queues, with the primary queue handling incoming data and the secondary queue acting as a dead-letter queue, the architect can ensure that data is not lost when the Lambda function is unable to process it due to the third-party service failure. This setup allows for the data to be reprocessed at a later time, thus improving the resiliency of the system.</p><p>The requirements are: &nbsp;</p><p>1. Ensure no data is lost (handle third-party service failures). &nbsp;</p><p>2. Enable delayed processing (retry failed jobs). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using two SQS queues: &nbsp;</p><p> &nbsp;- Primary queue: Buffers incoming weather data (decouples API Gateway from Lambda). &nbsp;</p><p> &nbsp;- Dead-letter queue (DLQ): Captures failed processing attempts (ensures no data loss). &nbsp;</p><p>- Lambda retries: Processes messages from the primary queue; failed messages move to the DLQ for later reprocessing. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Single SQS queue as DLQ for API Gateway is invalid (API Gateway cannot directly use SQS as DLQ). &nbsp;</p><p>- C & D: EventBridge lacks built-in retry logic (SQS is better for queuing and reprocessing). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- SQS + DLQ is the standard for resilient message processing. &nbsp;</p><p>- Decoupling (API Gateway → SQS → Lambda) prevents data loss during third-party failures. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "af87c51fa2fc4efc9e74550b865a27a0",
            "questionNumber": 381,
            "type": "multiple",
            "content": "<p>Question #381</p><p>A company built an ecommerce website on AWS using a three-tier web architecture. The application is Java-based and composed of an Amazon CloudFront distribution, an Apache web server layer of Amazon EC2 instances in an Auto Scaling group, and a backend Amazon Aurora MySQL database.</p><p><br></p><p>Last month, during a promotional sales event, users reported errors and timeouts while adding items to their shopping carts. The operations team recovered the logs created by the web servers and reviewed Aurora DB cluster performance metrics. Some of the web servers were terminated before logs could be collected and the Aurora metrics were not sufficient for query performance analysis.</p><p><br></p><p>Which combination of steps must the solutions architect take to improve application performance visibility during peak traffic events? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the Aurora MySQL DB cluster to publish slow query and error logs to Amazon CloudWatch Logs."
                },
                {
                    "label": "B",
                    "content": "Implement the AWS X-Ray SDK to trace incoming HTTP requests on the EC2 instances and implement tracing of SQL queries with the X-Ray SDK for Java."
                },
                {
                    "label": "C",
                    "content": "Conigure the Aurora MySQL DB cluster to stream slow query and error logs to Amazon Kinesis."
                },
                {
                    "label": "D",
                    "content": "Install and configure an Amazon CloudWatch Logs agent on the EC2 instances to send the Apache logs to CloudWatch Logs."
                },
                {
                    "label": "E",
                    "content": "Enable and configure AWS CloudTrail to collect and analyze application activity from Amazon EC2 and Aurora."
                },
                {
                    "label": "F",
                    "content": "Enable Aurora MySQL DB cluster performance benchmarking and publish the stream to AWS X-Ray."
                }
            ],
            "correctAnswer": "ABD",
            "explanation": "<p>Option A is correct as publishing slow query and error logs to CloudWatch Logs can help in identifying and diagnosing database performance issues. Option B is correct because implementing AWS X-Ray can provide insights into the performance of the application and its underlying services, including the ability to trace SQL queries. Option D is also correct as configuring the CloudWatch Logs agent on EC2 instances can centralize log data for analysis, which is crucial for troubleshooting and performance monitoring.</p><p>The requirements are to improve performance visibility during peak traffic by: &nbsp;</p><p>1. Capturing web server logs (prevent loss during Auto Scaling terminations). &nbsp;</p><p>2. Analyzing database query performance (slow queries/errors). &nbsp;</p><p>3. Tracing request flows (identify bottlenecks). &nbsp;</p><p>Options A, B, and D meet these needs by: &nbsp;</p><p>- A: Aurora slow query/error logs in CloudWatch → Diagnose DB performance issues. &nbsp;</p><p>- B: AWS X-Ray → Trace HTTP requests and SQL queries end-to-end. &nbsp;</p><p>- D: CloudWatch Logs agent → Persist Apache logs (survives instance termination). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- C: Kinesis for Aurora logs is overkill (CloudWatch Logs suffices). &nbsp;</p><p>- E: CloudTrail tracks API calls, not application performance. &nbsp;</p><p>- F: X-Ray doesn’t benchmark Aurora (it traces requests, unlike Performance Insights). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- CloudWatch Logs centralizes logs for Auto Scaling workloads. &nbsp;</p><p>- X-Ray maps distributed transaction flows. &nbsp;</p><p>- Aurora logs in CloudWatch enable historical query analysis. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5b5643440c1a4d4ea019d352019f1802",
            "questionNumber": 382,
            "type": "single",
            "content": "<p>Question #382</p><p>A company that provisions job boards for a seasonal workforce is seeing an increase in traffic and usage. The backend services run on a pair of Amazon EC2 instances behind an Application Load Balancer with Amazon DynamoDB as the datastore. Application read and write traffic is slow during peak seasons.</p><p><br></p><p>Which option provides a scalable application architecture to handle peak seasons with the LEAST development effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the backend services to AWS Lambda. Increase the read and write capacity of DynamoDB."
                },
                {
                    "label": "B",
                    "content": "Migrate the backend services to AWS Lambda. Configure DynamoDB to use global tables."
                },
                {
                    "label": "C",
                    "content": "Use Auto Scaling groups for the backend services. Use DynamoDB auto scaling."
                },
                {
                    "label": "D",
                    "content": "Use Auto Scaling groups for the backend services. Use Amazon Simple Queue Service (Amazon SQS) and an AWS Lambda function to write to DynamoDB."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the scenario where a job board application with EC2 instances and DynamoDB is experiencing slow read and write traffic during peak seasons, the option that provides a scalable architecture with the least development effort is:</p><p>C. Use Auto Scaling groups for the backend services. Use DynamoDB auto scaling.</p><p>Here's why this is the best option:</p><p>Least Development Effort:</p><p>This approach builds on the existing architecture (EC2 instances and DynamoDB) without requiring a significant rewrite of the application code.</p><p>The current backend services can continue to run on EC2 instances with minimal modifications.</p><p>Setting up Auto Scaling groups and DynamoDB auto scaling is primarily a configuration task rather than a development task.</p><p>Scalability Benefits:</p><p>Auto Scaling groups automatically adjust the number of EC2 instances based on demand, ensuring the backend services can handle increased traffic during peak seasons.</p><p>DynamoDB auto scaling automatically adjusts read and write capacity units based on actual usage patterns, addressing the slow read and write traffic issues.</p><p>Both components scale independently and automatically in response to actual load.</p><p>Why the other options require more effort or are less suitable:</p><p>Option A (Migrate to Lambda with increased DynamoDB capacity):</p><p>Requires rewriting the backend services to run as Lambda functions</p><p>Manually increasing DynamoDB capacity doesn't provide the automatic scaling needed for seasonal fluctuations</p><p>Option B (Migrate to Lambda with DynamoDB global tables):</p><p>Requires rewriting the backend services for Lambda</p><p>Global tables are designed for multi-region replication, which adds complexity and cost but doesn't directly address the scaling issue for a single-region application</p><p>Option D (Auto Scaling groups with SQS and Lambda):</p><p>Introduces additional components (SQS and Lambda) that would require new development work</p><p>Adds complexity to the architecture that may not be necessary to solve the specific problem</p><p>Option C leverages AWS's managed scaling capabilities for both compute (EC2 Auto Scaling) and database (DynamoDB auto scaling) layers, providing a comprehensive scaling solution with minimal development effort. It allows the application to automatically adjust resources based on demand, which is ideal for handling seasonal traffic patterns.</p><p>Sources</p><p>Choose the right throughput strategy for Amazon DynamoDB applications | AWS Database Blog （https://aws.amazon.com/blogs/database/choose-the-right-throughput-strategy-for-amazon-dynamodb-applications/）</p><p>Managing throughput capacity automatically with DynamoDB auto scaling - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/AutoScaling.html）</p><p>Scaling and event processing with SNS and SQS | AWS re:Post （https://repost.aws/questions/QUcnYW9ZAkSA-Px9tyHm_sYw/scaling-and-event-processing-with-sns-and-sqs）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ed8298ec63704b4aa5a8531a5c2e9f33",
            "questionNumber": 383,
            "type": "single",
            "content": "<p>Question #383</p><p>A company is migrating to the cloud. It wants to evaluate the configurations of virtual machines in its existing data center environment to ensure that it can size new Amazon EC2 instances accurately. The company wants to collect metrics, such as CPU, memory, and disk utilization, and it needs an inventory of what processes are running on each instance. The company would also like to monitor network connections to map communications between servers.</p><p><br></p><p>Which would enable the collection of this data MOST cost effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center."
                },
                {
                    "label": "B",
                    "content": "Configure the Amazon CloudWatch agent on all servers within the local environment and publish metrics to Amazon CloudWatch Logs."
                },
                {
                    "label": "C",
                    "content": "Use AWS Application Discovery Service and enable agentless discovery in the existing virtualization environment."
                },
                {
                    "label": "D",
                    "content": "Enable AWS Application Discovery Service in the AWS Management Console and configure the corporate firewall to allow scans over a VPN."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the company's requirements to collect metrics such as CPU, memory, disk utilization, running processes, and network connections between servers for EC2 instance sizing, the most cost-effective option is:</p><p>A. Use AWS Application Discovery Service and deploy the data collection agent to each virtual machine in the data center.</p><p>Here's why this is the most cost-effective solution:</p><p>Comprehensive Data Collection: The AWS Application Discovery Service with agent-based collection provides all the required data:</p><p>System metrics (CPU, memory, disk utilization)</p><p>Inventory of running processes</p><p>Network connections to map communications between servers</p><p>Detailed time-series performance data</p><p>Cost Effectiveness:</p><p>AWS Application Discovery Service is provided at no additional charge</p><p>The data collected is stored in AWS for 90 days at no cost</p><p>It's specifically designed for migration planning, so it collects exactly what's needed without excess</p><p>Purpose-Built for Migration: This service is specifically designed to help with cloud migration planning and EC2 instance sizing, making it the most appropriate tool for the stated requirements.</p><p>Why the other options are less suitable:</p><p>B. Configure the Amazon CloudWatch agent on all servers:</p><p>Incurs costs for metrics collection and storage in CloudWatch</p><p>Primarily designed for ongoing monitoring rather than migration planning</p><p>Would require additional configuration to collect all the required data</p><p>Not as seamlessly integrated with AWS migration tools</p><p>C. Use AWS Application Discovery Service with agentless discovery:</p><p>While also free to use, the agentless approach cannot collect information about running processes</p><p>Cannot map detailed network connections between servers</p><p>Limited to VMware environments and provides less detailed information</p><p>Doesn't meet all the requirements specified by the company</p><p>The agent-based AWS Application Discovery Service is the most cost-effective solution because it's free to use, purpose-built for migration planning, and collects all the required data types (system metrics, process inventory, and network connections). This makes it the ideal choice for accurately sizing EC2 instances based on the company's existing virtual machine configurations.</p><p>Sources</p><p>AWS Application Discovery Agent - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-agent.html）</p><p>Welcome - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/APIReference/Welcome.html）</p><p>Discover on-premises resources using AWS Migration Hub discovery tools - AWS Migration Hub （https://docs.aws.amazon.com/migrationhub/latest/ug/gs-new-user-discovery.html）</p><p>AWS Application Discovery Service Update – Agentless Discovery for VMware | AWS News Blog （https://aws.amazon.com/cn/blogs/aws/aws-application-discovery-service-update-agentless-discovery-for-vmware/）</p><p>Application Discovery Service Agentless Collector - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/userguide/agentless-collector.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a0ba3cd209584b1ba6efc6d80097f34b",
            "questionNumber": 384,
            "type": "single",
            "content": "<p>Question #384</p><p>A company provides a software as a service (SaaS) application that runs in the AWS Cloud. The application runs on Amazon EC2 instances behind a Network Load Balancer (NLB). The instances are in an Auto Scaling group and are distributed across three Availability Zones in a single AWS Region.</p><p><br></p><p>The company is deploying the application into additional Regions. The company must provide static IP addresses for the application to customers so that the customers can add the IP addresses to allow lists. The solution must automatically route customers to the Region that is geographically closest to them.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon CloudFront distribution. Create a CloudFront origin group. Add the NLB for each additional Region to the origin group. Provide customers with the IP address ranges of the distribution&rsquo;s edge locations."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Global Accelerator standard accelerator. Create a standard accelerator endpoint for the NLB in each additional Region. Provide customers with the Global Accelerator IP address."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon CloudFront distribution. Create a custom origin for the NLB in each additional Region. Provide customers with the IP address ranges of the distribution&rsquo;s edge locations."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Global Accelerator custom routing accelerator. Create a listener for the custom routing accelerator. Add the IP address and ports for the NLB in each additional Region. Provide customers with the Global Accelerator IP address."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. AWS Global Accelerator can direct traffic to the optimal regional endpoint based on the location of the user, providing the fastest response times. By creating a standard accelerator and providing the Global Accelerator IP address to customers, the company can ensure that users are routed to the closest Region without exposing the underlying NLB IP addresses.</p><p>The requirements are: &nbsp;</p><p>1. Provide static IP addresses for customers to whitelist. &nbsp;</p><p>2. Route customers to the nearest Region (geographic proximity). &nbsp;</p><p>3. Support multi-Region deployment with NLBs. &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using AWS Global Accelerator: &nbsp;</p><p> &nbsp;- Provides two static anycast IPs (simplifies allow lists). &nbsp;</p><p> &nbsp;- Automatically routes traffic to the nearest Region based on latency. &nbsp;</p><p> &nbsp;- Supports NLB endpoints in each Region. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & C: CloudFront uses dynamic edge location IPs (not static), and origin groups don’t support NLB. &nbsp;</p><p>- D: Custom routing accelerator is for non-standard ports/protocols (overkill for this use case). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Global Accelerator is ideal for static IPs + low-latency multi-Region routing. &nbsp;</p><p>- NLB integration ensures high availability. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c725d7c5454642b68fbd0c1a18b2985c",
            "questionNumber": 385,
            "type": "single",
            "content": "<p>Question #385</p><p>A company is running multiple workloads in the AWS Cloud. The company has separate units for software development. The company uses AWS Organizations and federation with SAML to give permissions to developers to manage resources in their AWS accounts. The development units each deploy their production workloads into a common production account.</p><p><br></p><p>Recently, an incident occurred in the production account in which members of a development unit terminated an EC2 instance that belonged to a different development unit. A solutions architect must create a solution that prevents a similar incident from happening in the future. The solution also must allow developers the possibility to manage the instances used for their workloads.</p><p><br></p><p>Which strategy will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create separate OUs in AWS Organizations for each development unit. Assign the created OUs to the company AWS accounts. Create separate SCP with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag that matches the development unit name. Assign the SCP to the corresponding OU."
                },
                {
                    "label": "B",
                    "content": "Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Update the IAM policy for the developers&rsquo; assumed IAM role with a deny action and a StringNotEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit."
                },
                {
                    "label": "C",
                    "content": "Pass an attribute for DevelopmentUnit as an AWS Security Token Service (AWS STS) session tag during SAML federation. Create an SCP with an allow action and a StringEquals condition for the DevelopmentUnit resource tag and aws:PrincipalTag/DevelopmentUnit. Assign the SCP to the root OU."
                },
                {
                    "label": "D",
                    "content": "Create separate IAM policies for each development unit. For every IAM policy, add an allow action and a StringEquals condition for the DevelopmentUnit resource tag and the development unit name. During SAML federation, use AWS Security Token Service (AWS STS) to assign the IAM policy and match the development unit name to the assumed IAM role."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. Prevent developers from managing other teams’ EC2 instances (isolate workloads by development unit). &nbsp;</p><p>2. Allow developers to manage their own instances (least privilege). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using SAML + STS session tags: &nbsp;</p><p> &nbsp;- Pass a `DevelopmentUnit` tag during federation (e.g., `TeamA`, `TeamB`). &nbsp;</p><p>- IAM policy with `StringNotEquals` condition: &nbsp;</p><p> &nbsp;- Denies actions if the `aws:PrincipalTag/DevelopmentUnit` (user’s team) doesn’t match the `DevelopmentUnit` tag on the EC2 instance. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: SCPs with `StringNotEquals` are overly broad (SCPs apply to entire OUs, not fine-grained resources). &nbsp;</p><p>- C: SCPs with `allow` + `StringEquals` would block all untagged resources (risky for production). &nbsp;</p><p>- D: Static IAM policies per team are hard to scale (STS session tags dynamically enforce isolation). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- STS session tags enable dynamic, attribute-based access control (ABAC). &nbsp;</p><p>- IAM conditions (`aws:PrincipalTag`) enforce team-based isolation without manual policy updates. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "581bb0949fbb4cf297d1bbc0cf7a33f8",
            "questionNumber": 386,
            "type": "multiple",
            "content": "<p>Question #386</p><p>An enterprise company is building an infrastructure services platform for its users. The company has the following requirements:</p><p><br></p><p>- Provide least privilege access to users when launching AWS infrastructure so users cannot provision unapproved services.</p><p>- Use a central account to manage the creation of infrastructure services.</p><p>- Provide the ability to distribute infrastructure services to multiple accounts in AWS Organizations.</p><p>- Provide the ability to enforce tags on any infrastructure that is started by users.</p><p><br></p><p>Which combination of actions using AWS services will meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Develop infrastructure services using AWS CloudFormation templates. Add the templates to a central Amazon S3 bucket and add the IAM roles or users that require access to the S3 bucket policy."
                },
                {
                    "label": "B",
                    "content": "Develop infrastructure services using AWS CloudFormation templates. Upload each template as an AWS Service Catalog product to portfolios created in a central AWS account. Share these portfolios with the Organizations structure created for the company."
                },
                {
                    "label": "C",
                    "content": "Allow user IAM roles to have AWSCloudFormationFullAccess and AmazonS3ReadOnlyAccess permissions. Add an Organizations SCP at the AWS account root user level to deny all services except AWS CloudFormation and Amazon S3."
                },
                {
                    "label": "D",
                    "content": "Allow user IAM roles to have ServiceCatalogEndUserAccess permissions only. Use an automation script to import the central portfolios to local AWS accounts, copy the TagOption, assign users access, and apply launch constraints."
                },
                {
                    "label": "E",
                    "content": "Use the AWS Service Catalog TagOption Library to maintain a list of tags required by the company. Apply the TagOption to AWS Service Catalog products or portfolios."
                },
                {
                    "label": "F",
                    "content": "Use the AWS CloudFormation Resource Tags property to enforce the application of tags to any CloudFormation templates that will be created for users."
                }
            ],
            "correctAnswer": "BDE",
            "explanation": "<p>Option B is correct as it allows the creation of standardized infrastructure services using AWS CloudFormation templates within AWS Service Catalog, which can be shared across the organization. Option D is correct as it enables the enforcement of least privilege access by restricting user IAM roles to ServiceCatalogEndUserAccess permissions and using automation to manage access and apply launch constraints. Option E is correct as it allows for the enforcement of tagging by using the AWS Service Catalog TagOption Library, ensuring that all infrastructure provisioned through Service Catalog will have the required tags.</p><p>The requirements are: &nbsp;</p><p>1. Least privilege access (users can only launch approved services). &nbsp;</p><p>2. Central management (templates in a central account). &nbsp;</p><p>3. Multi-account distribution (via AWS Organizations). &nbsp;</p><p>4. Tag enforcement (mandatory tags on resources). &nbsp;</p><p>Options B, E, and D meet these requirements by: &nbsp;</p><p>- B: Service Catalog in a central account → CloudFormation templates are shared as products/portfolios across accounts. &nbsp;</p><p>- E: TagOption Library → Enforces consistent tags across all provisioned resources. &nbsp;</p><p>- D: Restrict users to `ServiceCatalogEndUserAccess` → Limits users to launching only approved Service Catalog products. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: S3 + CloudFormation lacks governance (no approval workflow or tag enforcement). &nbsp;</p><p>- C: SCP to deny all except CloudFormation/S3 is too permissive (users can still deploy unapproved templates). &nbsp;</p><p>- F: CloudFormation Resource Tags are manual (Service Catalog automates enforcement). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Service Catalog centralizes approved infrastructure as code (IaC). &nbsp;</p><p>- TagOptions ensure compliance with tagging policies. &nbsp;</p><p>- Least privilege is achieved via `ServiceCatalogEndUserAccess`. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f3afab55a7164556a6c5615c473f5563",
            "questionNumber": 387,
            "type": "single",
            "content": "<p>Question #387</p><p>A company deploys a new web application. As part of the setup, the company configures AWS WAF to log to Amazon S3 through Amazon Kinesis Data Firehose. The company develops an Amazon Athena query that runs once daily to return AWS WAF log data from the previous 24 hours. The volume of daily logs is constant. However, over time, the same query is taking more time to run.</p><p><br></p><p>A solutions architect needs to design a solution to prevent the query time from continuing to increase. The solution must minimize operational overhead.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Lambda function that consolidates each day&#39;s AWS WAF logs into one log file."
                },
                {
                    "label": "B",
                    "content": "Reduce the amount of data scanned by configuring AWS WAF to send logs to a different S3 bucket each day."
                },
                {
                    "label": "C",
                    "content": "Update the Kinesis Data Firehose configuration to partition the data in Amazon S3 by date and time. Create external tables for Amazon Redshift. Configure Amazon Redshift Spectrum to query the data source."
                },
                {
                    "label": "D",
                    "content": "Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the requirement to prevent increasing query times for AWS WAF logs in Amazon Athena while minimizing operational overhead, the best solution is:</p><p>D. Modify the Kinesis Data Firehose configuration and Athena table definition to partition the data by date and time. Change the Athena query to view the relevant partitions.</p><p>Here's why this is the optimal solution:</p><p>Root Cause of Increasing Query Times: The query time is increasing because Athena is scanning an ever-growing amount of data in S3. Without partitioning, Athena must scan all WAF logs, even though the query only needs the last 24 hours of data.</p><p>Benefits of Partitioning:</p><p>Query Performance: Partitioning by date and time allows Athena to scan only the relevant partitions (last 24 hours) instead of the entire dataset, dramatically improving query performance.</p><p>Consistent Performance Over Time: As the total data volume grows, query performance remains stable because Athena only scans the partitions specified in the query.</p><p>Cost Efficiency: Athena pricing is based on the amount of data scanned, so partitioning reduces costs by limiting the scan to relevant data.</p><p>Implementation with Minimal Operational Overhead:</p><p>Kinesis Data Firehose can be configured to automatically partition data by date and time using dynamic prefixes like year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/</p><p>Athena's partition projection feature automatically manages partitions without manual intervention</p><p>Once configured, this solution runs automatically without daily maintenance</p><p>Why Option A is Less Optimal: Creating a Lambda function to consolidate daily logs would:</p><p>Introduce additional complexity and a potential point of failure</p><p>Require ongoing maintenance of the Lambda function</p><p>Not address the fundamental issue of scanning unnecessary data</p><p>Increase operational overhead rather than minimize it</p><p>The partitioning solution is a one-time configuration change that provides an automated, scalable approach to maintaining query performance as data volume grows. By modifying the Kinesis Data Firehose configuration to partition data and updating the Athena table definition to recognize these partitions, the company can ensure that queries only scan relevant data, keeping query times consistent regardless of how much historical data accumulates in S3.</p><p>Sources</p><p>Example queries for AWS WAF logs - Amazon Athena （https://docs.aws.amazon.com/athena/latest/ug/query-examples-waf-logs.html）</p><p>How to use Amazon Athena queries to analyze AWS WAF logs and provide the visibility needed for threat detection | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/how-to-use-amazon-athena-queries-to-analyze-aws-waf-logs-and-provide-the-visibility-needed-for-threat-detection/）</p><p>Amazon Data Firehose example - Amazon Athena （https://docs.aws.amazon.com/athena/latest/ug/partition-projection-kinesis-firehose-example.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "0cf1327e286849a78d9b92c9fac9967e",
            "questionNumber": 388,
            "type": "single",
            "content": "<p>Question #388</p><p>A company is developing a web application that runs on Amazon EC2 instances in an Auto Scaling group behind a public-facing Application Load Balancer (ALB). Only users from a specific country are allowed to access the application. The company needs the ability to log the access requests that have been blocked. The solution should require the least possible maintenance.</p><p><br></p><p>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an IPSet containing a list of IP ranges that belong to the specified country. Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from an IP range in the IPSet. Associate the rule with the web ACL. Associate the web ACL with the ALB."
                },
                {
                    "label": "B",
                    "content": "Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from the specified country. Associate the rule with the web ACL. Associate the web ACL with the ALB."
                },
                {
                    "label": "C",
                    "content": "Configure AWS Shield to block any requests that do not originate from the specified country. Associate AWS Shield with the ALB."
                },
                {
                    "label": "D",
                    "content": "Create a security group rule that allows ports 80 and 443 from IP ranges that belong to the specified country. Associate the security group with the ALB."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the requirement to restrict access to a web application to users from a specific country while logging blocked requests with minimal maintenance, the best solution is:</p><p>B. Create an AWS WAF web ACL. Configure a rule to block any requests that do not originate from the specified country. Associate the rule with the web ACL. Associate the web ACL with the ALB.</p><p>Here's why this is the optimal solution:</p><p>Minimal Maintenance Requirement:</p><p>AWS WAF's built-in geo-matching feature automatically maintains up-to-date country-to-IP mappings</p><p>AWS handles all updates to the geolocation database, eliminating the need for manual updates</p><p>This approach requires significantly less maintenance compared to manually creating and maintaining an IPSet of country-specific IP ranges</p><p>Accuracy and Reliability:</p><p>The built-in geo-matching feature provides better accuracy as AWS continuously updates its geolocation database</p><p>This reduces false positives (blocking legitimate users) and false negatives (allowing unauthorized access)</p><p>IP ranges for countries change frequently, making manual maintenance prone to errors</p><p>Logging Capabilities:</p><p>AWS WAF can log all blocked requests to Amazon CloudWatch Logs, Amazon S3, or Amazon Kinesis Data Firehose</p><p>Logs include detailed information about the requests, including geographic origin</p><p>This satisfies the requirement to log access requests that have been blocked</p><p>Implementation Simplicity:</p><p>Creating a geo-match rule is straightforward in the AWS WAF console</p><p>You simply select the country you want to allow and set the rule to block requests not matching that country</p><p>Why Option A is Less Optimal:</p><p>Creating and maintaining an IPSet with country IP ranges requires significant ongoing maintenance</p><p>IP ranges change frequently, requiring regular updates to the IPSet</p><p>This approach is more prone to errors and gaps in coverage</p><p>It doesn't align with the requirement for \"least possible maintenance\"</p><p>The solution in Option B leverages AWS's built-in capabilities to handle the complexity of geo-matching, providing a more accurate, reliable, and low-maintenance approach to restricting access by country while ensuring all blocked requests are properly logged.</p><p>Sources</p><p>AWS WAF - AWS Best Practices for DDoS Resiliency （https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/aws-waf.html）</p><p>Restrict access based on IP address or geolocation by using AWS WAF - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/aws-waf-restrict-access-geolocation.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d692eeedb7b94534b1bf195bc20a6e88",
            "questionNumber": 389,
            "type": "single",
            "content": "<p>Question #389</p><p>A company is migrating an application from on-premises infrastructure to the AWS Cloud. During migration design meetings, the company expressed concerns about the availability and recovery options for its legacy Windows file server. The file server contains sensitive business critical data that cannot be recreated in the event of data corruption or data loss. According to compliance requirements, the data must not travel across the public internet. The company wants to move to AWS managed services where possible.</p><p><br></p><p>The company decides to store the data in an Amazon FSx for Windows File Server file system. A solutions architect must design a solution that copies the data to another AWS Region for disaster recovery (DR) purposes.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a destination Amazon S3 bucket in the DR Region. Establish connectivity between the FSx for Windows File Server file system in the primary Region and the S3 bucket in the DR Region by using Amazon FSx File Gateway. Configure the S3 bucket as a continuous backup source in FSx File Gateway."
                },
                {
                    "label": "B",
                    "content": "Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC the primary Region and the VPC in the DR Region by using AWS Site-to-Site VPN. Configure AWS DataSync to communicate by using VPN endpoints."
                },
                {
                    "label": "C",
                    "content": "Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using VPC peering. Configure AWS DataSync to communicate by using interface VPC endpoints with AWS PrivateLink."
                },
                {
                    "label": "D",
                    "content": "Create an FSx for Windows File Server file system in the DR Region. Establish connectivity between the VPC in the primary Region and the VPC in the DR Region by using AWS Transit Gateway in each Region. Use AWS Transfer Family to copy files between the FSx for Windows File Server file system in the primary Region and the FSx for Windows File Server file system in the DR Region over the private AWS backbone network."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct solution as it meets the requirement of not allowing data to travel across the public internet by using VPC peering and AWS PrivateLink for a secure and private connection between the primary and DR Regions. Additionally, by using AWS DataSync, the company can efficiently copy data to the FSx for Windows File Server file system in the DR Region, ensuring business continuity and disaster recovery preparedness. This solution leverages AWS managed services and maintains compliance with the company's data travel restrictions.</p><p>The requirements are: &nbsp;</p><p>1. Migrate a legacy Windows file server to FSx for Windows (AWS managed service). &nbsp;</p><p>2. Replicate data to another Region for DR (no public internet traffic). &nbsp;</p><p>3. Compliance: Data must not traverse the public internet. &nbsp;</p><p>Option C meets these requirements by: &nbsp;</p><p>- Using FSx for Windows in both Regions → Fully managed, compatible with Windows file servers. &nbsp;</p><p>- VPC peering + PrivateLink → Ensures private replication (no internet exposure). &nbsp;</p><p>- AWS DataSync → Efficient, secure cross-Region file synchronization (supports PrivateLink). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: FSx File Gateway + S3 is for hybrid storage (not cross-Region DR for FSx). S3 is not a direct replacement for FSx. &nbsp;</p><p>- B: Site-to-Site VPN works but is less scalable than VPC peering/PrivateLink. &nbsp;</p><p>- D: Transfer Family is for SFTP/FTP (not designed for FSx replication). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- FSx for Windows supports native Windows features (SMB, Active Directory integration). &nbsp;</p><p>- DataSync + PrivateLink ensures encrypted, low-latency replication over AWS backbone. &nbsp;</p><p>- VPC peering simplifies private connectivity between Regions. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8e0a4a5539524a4c89d8e5bcc7695d38",
            "questionNumber": 390,
            "type": "single",
            "content": "<p>Question #390</p><p>A company is currently in the design phase of an application that will need an RPO of less than 5 minutes and an RTO of less than 10 minutes. The solutions architecture team is forecasting that the database will store approximately 10 TB of data. As part of the design, they are looking for a database solution that will provide the company with the ability to fail over to a secondary Region.</p><p><br></p><p>Which solution will meet these business requirements at the LOWEST cost?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy an Amazon Aurora DB cluster and take snapshots of the cluster every 5 minutes. Once a snapshot is complete, copy the snapshot to a secondary Region to serve as a backup in the event of a failure."
                },
                {
                    "label": "B",
                    "content": "Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary."
                },
                {
                    "label": "C",
                    "content": "Deploy an Amazon Aurora DB cluster in the primary Region and another in a secondary Region. Use AWS DMS to keep the secondary Region in sync."
                },
                {
                    "label": "D",
                    "content": "Deploy an Amazon RDS instance with a read replica in the same Region. In the event of a failure, promote the read replica to become the primary."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the requirement for a database solution with an RPO of less than 5 minutes, an RTO of less than 10 minutes, and the ability to fail over to a secondary Region for a 10TB database, the solution that will meet these business requirements at the LOWEST cost is:</p><p>B. Deploy an Amazon RDS instance with a cross-Region read replica in a secondary Region. In the event of a failure, promote the read replica to become the primary.</p><p>Here's why this is the most cost-effective solution that meets all requirements:</p><p>RPO (Recovery Point Objective) &lt; 5 minutes:</p><p>Cross-Region read replicas in Amazon RDS typically have replication lag measured in seconds, not minutes</p><p>This provides an RPO of seconds rather than minutes, well within the requirement of less than 5 minutes</p><p>The snapshot approach (Option A) would meet the 5-minute RPO requirement but with less margin for error</p><p>RTO (Recovery Time Objective) &lt; 10 minutes:</p><p>Promoting a read replica to primary is a relatively quick operation that can be completed within minutes</p><p>This allows the solution to meet the RTO requirement of less than 10 minutes</p><p>In contrast, restoring a 10TB database from a snapshot would take several hours, far exceeding the 10-minute RTO requirement</p><p>Failover to Secondary Region:</p><p>The cross-Region read replica is already running and contains a near real-time copy of the data</p><p>Failover simply requires promoting the read replica to primary status</p><p>This provides a much faster and more reliable failover mechanism than restoring from snapshots</p><p>Cost Considerations:</p><p>While maintaining a cross-Region read replica does incur ongoing costs for the additional instance and storage, it's the only option that can meet the strict RTO requirement of less than 10 minutes</p><p>The snapshot approach would be less expensive for ongoing operations but would fail to meet the RTO requirement by a significant margin</p><p>For a 10TB database, restoring from a snapshot would take hours, not minutes</p><p>Additional Benefits:</p><p>The read replica can serve read traffic in the secondary region, potentially improving application performance</p><p>The replica provides a warm standby that's ready to take over immediately</p><p>The solution offers better testing capabilities as you can verify the replica's data integrity without disrupting production</p><p>Option A (Aurora DB cluster with snapshots every 5 minutes) would be less expensive from an ongoing operational standpoint, but it cannot meet the 10-minute RTO requirement for a 10TB database. The time required to restore a 10TB snapshot in a secondary region would be several hours, making this approach unsuitable despite its lower cost.</p><p>Therefore, Option B is the LOWEST cost solution that meets ALL the business requirements, including the critical RTO of less than 10 minutes.</p><p>Sources</p><p>Disaster recovery options in the cloud - Disaster Recovery of Workloads on AWS: Recovery in the Cloud （https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html）</p><p>Modernize SQL Server databases - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/modernize-sql-server.html）</p><p>DR solution decision matrix - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/dr-standard-edition-amazon-rds/dr-matrix.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "56b98cb7ec274e9b84fcb7e2ce4357c4",
            "questionNumber": 391,
            "type": "single",
            "content": "<p>Question #391</p><p>A financial company needs to create a separate AWS account for a new digital wallet application. The company uses AWS Organizations to manage its accounts. A solutions architect uses the IAM user Support1 from the management account to create a new member account with finance1@example.com as the email address.</p><p><br></p><p>What should the solutions architect do to create IAM users in the new member account?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Sign in to the AWS Management Console with AWS account root user credentials by using the 64-character password from the initial AWS Organizations email sent to finance1@example.com. Set up the IAM users as required."
                },
                {
                    "label": "B",
                    "content": "From the management account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required."
                },
                {
                    "label": "C",
                    "content": "Go to the AWS Management Console sign-in page. Choose &ldquo;Sign in using root account credentials.&rdquo; Sign in by using the email address finance1@example.com and the management account&#39;s root password. Set up the IAM users as required."
                },
                {
                    "label": "D",
                    "content": "Go to the AWS Management Console sign-in page. Sign in by using the account ID of the new member account and the Support1 IAM credentials. Set up the IAM users as required."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided about creating IAM users in a new member account within AWS Organizations, the correct approach is:</p><p>B. From the management account, switch roles to assume the OrganizationAccountAccessRole role with the account ID of the new member account. Set up the IAM users as required.</p><p>Here's why this is the correct approach:</p><p>Automatic Role Creation: When a new member account is created in AWS Organizations, an IAM role called OrganizationAccountAccessRole is automatically created in that account. This role is specifically designed to provide administrative access from the management account.</p><p>Administrative Permissions: The OrganizationAccountAccessRole has full administrative permissions (via the AdministratorAccess policy) in the member account, which allows for creating IAM users and performing any other necessary account setup tasks.</p><p>Security Best Practice: Using the OrganizationAccountAccessRole follows AWS security best practices by avoiding the use of root user credentials. AWS recommends minimizing the use of root credentials and instead using IAM roles for administrative tasks.</p><p>Centralized Management: This approach leverages the centralized management capabilities of AWS Organizations, allowing administrators to manage multiple accounts from the management account without needing separate credentials for each account.</p><p>Process: The solutions architect would:</p><p>Sign in to the AWS Management Console using their IAM user (Support1) credentials in the management account</p><p>Click on their account name in the top right corner and select \"Switch Role\"</p><p>Enter the new member account ID and the role name \"OrganizationAccountAccessRole\"</p><p>After switching roles, they would have administrative access to create IAM users in the new member account</p><p>Option A (using root user credentials) is not recommended because:</p><p>It requires accessing and using the root user of the member account, which is against AWS security best practices</p><p>Root user access should be reserved for only a few specific tasks that cannot be performed by IAM users or roles</p><p>Using root credentials increases security risks and makes it harder to track and audit administrative actions</p><p>The OrganizationAccountAccessRole provides a secure, auditable, and centralized way to manage member accounts within AWS Organizations, making Option B the correct approach for creating IAM users in the new member account.</p><p>Sources</p><p>Add initial users - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/transitioning-to-multiple-aws-accounts/add-initial-users.html）</p><p>Accessing a member account that has OrganizationAccountAccessRole with AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_accounts_access-cross-account-role.html）</p><p>Secure root user access for member accounts in AWS Organizations | AWS Security Blog （https://aws.amazon.com/cn/blogs/security/secure-root-user-access-for-member-accounts-in-aws-organizations/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8035705bdef240a78122f36d7f95d843",
            "questionNumber": 392,
            "type": "single",
            "content": "<p>Question #392</p><p>A car rental company has built a serverless REST API to provide data to its mobile app. The app consists of an Amazon API Gateway API with a Regional endpoint, AWS Lambda functions, and an Amazon Aurora MySQL Serverless DB cluster. The company recently opened the API to mobile apps of partners. A significant increase in the number of requests resulted, causing sporadic database memory errors.</p><p><br></p><p>Analysis of the API traffic indicates that clients are making multiple HTTP GET requests for the same queries in a short period of time. Traffic is concentrated during business hours, with spikes around holidays and other events.</p><p><br></p><p>The company needs to improve its ability to support the additional usage while minimizing the increase in costs associated with the solution.</p><p><br></p><p>Which strategy meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Convert the API Gateway Regional endpoint to an edge-optimized endpoint. Enable caching in the production stage."
                },
                {
                    "label": "B",
                    "content": "Implement an Amazon ElastiCache for Redis cache to store the results of the database calls. Modify the Lambda functions to use the cache."
                },
                {
                    "label": "C",
                    "content": "Modify the Aurora Serverless DB cluster configuration to increase the maximum amount of available memory."
                },
                {
                    "label": "D",
                    "content": "Enable throttling in the API Gateway production stage. Set the rate and burst values to limit the incoming calls."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Correct Answer: B &nbsp;</p><p>Option B: Implement an Amazon ElastiCache for Redis cache to store database query results and modify Lambda functions to use the cache. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements Analysis &nbsp;</p><p>- Problem: Repeated database queries lead to memory errors due to traffic spikes. &nbsp;</p><p>- Goal: Reduce database load without restricting functionality or incurring high costs. &nbsp;</p><p><br></p><p> Option Analysis &nbsp;</p><p>1. Option B (Correct) &nbsp;</p><p> &nbsp; - Strategy: Use ElastiCache for Redis as an in-memory cache to store query results. Lambda functions first check the cache before accessing the database. &nbsp;</p><p> &nbsp; - Advantages: &nbsp;</p><p> &nbsp; &nbsp; - Addresses root cause: Caches frequent queries to minimize redundant database calls, reducing memory pressure. &nbsp;</p><p> &nbsp; &nbsp; - Cost-efficient: ElastiCache scales dynamically and reduces database compute usage during spikes. &nbsp;</p><p> &nbsp; &nbsp; - Performance improvement: Cache responses are faster than database queries, enhancing user experience. &nbsp;</p><p>2. Option A (Incorrect) &nbsp;</p><p> &nbsp; - Issue: Converting to an edge-optimized API Gateway endpoint reduces latency for global users but does not address repeated queries. &nbsp;</p><p> &nbsp; - Limitation: API Gateway caching (up to 1 MB) is less effective for complex database results and requires per-route configuration, making it less suitable for deep query caching. &nbsp;</p><p>3. Option C (Incorrect) &nbsp;</p><p> &nbsp; - Flaw: Increasing Aurora Serverless memory directly raises costs without resolving redundant queries. &nbsp;</p><p> &nbsp; - Trade-off: This is a reactive solution that does not optimize resource utilization for repetitive requests. &nbsp;</p><p>4. Option D (Incorrect) &nbsp;</p><p> &nbsp; - Problem: Enabling API Gateway throttling limits incoming requests, which may disrupt partner apps and restrict legitimate traffic growth. &nbsp;</p><p> &nbsp; - Mismatch: Throttling addresses volume but not the underlying issue of redundant queries. &nbsp;</p><p> Conclusion &nbsp;</p><p>Option B optimizes performance by caching query results, reducing database load and memory pressure while maintaining cost efficiency. This approach directly targets the root cause of repeated queries without compromising functionality or incurring unnecessary expenses.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7bcce5fffafd4b839db11369e4dfe3f1",
            "questionNumber": 393,
            "type": "single",
            "content": "<p>Question #393</p><p>A company is migrating an on-premises application and a MySQL database to AWS. The application processes highly sensitive data, and new data is constantly updated in the database. The data must not be transferred over the internet. The company also must encrypt the data in transit and at rest.</p><p><br></p><p>The database is 5 TB in size. The company already has created the database schema in an Amazon RDS for MySQL DB instance. The company has set up a 1 Gbps AWS Direct Connect connection to AWS. The company also has set up a public VIF and a private VIF. A solutions architect needs to design a solution that will migrate the data to AWS with the least possible downtime.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Perform a database backup. Copy the backup files to an AWS Snowball Edge Storage Optimized device. Import the backup to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."
                },
                {
                    "label": "B",
                    "content": "Use AWS Database Migration Service (AWS DMS) to migrate the data to AWS. Create a DMS replication instance in a private subnet. Create VPC endpoints for AWS DMS. Configure a DMS task to copy data from the on-premises database to the DB instance by using full load plus change data capture (CDC). Use the AWS Key Management Service (AWS KMS) default key for encryption at rest. Use TLS for encryption in transit."
                },
                {
                    "label": "C",
                    "content": "Perform a database backup. Use AWS DataSync to transfer the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."
                },
                {
                    "label": "D",
                    "content": "Use Amazon S3 File Gateway. Set up a private connection to Amazon S3 by using AWS PrivateLink. Perform a database backup. Copy the backup files to Amazon S3. Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3) for encryption at rest. Use TLS for encryption in transit. Import the data from Amazon S3 to the DB instance."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct solution for migrating the data with the least possible downtime while meeting the encryption requirements. AWS DMS supports the migration of large databases like the one described (5 TB) and can perform the migration over the Direct Connect connection, which ensures that the data does not travel over the public internet. By using CDC, the migration can be done with minimal downtime as only the initial full load is required to be complete, followed by the ongoing replication of changes. Additionally, AWS DMS can integrate with VPC endpoints for a secure and private connection, and the use of AWS KMS for encryption at rest aligns with the requirement for data encryption.</p><p>The requirements are: &nbsp;</p><p>1. Migrate a 5 TB MySQL database to AWS without internet exposure (use Direct Connect). &nbsp;</p><p>2. Encrypt data in transit and at rest (TLS + KMS). &nbsp;</p><p>3. Minimize downtime (near-zero RPO/RTO). &nbsp;</p><p>Option B meets these requirements by: &nbsp;</p><p>- Using AWS DMS with Direct Connect private VIF: &nbsp;</p><p> &nbsp;- Full load + CDC replicates existing data and ongoing changes with minimal downtime. &nbsp;</p><p> &nbsp;- Private subnet + VPC endpoints ensure no internet exposure. &nbsp;</p><p> &nbsp;- TLS encryption for in-transit data + AWS KMS for at-rest encryption. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A & C: Snowball/DataSync + S3 requires manual import steps (downtime during restore). &nbsp;</p><p>- D: S3 File Gateway is for file storage (not optimal for database migration). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- DMS with CDC is the gold standard for minimal-downtime database migrations. &nbsp;</p><p>- Direct Connect private VIF provides secure, high-throughput connectivity. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2e0211249b4f48c9ab84d45f44bc9031",
            "questionNumber": 394,
            "type": "single",
            "content": "<p>Question #394</p><p>A company is deploying a new cluster for big data analytics on AWS. The cluster will run across many Linux Amazon EC2 instances that are spread across multiple Availability Zones.</p><p><br></p><p>All of the nodes in the cluster must have read and write access to common underlying file storage. The file storage must be highly available, must be resilient, must be compatible with the Portable Operating System Interface (POSIX), and must accommodate high levels of throughput.</p><p><br></p><p>Which storage solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Provision an AWS Storage Gateway file gateway NFS file share that is attached to an Amazon S3 bucket. Mount the NFS file share on each EC2 instance in the cluster."
                },
                {
                    "label": "B",
                    "content": "Provision a new Amazon Elastic File System (Amazon EFS) file system that uses General Purpose performance mode. Mount the EFS file system on each EC2 instance in the cluster."
                },
                {
                    "label": "C",
                    "content": "Provision a new Amazon Elastic Block Store (Amazon EBS) volume that uses the io2 volume type. Attach the EBS volume to all of the EC2 instances in the cluster."
                },
                {
                    "label": "D",
                    "content": "Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the requirements for a big data analytics cluster that needs shared file storage across multiple EC2 instances in different Availability Zones, the correct solution is:</p><p>D. Provision a new Amazon Elastic File System (Amazon EFS) file system that uses Max I/O performance mode. Mount the EFS file system on each EC2 instance in the cluster.</p><p>Here's why Amazon EFS with Max I/O performance mode is the appropriate choice for this scenario:</p><p>Shared Access Requirement:</p><p>The requirement states that \"All of the nodes in the cluster must have read and write access to common underlying file storage\"</p><p>Amazon EFS is designed to be mounted concurrently by thousands of EC2 instances, allowing all cluster nodes to have simultaneous read and write access</p><p>EBS volumes (even io2) can only be attached to a single EC2 instance at a time in standard mode, or up to 16 instances with Multi-Attach (which still wouldn't support a large cluster)</p><p>Multi-Availability Zone Support:</p><p>The cluster will run across EC2 instances \"spread across multiple Availability Zones\"</p><p>Amazon EFS is natively designed to work across multiple Availability Zones within a region</p><p>EBS volumes are restricted to a single Availability Zone, making them unsuitable for a multi-AZ deployment</p><p>POSIX Compatibility:</p><p>The requirement explicitly states that the storage \"must be compatible with the Portable Operating System Interface (POSIX)\"</p><p>Amazon EFS is fully POSIX-compliant, supporting standard file operations, permissions, and locking</p><p>EBS provides block-level storage that requires additional configuration to achieve POSIX compliance</p><p>High Availability and Resilience:</p><p>The storage must be \"highly available\" and \"resilient\"</p><p>Amazon EFS is designed for 99.999999999% (11 nines) durability and high availability</p><p>EFS automatically stores data redundantly across multiple AZs, providing built-in resilience</p><p>High Throughput:</p><p>The requirement states the storage must \"accommodate high levels of throughput\"</p><p>Amazon EFS with Max I/O performance mode is specifically designed for high-throughput workloads</p><p>Max I/O mode is optimized for highly parallelized applications like big data analytics</p><p>Option C (using an EBS io2 volume) would not meet the requirements because:</p><p>EBS volumes cannot be simultaneously attached to multiple EC2 instances across different Availability Zones</p><p>EBS volumes are limited to a single Availability Zone, lacking the multi-AZ resilience required</p><p>Even with Multi-Attach, an io2 volume can only be attached to up to 16 instances, which may not be sufficient for a big data cluster</p><p>Amazon EFS with Max I/O performance mode provides the shared access, high availability across multiple AZs, POSIX compatibility, and high throughput capabilities required for this big data analytics cluster.</p><p>Sources</p><p>Amazon EFS performance specifications - Amazon Elastic File System （https://docs.aws.amazon.com/efs/latest/ug/performance.html）</p><p>Features and benefits of Amazon EBS volumes - Amazon EBS （https://docs.aws.amazon.com/ebs/latest/userguide/EBSFeatures.html）</p><p>Amazon EFS volumes - Amazon Elastic Container Service （https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/storage-efs.html）</p><p>Amazon EBS Provisioned IOPS Volume - Amazon Web Services （https://aws.amazon.com/cn/ebs/provisioned-iops/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "dea222c43f4b4c0ba29160e7436df769",
            "questionNumber": 395,
            "type": "single",
            "content": "<p>Question #395</p><p>A company hosts a software as a service (SaaS) solution on AWS. The solution has an Amazon API Gateway API that serves an HTTPS endpoint. The API uses AWS Lambda functions for compute. The Lambda functions store data in an Amazon Aurora Serverless v1 database.</p><p><br></p><p>The company used the AWS Serverless Application Model (AWS SAM) to deploy the solution. The solution extends across multiple Availability Zones and has no disaster recovery (DR) plan.</p><p><br></p><p>A solutions architect must design a DR strategy that can recover the solution in another AWS Region. The solution has an RTO of 5 minutes and an RPO of 1 minute.</p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a read replica of the Aurora Serverless v1 database in the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region. Promote the read replica to primary in case of disaster."
                },
                {
                    "label": "B",
                    "content": "Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Use AWS SAM to create a runbook to deploy the solution to the target Region."
                },
                {
                    "label": "C",
                    "content": "Create an Aurora Serverless v1 DB cluster that has multiple writer instances in the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration."
                },
                {
                    "label": "D",
                    "content": "Change the Aurora Serverless v1 database to a standard Aurora MySQL global database that extends across the source Region and the target Region. Launch the solution in the target Region. Configure the two Regional solutions to work in an active-passive configuration."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct approach to meet the specified RTO and RPO requirements for disaster recovery. By changing the Aurora Serverless v1 database to a standard Aurora MySQL global database, the company can leverage Aurora's global database feature to replicate data across Regions. This setup allows for a fast failover to the target Region while ensuring that the data is replicated in near real-time, which aligns with the 1-minute RPO. Additionally, launching the solution in the target Region and configuring it to work in an active-passive configuration helps meet the 5-minute RTO. This configuration ensures that in the event of a disaster, the application can quickly switch over to the secondary Region with minimal downtime.</p><p>The requirements are: &nbsp;</p><p>1. Disaster Recovery (DR) with RTO ≤5 min and RPO ≤1 min (near-zero data loss). &nbsp;</p><p>2. Multi-Region resilience for a serverless SaaS application. &nbsp;</p><p>Option D meets these requirements by: &nbsp;</p><p>- Using Aurora Global Database: &nbsp;</p><p> &nbsp;- Cross-Region replication with &lt;1s latency (meets RPO). &nbsp;</p><p> &nbsp;- 1-click promotion of secondary Region (meets RTO). &nbsp;</p><p>- Active-Passive deployment: &nbsp;</p><p> &nbsp;- Pre-deployed infrastructure in DR Region (API Gateway + Lambda). &nbsp;</p><p> &nbsp;- Fast failover by promoting the secondary Aurora cluster. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A: Aurora Serverless v1 read replicas cannot be promoted (no DR capability). &nbsp;</p><p>- B: Global Database + runbook lacks pre-deployed compute (violates RTO). &nbsp;</p><p>- C: Aurora Serverless v1 doesn’t support multi-writer (invalid configuration). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- Aurora Global Database is the only serverless-compatible solution for sub-minute RPO. &nbsp;</p><p>- Pre-deployed serverless components (Lambda/API Gateway) ensure 5-minute RTO. </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ace88f4702c74f85a53daf0bcfcfda93",
            "questionNumber": 396,
            "type": "single",
            "content": "<p>Question #396</p><p>A company owns a chain of travel agencies and is running an application in the AWS Cloud. Company employees use the application to search for information about travel destinations. Destination content is updated four times each year.</p><p><br></p><p>Two fixed Amazon EC2 instances serve the application. The company uses an Amazon Route 53 public hosted zone with a multivalue record of travel.example.com that returns the Elastic IP addresses for the EC2 instances. The application uses Amazon DynamoDB as its primary data store. The company uses a self-hosted Redis instance as a caching solution.</p><p><br></p><p>During content updates, the load on the EC2 instances and the caching solution increases drastically. This increased load has led to downtime on several occasions. A solutions architect must update the application so that the application is highly available and can handle the load that is generated by the content updates.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up DynamoDB Accelerator (DAX) as an in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB&#39;s DNS alias. Configure scheduled scaling for the EC2 instances before the content updates."
                },
                {
                    "label": "B",
                    "content": "Set up Amazon ElastiCache for Redis. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution&rsquo;s DNS alias. Manually scale up EC2 instances before the content updates."
                },
                {
                    "label": "C",
                    "content": "Set up Amazon ElastiCache for Memcached. Update the application to use ElastiCache. Create an Auto Scaling group for the EC2 instances. Create an Application Load Balancer (ALB). Set the Auto Scaling group as a target for the ALB. Update the Route 53 record to use a simple routing policy that targets the ALB&#39;s DNS alias. Configure scheduled scaling for the application before the content updates."
                },
                {
                    "label": "D",
                    "content": "Set up DynamoDB Accelerator (DAX) as an in-memory cache. Update the application to use DAX. Create an Auto Scaling group for the EC2 instances. Create an Amazon CloudFront distribution, and set the Auto Scaling group as an origin for the distribution. Update the Route 53 record to use a simple routing policy that targets the CloudFront distribution&#39;s DNS alias. Manually scale up EC2 instances before the content updates."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct solution to ensure high availability and the ability to handle increased load during content updates. By setting up DAX as an in-memory cache, the application can significantly reduce the latency of read operations on DynamoDB, which is crucial during peak load times. The use of an Auto Scaling group for EC2 instances ensures that the compute resources can scale out to meet demand, and an ALB can distribute the incoming traffic across these instances. Additionally, updating the Route 53 record to target the ALB's DNS alias allows for smooth traffic routing, and configuring scheduled scaling can proactively increase the EC2 capacity before anticipated content updates, thus preventing downtime.</p><p>The requirements are: &nbsp;</p><p>1. Handle high load during quarterly content updates (predictable spikes). &nbsp;</p><p>2. Maintain high availability (eliminate downtime). &nbsp;</p><p>3. Replace self-hosted Redis with a managed solution. &nbsp;</p><p>Option A meets these requirements by: &nbsp;</p><p>- Replacing Redis with DynamoDB Accelerator (DAX): &nbsp;</p><p> &nbsp;- Fully managed, in-memory cache for DynamoDB (reduces database load). &nbsp;</p><p>- Auto Scaling + ALB: &nbsp;</p><p> &nbsp;- Scales EC2 instances horizontally during traffic spikes. &nbsp;</p><p> &nbsp;- Distributes traffic evenly across instances. &nbsp;</p><p>- Scheduled scaling: &nbsp;</p><p> &nbsp;- Proactively scales before content updates (avoids reactive scaling delays). &nbsp;</p><p>- Route 53 simple routing: &nbsp;</p><p> &nbsp;- Directs traffic to the ALB (simpler than multivalue routing). &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: CloudFront is unnecessary (content updates are backend-driven, not static). Manual scaling is less reliable than scheduled scaling. &nbsp;</p><p>- C: Memcached lacks persistence (not ideal for travel data caching). &nbsp;</p><p>- D: CloudFront + DAX is mismatched (CloudFront caches static content, not DynamoDB queries). &nbsp;</p><p> Best Practices: &nbsp;</p><p>- DAX reduces DynamoDB read latency by 10x (ideal for high-read workloads). &nbsp;</p><p>- Scheduled scaling ensures capacity is ready before traffic spikes. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d8c885d561c0492eaeac1a2336ddccd8",
            "questionNumber": 397,
            "type": "multiple",
            "content": "<p>Question #397</p><p>A company needs to store and process image data that will be uploaded from mobile devices using a custom mobile app. Usage peaks between 8 AM and 5 PM on weekdays, with thousands of uploads per minute. The app is rarely used at any other time. A user is notified when image processing is complete.</p><p><br></p><p>Which combination of actions should a solutions architect take to ensure image processing can scale to handle the load? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon MQ queue."
                },
                {
                    "label": "B",
                    "content": "Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue."
                },
                {
                    "label": "C",
                    "content": "Invoke an AWS Lambda function to perform image processing when a message is available in the queue."
                },
                {
                    "label": "D",
                    "content": "Invoke an S3 Batch Operations job to perform image processing when a message is available in the queue."
                },
                {
                    "label": "E",
                    "content": "Send a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS) when processing is complete."
                },
                {
                    "label": "F",
                    "content": "Send a push notification to the mobile app by using Amazon Simple Email Service (Amazon SES) when processing is complete."
                }
            ],
            "correctAnswer": "BCE",
            "explanation": "<p>Based on the requirement to store and process image data uploaded from mobile devices with high peak usage during business hours, the best combination of actions to ensure scalable image processing is:</p><p>B. Upload files from the mobile software directly to Amazon S3. Use S3 event notifications to create a message in an Amazon Simple Queue Service (Amazon SQS) standard queue.</p><p>C. Invoke an AWS Lambda function to perform image processing when a message is available in the queue.</p><p>E. Send a push notification to the mobile app by using Amazon Simple Notification Service (Amazon SNS) when processing is complete.</p><p>Here's why this combination provides the optimal solution:</p><p>Direct S3 Uploads (Option B):</p><p>Allows mobile devices to upload images directly to S3 without requiring an intermediary server</p><p>S3 can handle thousands of concurrent uploads per minute, easily meeting the peak demand</p><p>S3 event notifications automatically trigger the next step in the workflow when new files are uploaded</p><p>SQS standard queue provides a buffer between uploads and processing, absorbing traffic spikes</p><p>This decoupling ensures the system remains responsive even during peak upload times</p><p>Lambda for Processing (Option C):</p><p>Lambda functions automatically scale to process messages from the SQS queue</p><p>Lambda's serverless nature means you only pay for actual processing time, ideal for the variable workload described (busy during business hours, minimal usage otherwise)</p><p>Lambda can be configured to process multiple images concurrently, handling the high volume during peak times</p><p>The queue-based approach ensures no uploads are lost even if processing temporarily falls behind</p><p>SNS Push Notifications (Option E):</p><p>After processing is complete, SNS can send push notifications directly to the mobile app</p><p>This satisfies the requirement that \"A user is notified when image processing is complete\"</p><p>SNS is highly scalable and can handle thousands of notifications per minute</p><p>This architecture creates a fully serverless, event-driven pipeline that:</p><p>Scales automatically to handle the variable load (thousands of uploads during business hours, minimal usage otherwise)</p><p>Decouples each step of the process for better resilience</p><p>Provides end-to-end notification when processing is complete</p><p>Minimizes costs by only using resources when needed (pay-per-use model)</p><p>Requires minimal operational overhead</p><p>The combination of S3 for storage, SQS for buffering, Lambda for processing, and SNS for notifications creates a robust, scalable solution that perfectly addresses the requirements for this image processing workflow.</p><p>Sources</p><p>What is the recommended convention for storing images in a serverless event-driven architecture? Base64 image in SNS event exceeds message size limit. | AWS re:Post （https://repost.aws/questions/QUde2ybRheSWiw6Mr0uA0_2A/what-is-the-recommended-convention-for-storing-images-in-a-serverless-event-driven-architecture-base64-image-in-sns-event-exceeds-message-size-limit）</p><p>Performance design patterns for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-design-patterns.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "0c3790537cc14abd8e0449aa1e5dac8d",
            "questionNumber": 398,
            "type": "single",
            "content": "<p>Question #398</p><p>A company is building an application on AWS. The application sends logs to an Amazon OpenSearch Service cluster for analysis. All data must be stored within a VPC.</p><p><br></p><p>Some of the company’s developers work from home. Other developers work from three different company office locations. The developers need to access OpenSearch Service to analyze and visualize logs directly from their local development machines.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure and set up an AWS Client VPN endpoint. Associate the Client VPN endpoint with a subnet in the VPC. Configure a Client VPN self-service portal. Instruct the developers to connect by using the client for Client VPN."
                },
                {
                    "label": "B",
                    "content": "Create a transit gateway, and connect it to the VPC. Create an AWS Site-to-Site VPN. Create an attachment to the transit gateway. Instruct the developers to connect by using an OpenVPN client."
                },
                {
                    "label": "C",
                    "content": "Create a transit gateway, and connect it to the VPC. Order an AWS Direct Connect connection. Set up a public VIF on the Direct Connect connection. Associate the public VIF with the transit gateway. Instruct the developers to connect to the Direct Connect connection."
                },
                {
                    "label": "D",
                    "content": "Create and configure a bastion host in a public subnet of the VPC. Configure the bastion host security group to allow SSH access from the company CIDR ranges. Instruct the developers to connect by using SSH."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the requirement to provide developers with access to an Amazon OpenSearch Service cluster within a VPC from both home and office locations, the best solution is:</p><p>A. Configure and set up an AWS Client VPN endpoint. Associate the Client VPN endpoint with a subnet in the VPC. Configure a Client VPN self-service portal. Instruct the developers to connect by using the client for Client VPN.</p><p>Here's why this is the correct solution:</p><p>Access Requirements:</p><p>The scenario involves developers working from multiple locations (home and three different office locations)</p><p>Developers need direct access from their local development machines to the OpenSearch Service cluster</p><p>AWS Client VPN is specifically designed for individual user remote access to AWS resources in a VPC</p><p>AWS Client VPN Benefits for This Scenario:</p><p>Provides secure, encrypted connections for individual developers regardless of their location</p><p>Works consistently whether developers are at home or in any of the three office locations</p><p>Allows direct access from developers' local machines to the OpenSearch Service cluster</p><p>Supports user-based authentication and authorization for granular access control</p><p>Self-Service Portal Advantage:</p><p>Makes it easy for developers to download the necessary client software and configuration files</p><p>Simplifies the onboarding process for new developers</p><p>Provides a consistent connection method across all developer locations</p><p>Implementation Process:</p><p>The Client VPN endpoint is associated with a subnet in the VPC where the OpenSearch Service cluster resides</p><p>Developers connect using an OpenVPN-compatible client (such as the AWS provided client)</p><p>Once connected, developers can directly access the OpenSearch Service cluster to analyze and visualize logs</p><p>Why Option B is Not Optimal:</p><p>AWS Site-to-Site VPN with Transit Gateway is designed for connecting entire networks, not individual users</p><p>It would be overly complex for individual developer access from home locations</p><p>It would require additional VPN infrastructure at each developer's home</p><p>It's not designed for the individual access pattern described in the scenario</p><p>AWS Client VPN provides the most straightforward and secure solution for this specific use case, allowing developers to access the OpenSearch Service cluster from any location while ensuring all data remains within the VPC as required. The self-service portal further simplifies the process by providing an easy way for developers to get the necessary connection tools and configurations.</p><p>Sources</p><p>Client VPN with VPC peering vs Client VPN withTransit gateway | AWS re:Post （https://repost.aws/questions/QUUotXjy2mTV6noMA-J86AXQ/client-vpn-with-vpc-peering-vs-client-vpn-withtransit-gateway）</p><p>Building secure multicloud access with AWS Client VPN and AWS Site-to-Site VPN | Networking & Content Delivery （https://aws.amazon.com/blogs/networking-and-content-delivery/building-secure-multicloud-access-with-aws-client-vpn-and-aws-site-to-site-vpn/）</p><p>AWS network connectivity management tools - Management and Governance Cloud Environment Guide （https://docs.aws.amazon.com/wellarchitected/latest/management-and-governance-guide/aws-network-connectivity-management-tools.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "250bf13457d14ebbb6cac833f7d174b6",
            "questionNumber": 399,
            "type": "multiple",
            "content": "<p>Question #399</p><p>A company wants to migrate its website from an on-premises data center onto AWS. At the same time, it wants to migrate the website to a containerized microservice-based architecture to improve the availability and cost efficiency. The company’s security policy states that privileges and network permissions must be configured according to best practice, using least privilege.</p><p><br></p><p>A solutions architect must create a containerized architecture that meets the security requirements and has deployed the application to an Amazon ECS cluster.</p><p><br></p><p>What steps are required after the deployment to meet the requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create tasks using the bridge network mode."
                },
                {
                    "label": "B",
                    "content": "Create tasks using the awsvpc network mode."
                },
                {
                    "label": "C",
                    "content": "Apply security groups to Amazon EC2 instances, and use IAM roles for EC2 instances to access other resources."
                },
                {
                    "label": "D",
                    "content": "Apply security groups to the tasks, and pass IAM credentials into the container at launch time to access other resources."
                },
                {
                    "label": "E",
                    "content": "Apply security groups to the tasks, and use IAM roles for tasks to access other resources."
                }
            ],
            "correctAnswer": "BE",
            "explanation": "<p>Based on the requirement to migrate a website to a containerized microservice-based architecture on AWS while ensuring privileges and network permissions follow least privilege best practices, the two steps required after deploying the application to an Amazon ECS cluster are:</p><p>B. Create tasks using the awsvpc network mode.</p><p>E. Apply security groups to the tasks, and use IAM roles for tasks to access other resources.</p><p>Here's why these are the correct choices:</p><p>Create tasks using the awsvpc network mode (Option B):</p><p>The awsvpc network mode provides each task with its own elastic network interface (ENI) and a private IP address</p><p>This creates network isolation at the task level, improving security by separating network traffic between different microservices</p><p>It enables you to apply security groups directly to individual tasks/microservices, allowing for fine-grained network access control</p><p>This network mode is essential for implementing least privilege at the network level, as it allows you to restrict network traffic precisely to what each microservice needs</p><p>It's considered a best practice for containerized microservice architectures on ECS</p><p>Apply security groups to the tasks, and use IAM roles for tasks to access other resources (Option E):</p><p>Security groups act as virtual firewalls that control inbound and outbound traffic at the task level when using awsvpc mode</p><p>This allows you to implement least privilege by restricting network access to only what each microservice requires</p><p>IAM roles for tasks enable you to grant specific AWS service permissions to individual containers/tasks</p><p>This implements least privilege for AWS API access by ensuring each microservice only has the permissions it needs</p><p>Using task-specific IAM roles instead of broader instance-level permissions is a security best practice</p><p>Why the other options are not correct:</p><p>Create tasks using the bridge network mode (Option A):</p><p>Bridge mode doesn't allow for security groups at the task level</p><p>It makes it difficult to implement least privilege for network access</p><p>It requires opening broader port ranges on the host, which contradicts the least privilege principle</p><p>It doesn't provide the same level of network isolation between microservices</p><p>The combination of awsvpc network mode with security groups and IAM roles for tasks creates a secure containerized architecture that follows the principle of least privilege by:</p><p>Isolating network traffic between microservices</p><p>Restricting network access through task-specific security groups</p><p>Limiting AWS service permissions through task-specific IAM roles</p><p>This approach ensures both network permissions and service privileges are configured according to security best practices, as required by the company's security policy.</p><p>Sources</p><p>Security considerations for running containers on Amazon ECS | AWS Security Blog （https://aws.amazon.com/blogs/security/security-considerations-for-running-containers-on-amazon-ecs/）</p><p>Using IAM roles with Amazon ECS tasks - Amazon Elastic Container Service （https://docs.aws.amazon.com/AmazonECS/latest/bestpracticesguide/security-iam-roles.html）</p><p>Amazon EC2 container instance security considerations for Amazon ECS - Amazon Elastic Container Service （https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ec2-security-considerations.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "32bed8c81b8b444f91bb0b42d0c49cc4",
            "questionNumber": 400,
            "type": "multiple",
            "content": "<p>Question #400</p><p>A company is running a serverless application that consists of several AWS Lambda functions and Amazon DynamoDB tables. The company has created new functionality that requires the Lambda functions to access an Amazon Neptune DB cluster. The Neptune DB cluster is located in three subnets in a VPC.</p><p><br></p><p>Which of the possible solutions will allow the Lambda functions to access the Neptune DB cluster and DynamoDB tables? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create three public subnets in the Neptune VPC, and route traffic through an internet gateway. Host the Lambda functions in the three new public subnets."
                },
                {
                    "label": "B",
                    "content": "Create three private subnets in the Neptune VPC, and route internet traffic through a NAT gateway. Host the Lambda functions in the three new private subnets."
                },
                {
                    "label": "C",
                    "content": "Host the Lambda functions outside the VPC. Update the Neptune security group to allow access from the IP ranges of the Lambda functions."
                },
                {
                    "label": "D",
                    "content": "Host the Lambda functions outside the VPC. Create a VPC endpoint for the Neptune database, and have the Lambda functions access Neptune over the VPC endpoint."
                },
                {
                    "label": "E",
                    "content": "Create three private subnets in the Neptune VPC. Host the Lambda functions in the three new isolated subnets. Create a VPC endpoint for DynamoDB, and route DynamoDB traffic to the VPC endpoint."
                }
            ],
            "correctAnswer": "BE",
            "explanation": "<p>Based on the requirement to allow Lambda functions to access both an Amazon Neptune DB cluster in a VPC and DynamoDB tables, the two correct solutions are:</p><p>B. Create three private subnets in the Neptune VPC, and route internet traffic through a NAT gateway. Host the Lambda functions in the three new private subnets.</p><p>E. Create three private subnets in the Neptune VPC. Host the Lambda functions in the three new isolated subnets. Create a VPC endpoint for DynamoDB, and route DynamoDB traffic to the VPC endpoint.</p><p>Here's why these are the correct solutions:</p><p>Option B: Private Subnets with NAT Gateway</p><p>This solution places Lambda functions in private subnets within the same VPC as the Neptune DB cluster</p><p>Lambda functions can directly access the Neptune DB cluster through the VPC's internal network</p><p>The NAT gateway provides outbound internet access for the Lambda functions to reach DynamoDB (which is an AWS service accessible via public endpoints)</p><p>This approach maintains security by keeping Lambda functions in private subnets while still allowing them to access both Neptune (internal) and DynamoDB (external)</p><p>Option E: Private Subnets with VPC Endpoint for DynamoDB</p><p>This solution also places Lambda functions in private subnets within the same VPC as the Neptune DB cluster</p><p>Lambda functions can directly access the Neptune DB cluster through the VPC's internal network</p><p>The VPC endpoint for DynamoDB allows Lambda functions to access DynamoDB without going through the public internet</p><p>This approach provides enhanced security and performance by keeping all traffic within the AWS network</p><p>Why the other option is incorrect:</p><p>Option A: Public Subnets with Internet Gateway</p><p>While this would technically allow Lambda functions to access both Neptune and DynamoDB, it's not a recommended approach from a security perspective</p><p>Placing Lambda functions in public subnets unnecessarily exposes them to potential security risks</p><p>This violates the principle of least privilege and AWS best practices for serverless applications</p><p>Both options B and E provide more secure alternatives that achieve the same connectivity requirements</p><p>Both correct solutions (B and E) allow Lambda functions to access:</p><p>The Neptune DB cluster directly within the VPC</p><p>DynamoDB tables either through a NAT gateway (Option B) or through a VPC endpoint (Option E)</p><p>Option E with the VPC endpoint is generally considered the better practice as it provides better security, lower latency, and reduced data transfer costs compared to using a NAT gateway. However, both solutions will successfully meet the technical requirements specified in the question.</p><p>Sources</p><p>How to get Dynamo DB access from VPC lambda function? | AWS re:Post （https://repost.aws/questions/QUuzLLsN-5Tlyavc8otBGgMw/how-to-get-dynamo-db-access-from-vpc-lambda-function）</p><p>Accessing s3 from a Lambda within a vpc | AWS re:Post （https://repost.aws/questions/QUDdulOqQTQV6zIuR887sEdQ/accessing-s3-from-a-lambda-within-a-vpc）</p><p>AWS Lambda function not connecting to external services | AWS re:Post （https://repost.aws/questions/QUhE14aOWtQFmqgh5eBlF82g/aws-lambda-function-not-connecting-to-external-services）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        }
    ],
    "totalQuestions": 529,
    "hasNextPage": true,
    "page": 4,
    "pageSize": 100
}

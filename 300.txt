{
    "questions": [
        {
            "id": "84dcf6fc3afb4e1989338d66e4463330",
            "questionNumber": 201,
            "type": "single",
            "content": "Question #201<p>A solutions architect needs to improve an application that is hosted in the AWS Cloud. The application uses an Amazon Aurora MySQL DB instance that is experiencing overloaded connections. Most of the application’s operations insert records into the database. The application currently stores credentials in a text-based configuration file.</p><p><br></p><p>The solutions architect needs to implement a solution so that the application can handle the current connection load. The solution must keep the credentials secure and must provide the ability to rotate the credentials automatically on a regular basis.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager."
                },
                {
                    "label": "B",
                    "content": "Deploy an Amazon RDS Proxy layer in front of the DB instance. Store the connection credentials in AWS Systems Manager Parameter Store."
                },
                {
                    "label": "C",
                    "content": "Create an Aurora Replica. Store the connection credentials as a secret in AWS Secrets Manager."
                },
                {
                    "label": "D",
                    "content": "Create an Aurora Replica. Store the connection credentials in AWS Systems Manager Parameter Store."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the requirements to handle connection load for an Aurora MySQL database experiencing overloaded connections, while securely storing credentials with automatic rotation capability, the answer is:</p><p>A. Deploy an Amazon RDS Proxy layer. In front of the DB instance. Store the connection credentials as a secret in AWS Secrets Manager.</p><p>This is the correct choice because:</p><p>RDS Proxy specifically addresses the connection overload issue by providing connection pooling. It efficiently manages database connections, allowing the application to handle more traffic without overwhelming the database with too many connections.</p><p>Since most of the application's operations are inserts, RDS Proxy is particularly beneficial as it maintains persistent connections to the database, reducing the overhead of establishing new connections for each insert operation.</p><p>AWS Secrets Manager provides secure storage for database credentials, eliminating the security risk of storing them in a text-based configuration file.</p><p>Secrets Manager offers built-in functionality for automatic credential rotation on a regular basis, which directly fulfills the requirement for automatic credential rotation.</p><p>RDS Proxy integrates natively with Secrets Manager, creating a comprehensive solution that addresses both the connection management and credential security requirements.</p><p>Option C (Create an Aurora Replica and store credentials in Secrets Manager) would not effectively address the connection overload issue for an insert-heavy workload, as write operations would still go to the primary instance. While it would solve the credential security and rotation requirements, it wouldn't resolve the primary problem of overloaded connections.</p><p>Sources</p><p>Amazon RDS Proxy - Oracle to Aurora MySQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/oracle-to-aurora-mysql-migration-playbook/chap-oracle-aurora-mysql.tools.rdsproxy.html）</p><p>Amazon RDS Proxy overview - SQL Server to Aurora MySQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/sql-server-to-aurora-mysql-migration-playbook/chap-sql-server-aurora-mysql.tools.rdsproxy.html）</p><p>Automate Amazon RDS credential rotation with AWS Secrets Manager for primary instances with read replicas | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/automate-amazon-rds-credential-rotation-with-aws-secrets-manager-for-primary-instances-with-read-replicas/）</p><p>Read replica of RDS with Secrets Manager integration | AWS re:Post （https://repost.aws/questions/QUdLdC7_nNRWm93ScLB2WO5Q/read-replica-of-rds-with-secrets-manager-integration）</p><p>Amazon RDS Proxy for Aurora - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html）</p><p>Amazon RDS Proxy - Oracle to Aurora PostgreSQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/oracle-to-aurora-postgresql-migration-playbook/chap-oracle-aurora-pg.tools.rdsproxy.html）</p><p>AWS managed RDS password rotation | AWS re:Post （https://repost.aws/questions/QUZiq-Gwb3R46N9TDG-zTWRw/aws-managed-rds-password-rotation）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "70c727893bcc4f6badcd4467d46a27c3",
            "questionNumber": 202,
            "type": "single",
            "content": "<p>Question #202</p><p>A company needs to build a disaster recovery (DR) solution for its ecommerce website. The web application is hosted on a fleet of t3.large Amazon EC2 instances and uses an Amazon RDS for MySQL DB instance. The EC2 instances are in an Auto Scaling group that extends across multiple Availability Zones. </p><p><br></p><p>In the event of a disaster, the web application must fail over to the secondary environment with an RPO of 30 seconds and an RTO of 10 minutes. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Recover the EC2 instances from the latest EC2 backup. Use an Amazon Route 53 geolocation routing policy to automatically fail over to the DR Region in the event of a disaster."
                },
                {
                    "label": "B",
                    "content": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create a cross-Region read replica for the DB instance. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the EC2 instances at the minimum capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. Increase the desired capacity of the Auto Scaling group."
                },
                {
                    "label": "C",
                    "content": "Set up a backup plan in AWS Backup to create cross-Region backups for the EC2 instances and the DB instance. Create a cron expression to back up the EC2 instances and the DB instance every 30 seconds to the DR Region. Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Manually restore the backed-up data on new instances. Use an Amazon Route 53 simple routing policy to automatically fail over to the DR Region in the event of a disaster."
                },
                {
                    "label": "D",
                    "content": "Use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. Create an Amazon Aurora global database. Set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. Run the Auto Scaling group of EC2 instances at full capacity in the DR Region. Use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The most cost-effective solution that meets the RPO of 30 seconds and RTO of 10 minutes is:</p><p> Correct Answer: B &nbsp;</p><p> Why Option B? &nbsp;</p><p>1. Cross-Region Read Replica for RDS MySQL &nbsp;</p><p> &nbsp; - Ensures low RPO (30 seconds) by continuously replicating data to the DR Region. &nbsp;</p><p> &nbsp; - Promotes the replica to primary in case of disaster. &nbsp;</p><p>2. AWS Elastic Disaster Recovery (DRS) for EC2 Instances &nbsp;</p><p> &nbsp; - Continuously replicates EC2 instances to the DR Region, meeting the RPO requirement. &nbsp;</p><p> &nbsp; - Allows minimal capacity in the DR Region (cost-effective) before scaling up during failover. &nbsp;</p><p>3. Auto Scaling Group in DR Region &nbsp;</p><p> &nbsp; - Running at minimum capacity reduces costs while allowing quick scaling (meeting RTO of 10 minutes). &nbsp;</p><p>4. Route 53 Failover Routing Policy &nbsp;</p><p> &nbsp; - Automatically redirects traffic to the DR Region when the primary fails. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A: Backing up EC2 every 30 seconds is impractical (AWS Backup doesn’t support such frequent backups). Restoring from backup is slow (high RTO). &nbsp;</p><p>- C: Manual restores violate RTO, and 30-second backups are unrealistic. &nbsp;</p><p>- D: Aurora Global Database is overkill (original DB is MySQL, not Aurora). Running EC2 at full capacity in DR is costly. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the most cost-effective while meeting RPO (30s) and RTO (10 min). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "117b424707a84bdd810ec8552e883a04",
            "questionNumber": 203,
            "type": "single",
            "content": "<p>Question #203</p><p>A company is planning a one-time migration of an on-premises MySQL database to Amazon Aurora MySQL in the us-east-1 Region. The company's current internet connection has limited bandwidth. The on-premises MySQL database is 60 TB in size. The company estimates that it will take a month to transfer the data to AWS over the current internet connection. The company needs a migration solution that will migrate the database more quickly. <br><br>Which solution will migrate the database in the LEAST amount of time?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Request a 1 Gbps AWS Direct Connect connection between the on-premises data center and AWS. Use AWS Database Migration Service (AWS DMS) to migrate the on-premises MySQL database to Aurora MySQL."
                },
                {
                    "label": "B",
                    "content": "Use AWS DataSync with the current internet connection to accelerate the data transfer between the on-premises data center and AWS. Use AWS Application Migration Service to migrate the on-premises MySQL database to Aurora MySQL."
                },
                {
                    "label": "C",
                    "content": "Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL."
                },
                {
                    "label": "D",
                    "content": "Order an AWS Snowball device. Load the data into an Amazon S3 bucket by using the S3 Adapter for Snowball. Use AWS Application Migration Service to migrate the data from Amazon S3 to Aurora MySQL."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirement to migrate a 60 TB on-premises MySQL database to Aurora MySQL in the least amount of time, the answer is:</p><p>C. Order an AWS Snowball Edge device. Load the data into an Amazon S3 bucket by using the S3 interface. Use AWS Database Migration Service (AWS DMS) to migrate the data from Amazon S3 to Aurora MySQL.</p><p>This is the correct choice because:</p><p>The company's current internet connection has limited bandwidth, and they estimate it would take a month to transfer the data over this connection.</p><p>Even with a 1 Gbps AWS Direct Connect connection (Option A), transferring 60 TB of data would still take approximately 5.5 days of continuous transfer at optimal speeds, plus additional time for AWS DMS processing and data validation.</p><p>Using AWS Snowball Edge provides a physical data transport solution that bypasses internet transfer limitations entirely for the bulk of the data:</p><p>Data export to Snowball Edge: 1-2 days</p><p>Shipping time: 1-3 days</p><p>Data import to S3: 1-2 days</p><p>AWS DMS migration from S3 to Aurora MySQL: 1-2 days</p><p>Total estimated time: 4-9 days</p><p>The Snowball Edge approach is specifically designed for large-scale data migrations where internet bandwidth is a constraint, making it ideal for this 60 TB migration scenario.</p><p>This approach also minimizes the impact on the production database compared to a prolonged migration over a network connection.</p><p>While Option A (AWS Direct Connect with AWS DMS) would provide a dedicated network connection, it would still be constrained by the 1 Gbps bandwidth limit, making it slower than the physical transfer method for this large dataset.</p><p>Sources</p><p>Migrate a self-managed MySQL database to Amazon Aurora MySQL using AWS DMS homogeneous data migrations | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/migrate-a-self-managed-mysql-database-to-amazon-aurora-mysql-using-aws-dms-homogeneous-data-migrations/）</p><p>Planning your database migration process - Migrating Your Databases to Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html）</p><p>Database Migration Step-by-Step Walkthroughs - Database Migration Guide （https://docs.aws.amazon.com/dms/latest/sbs/dms-sbs-welcome.html）</p><p>Migrate very large databases to Amazon Aurora MySQL using MyDumper and MyLoader | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/migrate-very-large-databases-to-amazon-aurora-mysql-using-mydumper-and-myloader/）</p><p>Full load MySQL database migration options performance comparison - Database Migration Guide（https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.mysql2rds.performance.html）</p><p>Time and performance - An Overview of AWS Cloud Data Migration Services （https://docs.aws.amazon.com/decision-guides/latest/migration-on-aws-how-to-choose/migration-on-aws-how-to-choose.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2bd75013208f490ea4f5b77a7e863659",
            "questionNumber": 204,
            "type": "single",
            "content": "<p>Question #204</p><p>A company has an application in the AWS Cloud. The application runs on a fleet of 20 Amazon EC2 instances. The EC2 instances are persistent and store data on multiple attached Amazon Elastic Block Store (Amazon EBS) volumes.</p><p><br></p><p>The company must maintain backups in a separate AWS Region. The company must be able to recover the EC2 instances and their configuration within 1 business day, with loss of no more than 1 day's worth of data. The company has limited staff and needs a backup solution that optimizes operational efficiency and cost. The company already has created an AWS CloudFormation template that can deploy the required network configuration in a secondary Region. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a second CloudFormation template that can recreate the EC2 instances in the secondary Region. Run daily multivolume snapshots by using AWS Systems Manager Automation runbooks. Copy the snapshots to the secondary Region. In the event of a failure launch the CloudFormation templates, restore the EBS volumes from snapshots, and transfer usage to the secondary Region."
                },
                {
                    "label": "B",
                    "content": "Use Amazon Data Lifecycle Manager (Amazon DLM) to create daily multivolume snapshots of the EBS volumes. In the event of a failure, launch the CloudFormation template and use Amazon DLM to restore the EBS volumes and transfer usage to the secondary Region."
                },
                {
                    "label": "C",
                    "content": "Use AWS Backup to create a scheduled daily backup plan for the EC2 instances. Configure the backup task to copy the backups to a vault in the secondary Region. In the event of a failure, launch the CloudFormation template, restore the instance volumes and configurations from the backup vault, and transfer usage to the secondary Region."
                },
                {
                    "label": "D",
                    "content": "Deploy EC2 instances of the same size and configuration to the secondary Region. Configure AWS DataSync daily to copy data from the primary Region to the secondary Region. In the event of a failure, launch the CloudFormation template and transfer usage to the secondary Region."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct answer because it utilizes AWS Backup, which is a fully managed service that can back up the Amazon EBS volumes and the EC2 instance configurations. By scheduling daily backups and copying them to a vault in a secondary region, the company can ensure that backups are maintained offsite and are available for a rapid recovery. The use of AWS Backup also provides a centralized management console for monitoring and reporting on backup activities, which aligns with the company's need for a solution that optimizes operational efficiency and cost. In the event of a failure, the company can use the CloudFormation template to recreate the network environment and restore the instance volumes and configurations from the backup vault within one business day, adhering to the company's recovery time objective (RTO) and recovery point objective (RPO).</p><p> Key Requirements:</p><p>1. Cross-Region Backups: Must maintain backups in a separate AWS Region.</p><p>2. Recovery Time Objective (RTO): Recover within 1 business day.</p><p>3. Recovery Point Objective (RPO): No more than 1 day's worth of data loss.</p><p>4. Operational Efficiency: Limited staff, so automation is crucial.</p><p>5. Cost Optimization: Need a cost-effective solution.</p><p>6. Existing CloudFormation Template: Already available for network configuration in the secondary Region.</p><p> Why Option C is Best?</p><p>- AWS Backup is a fully managed service that automates backup scheduling, retention, and cross-region copying.</p><p>- It supports EC2 instances and EBS volumes, ensuring consistent backups.</p><p>- Cross-Region Copy can be configured to replicate backups to the secondary Region.</p><p>- Restoration is simple: In case of failure, the CloudFormation template can deploy the network, and AWS Backup can restore the instances and EBS volumes.</p><p>- Minimal operational overhead: AWS Backup handles lifecycle management, reducing manual work.</p><p> Why Other Options Are Less Suitable?</p><p>- A: While possible, managing snapshots via Systems Manager Automation runbooks is more complex than using AWS Backup.</p><p>- B: Amazon DLM only manages EBS snapshots, not full instance recovery (including configurations). It doesn’t handle cross-region copying as seamlessly as AWS Backup.</p><p>- D: AWS DataSync is for active data replication, not backups. Running duplicate instances is costly, and this approach doesn’t guarantee point-in-time recovery.</p><p> Conclusion</p><p>AWS Backup (Option C) is the most automated, cost-effective, and compliant solution for meeting the RTO, RPO, and cross-region backup requirements.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "0d5e1226f9f94cc5be1ee81a3e96509b",
            "questionNumber": 205,
            "type": "multiple",
            "content": "<p>Question #205</p><p>A company is designing a new website that hosts static content. The website will give users the ability to upload and download large files. According to company requirements, all data must be encrypted in transit and at rest. A solutions architect is building the solution by using Amazon S3 and Amazon CloudFront. Which combination of steps will meet the encryption requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Turn on S3 server-side encryption for the S3 bucket that the web application uses."
                },
                {
                    "label": "B",
                    "content": "Add a policy attribute of &quot;aws:SecureTransport&quot;: &quot;true&quot; for read and write operations in the S3 ACLs."
                },
                {
                    "label": "C",
                    "content": "Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses."
                },
                {
                    "label": "D",
                    "content": "Configure encryption at rest on CloudFront by using server-side encryption with AWS KMS keys (SSE-KMS)."
                },
                {
                    "label": "E",
                    "content": "Configure redirection of HTTP requests to HTTPS requests in CloudFront."
                },
                {
                    "label": "F",
                    "content": "Use the RequireSSL option in the creation of presigned URLs for the S3 bucket that the web application uses."
                }
            ],
            "correctAnswer": "ACE",
            "explanation": "<p>The correct combination of steps to meet the encryption requirements (encrypted in transit and at rest) are:</p><p>A. Turn on S3 server-side encryption for the S3 bucket that the web application uses. &nbsp;</p><p>→ Ensures data is encrypted at rest in Amazon S3. &nbsp;</p><p>C. Create a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses. &nbsp;</p><p>→ Enforces encryption in transit by denying unencrypted (HTTP) requests to S3. &nbsp;</p><p>E. Configure redirection of HTTP requests to HTTPS requests in CloudFront. &nbsp;</p><p>→ Ensures all traffic between users and CloudFront is encrypted in transit by forcing HTTPS. &nbsp;</p><p> Why not the others? &nbsp;</p><p>- B: S3 ACLs (Access Control Lists) do not support the `aws:SecureTransport` condition; this must be set in a bucket policy, not ACLs. &nbsp;</p><p>- D: CloudFront does not store data persistently, so \"encryption at rest\" does not apply (it's for data in transit). &nbsp;</p><p>- F: Presigned URLs can enforce HTTPS, but this is not a primary solution for general encryption in transit (bucket policy and CloudFront HTTPS redirection are better). &nbsp;</p><p> Final Answer: &nbsp;</p><p>A. Enabling S3 server-side encryption ensures that the data stored in the S3 bucket is encrypted at rest.</p><p>C. A bucket policy that denies unencrypted operations ensures that any attempt to access the bucket without using HTTPS will be rejected, thus enforcing the use of encryption in transit for all operations with the bucket.</p><p>E. Configuring CloudFront to redirect HTTP requests to HTTPS requests ensures that all user interactions with the website are encrypted in transit.</p><p>While options D and F might seem relevant, they are not necessary for this scenario. Option D is about encrypting data at rest on CloudFront, which is redundant if the data is already encrypted at rest on S3. Option F, the RequireSSL option for pre-signed URLs, is an additional security measure but not a requirement for the given scenario since the question focuses on general data encryption at rest and in transit.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "bbfe6e28f5d640768ff6e82a15b12064",
            "questionNumber": 206,
            "type": "single",
            "content": "<p>Question #206</p><p>A company is implementing a serverless architecture by using AWS Lambda functions that need to access a Microsoft SQL Server DB instance on Amazon RDS. The company has separate environments for development and production, including a clone of the database system. </p><p><br></p><p>The company's developers are allowed to access the credentials for the development database. However, the credentials for the production database must be encrypted with a key that only members of the IT security team's IAM user group can access. This key must be rotated on a regular basis. </p><p><br></p><p>What should a solutions architect do in the production environment to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Store the database credentials in AWS Systems Manager Parameter Store by using a SecureString parameter that is encrypted by an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the SecureString parameter. Restrict access to the SecureString parameter and the customer managed key so that only the IT security team can access the parameter and the key."
                },
                {
                    "label": "B",
                    "content": "Encrypt the database credentials by using the AWS Key Management Service (AWS KMS) default Lambda key. Store the credentials in the environment variables of each Lambda function. Load the credentials from the environment variables in the Lambda code. Restrict access to the KMS key so that only the IT security team can access the key."
                },
                {
                    "label": "C",
                    "content": "Store the database credentials in the environment variables of each Lambda function. Encrypt the environment variables by using an AWS Key Management Service (AWS KMS) customer managed key. Restrict access to the customer managed key so that only the IT security team can access the key."
                },
                {
                    "label": "D",
                    "content": "Store the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS) customer managed key. Attach a role to each Lambda function to provide access to the secret. Restrict access to the secret and the customer managed key so that only the IT security team can access the secret and the key."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. AWS Secrets Manager is designed to securely manage and rotate secrets, such as database credentials. By storing the credentials in AWS Secrets Manager and encrypting them with a customer managed key in AWS KMS, the solution architect can ensure that the production database credentials are encrypted and only accessible to the IT security team. Additionally, Secrets Manager can rotate the credentials on a regular basis, meeting the requirement for key rotation. Attaching a role to each Lambda function allows the functions to access the secrets as needed, and restricting access to the secret and the KMS key ensures that only authorized individuals can manage the credentials.Here's why:</p><p> Requirements Recap:</p><p>1. Secure Storage of Production DB Credentials: The credentials must be encrypted, and only the IT security team should have access to the encryption key.</p><p>2. Key Rotation: The encryption key must be rotated regularly.</p><p>3. Lambda Access: Lambda functions need access to the credentials securely.</p><p>4. Restricted Access: Only the IT security team should manage the key and credentials.</p><p> Why Option D is Correct?</p><p>- AWS Secrets Manager is designed for securely storing and managing secrets (like database credentials).</p><p> &nbsp;- It integrates with AWS KMS for encryption using a customer-managed key (allowing key rotation).</p><p> &nbsp;- You can restrict access to the secret and the KMS key via IAM policies, ensuring only the IT security team can manage them.</p><p> &nbsp;- Lambda functions can retrieve the credentials securely by assuming an IAM role with permissions to access the secret.</p><p> Why Other Options Are Incorrect?</p><p>- Option A: While Systems Manager Parameter Store (SecureString) can store encrypted values, Secrets Manager is better suited for database credentials because it offers automatic rotation, tighter integration with RDS, and better access control.</p><p>- Option B: Storing credentials in environment variables (even if encrypted) is less secure than using Secrets Manager. Environment variables can be exposed in logs or via AWS APIs.</p><p>- Option C: Similar to B, storing credentials in environment variables is not best practice for sensitive data like production DB credentials.</p><p> Conclusion:</p><p>Option D is the most secure and AWS-recommended approach for managing database credentials in a serverless architecture while meeting the given requirements.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4842601e8f834275968ced83b77beeea",
            "questionNumber": 207,
            "type": "single",
            "content": "<p>Question #207</p><p>An online retail company is migrating its legacy on-premises .NET application to AWS. The application runs on load-balanced frontend web servers, load-balanced application servers, and a Microsoft SQL Server database. </p><p><br></p><p>The company wants to use AWS managed services where possible and does not want to rewrite the application. A solutions architect needs to implement a solution to resolve scaling issues and minimize licensing costs as the application scales. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database."
                },
                {
                    "label": "B",
                    "content": "Create images of all the servers by using AWS Database Migration Service (AWS DMS). Deploy Amazon EC2 instances that are based on the on-premises imports. Deploy the instances in an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon DynamoDB as the database tier."
                },
                {
                    "label": "C",
                    "content": "Containerize the web frontend tier and the application tier. Provision an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Create an Auto Scaling group behind a Network Load Balancer for the web tier and for the application tier. Use Amazon RDS for SQL Server to host the database."
                },
                {
                    "label": "D",
                    "content": "Separate the application functions into AWS Lambda functions. Use Amazon API Gateway for the web frontend tier and the application tier. Migrate the data to Amazon S3. Use Amazon Athena to query the data."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the requirements to migrate a legacy .NET application to AWS using managed services where possible, without rewriting the application, while resolving scaling issues and minimizing licensing costs, the answer is:</p><p>A. Deploy Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer for the web tier and for the application tier. Use Amazon Aurora PostgreSQL with Babelfish turned on to replatform the SQL Server database.</p><p>This is the correct choice because:</p><p>It maintains the existing application architecture (web tier, application tier, database) without requiring a rewrite of the application code, which is a key requirement.</p><p>Using EC2 instances in Auto Scaling groups addresses the scaling issues by automatically adjusting capacity based on demand for both the web and application tiers.</p><p>Amazon Aurora PostgreSQL with Babelfish provides SQL Server compatibility while significantly reducing licensing costs. Babelfish allows Aurora PostgreSQL to understand T-SQL and TDS protocol, enabling SQL Server applications to work with Aurora PostgreSQL with minimal code changes.</p><p>This approach leverages AWS managed services (Auto Scaling, ALB, Aurora) while maintaining compatibility with the existing .NET application.</p><p>Option D (using Lambda, API Gateway, S3, and Athena) would require completely redesigning and rewriting the application into a serverless architecture, which violates the requirement of not rewriting the application. While serverless could potentially be cost-effective, the extensive redevelopment effort would be significant and unnecessary given the constraints.</p><p>The solution in option A provides the best balance of using AWS managed services, addressing scaling issues, minimizing licensing costs, and avoiding application rewrites.</p><p>Sources</p><p>Migrating SQL Server - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-microsoft-workloads-aws/migrating-sql-server-workloads.html）</p><p>How to optimize costs for Microsoft workloads on AWS | Microsoft Workloads on AWS （https://aws.amazon.com/cn/blogs/modernizing-with-aws/how-to-optimize-costs-for-microsoft-workloads-on-aws/）</p><p>Paths to the cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-microsoft-workloads-aws/cloud-paths.html）</p><p>Make SQL Server end of support the start of your AWS cloud journey | Microsoft Workloads on AWS（https://aws.amazon.com/cn/blogs/modernizing-with-aws/make-sql-server-end-of-support-the-start-of-your-aws-cloud-journey/） </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "e3f258dd832344538fe923d736837089",
            "questionNumber": 208,
            "type": "single",
            "content": "<p>Question #208</p><p>A software-as-a-service (SaaS) provider exposes APIs through an Application Load Balancer (ALB). The ALB connects to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that is deployed in the us-east-1 Region. The exposed APIs contain usage of a few non-standard REST methods: LINK, UNLINK, LOCK, and UNLOCK. </p><p><br></p><p>Users outside the United States are reporting long and inconsistent response times for these APIs. A solutions architect needs to resolve this problem with a solution that minimizes operational overhead. </p><p><br></p><p>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Add an Amazon CloudFront distribution. Configure the ALB as the origin."
                },
                {
                    "label": "B",
                    "content": "Add an Amazon API Gateway edge-optimized API endpoint to expose the APIs. Configure the ALB as the target."
                },
                {
                    "label": "C",
                    "content": "Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin"
                },
                {
                    "label": "D",
                    "content": "Deploy the APIs to two additional AWS Regions: eu-west-1 and ap-southeast-2. Add latency-based routing records in Amazon Route 53."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirements to improve API response times for users outside the United States while minimizing operational overhead, the answer is:</p><p>C. Add an accelerator in AWS Global Accelerator. Configure the ALB as the origin.</p><p>This is the correct choice because:</p><p>AWS Global Accelerator is specifically designed to improve the availability and performance of applications for global users by routing traffic through AWS's global network infrastructure rather than the public internet.</p><p>Global Accelerator works seamlessly with ALBs and supports all TCP traffic, including non-standard HTTP methods like LINK, UNLINK, LOCK, and UNLOCK mentioned in the question.</p><p>Implementation requires minimal changes to the existing architecture - you simply create an accelerator and add the ALB as an endpoint, which aligns with the requirement for minimal operational overhead.</p><p>Global Accelerator provides static IP addresses that serve as fixed entry points to your application, making it easier for global users to connect with lower latency.</p><p>Let's compare with the other options:</p><p>Option A (CloudFront): While CloudFront can improve global performance, it's primarily designed for content delivery and caching. It may require additional configuration for non-standard HTTP methods and might not be optimal for dynamic API requests.</p><p>Option B (API Gateway): Would require significant architectural changes to your existing setup and potentially introduce new complexities, increasing operational overhead.</p><p>Option D (Multi-region deployment): While effective for performance, this approach requires deploying and maintaining the EKS cluster in multiple regions, which significantly increases operational overhead and costs.</p><p>AWS Global Accelerator provides the best balance of performance improvement for global users and minimal operational overhead for your specific use case with non-standard REST methods.</p><p>Sources</p><p>PERF05-BP02 Evaluate available networking features - AWS Well-Architected Framework (2023-04-10)（https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/perf_select_network_evaluate_features.html） </p><p>Get suggestions to optimize application performance in Internet Monitor (Optimize page) - Amazon CloudWatch （https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-IM-insights.html）</p><p>PERF05-BP02 Evaluate available networking features - AWS Well-Architected Framework (2022-03-31) （https://docs.aws.amazon.com/wellarchitected/2022-03-31/framework/perf_select_network_evaluate_features.html）</p><p>Amazon API Gateway - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/amazon-api-gateway.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "432fb1723c434d56b169e3fd23b2584c",
            "questionNumber": 209,
            "type": "single",
            "content": "<p>Question #209</p><p>A company runs an IoT application in the AWS Cloud. The company has millions of sensors that collect data from houses in the United States. The sensors use the MQTT protocol to connect and send data to a custom MQTT broker. The MQTT broker stores the data on a single Amazon EC2 instance. The sensors connect to the broker through the domain named iot.example.com. The company uses Amazon Route 53 as its DNS service. The company stores the data in Amazon DynamoDB. </p><p><br></p><p>On several occasions, the amount of data has overloaded the MQTT broker and has resulted in lost sensor data. The company must improve the reliability of the solution. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. Use the Auto Scaling group as the target for the ALB. Update the DNS record in Route 53 to an alias record. Point the alias record to the ALB. Use the MQTT broker to store the data."
                },
                {
                    "label": "B",
                    "content": "Set up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record in Route 53 to point to the AWS IoT Core Data-ATS endpoint. Configure an AWS IoT rule to store the data."
                },
                {
                    "label": "C",
                    "content": "Create a Network Load Balancer (NLB). Set the MQTT broker as the target. Create an AWS Global Accelerator accelerator. Set the NLB as the endpoint for the accelerator. Update the DNS record in Route 53 to a multivalue answer record. Set the Global Accelerator IP addresses as values. Use the MQTT broker to store the data."
                },
                {
                    "label": "D",
                    "content": "Set up AWS IoT Greengrass to receive the sensor data. Update the DNS record in Route 53 to point to the AWS IoT Greengrass endpoint. Configure an AWS IoT rule to invoke an AWS Lambda function to store the data."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. Set up AWS IoT Core to receive the sensor data. Create and configure a custom domain to connect to AWS IoT Core. Update the DNS record in Route 53 to point to the AWS IoT Core Data-ATS endpoint. Configure an AWS IoT rule to store the data.</p><p> Why Option B is the Best Solution?</p><p>1. AWS IoT Core is a fully managed service designed to handle millions of IoT devices securely and reliably, eliminating the need for a self-managed MQTT broker.</p><p>2. Custom Domain allows the company to retain the existing domain (`iot.example.com`) while routing traffic to AWS IoT Core.</p><p>3. Route 53 Alias Record can point directly to the AWS IoT Core ATS endpoint, ensuring secure and reliable connectivity.</p><p>4. AWS IoT Rules can automatically store incoming data in Amazon DynamoDB, removing the bottleneck caused by the single EC2 instance.</p><p>5. Scalability & Reliability: AWS IoT Core scales automatically, preventing data loss due to overload.</p><p> Why Other Options Are Not Ideal?</p><p>- Option A: Using an ALB + Auto Scaling still requires managing an MQTT broker, which is less scalable and reliable than AWS IoT Core.</p><p>- Option C: A Network Load Balancer (NLB) + Global Accelerator improves availability but still relies on a self-managed broker, which is not as scalable as IoT Core.</p><p>- Option D: AWS IoT Greengrass is meant for edge computing, not for replacing an MQTT broker in the cloud.</p><p> Conclusion</p><p>AWS IoT Core is the best solution because it is fully managed, highly scalable, and integrates seamlessly with DynamoDB, ensuring no data loss due to broker overload. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "41a73d8ef3c9433b9d64db398533d109",
            "questionNumber": 210,
            "type": "single",
            "content": "<p>Question #210</p><p>A company has Linux-based Amazon EC2 instances. Users must access the instances by using SSH with EC2 SSH key pairs. Each machine requires a unique EC2 key pair. </p><p><br></p><p>The company wants to implement a key rotation policy that will, upon request, automatically rotate all the EC2 key pairs and keep the keys in a securely encrypted place. The company will accept less than 1 minute of downtime during key rotation. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Secrets Manager."
                },
                {
                    "label": "B",
                    "content": "Store all the keys in Parameter Store, a capability of AWS Systems Manager, as a string. Define a Systems Manager maintenance window to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Parameter Store."
                },
                {
                    "label": "C",
                    "content": "Import the EC2 key pairs into AWS Key Management Service (AWS KMS). Configure automatic key rotation for these key pairs. Create an Amazon EventBridge scheduled rule to invoke an AWS Lambda function to initiate the key rotation in AWS KMS."
                },
                {
                    "label": "D",
                    "content": "Add all the EC2 instances to Fleet Manager, a capability of AWS Systems Manager. Define a Systems Manager maintenance window to issue a Systems Manager Run Command document to generate new key pairs and to rotate public keys to all the instances in Fleet Manager."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>For automatically rotating EC2 SSH key pairs while keeping them securely encrypted with minimal downtime, the answer is:</p><p>A. Store all the keys in AWS Secrets Manager. Define a Secrets Manager rotation schedule to invoke an AWS Lambda function to generate new key pairs. Replace public keys on EC2 instances. Update the private keys in Secrets Manager.</p><p>This is the correct choice because:</p><p>AWS Secrets Manager is specifically designed for secure storage and automated rotation of sensitive information like SSH keys. It provides built-in rotation capabilities that can be scheduled according to your security policies.</p><p>Secrets Manager offers native encryption at rest using AWS KMS, ensuring your SSH keys remain securely encrypted.</p><p>The rotation process using Lambda functions allows for programmatic generation of new key pairs and updating of the public keys on EC2 instances, which can be accomplished with minimal downtime (less than 1 minute as required).</p><p>Secrets Manager provides fine-grained access control through IAM policies, ensuring that only authorized users and services can access the SSH keys.</p><p>The solution is fully automated once configured, meeting the requirement for automatic rotation upon request.</p><p>Option B (using Parameter Store) is less suitable because:</p><p>Parameter Store doesn't have built-in rotation capabilities like Secrets Manager</p><p>While Parameter Store can store encrypted strings, it lacks the specialized features for secret rotation that Secrets Manager provides</p><p>Parameter Store is primarily designed for configuration data rather than sensitive credentials that require regular rotation</p><p>AWS Secrets Manager is purpose-built for this exact use case - securely storing sensitive credentials and automating their rotation, making it the optimal solution for the company's requirements.</p><p>Sources</p><p>AWS Secrets Manager best practices - AWS Secrets Manager （https://docs.aws.amazon.com/secretsmanager/latest/userguide/best-practices.html）</p><p>Secrets management - Amazon Elastic Container Service （https://docs.aws.amazon.com/AmazonECS/latest/developerguide/specifying-sensitive-data.html）</p><p>Choosing an AWS cryptography service - Choosing an AWS cryptography service （https://docs.aws.amazon.com/decision-guides/latest/cryptography-on-aws-how-to-choose/guide.html）</p><p>Best practice for storing EC2 SSH keys, SQL Server encryption keys (SMK, DMK), other encryption keys | AWS re:Post （https://repost.aws/questions/QUFySsI0MDRsGEJ1C3dq0-hw/best-practice-for-storing-ec2-ssh-keys-sql-server-encryption-keys-smk-dmk-other-encryption-keys）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f2f2e62b9dff41c4af6bcd201c0e5334",
            "questionNumber": 211,
            "type": "single",
            "content": "<p>Question #211</p><p>A company wants to migrate to AWS. The company is running thousands of VMs in a VMware ESXi environment. The company has no configuration management database and has little knowledge about the utilization of the VMware portfolio. </p><p><br></p><p>A solutions architect must provide the company with an accurate inventory so that the company can plan for a cost-effective migration. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Systems Manager Patch Manager to deploy Migration Evaluator to each VM. Review the collected data in Amazon QuickSight. Identify servers that have high utilization. Remove the servers that have high utilization from the migration list. Import the data to AWS Migration Hub."
                },
                {
                    "label": "B",
                    "content": "Export the VMware portfolio to a .csv file. Check the disk utilization for each server. Remove servers that have high utilization. Export the data to AWS Application Migration Service. Use AWS Server Migration Service (AWS SMS) to migrate the remaining servers."
                },
                {
                    "label": "C",
                    "content": "Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub."
                },
                {
                    "label": "D",
                    "content": "Deploy the AWS Application Migration Service Agent to each VM. When the data is collected, use Amazon Redshift to import and analyze the data. Use Amazon QuickSight for data visualization."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirement to provide an accurate inventory for a cost-effective migration with the least operational overhead, the answer is:</p><p>C. Deploy the Migration Evaluator agentless collector to the ESXi hypervisor. Review the collected data in Migration Evaluator. Identify inactive servers. Remove the inactive servers from the migration list. Import the data to AWS Migration Hub.</p><p>This is the correct choice because:</p><p>Migration Evaluator (formerly TSO Logic) is specifically designed to provide detailed discovery and assessment of on-premises environments with minimal operational overhead. The agentless collector can be deployed directly to the ESXi hypervisor without requiring agents on each VM.</p><p>For an environment with thousands of VMs and little knowledge about utilization, the automated discovery and assessment capabilities of Migration Evaluator provide comprehensive data collection with minimal manual effort.</p><p>Migration Evaluator automatically collects detailed information about server specifications, utilization metrics, and dependencies, which is crucial when there's no configuration management database available.</p><p>The solution allows for identifying inactive servers based on actual utilization data, enabling the company to make informed decisions about which servers to migrate, which helps achieve a cost-effective migration.</p><p>Integration with AWS Migration Hub streamlines the migration planning and execution process, providing a central location to track migration progress across multiple AWS and partner tools.</p><p>Option B involves manually exporting data to CSV files and manually checking disk utilization for each server, which would require significantly more operational overhead for thousands of VMs. This manual approach is more time-consuming, error-prone, and less comprehensive than the automated discovery provided by Migration Evaluator.</p><p>The Migration Evaluator approach provides the most efficient path to creating an accurate inventory with minimal operational overhead, making it the best solution for this scenario.</p><p>Sources</p><p>VMware ESXI to AWS migration | AWS re:Post （https://repost.aws/questions/QUyuhMhfmQTvGlvy0lI8-f9w/vmware-esxi-to-aws-migration）</p><p>VMware migration - AWS Transform （https://docs.aws.amazon.com/transform/latest/userguide/transform-app-vmware.html）</p><p>AWS Agentic AI Options for migrating VMware based workloads | Migration & Modernization （https://aws.amazon.com/cn/blogs/migration-and-modernization/aws-agentic-ai-options-for-migrating-vmware-based-workloads/）</p><p>Choosing a migration approach for relocating your VMware applications and workloads to the AWS Cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-vmware-aws/welcome.html）</p><p>Migrate VMware SDDC to VMware Cloud on AWS using VMware HCX - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-vmware-sddc-to-vmware-cloud-on-aws-using-vmware-hcx.html）</p><p>Migrate VMs to VMware Cloud on AWS by using HCX OS Assisted Migration - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-vms-to-vmware-cloud-on-aws-by-using-hcx-os-assisted-migration.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "28639344ac1e4a318279e45a386eea61",
            "questionNumber": 212,
            "type": "single",
            "content": "<p>Question #212</p><p>A company runs a microservice as an AWS Lambda function. The microservice writes data to an on-premises SQL database that supports a limited number of concurrent connections. When the number of Lambda function invocations is too high, the database crashes and causes application downtime. The company has an AWS Direct Connect connection between the company's VPC and the on-premises data center. The company wants to protect the database from crashes. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Write the data to an Amazon Simple Queue Service (Amazon SQS) queue. Configure the Lambda function to read from the queue and write to the existing database. Set a reserved concurrency limit on the Lambda function that is less than the number of connections that the database supports."
                },
                {
                    "label": "B",
                    "content": "Create a new Amazon Aurora Serverless DB cluster. Use AWS DataSync to migrate the data from the existing database to Aurora Serverless. Reconfigure the Lambda function to write to Aurora."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon RDS Proxy DB instance. Attach the RDS Proxy DB instance to the Amazon RDS DB instance. Reconfigure the Lambda function to write to the RDS Proxy DB instance."
                },
                {
                    "label": "D",
                    "content": "Write the data to an Amazon Simple Notification Service (Amazon SNS) topic. Invoke the Lambda function to write to the existing database when the topic receives new messages. Configure provisioned concurrency for the Lambda function to be equal to the number of connections that the database supports."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer. By writing data to an Amazon SQS queue, the Lambda function can offload the workload from the on-premises SQL database. The queue acts as a buffer that smooths out the invocation rate, preventing the database from being overwhelmed by too many concurrent connections. Configuring the Lambda function to process messages from the queue at a controlled rate ensures that the number of connections to the database does not exceed its capacity. Setting a reserved concurrency limit on the Lambda function helps to further control the rate of execution and protect the database from being overloaded.</p><p>The issue is that the on-premises SQL database crashes due to too many concurrent connections from the Lambda function. To solve this, we need to: &nbsp;</p><p>1. Throttle the Lambda function's concurrency to match the database's connection limit. &nbsp;</p><p>2. Use an SQS queue as a buffer to handle spikes in requests without overwhelming the database. &nbsp;</p><p>Option A is correct because: &nbsp;</p><p>- SQS decouples the Lambda function from the database, allowing writes to be processed at a controlled rate. &nbsp;</p><p>- Reserved concurrency ensures Lambda doesn’t exceed the database’s connection limit. &nbsp;</p><p>- Direct Connect is still used (no need to migrate data or change the database). &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- B (Aurora Serverless + DataSync): Migrating to Aurora is unnecessary and doesn’t solve the concurrency issue directly. &nbsp;</p><p>- C (RDS Proxy): RDS Proxy manages database connections, but it only works with AWS RDS databases, not on-premises SQL databases. &nbsp;</p><p>- D (SNS + Provisioned Concurrency): SNS doesn’t buffer requests like SQS, and provisioned concurrency doesn’t limit concurrent executions (it only pre-initializes instances). &nbsp;</p><p> Final Answer: &nbsp;</p><p>✅ A. Write the data to an Amazon SQS queue, configure the Lambda function to read from the queue, and set a reserved concurrency limit.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6c940ba5cd8e42849d4349701c97b458",
            "questionNumber": 213,
            "type": "single",
            "content": "<p>Question #213</p><p>A company uses a Grafana data visualization solution that runs on a single Amazon EC2 instance to monitor the health of the company's AWS workloads. The company has invested time and effort to create dashboards that the company wants to preserve. The dashboards need to be highly available and cannot be down for longer than 10 minutes. The company needs to minimize ongoing maintenance. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate to Amazon CloudWatch dashboards. Recreate the dashboards to match the existing Grafana dashboards. Use automatic dashboards where possible."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing Grafana instance. Import the dashboards into the new workspace."
                },
                {
                    "label": "C",
                    "content": "Create an AMI that has Grafana pre-installed. Store the existing dashboards in Amazon Elastic File System (Amazon EFS). Create an Auto Scaling group that uses the new AMI. Set the Auto Scaling group&#39;s minimum, desired, and maximum number of instances to one. Create an Application Load Balancer that serves at least two Availability Zones."
                },
                {
                    "label": "D",
                    "content": "Configure AWS Backup to back up the EC2 instance that runs Grafana once each hour. Restore the EC2 instance from the most recent snapshot in an alternate Availability Zone when required."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the requirements to make the Grafana dashboards highly available with less than 10 minutes of downtime while minimizing ongoing maintenance and operational overhead, the answer is:</p><p>B. Create an Amazon Managed Grafana workspace. Configure a new Amazon CloudWatch data source. Export dashboards from the existing Grafana instance. Import the dashboards into the new workspace.</p><p>This is the correct choice because:</p><p>Amazon Managed Grafana is a fully managed service that handles provisioning, setup, scaling, and maintenance automatically, which directly addresses the requirement to minimize ongoing maintenance and operational overhead.</p><p>The company has invested time and effort in creating Grafana dashboards that they want to preserve. Amazon Managed Grafana allows for direct export/import of existing Grafana dashboards, preserving their investment without requiring recreation of dashboards from scratch.</p><p>Amazon Managed Grafana provides built-in high availability across multiple Availability Zones, meeting the requirement for high availability.</p><p>The migration process of exporting dashboards from the existing Grafana instance and importing them into Amazon Managed Grafana can be completed within the 10-minute downtime constraint.</p><p>Amazon Managed Grafana maintains the familiar Grafana interface and functionality that the company's team is already accustomed to, reducing any learning curve.</p><p>Option A (migrating to CloudWatch dashboards) would require recreating all dashboards from scratch, which represents significantly more operational overhead and might not preserve all the visualization capabilities and customizations of the existing Grafana dashboards. This approach would likely take longer and require more effort than the 10-minute downtime constraint allows.</p><p>Amazon Managed Grafana provides the best balance of preserving existing dashboards, ensuring high availability, minimizing downtime, and reducing ongoing operational overhead.</p><p>Sources</p><p>Amazon Managed Grafana for dashboarding and visualization - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/implementing-logging-monitoring-cloudwatch/amg-dashboarding-visualization.html）</p><p>Configuring AWS Managed Grafana with CloudWatch Integration using CloudFormation | AWS re:Post （https://repost.aws/articles/ARSRP9KmeUSWyrGUiCg64DOg/configuring-aws-managed-grafana-with-cloudwatch-integration-using-cloudformation）</p><p>Enhance your SAP Observability using Amazon Managed Prometheus and Grafana | AWS for SAP （https://aws.amazon.com/cn/blogs/awsforsap/enhance-your-sap-observability-using-amazon-managed-prometheus-and-amazon-managed-grafana/）</p><p>Amazon Managed Grafana Features | Amazon Web Services （https://aws.amazon.com/cn/grafana/features/）</p><p>Setting up an Amazon Managed Grafana workspace - Amazon SageMaker AI （https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-cluster-observability-slurm-managed-grafana-ws.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b548e4f17ac442aebbd9c26318678c21",
            "questionNumber": 214,
            "type": "single",
            "content": "<p>Question #214</p><p>A company needs to migrate its customer transactions database from on premises to AWS. The database resides on an Oracle DB instance that runs on a Linux server. According to a new security requirement, the company must rotate the database password each year. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Convert the database to Amazon DynamoDB by using the AWS Schema Conversion Tool (AWS SCT). Store the password in AWS Systems Manager Parameter Store. Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly password rotation."
                },
                {
                    "label": "B",
                    "content": "Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a yearly rotation schedule."
                },
                {
                    "label": "C",
                    "content": "Migrate the database to an Amazon EC2 instance. Use AWS Systems Manager Parameter Store to keep and rotate the connection string by using an AWS Lambda function on a yearly schedule."
                },
                {
                    "label": "D",
                    "content": "Migrate the database to Amazon Neptune by using the AWS Schema Conversion Tool (AWS SCT). Create an Amazon CloudWatch alarm to invoke an AWS Lambda function for yearly password rotation."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the requirement to migrate an Oracle database from on-premises to AWS with yearly password rotation while minimizing operational overhead, the answer is:</p><p>B. Migrate the database to Amazon RDS for Oracle. Store the password in AWS Secrets Manager. Turn on automatic rotation. Configure a yearly rotation schedule.</p><p>This is the correct choice because:</p><p>Amazon RDS for Oracle is a managed database service that significantly reduces operational overhead by automating routine database tasks such as hardware provisioning, database setup, patching, and backups.</p><p>AWS Secrets Manager is specifically designed for storing and automatically rotating credentials, including database passwords. It has built-in functionality for automatic rotation on a schedule (such as yearly), which directly addresses the security requirement.</p><p>The integration between RDS for Oracle and Secrets Manager is seamless, allowing for automated password rotation without application downtime or manual intervention.</p><p>This solution requires the least amount of custom code or manual processes to maintain compared to the other options.</p><p>Let's compare with the alternatives:</p><p>Option C (EC2 with Systems Manager Parameter Store and Lambda) would require managing the EC2 instance, installing and configuring Oracle, and creating custom Lambda functions for password rotation. This involves significantly more operational overhead than the managed RDS solution.</p><p>Option D (Amazon Neptune with AWS SCT) would require not only migrating but also converting the Oracle database to a completely different database type (graph database), which would likely require application changes and is not a direct replacement for an Oracle relational database. This would involve the highest operational overhead and complexity.</p><p>The combination of Amazon RDS for Oracle and AWS Secrets Manager provides the most streamlined, automated solution with the least operational overhead while meeting the yearly password rotation requirement.</p><p>Sources</p><p>Strategies for Migrating Oracle Databases to AWS - Strategies for Migrating Oracle Databases to AWS（https://docs.aws.amazon.com/whitepapers/latest/strategies-migrating-oracle-db-to-aws/strategies-migrating-oracle-db-to-aws.html） </p><p>Migrate an on-premises Oracle database to Amazon RDS for Oracle - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-oracle-database-to-amazon-rds-for-oracle.html）</p><p>Oracle database migration strategies - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-oracle-database/strategies.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8ee384473e694912812ab77dc8568ddb",
            "questionNumber": 215,
            "type": "multiple",
            "content": "<p>Question #215</p><p>A solutions architect is designing an AWS account structure for a company that consists of multiple teams. All the teams will work in the same AWS Region. The company needs a VPC that is connected to the on-premises network. The company expects less than 50 Mbps of total traffic to and from the on-premises network. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to each AWS account."
                },
                {
                    "label": "B",
                    "content": "Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager."
                },
                {
                    "label": "C",
                    "content": "Use AWS Transit Gateway along with an AWS Site-to-Site VPN for connectivity to the on-premises network. Share the transit gateway by using AWS Resource Access Manager."
                },
                {
                    "label": "D",
                    "content": "Use AWS Site-to-Site VPN for connectivity to the on-premises network."
                },
                {
                    "label": "E",
                    "content": "Use AWS Direct Connect for connectivity to the on-premises network."
                }
            ],
            "correctAnswer": "BD",
            "explanation": "<p>Based on the requirements to design a cost-effective AWS account structure with on-premises connectivity for multiple teams with less than 50 Mbps of traffic, the answers are:</p><p>B. Create an AWS CloudFormation template that provisions a VPC and the required subnets. Deploy the template to a shared services account. Share the subnets by using AWS Resource Access Manager.</p><p>D. Use AWS Site-to-Site VPN for connectivity to the on-premises network.</p><p>These are the correct choices because:</p><p>Option B (Shared VPC with Resource Access Manager):</p><p>Creating a single VPC in a shared services account and sharing subnets using AWS Resource Access Manager is more cost-effective than creating separate VPCs in each team's account.</p><p>This approach reduces the number of networking resources to manage and maintain.</p><p>It centralizes network management while still allowing individual teams to deploy their resources in the shared subnets.</p><p>It improves IP address utilization and simplifies the overall network architecture.</p><p>Option D (AWS Site-to-Site VPN):</p><p>For the specified traffic requirement of less than 50 Mbps, AWS Site-to-Site VPN is significantly more cost-effective than Direct Connect.</p><p>Site-to-Site VPN provides encrypted connectivity over the internet without requiring dedicated hardware or circuits.</p><p>It's easier and faster to set up compared to Direct Connect, which requires physical connections and can take weeks to provision.</p><p>The monthly cost of Site-to-Site VPN is much lower than Direct Connect for this bandwidth requirement.</p><p>Option A (separate VPCs in each account) would increase complexity and cost by requiring multiple VPCs, potentially leading to IP address space fragmentation and more complex routing.</p><p>Option E (AWS Direct Connect) would be unnecessarily expensive for the stated bandwidth requirement of less than 50 Mbps. Direct Connect is more suitable for higher bandwidth needs or when consistent network performance is critical.</p><p>The combination of a shared VPC using Resource Access Manager and Site-to-Site VPN provides the most cost-effective solution that meets all the stated requirements.</p><p>Sources</p><p>VPC sharing - Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/amazon-vpc-sharing.html）</p><p>Transit gateway attachment configuration | AWS re:Post （https://repost.aws/questions/QUvX0QyO3pRQWkBQspwcgVKg/transit-gateway-attachment-configuration）</p><p>VPC peering - Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/vpc-peering.html）</p><p>Client VPN with VPC peering vs Client VPN withTransit gateway | AWS re:Post （https://repost.aws/questions/QUUotXjy2mTV6noMA-J86AXQ/client-vpn-with-vpc-peering-vs-client-vpn-withtransit-gateway）</p><p>AWS Direct Connect + AWS Transit Gateway + AWS Site-to-Site VPN - Amazon Virtual Private Cloud Connectivity Options （https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway-vpn.html）</p><p>Centralize network connectivity using AWS Transit Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/centralize-network-connectivity-using-aws-transit-gateway.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d9aa6ac6c0274e48a5e6ff89a5e240a6",
            "questionNumber": 216,
            "type": "single",
            "content": "<p>Question #216</p><p>A solutions architect at a large company needs to set up network security for outbound traffic to the internet from all AWS accounts within an organization in AWS Organizations. The organization has more than 100 AWS accounts, and the accounts route to each other by using a centralized AWS Transit Gateway. Each account has both an internet gateway and a NAT gateway for outbound traffic to the internet. The company deploys resources only into a single AWS Region. </p><p><br></p><p>The company needs the ability to add centrally managed rule-based filtering on all outbound traffic to the internet for all AWS accounts in the organization. The peak load of outbound traffic will not exceed 25 Gbps in each Availability Zone. </p><p><br></p><p>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Create an Auto Scaling group of Amazon EC2 instances that run an open-source internet proxy for rule-based filtering across all Availability Zones in the Region. Modify all default routes to point to the proxy&#39;s Auto Scaling group."
                },
                {
                    "label": "B",
                    "content": "Create a new VPC for outbound traffic to the internet. Connect the existing transit gateway to the new VPC. Configure a new NAT gateway. Use an AWS Network Firewall firewall for rule-based filtering. Create Network Firewall endpoints in each Availability Zone. Modify all default routes to point to the Network Firewall endpoints."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Network Firewall firewall for rule-based filtering in each AWS account. Modify all default routes to point to the Network Firewall firewalls in each account."
                },
                {
                    "label": "D",
                    "content": "In each AWS account, create an Auto Scaling group of network-optimized Amazon EC2 instances that run an open-source internet proxy for rule-based filtering. Modify all default routes to point to the proxy&#39;s Auto Scaling group."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. By creating a new VPC dedicated to outbound internet traffic and connecting it to the existing transit gateway, the company can centralize the management of outbound traffic. Configuring a new NAT gateway within this VPC ensures that traffic to the internet is properly routed and translated. The use of AWS Network Firewall allows for rule-based filtering, and by creating endpoints in each Availability Zone, the solution provides a scalable and highly available architecture. Modifying the default routes to point to the Network Firewall endpoints ensures that all outbound traffic from the VPCs is subject to the filtering policies, meeting the company's security requirements with minimal operational overhead.</p><p>The requirements are: &nbsp;</p><p>1. Centrally managed rule-based filtering for outbound internet traffic across 100+ AWS accounts. &nbsp;</p><p>2. Peak traffic ≤ 25 Gbps per Availability Zone (AWS Network Firewall supports up to 30 Gbps per endpoint). &nbsp;</p><p>3. Single AWS Region deployment with a centralized Transit Gateway for inter-account routing. &nbsp;</p><p>4. No open-source proxy maintenance (AWS Network Firewall is a managed service). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>✅ Centralized Security VPC: A dedicated VPC with AWS Network Firewall provides rule-based filtering for all outbound traffic. &nbsp;</p><p>✅ Transit Gateway Integration: The existing Transit Gateway connects all accounts to the centralized firewall. &nbsp;</p><p>✅ Scalability: Network Firewall endpoints in each AZ handle up to 30 Gbps, meeting the 25 Gbps requirement. &nbsp;</p><p>✅ No Proxy Maintenance: Unlike open-source proxies (Option A & D), AWS Network Firewall is fully managed. &nbsp;</p><p>✅ Route Modification: Default routes in all VPCs point to the Network Firewall endpoints, enforcing filtering. &nbsp;</p><p> Why Other Options Are Wrong? &nbsp;</p><p>❌ A: Open-source proxy on EC2 requires management overhead and lacks AWS’s scalability. &nbsp;</p><p>❌ C: Deploying Network Firewall per account is not centralized and increases management complexity. &nbsp;</p><p>❌ D: Open-source proxies on EC2 lack centralized control and require manual scaling. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the most scalable, manageable, and AWS-native solution for centralized outbound traffic filtering. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "700281c1cc8f4e1b9f991115453ea0c8",
            "questionNumber": 217,
            "type": "single",
            "content": "<p>Question #217</p><p>A company uses a load balancer to distribute traffic to Amazon EC2 instances in a single Availability Zone. The company is concerned about security and wants a solutions architect to re-architect the solution to meet the following requirements:：</p><p><br></p><p>- Inbound requests must be filtered for common vulnerability attacks.</p><p>- Rejected requests must be sent to a third-party auditing application.</p><p>- All resources should be highly available.</p><p><br></p><p>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure a Multi-AZ Auto Scaling group using the application&#39;s AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Use Amazon Inspector to monitor traffic to the ALB and EC2 instances. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB. Use an AWS Lambda function to frequently push the Amazon Inspector report to the third-party auditing application."
                },
                {
                    "label": "B",
                    "content": "Configure an Application Load Balancer (ALB) and add the EC2 instances as targets. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB name and enable logging with Amazon CloudWatch Logs. Use an AWS Lambda function to frequently push the logs to the third-party auditing application."
                },
                {
                    "label": "C",
                    "content": "Configure an Application Load Balancer (ALB) along with a target group adding the EC2 instances as targets. Create an Amazon Kinesis Data Firehose with the destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the web ACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber."
                },
                {
                    "label": "D",
                    "content": "Configure a Multi-AZ Auto Scaling group using the application&#39;s AMI. Create an Application Load Balancer (ALB) and select the previously created Auto Scaling group as the target. Create an Amazon Kinesis Data Firehose with a destination of the third-party auditing application. Create a web ACL in WAF. Create an AWS WAF using the WebACL and ALB then enable logging by selecting the Kinesis Data Firehose as the destination. Subscribe to AWS Managed Rules in AWS Marketplace, choosing the WAF as the subscriber."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. This solution ensures high availability by using a Multi-AZ Auto Scaling group, which allows the application to remain available even if one Availability Zone goes down. The Application Load Balancer (ALB) distributes traffic to the EC2 instances. AWS WAF (Web Application Firewall) is used to filter the inbound requests for common vulnerability attacks, and a web ACL (Web Access Control List) is created to define the rules for these filters. The use of Amazon Kinesis Data Firehose allows the rejected requests to be sent to the third-party auditing application. Additionally, enabling logging with the Kinesis Data Firehose as the destination ensures that logs are sent to the auditing application. Subscribing to AWS Managed Rules in AWS Marketplace provides additional security measures by leveraging predefined security rules.</p><p>1. Inbound requests filtered for common vulnerability attacks: &nbsp;</p><p> &nbsp; - AWS WAF with a web ACL and AWS Managed Rules (from AWS Marketplace) helps filter and block common web exploits like SQL injection, XSS, etc.</p><p>2. Rejected requests sent to a third-party auditing application: &nbsp;</p><p> &nbsp; - Enabling WAF logging and configuring Kinesis Data Firehose to deliver logs to the third-party auditing application fulfills this requirement.</p><p>3. High availability: &nbsp;</p><p> &nbsp; - A Multi-AZ Auto Scaling group ensures that EC2 instances are distributed across multiple Availability Zones, providing fault tolerance. &nbsp;</p><p> &nbsp; - The ALB also inherently supports high availability by distributing traffic across healthy instances.</p><p> Why not the other options?</p><p>- A: Uses Amazon Inspector, which is for vulnerability assessment of EC2 instances, not real-time traffic filtering. Also, Lambda pushing reports is less efficient than direct logging via Kinesis Firehose.</p><p>- B: Lacks Multi-AZ Auto Scaling, risking availability. Also, CloudWatch Logs + Lambda is less direct than Kinesis Firehose for log delivery.</p><p>- C: Misses Multi-AZ Auto Scaling, compromising high availability.</p><p> Correct Answer: D ✅ &nbsp;</p><p>This solution covers WAF filtering, log delivery via Kinesis Firehose, and high availability with Multi-AZ Auto Scaling.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5329b80a37bd474b9d97871e17a127b0",
            "questionNumber": 218,
            "type": "single",
            "content": "<p>Question #218</p><p>A company is running an application in the AWS Cloud. The application consists of microservices that run on a fleet of Amazon EC2 instances in multiple Availability Zones behind an Application Load Balancer. The company recently added a new REST API that was implemented in Amazon API Gateway. Some of the older microservices that run on EC2 instances need to call this new API. </p><p><br></p><p>The company does not want the API to be accessible from the public internet and does not want proprietary data to traverse the public internet. </p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Site-to-Site VPN connection between the VPC and the API Gateway. Use API Gateway to generate a unique API Key for each microservice. Configure the API methods to require the key."
                },
                {
                    "label": "B",
                    "content": "Create an interface VPC endpoint for API Gateway, and set an endpoint policy to only allow access to the specific API. Add a resource policy to API Gateway to only allow access from the VPC endpoint. Change the API Gateway endpoint type to private."
                },
                {
                    "label": "C",
                    "content": "Modify the API Gateway to use IAM authentication. Update the IAM policy for the IAM role that is assigned to the EC2 instances to allow access to the API Gateway. Move the API Gateway into a new VPC. Deploy a transit gateway and connect the VPCs."
                },
                {
                    "label": "D",
                    "content": "Create an accelerator in AWS Global Accelerator, and connect the accelerator to the API Gateway. Update the route table for all VPC subnets with a route to the created Global Accelerator endpoint IP address. Add an API key for each service to use for authentication."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. By creating an interface VPC endpoint for API Gateway, the architect can establish a private connection between the VPC and the API Gateway, ensuring that the API is not accessible from the public internet. Setting an endpoint policy to only allow access to the specific API and adding a resource policy to API Gateway to only allow access from the VPC endpoint further enhances security. Changing the API Gateway endpoint type to private ensures that the API is not exposed to the public internet, thus protecting proprietary data from traversing the public internet.</p><p>1. The API should not be accessible from the public internet.</p><p>2. Proprietary data should not traverse the public internet.</p><p>3. Only the older microservices (EC2 instances) should be able to call the new API.</p><p> Solution Breakdown (Option B):</p><p>1. Create an interface VPC endpoint for API Gateway &nbsp;</p><p> &nbsp; - This allows private connectivity between the VPC (where EC2 instances are running) and API Gateway without going over the public internet.</p><p> &nbsp; - The endpoint policy restricts access to only the specific API.</p><p>2. Set an API Gateway resource policy to allow access only from the VPC endpoint &nbsp;</p><p> &nbsp; - Ensures that only requests originating from the VPC (via the endpoint) can access the API.</p><p>3. Change the API Gateway endpoint type to private &nbsp;</p><p> &nbsp; - This makes the API accessible only via the VPC endpoint, blocking public internet access entirely.</p><p> Why Not the Other Options?</p><p>- A: A VPN is unnecessary since API Gateway supports private VPC endpoints. Also, API keys alone don't enforce private network access.</p><p>- C: IAM authentication is good for access control, but it doesn't prevent traffic from going over the internet unless combined with a private endpoint (which is already covered in Option B).</p><p>- D: Global Accelerator improves performance but still uses public IPs, so data traverses the internet, violating the requirement.</p><p> Conclusion:</p><p>Option B is the most secure and efficient solution, ensuring private connectivity between the EC2 instances and API Gateway without exposing the API to the internet.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "81d294e5725c458781ab8870674669e3",
            "questionNumber": 219,
            "type": "single",
            "content": "<p>Question #219</p><p>A company has set up its entire infrastructure on AWS. The company uses Amazon EC2 instances to host its ecommerce website and uses Amazon S3 to store static data. Three engineers at the company handle the cloud administration and development through one AWS account. Occasionally, an engineer alters an EC2 security group configuration of another engineer and causes noncompliance issues in the environment. </p><p><br></p><p>A solutions architect must set up a system that tracks changes that the engineers make. The system must send alerts when the engineers make noncompliant changes to the security settings for the EC2 instances. </p><p><br></p><p>What is the FASTEST way for the solutions architect to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up AWS Organizations for the company. Apply SCPs to govern and track noncompliant security group changes that are made to the AWS account."
                },
                {
                    "label": "B",
                    "content": "Enable AWS CloudTrail to capture the changes to EC2 security groups. Enable Amazon CloudWatch rules to provide alerts when noncompliant security settings are detected."
                },
                {
                    "label": "C",
                    "content": "Enable SCPs on the AWS account to provide alerts when noncompliant security group changes are made to the environment."
                },
                {
                    "label": "D",
                    "content": "Enable AWS Config on the EC2 security groups to track any noncompliant changes. Send the changes as alerts through an Amazon Simple Notification Service (Amazon SNS) topic."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the fastest way to meet the requirements. AWS Config is designed to track and evaluate the configurations of AWS resources, including EC2 security groups. By enabling AWS Config, the architect can quickly set up monitoring and alerting for configuration changes. Any noncompliant changes to the EC2 security groups can be sent as alerts through an Amazon SNS topic, allowing for immediate notification when changes occur. This approach provides a direct and efficient method for tracking changes and ensuring compliance, without the need for additional infrastructure or more complex setups.</p><p>- AWS Config is specifically designed to track configuration changes and assess compliance against predefined rules. It can detect noncompliant changes to EC2 security groups and trigger alerts via Amazon SNS.</p><p>- AWS CloudTrail (Option B) logs API calls but does not directly assess compliance or provide alerts without additional CloudWatch rules and custom logic.</p><p>- SCPs (Options A & C) are used in AWS Organizations to restrict permissions but do not track changes or send alerts.</p><p>- Option D provides a direct, automated solution for tracking and alerting on noncompliant security group changes without requiring complex setups.</p><p>Thus, Option D is the most efficient and fastest solution. </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "75beb12673b34e92b377dcd2a595eeed",
            "questionNumber": 220,
            "type": "multiple",
            "content": "<p>Question #220</p><p>A company has IoT sensors that monitor traffic patterns throughout a large city. The company wants to read and collect data from the sensors and perform aggregations on the data. A solutions architect designs a solution in which the IoT devices are streaming to Amazon Kinesis Data Streams. Several applications are reading from the stream. However, several consumers are experiencing throttling and are periodically encountering a ReadProvisionedThroughputExceeded error. Which actions should the solutions architect take to resolve this issue? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Reshard the stream to increase the number of shards in the stream."
                },
                {
                    "label": "B",
                    "content": "Use the Kinesis Producer Library (KPL). Adjust the polling frequency."
                },
                {
                    "label": "C",
                    "content": "Use consumers with the enhanced fan-out feature."
                },
                {
                    "label": "D",
                    "content": "Reshard the stream to reduce the number of shards in the stream."
                },
                {
                    "label": "E",
                    "content": "Use an error retry and exponential backoff mechanism in the consumer logic."
                },
                {
                    "label": "F",
                    "content": "Configure the stream to use dynamic partitioning."
                }
            ],
            "correctAnswer": "ACE",
            "explanation": "<p>Based on the scenario where IoT sensors are streaming data to Kinesis Data Streams and multiple applications are experiencing ReadProvisionedThroughputExceeded errors, the correct actions to resolve this issue are:</p><p>A. Reshard the stream to increase the number of shards in the stream. </p><p>C. Use consumers with the enhanced fan-out feature. </p><p>E. Use an error retry and exponential backoff mechanism in the consumer logic.</p><p>Here's why these are the correct choices:</p><p>A. Reshard the stream to increase the number of shards in the stream:</p><p>Each Kinesis shard provides a limited read throughput (2 MB/sec shared among all standard consumers).</p><p>When multiple applications are reading from the same stream, they're competing for this limited throughput.</p><p>Increasing the number of shards distributes the data and read operations across more shards, providing more total throughput capacity for the stream.</p><p>This directly addresses the ReadProvisionedThroughputExceeded error by providing more resources.</p><p>C. Use consumers with the enhanced fan-out feature:</p><p>Enhanced fan-out allows each consumer to get dedicated throughput of up to 2 MB/sec per shard.</p><p>This eliminates contention between consumers since each gets its own dedicated throughput.</p><p>Enhanced fan-out is specifically designed for scenarios with multiple consumers reading from the same stream.</p><p>This feature significantly reduces the likelihood of throughput-related errors.</p><p>E. Use an error retry and exponential backoff mechanism in the consumer logic:</p><p>Even with proper provisioning, occasional throttling can still occur.</p><p>Implementing retry logic with exponential backoff is a best practice for handling transient errors.</p><p>This approach allows the application to gracefully handle temporary throughput issues.</p><p>The exponential backoff prevents overwhelming the system with immediate retries.</p><p>The other options are incorrect because:</p><p>B (Using KPL and adjusting polling frequency): The Kinesis Producer Library is for producers, not consumers, so it wouldn't help with read throughput issues.</p><p>D (Reducing the number of shards): This would decrease available throughput and make the problem worse.</p><p>F (Dynamic partitioning): This is a feature of Kinesis Data Firehose, not Kinesis Data Streams, and wouldn't address the read throughput issue.</p><p>Sources</p><p>Troubleshoot Kinesis Data Streams consumers - Amazon Kinesis Data Streams （https://docs.aws.amazon.com/streams/latest/dev/troubleshooting-consumers.html）</p><p>GetRecords - Amazon Kinesis Data Streams Service （https://docs.aws.amazon.com/kinesis/latest/APIReference/API_GetRecords.html）</p><p>Use Amazon Kinesis Data Streams to deliver real-time data to Amazon OpenSearch Service domains with Amazon OpenSearch Ingestion | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/use-amazon-kinesis-data-streams-to-deliver-real-time-data-to-amazon-opensearch-service-domains-with-amazon-opensearch-ingestion/）</p><p>Handle startup, shutdown, and throttling - Amazon Kinesis Data Streams （https://docs.aws.amazon.com/streams/latest/dev/kinesis-record-processor-additional-considerations.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "be8b4dddfcc34da6be184b512bfd9199",
            "questionNumber": 221,
            "type": "single",
            "content": "<p>Question #221</p><p>A company uses AWS Organizations to manage its AWS accounts. The company needs a list of all its Amazon EC2 instances that have underutilized CPU or memory usage. The company also needs recommendations for how to downsize these underutilized instances. </p><p><br></p><p>Which solution will meet these requirements with the LEAST effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Install a CPU and memory monitoring tool from AWS Marketplace on all the EC2 instances. Store the findings in Amazon S3. Implement a Python script to identify underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options."
                },
                {
                    "label": "B",
                    "content": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in the organization&rsquo;s management account. Use the recommendations to downsize underutilized instances in all accounts of the organization."
                },
                {
                    "label": "C",
                    "content": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in each account of the organization. Use the recommendations to downsize underutilized instances in all accounts of the organization."
                },
                {
                    "label": "D",
                    "content": "Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Create an AWS Lambda function to extract CPU and memory usage from all the EC2 instances. Store the findings as files in Amazon S3. Use Amazon Athena to find underutilized instances. Reference EC2 instance pricing information for recommendations about downsizing options."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the requirement to get a list of underutilized EC2 instances across an organization with recommendations for downsizing with the least effort, the answer is:</p><p>B. Install the Amazon CloudWatch agent on all the EC2 instances by using AWS Systems Manager. Retrieve the resource optimization recommendations from AWS Cost Explorer in the organization's management account. Use the recommendations to downsize underutilized instances in all accounts of the organization.</p><p>This is the correct choice because:</p><p>Installing the CloudWatch agent using AWS Systems Manager allows for efficient collection of detailed memory and CPU metrics across all EC2 instances in the organization. Systems Manager provides a centralized way to deploy and configure the agent across multiple accounts.</p><p>AWS Cost Explorer in the organization's management account provides consolidated resource optimization recommendations for all member accounts in the AWS Organizations structure. This centralized approach allows you to view and analyze recommendations for all accounts from a single location.</p><p>Accessing the recommendations from the management account requires significantly less effort than accessing each individual account separately, as you only need to log in once and can see all recommendations in one place.</p><p>Option C suggests retrieving recommendations from each individual account in the organization, which would require logging into each account separately and manually consolidating the data. This approach would require much more effort, especially for organizations with many accounts.</p><p>The centralized approach in option B provides a comprehensive view of all underutilized instances across the organization from a single access point, making it the solution that requires the least effort to meet the company's requirements.</p><p>Sources</p><p>AWS Cost Optimization Hub and AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-coh.html）</p><p>Optimize Your AWS Spend with New Cost Savings Features in AWS Trusted Advisor | AWS Cloud Financial Management （https://aws.amazon.com/cn/blogs/aws-cloud-financial-management/optimize-your-aws-spend-with-new-cost-savings-features-in-aws-trusted-advisor/）</p><p>AWS Cost Optimization | AWS Cloud Financial Management （https://aws.amazon.com/cn/aws-cost-management/cost-optimization-hub/）</p><p>Best practices to optimize costs after mergers and acquisitions with AWS Organizations | AWS Cloud Operations Blog（https://aws.amazon.com/cn/blogs/mt/best-practices-to-optimize-costs-after-mergers-and-acquisitions-with-aws-organizations/） </p><p>Compute Optimizer Dashboard - Guidance for Advanced Cloud Observability with Cloud Intelligence Dashboards on AWS （https://docs.aws.amazon.com/guidance/latest/advanced-cloud-observability-with-cloud-intelligence-dashboard/compute-optimizer-dashboard.html）</p><p>Optimizing your cost with rightsizing recommendations - AWS Cost Management （https://docs.aws.amazon.com/cost-management/latest/userguide/ce-rightsizing.html）</p><p>Cost optimization recommendations (from Cost Optimization Hub) - AWS Data Exports（https://docs.aws.amazon.com/cur/latest/userguide/table-dictionary-cor.html） </p><p>Cost Optimization Hub and AWS Organizations trusted access - AWS Cost Management （https://docs.aws.amazon.com/cost-management/latest/userguide/coh-trusted-access.html）</p><p>Getting started with Cost Optimization Hub - AWS Cost Management （https://docs.aws.amazon.com/cost-management/latest/userguide/coh-getting-started.html）</p><p>CUR report for Organisation | AWS re:Post （https://repost.aws/questions/QURjr672h1QPGj4ja2fGGnRA/cur-report-for-organisation）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a93e7c2ddffb4974af781d9ca2be28ca",
            "questionNumber": 222,
            "type": "multiple",
            "content": "<p>Question #222</p><p>A company wants to run a custom network analysis software package to inspect traffic as traffic leaves and enters a VPC. The company has deployed the solution by using AWS CloudFormation on three Amazon EC2 instances in an Auto Scaling group. All network routing has been established to direct traffic to the EC2 instances. Whenever the analysis software stops working, the Auto Scaling group replaces an instance. The network routes are not updated when the instance replacement occurs. Which combination of steps will resolve this issue? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create alarms based on EC2 status check metrics that will cause the Auto Scaling group to replace the failed instance."
                },
                {
                    "label": "B",
                    "content": "Update the CloudFormation template to install the Amazon CloudWatch agent on the EC2 instances. Configure the CloudWatch agent to send process metrics for the application."
                },
                {
                    "label": "C",
                    "content": "Update the CloudFormation template to install AWS Systems Manager Agent on the EC2 instances. Configure Systems Manager Agent to send process metrics for the application."
                },
                {
                    "label": "D",
                    "content": "Create an alarm for the custom metric in Amazon CloudWatch for the failure scenarios. Configure the alarm to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic."
                },
                {
                    "label": "E",
                    "content": "Create an AWS Lambda function that responds to the Amazon Simple Notification Service (Amazon SNS) message to take the instance out of service. Update the network routes to point to the replacement instance."
                },
                {
                    "label": "F",
                    "content": "In the CloudFormation template, write a condition that updates the network routes when a replacement instance is launched."
                }
            ],
            "correctAnswer": "BDE",
            "explanation": "<p>The correct combination of steps to resolve the issue is B, D, and E. Option B involves updating the CloudFormation template to include the CloudWatch agent, which can monitor the health of the application running on the EC2 instances. Option D involves setting up an alarm in CloudWatch that triggers when the custom metric, indicative of the application's failure, is reached. This alarm can be configured to send a message to an SNS topic. Option E involves creating a Lambda function that is triggered by the SNS message. This function can then execute the logic required to update the network routes to point to the new instance launched by the Auto Scaling group, ensuring that the traffic is directed to the healthy instance.</p><p>- B: Install the Amazon CloudWatch agent to monitor the application process and send custom metrics to CloudWatch. This ensures we detect software failures, not just EC2-level issues. &nbsp;</p><p>- D: Create a CloudWatch alarm for the custom metric (application failure) and configure it to publish to an SNS topic. This triggers a notification when the software fails. &nbsp;</p><p>- E: Use a Lambda function subscribed to the SNS topic to update network routes when a replacement instance is launched. This ensures traffic is redirected properly. &nbsp;</p><p> Why not the others?</p><p>- A: EC2 status checks only detect instance-level failures, not application-level failures. &nbsp;</p><p>- C: Systems Manager Agent (SSM) is useful for management but not directly for triggering Auto Scaling or route updates. &nbsp;</p><p>- F: CloudFormation conditions are evaluated at stack creation/update, not dynamically during Auto Scaling events. &nbsp;</p><p>Thus, B, D, E is the correct combination.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "984db2f3ea204264aa5846723a951f01",
            "questionNumber": 223,
            "type": "single",
            "content": "<p>Question #223</p><p>A company is developing a new on-demand video application that is based on microservices. The application will have 5 million users at launch and will have 30 million users after 6 months. The company has deployed the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. The company developed the application by using ECS services that use the HTTPS protocol. </p><p><br></p><p>A solutions architect needs to implement updates to the application by using blue/green deployments. The solution must distribute traffic to each ECS service through a load balancer. The application must automatically adjust the number of tasks in response to an Amazon CloudWatch alarm. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Request increases to the service quota for tasks per service to meet the demand."
                },
                {
                    "label": "B",
                    "content": "Configure the ECS services to use the blue/green deployment type and a Network Load Balancer. Implement Auto Scaling group for each ECS service by using the Cluster Autoscaler."
                },
                {
                    "label": "C",
                    "content": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement an Auto Scaling group for each ECS service by using the Cluster Autoscaler."
                },
                {
                    "label": "D",
                    "content": "Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for each ECS service."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. Using an Application Load Balancer (ALB) is suitable for handling HTTPS traffic, which is required by the application. The blue/green deployment type ensures that updates can be implemented with minimal downtime. Service Auto Scaling, as mentioned in option D, allows each ECS service to automatically adjust the number of tasks in response to CloudWatch alarms, which aligns with the requirement for automatic scaling based on traffic demand. The Cluster Autoscaler is not needed for ECS services running on AWS Fargate, as Fargate manages the infrastructure scaling automatically.</p><p>1. Blue/Green Deployment with ECS: &nbsp;</p><p> &nbsp; - AWS supports blue/green deployments natively in Amazon ECS when using CodeDeploy. This allows traffic to be shifted gradually between the old (blue) and new (green) versions of the service.</p><p> &nbsp; - The Application Load Balancer (ALB) is required (not Network Load Balancer) because it supports path-based routing, host-based routing, and integration with ECS blue/green deployments via CodeDeploy.</p><p>2. Application Load Balancer (ALB) vs. Network Load Balancer (NLB): &nbsp;</p><p> &nbsp; - The question mentions the application uses HTTPS, which is best suited for ALB (Layer 7 load balancing) rather than NLB (Layer 4). &nbsp;</p><p> &nbsp; - ALB also integrates seamlessly with ECS blue/green deployments.</p><p>3. Auto Scaling for ECS Services: &nbsp;</p><p> &nbsp; - Service Auto Scaling (using Target Tracking Policies or Step Scaling) is the correct way to automatically adjust the number of tasks in response to a CloudWatch alarm. &nbsp;</p><p> &nbsp; - The Cluster Autoscaler (mentioned in options B and C) is used for scaling the underlying EC2 instances in an ECS cluster (if using EC2 launch type), but the question specifies AWS Fargate, which is serverless and does not require EC2 scaling. &nbsp;</p><p> &nbsp; - Service Auto Scaling is the proper way to scale tasks (containers) in Fargate.</p><p> Why Not the Other Options?</p><p>- A & B: Incorrect because they suggest using a Network Load Balancer (NLB), which is not optimal for HTTPS-based microservices (ALB is better for Layer 7 traffic). &nbsp;</p><p>- B & C: Incorrect because they mention Cluster Autoscaler, which is irrelevant for Fargate (only applies to EC2 launch type). &nbsp;</p><p>- C: Incorrect because it suggests using the Cluster Autoscaler instead of Service Auto Scaling.</p><p> Final Answer:</p><p>D. Configure the ECS services to use the blue/green deployment type and an Application Load Balancer. Implement Service Auto Scaling for each ECS service.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a35751c009744884a14a7858161c1026",
            "questionNumber": 224,
            "type": "single",
            "content": "<p>Question #224</p><p>A company is running a containerized application in the AWS Cloud. The application is running by using Amazon Elastic Container Service (Amazon ECS) on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group. </p><p><br></p><p>The company uses Amazon Elastic Container Registry (Amazon ECR) to store its container images. When a new image version is uploaded, the new image version receives a unique tag. </p><p><br></p><p>The company needs a solution that inspects new image versions for common vulnerabilities and exposures. The solution must automatically delete new image tags that have Critical or High severity findings. The solution also must notify the development team when such a deletion occurs. </p><p><br></p><p>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure scan on push on the repository. Use Amazon EventBridge to invoke an AWS Step Functions state machine when a scan is complete for images that have Critical or High severity findings. Use the Step Functions state machine to delete the image tag for those images and to notify the development team through Amazon Simple Notification Service (Amazon SNS)."
                },
                {
                    "label": "B",
                    "content": "Configure scan on push on the repository. Configure scan results to be pushed to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Lambda function when a new message is added to the SQS queue. Use the Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES)."
                },
                {
                    "label": "C",
                    "content": "Schedule an AWS Lambda function to start a manual image scan every hour. Configure Amazon EventBridge to invoke another Lambda function when a scan is complete. Use the second Lambda function to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Notification Service (Amazon SNS)."
                },
                {
                    "label": "D",
                    "content": "Configure periodic image scan on the repository. Configure scan results to be added to an Amazon Simple Queue Service (Amazon SQS) queue. Invoke an AWS Step Functions state machine when a new message is added to the SQS queue. Use the Step Functions state machine to delete the image tag for images that have Critical or High severity findings. Notify the development team by using Amazon Simple Email Service (Amazon SES)."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer. It provides an automated solution where the scan on push is configured for the ECR repository, and Amazon EventBridge is used to trigger a Step Functions state machine upon completion of the scan. If the scan identifies Critical or High severity issues, the state machine can then execute steps to delete the problematic image tag and notify the development team via Amazon SNS. This approach ensures that the process is automatic, efficient, and provides clear communication to the development team about the action taken.</p><p>The requirements are: &nbsp;</p><p>1. Scan new image versions for vulnerabilities – This is achieved by enabling scan on push in Amazon ECR. &nbsp;</p><p>2. Automatically delete image tags with Critical/High severity findings – Amazon EventBridge can detect scan completion events and trigger a Step Functions workflow to delete the image tag. &nbsp;</p><p>3. Notify the development team – The Step Functions workflow can use Amazon SNS to send notifications. &nbsp;</p><p> Why Option A is correct: &nbsp;</p><p>- Scan on push ensures images are scanned immediately upon upload. &nbsp;</p><p>- EventBridge can capture the ECR Image Scan event and trigger Step Functions when Critical/High findings are detected. &nbsp;</p><p>- Step Functions can orchestrate both the deletion of the image tag and sending an SNS notification. &nbsp;</p><p> Why other options are incorrect: &nbsp;</p><p>- B: While SQS can be used, it adds unnecessary complexity compared to EventBridge. Also, SES is not the best choice for real-time notifications (SNS is preferred). &nbsp;</p><p>- C: Manual scanning every hour is inefficient and does not guarantee immediate action on new image uploads. &nbsp;</p><p>- D: Periodic scans (instead of scan-on-push) introduce delays, and using SQS + SES is less efficient than EventBridge + SNS. &nbsp;</p><p>Thus, Option A is the most efficient and fully automated solution. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "9b7a5fcbc09c47f6a36fb24bcf4be26b",
            "questionNumber": 225,
            "type": "single",
            "content": "<p>Question #225</p><p>A company runs many workloads on AWS and uses AWS Organizations to manage its accounts. The workloads are hosted on Amazon EC2, AWS Fargate, and AWS Lambda. Some of the workloads have unpredictable demand. Accounts record high usage in some months and low usage in other months. </p><p><br></p><p>The company wants to optimize its compute costs over the next 3 years. A solutions architect obtains a 6-month average for each of the accounts across the organization to calculate usage. </p><p><br></p><p>Which solution will provide the MOST cost savings for all the organization's compute usage?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Purchase Reserved Instances for the organization to match the size and number of the most common EC2 instances from the member accounts."
                },
                {
                    "label": "B",
                    "content": "Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level."
                },
                {
                    "label": "C",
                    "content": "Purchase Reserved Instances for each member account that had high EC2 usage according to the data from the last 6 months."
                },
                {
                    "label": "D",
                    "content": "Purchase an EC2 Instance Savings Plan for each member account from the management account based on EC2 usage data from the last 6 months."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the scenario where a company runs workloads on EC2, Fargate, and Lambda with unpredictable demand across AWS Organizations, the solution that will provide the MOST cost savings is:</p><p>B. Purchase a Compute Savings Plan for the organization from the management account by using the recommendation at the management account level.</p><p>This is the correct choice because:</p><p>Compute Savings Plans offer the most flexibility across the company's diverse compute services (EC2, Fargate, and Lambda). Unlike Reserved Instances which only apply to EC2, Compute Savings Plans automatically apply to all eligible compute usage regardless of service type.</p><p>Compute Savings Plans are ideal for workloads with unpredictable demand, as mentioned in the scenario. They provide discounts in exchange for a commitment to a consistent amount of compute usage (measured in $/hour) rather than committing to specific instance types or configurations.</p><p>When purchased at the organization level from the management account, Compute Savings Plans automatically apply across all member accounts. This provides maximum flexibility as usage can shift between accounts without losing the discount benefits.</p><p>For workloads that vary month to month (high usage in some months, low in others), Compute Savings Plans will automatically apply to whatever eligible compute resources are being used, maximizing the utilization of the commitment.</p><p>Option A (Reserved Instances at the organization level) would be less effective because:</p><p>Reserved Instances only apply to EC2, not Fargate or Lambda</p><p>They are less flexible for unpredictable workloads as they're tied to specific instance families and sizes</p><p>They would require more management to ensure full utilization</p><p>Option C (Reserved Instances for individual member accounts) would be the least effective because:</p><p>It doesn't allow for sharing discounts across accounts when usage patterns shift</p><p>It requires more management overhead</p><p>It doesn't cover Fargate or Lambda usage</p><p>Compute Savings Plans at the organization level provide the optimal balance of flexibility, coverage across multiple compute services, and administrative simplicity for the company's diverse and unpredictable workloads.</p><p>Sources</p><p>Optimize spending for Windows on Amazon EC2 - AWS Prescriptive Guidance（https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/savings-plans.html） </p><p>COST07-BP01 Perform pricing model analysis - Cost Optimization Pillar （https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_pricing_model_analysis.html）</p><p>COST07-BP01 Perform pricing model analysis - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/latest/framework/cost_pricing_model_analysis.html）</p><p>My boss asking me on cost optimization in AWS | AWS re:Post （https://repost.aws/questions/QU3MV37BYRR6ePI6WmUgOW0g/my-boss-asking-me-on-cost-optimization-in-aws）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "07b94f7a554146509d3bfbbedbf7e6b0",
            "questionNumber": 226,
            "type": "single",
            "content": "<p>Question #226</p><p>A company has hundreds of AWS accounts. The company uses an organization in AWS Organizations to manage all the accounts. The company has turned on all features. </p><p><br></p><p>A finance team has allocated a daily budget for AWS costs. The finance team must receive an email notification if the organization's AWS costs exceed 80% of the allocated budget. A solutions architect needs to implement a solution to track the costs and deliver the notifications. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "In the organization&#39;s management account, use AWS Budgets to create a budget that has a daily period. Add an alert threshold and set the value to 80%. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team."
                },
                {
                    "label": "B",
                    "content": "In the organization&rsquo;s management account, set up the organizational view feature for AWS Trusted Advisor. Create an organizational view report for cost optimization. Set an alert threshold of 80%. Configure notification preferences. Add the email addresses of the finance team."
                },
                {
                    "label": "C",
                    "content": "Register the organization with AWS Control Tower. Activate the optional cost control (guardrail). Set a control (guardrail) parameter of 80%. Configure control (guardrail) notification preferences. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team."
                },
                {
                    "label": "D",
                    "content": "Configure the member accounts to save a daily AWS Cost and Usage Report to an Amazon S3 bucket in the organization&#39;s management account. Use Amazon EventBridge to schedule a daily Amazon Athena query to calculate the organization&rsquo;s costs. Configure Athena to send an Amazon CloudWatch alert if the total costs are more than 80% of the allocated budget. Use Amazon Simple Notification Service (Amazon SNS) to notify the finance team."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer. AWS Budgets can be used to create a budget for the organization's AWS costs with a daily time period. By setting an alert threshold at 80% of the allocated budget, the finance team can be notified when costs are approaching the limit. Amazon SNS can be used to send email notifications to the finance team when the threshold is breached, providing a direct and automated method for cost monitoring and alerting.</p><p>AWS Budgets is the most appropriate service for this scenario because it allows you to: &nbsp;</p><p>- Set custom budgets to track costs across an entire AWS Organization. &nbsp;</p><p>- Define alert thresholds (e.g., 80% of the daily budget). &nbsp;</p><p>- Configure notifications via Amazon SNS to alert the finance team when costs exceed the threshold. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- B: AWS Trusted Advisor’s organizational view does not provide real-time budget alerts based on a percentage threshold. It focuses on cost optimization recommendations, not budget tracking. &nbsp;</p><p>- C: AWS Control Tower’s guardrails are not designed for daily budget tracking with percentage-based alerts. They enforce compliance rules rather than financial thresholds. &nbsp;</p><p>- D: While technically possible, this solution is overly complex compared to AWS Budgets, which provides built-in budget tracking and alerting without requiring manual queries or additional services like Athena and EventBridge. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option A is the simplest and most effective solution for tracking AWS costs and sending notifications when they exceed 80% of the daily budget. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "57349235830841df9fd35801ff26a516",
            "questionNumber": 227,
            "type": "single",
            "content": "<p>Question #227</p><p>A company provides auction services for artwork and has users across North America and Europe. The company hosts its application in Amazon EC2 instances in the us-east-1 Region. Artists upload photos of their work as large-size, high-resolution image files from their mobile phones to a centralized Amazon S3 bucket created in the us-east-1 Region. The users in Europe are reporting slow performance for their image uploads. </p><p><br></p><p>How can a solutions architect improve the performance of the image upload process?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Redeploy the application to use S3 multipart uploads."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon CloudFront distribution and point to the application as a custom origin."
                },
                {
                    "label": "C",
                    "content": "Configure the buckets to use S3 Transfer Acceleration."
                },
                {
                    "label": "D",
                    "content": "Create an Auto Scaling group for the EC2 instances and create a scaling policy."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Configure the buckets to use S3 Transfer Acceleration.</p><p>This solution will best improve the performance of the image upload process for users in Europe for the following reasons:</p><p>S3 Transfer Acceleration is specifically designed to optimize long-distance transfers between clients and S3 buckets, which directly addresses the problem of European users experiencing slow upload performance to a bucket in us-east-1.</p><p>It works by utilizing Amazon's global network of edge locations to route data more efficiently. When users in Europe upload files with Transfer Acceleration enabled, their data is routed through the nearest AWS edge location in Europe and then transferred to the destination bucket in us-east-1 using optimized network paths.</p><p>S3 Transfer Acceleration can improve upload speeds by 50-500% for long-distance transfers of larger files, which is particularly relevant for the \"large-size, high-resolution image files\" mentioned in the scenario.</p><p>It's designed specifically to minimize latency caused by geographic distance, which is the core issue in this scenario (European users uploading to a US-based bucket).</p><p>Option A (Redeploy the application to use S3 multipart uploads) would help with reliability and throughput for large files, but it doesn't specifically address the latency issues caused by geographic distance. While multipart uploads allow parallel uploading of parts, the data still needs to travel the same distance.</p><p>Option B (Create an Amazon CloudFront distribution and point to the application as a custom origin) is not appropriate for this scenario. CloudFront is primarily designed for content delivery (downloads) rather than uploads. It helps accelerate content delivery to users but doesn't provide the same benefits for upload acceleration.</p><p>S3 Transfer Acceleration is the most appropriate solution as it's specifically designed to address the exact problem described in the scenario - improving upload performance over long geographic distances for large files.</p><p>Sources</p><p>Use Amazon S3 Transfer Acceleration to Minimize Latency Caused by Distance - Best Practices Design Patterns: Optimizing Amazon S3 Performance （https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html）</p><p>Looking to compare and understand S3 transfer accelerator vs batch operations vs AWS Backup for S3 | AWS re:Post （https://repost.aws/questions/QUFnrSpEOdQ_mZuGpL2iQxVw/looking-to-compare-and-understand-s3-transfer-accelerator-vs-batch-operations-vs-aws-backup-for-s3）</p><p>I am getting very low speed while uploading data to AWS S3 deep archive storage class | AWS re:Post （https://repost.aws/questions/QUCyvqCmNLQVGFCjZcz147-Q/i-am-getting-very-low-speed-while-uploading-data-to-aws-s3-deep-archive-storage-class）</p><p>Performance guidelines for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-guidelines.html）</p><p>Using Amazon S3 Transfer Acceleration to Accelerate Geographically Disparate Data Transfers - Best Practices Design Patterns: Optimizing Amazon S3 Performance （https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html）</p><p>S3 Transfer Rates Capped at 2.8mb/s ??? How Can I Speed This Up? | AWS re:Post （https://repost.aws/questions/QUUdZKwva0TO6EwrPcFLtErA/s3-transfer-rates-capped-at-2-8mb-s-how-can-i-speed-this-up）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "03f927f647a44e49ac26899700cf1b2a",
            "questionNumber": 228,
            "type": "single",
            "content": "<p>Question #228</p><p>A company wants to containerize a multi-tier web application and move the application from an on-premises data center to AWS. The application includes web, application, and database tiers. The company needs to make the application fault tolerant and scalable. Some frequently accessed data must always be available across application servers. Frontend web servers need session persistence and must scale to meet increases in traffic. </p><p><br></p><p>Which solution will meet these requirements with the LEAST ongoing operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Run the application on Amazon Elastic Container Service (Amazon ECS) on AWS Fargate. Use Amazon Elastic File System (Amazon EFS) for data that is frequently accessed between the web and application tiers. Store the frontend web server session data in Amazon Simple Queue Service (Amazon SQS)."
                },
                {
                    "label": "B",
                    "content": "Run the application on Amazon Elastic Container Service (Amazon ECS) on Amazon EC2. Use Amazon ElastiCache for Redis to cache frontend web server session data. Use Amazon Elastic Block Store (Amazon EBS) with Multi-Attach on EC2 instances that are distributed across multiple Availability Zones."
                },
                {
                    "label": "C",
                    "content": "Run the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Use ReplicaSets to run the web servers and applications. Create an Amazon Elastic File System (Amazon EFS) file system. Mount the EFS file system across all EKS pods to store frontend web server session data."
                },
                {
                    "label": "D",
                    "content": "Deploy the application on Amazon Elastic Kubernetes Service (Amazon EKS). Configure Amazon EKS to use managed node groups. Run the web servers and application as Kubernetes deployments in the EKS cluster. Store the frontend web server session data in an Amazon DynamoDB table. Create an Amazon Elastic File System (Amazon EFS) volume that all applications will mount at the time of deployment."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. Deploying the application on Amazon EKS allows for container orchestration and scaling, which is essential for fault tolerance and scalability. Using managed node groups simplifies cluster management. Storing session data in DynamoDB ensures that it is highly available and scalable, while using EFS for shared application data provides a file system that can be mounted by all application pods. This combination of services offers a robust and scalable solution with minimal operational overhead.</p><p>Analysis of Options:</p><p> Option A:</p><p>- Uses Amazon ECS on Fargate (serverless, low operational overhead).</p><p>- Uses Amazon EFS for shared data (good for frequently accessed data).</p><p>- Problem: Amazon SQS is not suitable for session persistence (SQS is a queueing service, not a session store). This does not meet the session persistence requirement.</p><p> Option B:</p><p>- Uses Amazon ECS on EC2 (requires managing EC2 instances, higher operational overhead).</p><p>- Uses ElastiCache for Redis (good for session persistence).</p><p>- Problem: Amazon EBS with Multi-Attach is not a good solution for shared storage across Availability Zones (EBS volumes are AZ-bound and Multi-Attach is limited to a single AZ). This does not meet fault tolerance requirements.</p><p> Option C:</p><p>- Uses Amazon EKS with managed node groups (reduces operational overhead).</p><p>- Uses EFS for shared storage (good for frequently accessed data).</p><p>- Problem: EFS is not ideal for session persistence (high latency compared to in-memory solutions like ElastiCache or DynamoDB). This may not meet performance requirements for session persistence.</p><p> Option D:</p><p>- Uses Amazon EKS with managed node groups (reduces operational overhead).</p><p>- Uses DynamoDB for session persistence (scalable, highly available, and low-latency).</p><p>- Uses EFS for shared data (good for frequently accessed data).</p><p>- Meets all requirements:</p><p> &nbsp;- Fault-tolerant and scalable (EKS + DynamoDB + EFS).</p><p> &nbsp;- Session persistence handled by DynamoDB (better than EFS or SQS).</p><p> &nbsp;- Shared data handled by EFS.</p><p> &nbsp;- Minimal operational overhead (managed services).</p><p> Why Not Other Options?</p><p>- A: Incorrect session persistence solution (SQS is not suitable).</p><p>- B: EBS Multi-Attach is not multi-AZ, increasing operational overhead.</p><p>- C: EFS is not optimal for session persistence.</p><p> Conclusion:</p><p>Option D is the best solution because it:</p><p>- Uses managed services (EKS, DynamoDB, EFS) to minimize operational overhead.</p><p>- Provides scalability and fault tolerance.</p><p>- Uses DynamoDB for session persistence (better than EFS or SQS).</p><p>- Uses EFS for shared frequently accessed data.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6eff5b0434424c8eae0807c30279baeb",
            "questionNumber": 229,
            "type": "single",
            "content": "<p>Question #229</p><p>A solutions architect is planning to migrate critical Microsoft SQL Server databases to AWS. Because the databases are legacy systems, the solutions architect will move the databases to a modern data architecture. The solutions architect must migrate the databases with near-zero downtime. <br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Application Migration Service and the AWS Schema Conversion Tool (AWS SCT). Perform an in-place upgrade before the migration. Export the migrated data to Amazon Aurora Serverless after cutover. Repoint the applications to Amazon Aurora."
                },
                {
                    "label": "B",
                    "content": "Use AWS Database Migration Service (AWS DMS) to rehost the database. Set Amazon S3 as a target. Set up change data capture (CDC) replication. When the source and destination are fully synchronized, load the data from Amazon S3 into an Amazon RDS for Microsoft SQL Server DB instance."
                },
                {
                    "label": "C",
                    "content": "Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance."
                },
                {
                    "label": "D",
                    "content": "Use AWS Application Migration Service. Rehost the database server on Amazon EC2. When data replication is finished, detach the database and move the database to an Amazon RDS for Microsoft SQL Server DB instance. Reattach the database and then cut over all networking."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use native database high availability tools. Connect the source system to an Amazon RDS for Microsoft SQL Server DB instance. Configure replication accordingly. When data replication is finished, transition the workload to an Amazon RDS for Microsoft SQL Server DB instance.</p><p>This solution will meet the requirements for the following reasons:</p><p>Near-Zero Downtime: Using native SQL Server high availability tools with direct replication to RDS SQL Server provides a continuous replication stream between the on-premises source and the AWS target. This approach minimizes downtime during the migration process, as the databases remain operational throughout most of the migration.</p><p>Modern Data Architecture: By migrating directly to Amazon RDS for Microsoft SQL Server, the solution moves the legacy databases to a managed database service. RDS provides modern features like automated backups, patching, monitoring, and high availability options, which helps modernize the data architecture.</p><p>Streamlined Process: This approach involves a more direct path to the target state (RDS SQL Server) without intermediate steps, reducing complexity and potential points of failure.</p><p>Familiar Tools: Using native SQL Server replication capabilities leverages existing database expertise and provides a well-established migration path that's proven for critical database migrations.</p><p>Option D (using AWS Application Migration Service) has several disadvantages for this specific scenario:</p><p>It involves a two-step process: first rehosting on EC2, then moving to RDS. This adds complexity and potential for additional downtime during the second step.</p><p>The process of detaching the database from EC2 and reattaching it to RDS would likely cause more downtime than a properly configured replication approach.</p><p>While AWS Application Migration Service is excellent for rehosting entire servers, it's not the most efficient approach when the end goal is specifically to migrate databases to RDS.</p><p>The native database high availability tools approach provides a more direct path to the target state with near-zero downtime, which is critical for the migration of these legacy SQL Server databases. It also aligns better with the requirement to move to a modern data architecture by directly leveraging Amazon RDS for Microsoft SQL Server's managed database capabilities.</p><p>Sources</p><p>Best practices for migrating to Amazon RDS for SQL Server - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/best-practices.html）</p><p>Migrate legacy database | AWS re:Post （https://repost.aws/questions/QUTC4STLNPTNeCYjbQMSErcA/migrate-legacy-database）</p><p>Migrating SQL Server Databases to Amazon RDS for SQL Server - Database Migration Guide （https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.sql-server-rds-sql-server.html）</p><p>Migrate an on-premises Microsoft SQL Server database to Amazon EC2 - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-an-on-premises-microsoft-sql-server-database-to-amazon-ec2.html）</p><p>SQL Server database migration methods - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/methods.html）</p><p>Migrating SQL Server - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-microsoft-workloads-aws/migrating-sql-server-workloads.html）</p><p>SQL Server database migration strategies - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/strategies.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5953355494104344a5de0e14851ad304",
            "questionNumber": 230,
            "type": "multiple",
            "content": "<p>Question #230</p><p>A company's solutions architect is analyzing costs of a multi-application environment. The environment is deployed across multiple Availability Zones in a single AWS Region. After a recent acquisition, the company manages two organizations in AWS Organizations. The company has created multiple service provider applications as AWS PrivateLink-powered VPC endpoint services in one organization. The company has created multiple service consumer applications in the other organization. Data transfer charges are much higher than the company expected, and the solutions architect needs to reduce the costs. The solutions architect must recommend guidelines for developers to follow when they deploy services. These guidelines must minimize data transfer charges for the whole environment. Which guidelines meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Resource Access Manager to share the subnets that host the service provider applications with other accounts in the organization."
                },
                {
                    "label": "B",
                    "content": "Place the service provider applications and the service consumer applications in AWS accounts in the same organization."
                },
                {
                    "label": "C",
                    "content": "Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments."
                },
                {
                    "label": "D",
                    "content": "Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint&#39;s local DNS name."
                },
                {
                    "label": "E",
                    "content": "Create a Savings Plan that provides adequate coverage for the organization&#39;s planned inter-Availability Zone data transfer usage."
                }
            ],
            "correctAnswer": "CD",
            "explanation": "<p>The company faces high data transfer charges due to multi-application deployments across two AWS Organizations and multiple Availability Zones (AZs). The service provider applications (using AWS PrivateLink VPC endpoint services) and service consumer applications reside in different organizations, potentially incurring cross-organization or cross-AZ data transfer costs. &nbsp;</p><p><br></p><p> Correct Options &nbsp;</p><p><br></p><p> C. Turn off cross-zone load balancing for the Network Load Balancer in all service provider application deployments. &nbsp;</p><p>- Rationale: Cross-zone load balancing on a Network Load Balancer (NLB) distributes traffic across instances in different AZs, causing data transfer between AZs. Disabling this feature restricts traffic to the local AZ, minimizing cross-AZ data transfer charges. &nbsp;</p><p>- Cost Impact: Cross-AZ data transfer within a Region incurs fees. By limiting traffic to the local AZ, the company reduces unnecessary inter-AZ bandwidth usage. &nbsp;</p><p><br></p><p> D. Ensure that service consumer compute resources use the Availability Zone-specific endpoint service by using the endpoint's local DNS name. &nbsp;</p><p>- Rationale: Using the AZ-specific local DNS name for the VPC endpoint ensures that service consumers connect to the endpoint in their own AZ. This avoids cross-AZ traffic between consumers and providers, reducing data transfer costs. &nbsp;</p><p>- Cost Impact: Connecting to the local AZ endpoint prevents inter-AZ data transfer, which is a key source of high charges in multi-AZ environments. &nbsp;</p><p><br></p><p> Incorrect Options &nbsp;</p><p><br></p><p> B. Place the service provider applications and the service consumer applications in AWS accounts in the same organization. &nbsp;</p><p>- Issue: While AWS Organizations can simplify account management, data transfer between AWS accounts in the same Region—even across different organizations—via PrivateLink VPC endpoints is typically free. The primary cost driver here is cross-AZ traffic, not cross-organization account traffic. &nbsp;</p><p>- Why It's Not Optimal: PrivateLink within a Region does not charge for data transfer between accounts, regardless of organizational boundaries. Thus, reorganizing accounts does not address the core cost issue. &nbsp;</p><p><br></p><p> Key Takeaway &nbsp;</p><p>To minimize data transfer costs, focus on reducing cross-AZ traffic by disabling cross-zone load balancing (C) and using AZ-specific endpoints (D). These steps target the primary cost drivers in multi-AZ deployments without requiring organizational restructuring.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ba3b12852e8c416c93aba1a3349a126b",
            "questionNumber": 231,
            "type": "single",
            "content": "<p>Question #231</p><p>A company has an on-premises Microsoft SQL Server database that writes a nightly 200 GB export to a local drive. The company wants to move the backups to more robust cloud storage on Amazon S3. The company has set up a 10 Gbps AWS Direct Connect connection between the on-premises data center and AWS. <br><br>Which solution meets these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon FSx for Windows File Server Single-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon FSx for Windows File Server Multi-AZ file system within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to an SMB file share on the Amazon FSx file system. Enable nightly backups."
                },
                {
                    "label": "D",
                    "content": "Create a new S3 bucket. Deploy an AWS Storage Gateway volume gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share on the volume gateway, and automate copies of this data to an S3 bucket."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a new S3 bucket. Deploy an AWS Storage Gateway file gateway within the VPC that is connected to the Direct Connect connection. Create a new SMB file share. Write nightly database exports to the new SMB file share.</p><p>This solution meets the requirements most cost-effectively for the following reasons:</p><p>Direct Path to S3: AWS Storage Gateway file gateway provides a direct path for storing the SQL Server exports in Amazon S3, which is the company's stated goal for \"robust cloud storage.\" The file gateway acts as an intermediary that presents an SMB file share interface to the on-premises SQL Server while automatically transferring the data to S3.</p><p>Cost-Effective Storage: Amazon S3 offers significantly lower storage costs compared to file system storage options like FSx for Windows File Server, especially for this use case where the data is primarily for backup purposes rather than active file serving.</p><p>Efficient Use of Existing Infrastructure: The solution leverages the company's existing 10 Gbps Direct Connect connection, allowing for efficient data transfer from on-premises to AWS without additional networking costs.</p><p>Minimal Operational Overhead: Once configured, the file gateway requires minimal management, as it automatically handles the transfer of data from the SMB share to S3.</p><p>Compatibility with Existing Workflows: The SQL Server can continue writing exports to an SMB file share, which is a familiar protocol in Windows environments, requiring no changes to the existing export process.</p><p>Option B (using Amazon FSx for Windows File Server) would be less cost-effective because:</p><p>FSx for Windows File Server is designed as a high-performance file system for active workloads, which is unnecessary for nightly database exports.</p><p>FSx storage costs are higher than S3 storage costs, especially for data that doesn't require the performance characteristics of a Windows file system.</p><p>The solution would require an additional step to move the data from FSx to S3 if the ultimate goal is S3 storage, adding complexity and potential costs.</p><p>Enabling nightly backups of FSx would add another layer of storage costs that isn't necessary when the data can be stored directly in S3.</p><p>The AWS Storage Gateway file gateway solution provides the most direct and cost-effective path to meet the company's requirement of moving their SQL Server exports to robust cloud storage on Amazon S3.</p><p>Sources</p><p>Using the S3 File Gateway as an Enterprise SMB File Server | AWS re:Post （https://repost.aws/questions/QU_XkS2prlSwC_5oAQBD0kcQ/using-the-s3-file-gateway-as-an-enterprise-smb-file-server）</p><p>AWS Storage Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/storage-main-storage-gateway.html）</p><p>Writing to AWS from a Windows legacy app that supports only Network Paths for exports? | AWS re:Post（https://repost.aws/questions/QUJnyObhgiQhyWmvoqgVj8FA/writing-to-aws-from-a-windows-legacy-app-that-supports-only-network-paths-for-exports） </p><p>Please help me choose the right Amazon service(s) | AWS re:Post （https://repost.aws/questions/QUjGbInOcSSAWwhvHUFz8PzA/please-help-me-choose-the-right-amazon-service-s）</p><p>S3 File Gateway file limit per SMB share | AWS re:Post （https://repost.aws/questions/QUkeYgQIC8S1SLAv--BfMP4w/s3-file-gateway-file-limit-per-smb-share）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6ceedcf7e6914bbca1cd67333515e015",
            "questionNumber": 232,
            "type": "single",
            "content": "<p>Question #232</p><p>A company needs to establish a connection from its on-premises data center to AWS. The company needs to connect all of its VPCs that are located in different AWS Regions with transitive routing capabilities between VPC networks. The company also must reduce network outbound traffic costs, increase bandwidth throughput, and provide a consistent network experience for end users. <br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Create VPC peering connections that initiate from the central VPC to all other VPCs."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Site-to-Site VPN connection between the on-premises data center and a new central VPC. Use a transit gateway with dynamic routing. Connect the transit gateway to all other VPCs."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Direct Connect connection between the on-premises data center and AWS. Establish an AWS Site-to-Site VPN connection between all VPCs in each Region. Create VPC peering connections that initiate from the central VPC to all other VPCs."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS Direct Connect connection between the on-premises data center and AWS. Provision a transit VIF, and connect it to a Direct Connect gateway. Connect the Direct Connect gateway to all the other VPCs by using a transit gateway in each Region.</p><p>This solution will meet the requirements for the following reasons:</p><p>Transitive Routing Capabilities:</p><p>The combination of Direct Connect gateway and transit gateways provides true transitive routing between VPCs across different AWS Regions.</p><p>Transit gateways act as regional network hubs that enable communication between multiple VPCs and the on-premises network.</p><p>This architecture allows any VPC to communicate with any other VPC across regions, fulfilling the requirement for transitive routing capabilities.</p><p>Reduced Network Outbound Traffic Costs:</p><p>Direct Connect offers lower data transfer costs compared to VPN or internet-based connections, especially for high-volume traffic.</p><p>The predictable billing model of Direct Connect helps reduce overall network costs for consistent, high-volume traffic patterns.</p><p>Increased Bandwidth Throughput:</p><p>Direct Connect provides dedicated, high-bandwidth connections (available at 1 Gbps, 10 Gbps, and higher).</p><p>This significantly outperforms the maximum throughput of Site-to-Site VPN (limited to 1.25 Gbps per tunnel).</p><p>Consistent Network Experience:</p><p>Direct Connect provides a private, dedicated connection that bypasses the public internet.</p><p>This results in lower latency, less jitter, and more predictable performance compared to internet-based connections.</p><p>The consistent network experience is critical for end users accessing resources across multiple regions.</p><p>Option A (Site-to-Site VPN with VPC peering) would not meet the requirements because:</p><p>VPC peering does not support transitive routing - traffic from one VPC cannot route through the central VPC to reach another VPC.</p><p>Site-to-Site VPN connections traverse the public internet, which can lead to inconsistent network performance.</p><p>VPN connections have bandwidth limitations that may not be sufficient for high-throughput requirements.</p><p>Internet-based data transfer costs would be higher compared to Direct Connect.</p><p>The Direct Connect with transit VIF, Direct Connect gateway, and regional transit gateways solution provides a scalable, high-performance network architecture that meets all the stated requirements for transitive routing, cost reduction, bandwidth improvement, and consistent network experience.</p><p>Sources</p><p>AWS Direct Connect + AWS Transit Gateway + AWS Site-to-Site VPN - Amazon Virtual Private Cloud Connectivity Options （https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-aws-transit-gateway-vpn.html）</p><p>AWS Direct Connect &nbsp;- Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html）</p><p>Transit gateway attachment configuration | AWS re:Post （https://repost.aws/questions/QUvX0QyO3pRQWkBQspwcgVKg/transit-gateway-attachment-configuration）</p><p>Hybrid connectivity - Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/hybrid-connectivity.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "742e8d42973d4cb6a1c5ec60c8b7bea2",
            "questionNumber": 233,
            "type": "single",
            "content": "<p>Question #233</p><p>A company is migrating its development and production workloads to a new organization in AWS Organizations. The company has created a separate member account for development and a separate member account for production. Consolidated billing is linked to the management account. In the management account, a solutions architect needs to create an IAM user that can stop or terminate resources in both member accounts. <br><br>Which solution will meet this requirement?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an IAM user and a cross-account role in the management account. Configure the cross-account role with least privilege access to the member accounts."
                },
                {
                    "label": "B",
                    "content": "Create an IAM user in each member account. In the management account, create a cross-account role that has least privilege access. Grant the IAM users access to the cross-account role by using a trust policy."
                },
                {
                    "label": "C",
                    "content": "Create an IAM user in the management account. In the member accounts, create an IAM group that has least privilege access. Add the IAM user from the management account to each IAM group in the member accounts."
                },
                {
                    "label": "D",
                    "content": "Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided and security best practices, the correct answer is:</p><p>D. Create an IAM user in the management account. In the member accounts, create cross-account roles that have least privilege access. Grant the IAM user access to the roles by using a trust policy.</p><p>This solution will meet the requirement for the following reasons:</p><p>Proper Separation of Concerns: This approach maintains proper separation between the management account and member accounts, which is a security best practice in AWS Organizations.</p><p>Principle of Least Privilege: By creating specific cross-account roles in each member account (development and production), you can define precise permissions that follow the principle of least privilege. Each role can be tailored to include only the permissions needed to stop or terminate resources in that specific account.</p><p>Trust Relationship: The trust policy in each member account's role explicitly defines which entities (in this case, the IAM user in the management account) are allowed to assume the role, providing clear access control.</p><p>Reduced Risk: This approach reduces risk by not creating roles with cross-account access in the management account, which could potentially grant excessive permissions if compromised.</p><p>Better Auditability: When the IAM user assumes a role in a member account, AWS CloudTrail logs will clearly show which user performed actions in which account, improving auditability and compliance.</p><p>Option A (creating an IAM user and a cross-account role in the management account) is not the correct approach because:</p><p>It doesn't follow AWS best practices for cross-account access. The standard pattern is to create roles in the accounts where resources need to be accessed, not in the account where the user exists.</p><p>A cross-account role in the management account wouldn't have direct access to resources in member accounts without additional complex permission configurations.</p><p>This approach would be more difficult to audit and could potentially lead to privilege escalation issues.</p><p>The correct solution (Option D) follows AWS security best practices by implementing a clear separation of duties, maintaining the principle of least privilege, and establishing explicit trust relationships between accounts, while still allowing the IAM user in the management account to perform the required actions in both member accounts.</p><p>Sources</p><p>Secure root user access for member accounts in AWS Organizations | AWS Security Blog （https://aws.amazon.com/cn/blogs/security/secure-root-user-access-for-member-accounts-in-aws-organizations/）</p><p>Managing access permissions for an organization with AWS Organizations - AWS Organizations（https://docs.aws.amazon.com/organizations/latest/userguide/orgs_permissions_overview.html） </p><p>IAM tutorial: Delegate access across AWS accounts using IAM roles - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html）</p><p>WHY AWS Account is not relative each login? | AWS re:Post （https://repost.aws/questions/QUuVhhDp6ZS4CX1I8M_jSCzA/why-aws-account-is-not-relative-each-login）</p><p>AWS Organizations - Organizing Your AWS Environment Using Multiple Accounts （https://docs.aws.amazon.com/whitepapers/latest/organizing-your-aws-environment/aws-organizations.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d7583506be5046b583b92e5b327447b1",
            "questionNumber": 234,
            "type": "single",
            "content": "<p>Question #234</p><p>A company wants to use AWS for disaster recovery for an on-premises application. The company has hundreds of Windows-based servers that run the application. All the servers mount a common share. <br><br>The company has an RTO of 15 minutes and an RPO of 5 minutes. The solution must support native failover and fallback capabilities. <br><br>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Storage Gateway File Gateway. Schedule daily Windows server backups. Save the data to Amazon S3. During a disaster, recover the on-premises servers from the backup. During tailback, run the on-premises servers on Amazon EC2 instances."
                },
                {
                    "label": "B",
                    "content": "Create a set of AWS CloudFormation templates to create infrastructure. Replicate all data to Amazon Elastic File System (Amazon EFS) by using AWS DataSync. During a disaster, use AWS CodePipeline to deploy the templates to restore the on-premises servers. Fail back the data by using DataSync."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Cloud Development Kit (AWS CDK) pipeline to stand up a multi-site active-active environment on AWS. Replicate data into Amazon S3 by using the s3 sync command. During a disaster, swap DNS endpoints to point to AWS. Fail back the data by using the s3 sync command."
                },
                {
                    "label": "D",
                    "content": "Use AWS Elastic Disaster Recovery to replicate the on-premises servers. Replicate data to an Amazon FSx for Windows File Server file system by using AWS DataSync. Mount the file system to AWS servers. During a disaster, fail over the on-premises servers to AWS. Fail back to new or existing servers by using Elastic Disaster Recovery."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. AWS Elastic Disaster Recovery is designed to replicate servers to AWS with minimal downtime, which aligns with the company's RTO and RPO requirements. By replicating data to Amazon FSx for Windows File Server and using AWS DataSync, the company can ensure that the data is up-to-date and that there is a robust failover and fallback process in place. This solution also supports native failover and fallback capabilities, which is a requirement specified by the company.</p><p>The requirements include: &nbsp;</p><p>- RTO of 15 minutes and RPO of 5 minutes (fast recovery and minimal data loss). &nbsp;</p><p>- Native failover and fallback capabilities. &nbsp;</p><p>- Hundreds of Windows servers with a common share. &nbsp;</p><p>Option D meets these requirements most effectively and cost-efficiently: &nbsp;</p><p>- AWS Elastic Disaster Recovery (DRS) provides continuous replication of on-premises servers to AWS, enabling fast failover (meeting RTO/RPO). &nbsp;</p><p>- Amazon FSx for Windows File Server is a fully managed shared file storage solution compatible with Windows servers. &nbsp;</p><p>- AWS DataSync efficiently replicates data to FSx. &nbsp;</p><p>- Native failover and fallback are supported by Elastic Disaster Recovery. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: Storage Gateway + S3 backups do not meet the RTO/RPO requirements (slow recovery). &nbsp;</p><p>- B: CloudFormation + EFS + DataSync is complex and does not provide native failover/fallback for Windows servers. &nbsp;</p><p>- C: Multi-site active-active is expensive and overkill; `s3 sync` is not real-time enough for a 5-minute RPO. &nbsp;</p><p>Thus, D is the most cost-effective and compliant solution. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2d68dd72d1564408a149bf5933974435",
            "questionNumber": 235,
            "type": "multiple",
            "content": "<p>Question #235</p><p>A company has built a high performance computing (HPC) cluster in AWS for a tightly coupled workload that generates a large number of shared files stored in Amazon EFS. The cluster was performing well when the number of Amazon EC2 instances in the cluster was 100. However, when the company increased the cluster size to 1,000 EC2 instances, overall performance was well below expectations. Which collection of design choices should a solutions architect make to achieve the maximum performance from the HPC cluster? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Ensure the HPC cluster is launched within a single Availability Zone."
                },
                {
                    "label": "B",
                    "content": "Launch the EC2 instances and attach elastic network interfaces in multiples of four."
                },
                {
                    "label": "C",
                    "content": "Select EC2 instance types with an Elastic Fabric Adapter (EFA) enabled."
                },
                {
                    "label": "D",
                    "content": "Ensure the cluster is launched across multiple Availability Zones."
                },
                {
                    "label": "E",
                    "content": "Replace Amazon EFS with multiple Amazon EBS volumes in a RAID array."
                },
                {
                    "label": "F",
                    "content": "Replace Amazon EFS with Amazon FSx for Lustre."
                }
            ],
            "correctAnswer": "ACF",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Ensure the HPC cluster is launched within a single Availability Zone. </p><p>C. Select EC2 instance types with an ElasticFabric Adapter (EFA) enabled. </p><p>F. Replace Amazon EFS with Amazon FSx for Lustre.</p><p>These three design choices together will maximize the performance of the HPC cluster for the following reasons:</p><p>Ensure the HPC cluster is launched within a single Availability Zone (A):</p><p>For tightly coupled HPC workloads, minimizing network latency between instances is critical.</p><p>A single AZ deployment offers significantly lower latency and higher network throughput between instances compared to multi-AZ deployments.</p><p>When scaling to 1,000 instances, the inter-node communication becomes even more important, and cross-AZ traffic would introduce unnecessary latency that would severely impact performance.</p><p>Select EC2 instance types with an ElasticFabric Adapter (EFA) enabled (C):</p><p>EFA is specifically designed for HPC applications that require high levels of inter-node communications.</p><p>It provides lower and more consistent latency compared to traditional TCP transport by enabling OS-bypass capabilities.</p><p>EFA allows HPC applications to communicate directly with the network interface, bypassing the operating system kernel.</p><p>This is particularly beneficial for tightly coupled workloads that require frequent communication between nodes.</p><p>Replace Amazon EFS with Amazon FSx for Lustre (F):</p><p>FSx for Lustre is purpose-built for high-performance computing workloads.</p><p>It offers significantly higher throughput and lower latency compared to EFS, which is critical when scaling to 1,000 instances.</p><p>As a parallel file system, Lustre allows multiple clients to access data simultaneously, which is ideal for HPC clusters generating large numbers of shared files.</p><p>FSx for Lustre can deliver hundreds of gigabytes per second of throughput and millions of IOPS, which addresses the performance bottleneck likely caused by EFS when scaling up.</p><p>The other options would not be optimal:</p><p>Option D (multi-AZ deployment) would introduce higher latency between instances, degrading performance for tightly coupled workloads.</p><p>Option E (EBS RAID array) would not scale well to 1,000 instances and would create a bottleneck for shared files.</p><p>Option B (launching instances and attaching network interfaces in multiples of four) might offer some optimization but is not as significant for performance as the other three correct choices.</p><p>These three design choices together address the key aspects of HPC performance: minimizing network latency (single AZ), optimizing inter-node communication (EFA), and providing high-performance shared storage (FSx for Lustre).</p><p>Sources</p><p>Optimizing HPC workflows with automatically scaling clusters in Ansys Gateway powered by AWS | AWS HPC Blog （https://aws.amazon.com/cn/blogs/hpc/optimizing-hpc-workflows-with-automatically-scaling-clusters-in-ansys-gateway-powered-by-aws/）</p><p>Network architecture - High Performance Computing Lens （https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/network-architecture.html）</p><p>Storage architecture - High Performance Computing Lens （https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/storage-architecture.html）</p><p>Data management and transfer - Financial Services Grid Computing on AWS （https://docs.aws.amazon.com/whitepapers/latest/financial-services-grid-computing/data-management-and-transfer.html）</p><p>Shared storage - AWS ParallelCluster （https://docs.aws.amazon.com/parallelcluster/latest/ug/shared-storage-quotas-integration-v3.html）</p><p>Tightly coupled scenarios - High Performance Computing Lens （https://docs.aws.amazon.com/wellarchitected/latest/high-performance-computing-lens/tightly-coupled-scenarios.html）</p><p>Software - Computational Fluid Dynamics on AWS （https://docs.aws.amazon.com/whitepapers/latest/computational-fluid-dynamics-on-aws/software.html）</p><p>Storage and data sharing - Financial Services Grid Computing on AWS （https://docs.aws.amazon.com/whitepapers/latest/financial-services-grid-computing/storage-and-data-sharing.html）</p><p>Guidance for Deploying High Performance Computing Clusters on AWS （https://aws.amazon.com/cn/solutions/guidance/deploying-high-performance-computing-clusters-on-aws/）</p><p>File Server - Amazon FSx - AWS （https://aws.amazon.com/cn/fsx/intelligent-tiering/）</p><p>Amazon FSx for Lustre increases throughput to GPU instances by up to 12x | AWS News Blog (https://aws.amazon.com/cn/blogs/aws/amazon-fsx-for-lustre-unlocks-full-network-bandwidth-and-gpu-performance/)</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "79edbf46b0c44b60a42b07a21fa928bc",
            "questionNumber": 236,
            "type": "single",
            "content": "<p>Question #236</p><p>A company is designing an AWS Organizations structure. The company wants to standardize a process to apply tags across the entire organization. The company will require tags with specific values when a user creates a new resource. Each of the company's OUs will have unique tag values. <br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs."
                },
                {
                    "label": "B",
                    "content": "Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the organization&#39;s management account."
                },
                {
                    "label": "C",
                    "content": "Use an SCP to allow the creation of resources only when the resources have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs."
                },
                {
                    "label": "D",
                    "content": "Use an SCP to deny the creation of resources that do not have the required tags. Define the list of tags. Attach the SCP to the OUs."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Use an SCP to deny the creation of resources that do not have the required tags. Create a tag policy that includes the tag values that the company has assigned to each OU. Attach the tag policies to the OUs.</p><p>This solution will meet the requirements for the following reasons:</p><p>Service Control Policies (SCPs) to enforce tagging: Using an SCP to deny the creation of resources that don't have required tags ensures compliance with the company's tagging standards. This creates a preventive control that blocks resources from being created without proper tags.</p><p>Tag policies for defining allowed values: Tag policies are specifically designed to define and enforce standardized tag keys and values across an organization. They allow you to specify which tag keys must be used and what values are allowed for each key.</p><p>OU-specific tag values: By attaching tag policies to individual OUs rather than the management account, each OU can have its own unique tag values as specified in the requirements. This provides the flexibility needed when different parts of the organization require different tag values.</p><p>Comprehensive enforcement: The combination of SCPs (to require tags) and tag policies (to define allowed values) creates a complete tagging governance solution that both requires tags and ensures they have appropriate values.</p><p>Option B would not work because attaching tag policies only to the management account wouldn't allow for unique tag values for each OU. This approach would enforce the same tag values across the entire organization.</p><p>Option D would not be optimal because using only SCPs without tag policies would make it difficult to define and manage the allowed tag values for each OU. SCPs are primarily access control policies and aren't designed to efficiently manage and enforce specific tag values.</p><p>The correct solution (Option A) provides both the enforcement mechanism (SCPs) and the value definition capability (tag policies attached to OUs) needed to meet the company's requirements for standardized tagging with OU-specific values.</p><p>Sources</p><p>AWS Organizations – Tag policies - Best Practices for Tagging AWS Resources (https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/aws-organizations-tag-policies.html)</p><p>AWS Organizations tag policies - Tagging AWS Resources and Tag Editor (https://docs.aws.amazon.com/tag-editor/latest/userguide/tag-policies-orgs.html)</p><p>Tag policies - AWS Organizations (https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_tag-policies.html)</p><p>Measure, enforce, and evolve - AWS Prescriptive Guidance (https://docs.aws.amazon.com/prescriptive-guidance/latest/cost-allocation-tagging/measure-enforce-evolve.html)</p><p>Using TagOptions with AWS Organizations tag policies - AWS Service Catalog (https://docs.aws.amazon.com/servicecatalog/latest/adminguide/tagoption-policies.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "98b6d39689e3402a92cd3ec3f36f35fd",
            "questionNumber": 237,
            "type": "single",
            "content": "<p>Question #237</p><p>A company has more than 10,000 sensors that send data to an on-premises Apache Kafka server by using the Message Queuing Telemetry Transport (MQTT) protocol. The on-premises Kafka server transforms the data and then stores the results as objects in an Amazon S3 bucket. </p><p><br></p><p>Recently, the Kafka server crashed. The company lost sensor data while the server was being restored. A solutions architect must create a new design on AWS that is highly available and scalable to prevent a similar occurrence. <br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Launch two Amazon EC2 instances to host the Kafka server in an active/standby configuration across two Availability Zones. Create a domain name in Amazon Route 53. Create a Route 53 failover policy. Route the sensors to send the data to the domain name."
                },
                {
                    "label": "B",
                    "content": "Migrate the on-premises Kafka server to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create a Network Load Balancer (NLB) that points to the Amazon MSK broker. Enable NLB health checks. Route the sensors to send the data to the NLB."
                },
                {
                    "label": "C",
                    "content": "Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core."
                },
                {
                    "label": "D",
                    "content": "Deploy AWS IoT Core, and launch an Amazon EC2 instance to host the Kafka server. Configure AWS IoT Core to send the data to the EC2 instance. Route the sensors to send the data to AWS IoT Core."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Deploy AWS IoT Core, and connect it to an Amazon Kinesis Data Firehose delivery stream. Use an AWS Lambda function to handle data transformation. Route the sensors to send the data to AWS IoT Core.</p><p>This solution will meet the requirements for the following reasons:</p><p>High Availability:</p><p>AWS IoT Core is a fully managed service with built-in high availability across multiple Availability Zones.</p><p>Kinesis Data Firehose is also a fully managed service with automatic scaling and high availability.</p><p>Lambda functions run across multiple Availability Zones by default.</p><p>This serverless architecture eliminates the single point of failure that existed with the on-premises Kafka server.</p><p>Scalability:</p><p>AWS IoT Core can easily handle the 10,000+ sensors mentioned in the scenario and can scale to billions of devices and messages.</p><p>Kinesis Data Firehose automatically scales to match the throughput of the incoming data.</p><p>Lambda functions scale automatically based on the incoming request rate.</p><p>This solution can accommodate growth in the number of sensors without manual intervention.</p><p>Data Transformation:</p><p>Lambda functions can be used to perform the same data transformations that were previously handled by the Kafka server.</p><p>This maintains the required functionality while improving reliability.</p><p>Data Storage:</p><p>Kinesis Data Firehose can deliver the transformed data directly to Amazon S3, maintaining the existing storage solution.</p><p>Firehose provides at-least-once delivery guarantees to S3.</p><p>Fully Managed Services:</p><p>This solution uses fully managed services that AWS maintains, reducing operational overhead and eliminating the need to manage infrastructure.</p><p>Option D (AWS IoT Core with EC2-hosted Kafka) would not be optimal because:</p><p>It reintroduces the same potential point of failure (Kafka server) that caused the original data loss.</p><p>EC2 instances require manual configuration for high availability and scaling.</p><p>Managing Kafka on EC2 requires operational overhead and expertise.</p><p>This approach doesn't fully leverage the benefits of AWS managed services.</p><p>The serverless architecture in Option C provides the high availability and scalability required to prevent data loss during failures, which directly addresses the problem that occurred with the on-premises Kafka server. It also simplifies operations by using fully managed services rather than requiring the company to manage server infrastructure.</p><p>Sources</p><p>Architectural Patterns for real-time analytics using Amazon Kinesis Data Streams, Part 2: AI Applications | AWS Big Data Blog （https://aws.amazon.com/cn/blogs/big-data/architectural-patterns-for-real-time-analytics-using-amazon-kinesis-data-streams-part-2-ai-applications/）</p><p>Best Practice 10.1 – Store data before processing - IoT Lens Checklist （https://docs.aws.amazon.com/wellarchitected/latest/iot-lens-checklist/best-practice-10-1.html）</p><p>Key considerations while building streaming analytics - Build Modern Data Streaming Architectures on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-modern-data-streaming-analytics-architectures/key-considerations-while-building-streaming-analytics.html）</p><p>Unlocking Scalable IoT Analytics on AWS | The Internet of Things on AWS – Official Blog （https://aws.amazon.com/cn/blogs/iot/unlocking-scalable-iot-analytics-on-aws/）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a8556be72b224353a7a489ba9f3facb8",
            "questionNumber": 238,
            "type": "multiple",
            "content": "<p>Question #238</p><p>A company recently started hosting new application workloads in the AWS Cloud. The company is using Amazon EC2 instances, Amazon Elastic File System (Amazon EFS) file systems, and Amazon RDS DB instances. To meet regulatory and business requirements, the company must make the following changes for data backups:</p><p>• Backups must be retained based on custom daily, weekly, and monthly requirements.</p><p>• Backups must be replicated to at least one other AWS Region immediately after capture.</p><p>• The backup solution must provide a single source of backup status across the AWS environment.</p><p>• The backup solution must send immediate notifications upon failure of any resource backup.</p><p>Which combination of steps will meet these requirements with the LEAST amount of operational overhead? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Backup plan with a backup rule for each of the retention requirements."
                },
                {
                    "label": "B",
                    "content": "Configure an AWS Backup plan to copy backups to another Region."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Lambda function to replicate backups to another Region and send notification if a failure occurs."
                },
                {
                    "label": "D",
                    "content": "Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED."
                },
                {
                    "label": "E",
                    "content": "Create an Amazon Data Lifecycle Manager (Amazon DLM) snapshot lifecycle policy for each of the retention requirements."
                },
                {
                    "label": "F",
                    "content": "Set up RDS snapshots on each database."
                }
            ],
            "correctAnswer": "ABD",
            "explanation": "<p>The combination of steps that will meet the requirements with the least amount of operational overhead is A, B, and D.</p><p>A. Creating an AWS Backup plan with custom backup rules allows the company to meet the regulatory and business requirements for backup retention schedules.</p><p>B. Configuring the AWS Backup plan to copy backups to another Region ensures that backups are replicated immediately after capture, providing data protection and disaster recovery capabilities.</p><p>D. Adding an Amazon SNS topic to the backup plan enables immediate notifications to be sent upon the failure of any resource backup, allowing for quick response to any issues.</p><p>Option C is not necessary because AWS Backup can be configured to send notifications directly through the service. Option E is not required as AWS Backup can also handle the creation and lifecycle of EBS snapshots, and Amazon DLM is more focused on EBS snapshots rather than a comprehensive backup solution. Option F is not needed because AWS Backup can be used to automate RDS snapshot creation and management.</p><p>The question asks for the LEAST amount of operational overhead while meeting all the given backup requirements. The best approach is to use AWS Backup, as it provides a centralized backup solution for EC2, EFS, and RDS, along with cross-region replication, customizable retention policies, and notifications.</p><p> Correct Answers:</p><p>A. Create an AWS Backup plan with a backup rule for each of the retention requirements. &nbsp;</p><p>- AWS Backup allows you to define multiple backup rules with different retention periods (daily, weekly, monthly). &nbsp;</p><p>- This meets the requirement for custom retention policies.</p><p>B. Configure an AWS Backup plan to copy backups to another Region. &nbsp;</p><p>- AWS Backup supports cross-region copy, which automatically replicates backups to another Region. &nbsp;</p><p>- This meets the requirement for immediate replication.</p><p>D. Add an Amazon Simple Notification Service (Amazon SNS) topic to the backup plan to send a notification for finished jobs that have any status except BACKUP_JOB_COMPLETED. &nbsp;</p><p>- AWS Backup integrates with SNS to send notifications on backup failures. &nbsp;</p><p>- This meets the requirement for immediate failure notifications.</p><p> Why Not the Other Options?</p><p>- C. Lambda function for replication & notifications → Adds unnecessary complexity since AWS Backup natively supports these features.</p><p>- E. Amazon DLM policies → Only works for EBS snapshots (not RDS or EFS) and lacks centralized management.</p><p>- F. RDS snapshots → Manual setup per database, no centralized status tracking, and no built-in cross-region replication.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "e5790433f15346a0bb3fc6b2b06639b4",
            "questionNumber": 239,
            "type": "single",
            "content": "<p>Question #239</p><p>A company is developing a gene reporting device that will collect genomic information to assist researchers with collecting large samples of data from a diverse population. The device will push 8 KB of genomic data every second to a data platform that will need to process and analyze the data and provide information back to researchers. The data platform must meet the following requirements:<br><br></p><p>• Provide near-real-time analytics of the inbound genomic data</p><p>• Ensure the data is flexible, parallel, and durable</p><p>• Deliver results of processing to a data warehouse<br><br></p><p>Which strategy should a solutions architect use to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Amazon Kinesis Data Firehose to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon RDS instance."
                },
                {
                    "label": "B",
                    "content": "Use Amazon Kinesis Data Streams to collect the inbound sensor data, analyze the data with Kinesis clients, and save the results to an Amazon Redshift cluster using Amazon EMR."
                },
                {
                    "label": "C",
                    "content": "Use Amazon S3 to collect the inbound device data, analyze the data from Amazon SQS with Kinesis, and save the results to an Amazon Redshift cluster."
                },
                {
                    "label": "D",
                    "content": "Use an Amazon API Gateway to put requests into an Amazon SQS queue, analyze the data with an AWS Lambda function, and save the results to an Amazon Redshift cluster using Amazon EMR."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. Amazon Kinesis Data Streams can collect and process large volumes of genomic data in near-real-time. By using Kinesis clients, the data can be analyzed as it streams in. Amazon Redshift is a fully managed data warehouse service that can handle large-scale data analytics and is suitable for storing and querying the processed genomic data. Using Amazon EMR (Elastic MapReduce) can provide additional processing power for complex analytics if needed.</p><p>The requirements are: &nbsp;</p><p>1. Near-real-time analytics: Amazon Kinesis Data Streams is designed for real-time data ingestion and processing. &nbsp;</p><p>2. Flexible, parallel, and durable data: Kinesis Data Streams allows multiple consumers (Kinesis clients) to process data in parallel, and it is durable. &nbsp;</p><p>3. Deliver results to a data warehouse: Amazon Redshift is a data warehouse, and Amazon EMR can be used to process and load data into it. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: Amazon RDS is not a data warehouse solution like Redshift, and Kinesis Data Firehose is more for batch processing rather than real-time analytics. &nbsp;</p><p>- C: Amazon S3 is for storage, not real-time processing. SQS + Kinesis is not a typical pattern for real-time genomic data streaming. &nbsp;</p><p>- D: API Gateway + SQS + Lambda is more suited for request/response workloads, not continuous real-time data streaming. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "88b1063c95364a768443dce2d4a4a293",
            "questionNumber": 240,
            "type": "multiple",
            "content": "<p>Question #240</p><p>A solutions architect needs to define a reference architecture for a solution for three-tier applications with web, application, and NoSQL data layers. The reference architecture must meet the following requirements:</p><p>• High availability within an AWS Region</p><p>• Able to fail over in 1 minute to another AWS Region for disaster recovery</p><p>• Provide the most efficient solution while minimizing the impact on the user experience</p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use an Amazon Route 53 weighted routing policy set to 100/0 across the two selected Regions. Set Time to Live (TTL) to 1 hour."
                },
                {
                    "label": "B",
                    "content": "Use an Amazon Route 53 failover routing policy for failover from the primary Region to the disaster recovery Region. Set Time to Live (TTL) to 30 seconds."
                },
                {
                    "label": "C",
                    "content": "Use a global table within Amazon DynamoDB so data can be accessed in the two selected Regions."
                },
                {
                    "label": "D",
                    "content": "Back up data from an Amazon DynamoDB table in the primary Region every 60 minutes and then write the data to Amazon S3. Use S3 cross-Region replication to copy the data from the primary Region to the disaster recovery Region. Have a script import the data into DynamoDB in a disaster recovery scenario."
                },
                {
                    "label": "E",
                    "content": "Implement a hot standby model using Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use zonal Reserved Instances for the minimum number of servers and On-Demand Instances for any additional resources."
                },
                {
                    "label": "F",
                    "content": "Use Auto Scaling groups for the web and application layers across multiple Availability Zones in the Regions. Use Spot Instances for the required resources."
                }
            ],
            "correctAnswer": "BCE",
            "explanation": "<p> The combination of steps that will meet the requirements is B, C, and E.</p><p>B. Using an Amazon Route 53 failover routing policy with a TTL of 30 seconds allows for quick DNS failover to the disaster recovery Region in case of a primary Region failure.</p><p>C. Implementing a global table within Amazon DynamoDB ensures that data is replicated and accessible in both the primary and disaster recovery Regions, which supports high availability and disaster recovery.</p><p>E. A hot standby model with Auto Scaling groups across multiple Availability Zones provides high availability within an AWS Region and can quickly adapt to demand changes while using a mix of Reserved Instances and On-Demand Instances for cost efficiency.</p><p>Option A is not suitable due to the long TTL, which would delay failover. Option D is not the most efficient solution for disaster recovery as it relies on backups and manual data import scripts. Option F does not meet the requirement for high availability within an AWS Region as Spot Instances can be terminated with little notice, which could impact the availability of the application.</p><p>- B (Route 53 Failover Routing Policy with 30s TTL): &nbsp;</p><p> &nbsp;- Enables fast failover (within 1 minute) by quickly redirecting traffic to the standby region. &nbsp;</p><p> &nbsp;- A low TTL (30s) ensures DNS changes propagate quickly. &nbsp;</p><p>- C (DynamoDB Global Table): &nbsp;</p><p> &nbsp;- Provides multi-region replication for NoSQL data, ensuring the disaster recovery region has real-time access to data. &nbsp;</p><p> &nbsp;- Eliminates manual backup/restore delays, improving recovery time. &nbsp;</p><p>- E (Hot Standby with Auto Scaling & Reserved/On-Demand Instances): &nbsp;</p><p> &nbsp;- A hot standby model ensures resources are running in the DR region, allowing quick failover. &nbsp;</p><p> &nbsp;- Reserved Instances for baseline capacity + On-Demand for scaling balances cost and availability. &nbsp;</p><p> Why Not Others? &nbsp;</p><p>- A: Weighted routing (100/0) does not support automatic failover. &nbsp;</p><p>- D: Backing up DynamoDB every 60 minutes is too slow (RTO &gt; 1 minute). &nbsp;</p><p>- F: Spot Instances are unreliable for DR since they can be interrupted. &nbsp;</p><p> Conclusion: &nbsp;</p><p>B, C, E provide high availability, fast failover (&lt;1 min), and minimal user impact while being cost-efficient. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a453094f0d3a47e487c451972511917e",
            "questionNumber": 241,
            "type": "single",
            "content": "<p>Question #241</p><p>A company manufactures smart vehicles. The company uses a custom application to collect vehicle data. The vehicles use the MQTT protocol to connect to the application. The company processes the data in 5-minute intervals. The company then copies vehicle telematics data to on-premises storage. <br><br>Custom applications analyze this data to detect anomalies. The number of vehicles that send data grows constantly. Newer vehicles generate high volumes of data. The on-premises storage solution is not able to scale for peak traffic, which results in data loss. The company must modernize the solution and migrate the solution to AWS to resolve the scaling challenges. <br><br>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS IoT Greengrass to send the vehicle data to Amazon Managed Streaming for Apache Kafka (Amazon MSK). Create an Apache Kafka application to store the data in Amazon S3. Use a pretrained model in Amazon SageMaker to detect anomalies."
                },
                {
                    "label": "B",
                    "content": "Use AWS IoT Core to receive the vehicle data. Configure rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3. Create an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies."
                },
                {
                    "label": "C",
                    "content": "Use AWS IoT FleetWise to collect the vehicle data. Send the data to an Amazon Kinesis data stream. Use an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use the built-in machine learning transforms in AWS Glue to detect anomalies."
                },
                {
                    "label": "D",
                    "content": "Use Amazon MQ for RabbitMQ to collect the vehicle data. Send the data to an Amazon Kinesis Data Firehose delivery stream to store the data in Amazon S3. Use Amazon Lookout for Metrics to detect anomalies."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. AWS IoT Core is designed to handle the ingestion of data from IoT devices at scale, which is ideal for the growing number of smart vehicles sending data. By routing the data to an Amazon Kinesis Data Firehose delivery stream, the company can efficiently store the large volumes of data in Amazon S3. Additionally, Amazon Kinesis Data Analytics can be used to analyze the data stream for anomaly detection, which provides a scalable and serverless solution with minimal operational overhead. This approach leverages managed services and does not require the management of clusters or Kafka applications, which simplifies operations and reduces overhead compared to the other options.</p><p>1. AWS IoT Core natively supports MQTT, which the vehicles already use, minimizing changes to the existing setup.</p><p>2. Kinesis Data Firehose automatically scales to handle high data volumes and stores data directly in Amazon S3 without manual intervention.</p><p>3. Kinesis Data Analytics can process streaming data in real-time (or near-real-time) to detect anomalies, aligning with the 5-minute processing interval.</p><p>4. This approach is serverless, meaning AWS manages scaling, reducing operational overhead.</p><p> Why Not the Other Options?</p><p>- A: Uses AWS IoT Greengrass (requires edge deployment) and Amazon MSK (Kafka cluster management adds complexity).</p><p>- C: AWS IoT FleetWise is more specialized for automotive data collection but introduces unnecessary complexity compared to IoT Core.</p><p>- D: Amazon MQ for RabbitMQ is not optimized for IoT use cases like AWS IoT Core, and Lookout for Metrics is more for time-series anomaly detection rather than telemetry analysis.</p><p> Conclusion:</p><p>Option B provides a scalable, serverless, and low-operational-overhead solution by leveraging AWS IoT Core, Kinesis Firehose, and Kinesis Analytics. </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c5ee0927552c4bd08f4403976afacd56",
            "questionNumber": 242,
            "type": "single",
            "content": "<p>Question #242</p><p>During an audit, a security team discovered that a development team was putting IAM user secret access keys in their code and then committing it to an AWS CodeCommit repository. The security team wants to automatically find and remediate instances of this security vulnerability.<br><br></p><p>Which solution will ensure that the credentials are appropriately secured automatically?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Run a script nightly using AWS Systems Manager Run Command to search for credentials on the development instances. If found, use AWS Secrets Manager to rotate the credentials"
                },
                {
                    "label": "B",
                    "content": "Use a scheduled AWS Lambda function to download and scan the application code from CodeCommit. If credentials are found, generate new credentials and store them in AWS KMS."
                },
                {
                    "label": "C",
                    "content": "Configure Amazon Macie to scan for credentials in CodeCommit repositories. If credentials are found, trigger an AWS Lambda function to disable the credentials and notify the user."
                },
                {
                    "label": "D",
                    "content": "Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Configure a CodeCommit trigger to invoke an AWS Lambda function to scan new code submissions for credentials. If credentials are found, disable them in AWS IAM and notify the user.</p><p>This solution will ensure that credentials are appropriately secured automatically for the following reasons:</p><p>Real-time Detection: CodeCommit triggers activate immediately when new code is submitted, allowing for instant scanning of code for IAM credentials. This provides the fastest possible detection compared to scheduled or periodic scanning approaches.</p><p>Immediate Remediation: When credentials are found, they are disabled in IAM right away, minimizing the window of vulnerability. This is critical for security since exposed credentials could be exploited quickly.</p><p>User Notification: Notifying the user who committed the code creates awareness and provides an opportunity for education about secure coding practices, helping prevent similar issues in the future.</p><p>Preventive Approach: This solution prevents the exposed credentials from being used maliciously by disabling them as soon as they're detected, rather than just identifying them or rotating them while they remain exposed.</p><p>Automation: The entire process is automated from detection to remediation, requiring no manual intervention.</p><p>Option A (using Systems Manager Run Command with nightly scripts) would be less effective because:</p><p>Nightly scans create a potential 24-hour window where credentials could be exposed and exploited</p><p>It doesn't include notification to users who committed the credentials</p><p>It focuses on development instances rather than the source code repository itself</p><p>Option B (scheduled Lambda function to scan CodeCommit) would be less effective because:</p><p>Scheduled scans may miss credentials between scan intervals</p><p>Storing credentials in KMS doesn't address the immediate security risk of exposed credentials</p><p>It doesn't include disabling the compromised credentials or notifying users</p><p>The CodeCommit trigger with Lambda approach (Option D) provides the most comprehensive solution by combining immediate detection, automatic remediation, and user education, which aligns with security best practices for handling exposed credentials in code repositories.</p><p>Sources</p><p>IAM credentials for CodeCommit: Git credentials, SSH keys, and AWS access keys - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html）</p><p>CodeCommit auth fail | AWS re:Post （https://repost.aws/questions/QU_8egJwzFRqegRf5zMtsPgw/codecommit-auth-fail）</p><p>Security best practices in IAM - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "64a483d46eb2424fbd65dcca18cfd99c",
            "questionNumber": 243,
            "type": "multiple",
            "content": "<p>Question #243</p><p>A company has a data lake in Amazon S3 that needs to be accessed by hundreds of applications across many AWS accounts. The company's information security policy states that the S3 bucket must not be accessed over the public internet and that each application should have the minimum permissions necessary to function.</p><p>To meet these requirements, a solutions architect plans to use an S3 access point that is restricted to specific VPCs for each application.</p><p>Which combination of steps should the solutions architect take to implement this solution? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an S3 access point for each application in the AWS account that owns the S3 bucket. Configure each access point to be accessible only from the application&rsquo;s VPC. Update the bucket policy to require access from an access point."
                },
                {
                    "label": "B",
                    "content": "Create an interface endpoint for Amazon S3 in each application&#39;s VPC. Configure the endpoint policy to allow access to an S3 access point. Create a VPC gateway attachment for the S3 endpoint."
                },
                {
                    "label": "C",
                    "content": "Create a gateway endpoint for Amazon S3 in each application&#39;s VPC. Configure the endpoint policy to allow access to an S3 access point. Secify the route table that is used to access the access point.<p><br></p>"
                },
                {
                    "label": "D",
                    "content": "Create an S3 access point for each application in each AWS account and attach the access points to the S3 bucket. Configure each access point to be accessible only from the application&#39;s VPC. Update the bucket policy to require access from an access point."
                },
                {
                    "label": "E",
                    "content": "Create a gateway endpoint for Amazon S3 in the data lake&#39;s VPC. Attach an endpoint policy to allow access to the S3 bucket. Specify the route table that is used to access the bucket."
                }
            ],
            "correctAnswer": "AC",
            "explanation": "<p>The correct answers are A and C. Step A enables each application to have its own access point, which can be configured to allow only the necessary permissions for that application. This satisfies the requirement for least privilege access. Step C involves creating a gateway VPC endpoint for S3 in each application's VPC. This endpoint provides a private path for traffic between the VPC and the S3 bucket, ensuring that the data does not traverse the public internet. The endpoint policy should be configured to allow access to the specific S3 access point created for the application, maintaining the least privilege principle.</p><p>The requirements are:</p><p>1. No public internet access to the S3 bucket.</p><p>2. Minimum permissions for each application.</p><p>3. Access restricted via VPCs using S3 access points.</p><p> Option A: Correct</p><p>- S3 access points are created in the bucket's account (where the data lake resides).</p><p>- Each access point is restricted to a specific application VPC (using VPC restrictions).</p><p>- The bucket policy enforces that all access must come through an access point (ensuring no public internet access).</p><p> Option C: Correct</p><p>- A gateway endpoint for S3 (type `com.amazonaws.&lt;region&gt;.s3`) is needed in each application's VPC to allow private connectivity to S3.</p><p>- The endpoint policy can restrict access to only the required S3 access point.</p><p>- The route table ensures traffic to S3 goes through the gateway endpoint (keeping traffic within AWS network).</p><p> Why not the others?</p><p>- B: Incorrect because interface endpoints (PrivateLink) are not needed for S3; gateway endpoints are the correct choice for S3.</p><p>- D: Incorrect because S3 access points must be created in the bucket's account, not in each application's account.</p><p>- E: Incorrect because the gateway endpoint should be in each application's VPC, not the data lake's VPC. Also, the policy should allow access to the access point, not the bucket directly.</p><p> Summary:</p><p>- A: Sets up VPC-restricted access points in the bucket's account.</p><p>- C: Configures private connectivity via gateway endpoints in each application's VPC. </p><p>This ensures private access and least-privilege permissions. ✅</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "0bfd526cb8394369aa4cca707d99fd0b",
            "questionNumber": 244,
            "type": "single",
            "content": "<p>Question #244</p><p>A company has developed a hybrid solution between its data center and AWS. The company uses Amazon VPC and Amazon EC2 instances that send application logs to Amazon CloudWatch. The EC2 instances read data from multiple relational databases that are hosted on premises.<br><br></p><p>The company wants to monitor which EC2 instances are connected to the databases in near-real time. The company already has a monitoring solution that uses Splunk on premises. A solutions architect needs to determine how to send networking traffic to Splunk.<br><br></p><p>How should the solutions architect meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Enable VPC flows logs, and send them to CloudWatch. Create an AWS Lambda function to periodically export the CloudWatch logs to an Amazon S3 bucket by using the pre-defined export function. Generate ACCESS_KEY and SECRET_KEY AWS credentials. Configure Splunk to pull the logs from the S3 bucket by using those credentials."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination. Configure a pre-processing AWS Lambda function with a Kinesis Data Firehose stream processor that extracts individual log events from records sent by CloudWatch Logs subscription filters. Enable VPC flows logs, and send them to CloudWatch. Create a CloudWatch Logs subscription that sends log events to the Kinesis Data Firehose delivery stream."
                },
                {
                    "label": "C",
                    "content": "Ask the company to log every request that is made to the databases along with the EC2 instance IP address. Export the CloudWatch logs to an Amazon S3 bucket. Use Amazon Athena to query the logs grouped by database name. Export Athena results to another S3 bucket. Invoke an AWS Lambda function to automatically send any new file that is put in the S3 bucket to Splunk."
                },
                {
                    "label": "D",
                    "content": "Send the CloudWatch logs to an Amazon Kinesis data stream with Amazon Kinesis Data Analytics for SQL Applications. Configure a 1 minute sliding window to collect the events. Create a SQL query that uses the anomaly detection template to monitor any networking traffic anomalies in near-real time. Send the result to an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. This solution involves creating an Amazon Kinesis Data Firehose delivery stream with Splunk as the destination and configuring a pre-processing AWS Lambda function. The Lambda function is used to process the records from CloudWatch Logs before they are sent to the Kinesis Data Firehose stream. This allows for near-real-time delivery of VPC Flow Logs to Splunk, which meets the company's requirements for monitoring EC2 instances' connectivity to on-premises databases.</p><p>Requirements Recap:</p><p>1. Monitor EC2 instances connected to on-premises databases in near-real time.</p><p>2. Send networking traffic logs to Splunk (which is on-premises).</p><p>3. Current setup: &nbsp;</p><p> &nbsp; - Hybrid environment (AWS VPC + on-premises databases). &nbsp;</p><p> &nbsp; - EC2 instances send logs to Amazon CloudWatch. &nbsp;</p><p> &nbsp; - Splunk is already used on-premises for monitoring.</p><p> Analysis of Options:</p><p> Option A:</p><p>- Uses VPC Flow Logs → CloudWatch → S3 → Splunk (via Lambda & S3 pull). &nbsp;</p><p>- Issues: &nbsp;</p><p> &nbsp;- Not near-real time (S3 exports are periodic, not continuous). &nbsp;</p><p> &nbsp;- Manual credential management (ACCESS_KEY/SECRET_KEY) is less secure. &nbsp;</p><p> &nbsp;- Splunk pulling from S3 introduces latency. &nbsp;</p><p> Option B:</p><p>- Uses VPC Flow Logs → CloudWatch → Kinesis Data Firehose (with Lambda pre-processing) → Splunk. &nbsp;</p><p>- Advantages: &nbsp;</p><p> &nbsp;- Near-real time delivery (Kinesis Data Firehose streams logs directly to Splunk). &nbsp;</p><p> &nbsp;- No manual credential management (Firehose securely delivers logs). &nbsp;</p><p> &nbsp;- Supports CloudWatch Logs subscription filters for efficient log forwarding. &nbsp;</p><p> &nbsp;- Lambda pre-processing allows log transformation if needed. &nbsp;</p><p>- Best fits the requirement of near-real time monitoring. &nbsp;</p><p> Option C:</p><p>- Uses manual logging → S3 → Athena → Lambda → Splunk. &nbsp;</p><p>- Issues: &nbsp;</p><p> &nbsp;- Not near-real time (Athena queries and Lambda processing add latency). &nbsp;</p><p> &nbsp;- Overly complex for the requirement. &nbsp;</p><p> &nbsp;- Manual logging is error-prone. &nbsp;</p><p> Option D:</p><p>- Uses Kinesis Data Analytics for anomaly detection. &nbsp;</p><p>- Issues: &nbsp;</p><p> &nbsp;- Overkill for simply monitoring connections (anomaly detection is not required). &nbsp;</p><p> &nbsp;- More expensive and complex than necessary. &nbsp;</p><p> Conclusion:</p><p>Option B is the best solution because:</p><p>✔ Uses VPC Flow Logs (which track network connections, including EC2-to-database traffic). &nbsp;</p><p>✔ Kinesis Data Firehose provides near-real time delivery to Splunk. &nbsp;</p><p>✔ CloudWatch Logs subscription ensures efficient log forwarding. &nbsp;</p><p>✔ Minimal latency and complexity compared to other options. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "09c9cfd4d6f145fe9f879d93183991be",
            "questionNumber": 245,
            "type": "multiple",
            "content": "<p>Question #245</p><p>A company has five development teams that have each created five AWS accounts to develop and host applications. To track spending, the development teams log in to each account every month, record the current cost from the AWS Billing and Cost Management console, and provide the information to the company's finance team.</p><p>The company has strict compliance requirements and needs to ensure that resources are created only in AWS Regions in the United States. However, some resources have been created in other Regions.</p><p>A solutions architect needs to implement a solution that gives the finance team the ability to track and consolidate expenditures for all the accounts. The solution also must ensure that the company can create resources only in Regions in the United States.</p><p>Which combination of steps will meet these requirements in the MOST operationally efficient way? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new account to serve as a management account. Create an Amazon S3 bucket for the finance team. Use AWS Cost and Usage Reports to create monthly reports and to store the data in the finance team&#39;s S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Create a new account to serve as a management account. Deploy an organization in AWS Organizations with all features enabled. Invite all the existing accounts to the organization. Ensure that each account accepts the invitation."
                },
                {
                    "label": "C",
                    "content": "Create an OU that includes all the development teams. Create an SCP that allows the creation of resources only in Regions that are in the United States. Apply the SCP to the OU."
                },
                {
                    "label": "D",
                    "content": "Create an OU that includes all the development teams. Create an SCP that denies the creation of resources in Regions that are outside the United States. Apply the SCP to the OU."
                },
                {
                    "label": "E",
                    "content": "Create an IAM role in the management account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role. Use AWS Cost Explorer and the Billing and Cost Management console to analyze cost."
                },
                {
                    "label": "F",
                    "content": "Create an IAM role in each AWS account. Attach a policy that includes permissions to view the Billing and Cost Management console. Allow the finance team users to assume the role."
                }
            ],
            "correctAnswer": "BDE",
            "explanation": "<p>The correct answers are B, D, and E. To centralize billing and ensure compliance with regional restrictions, the following steps should be taken:</p><p>1. B: Create a new management account and set up AWS Organizations &nbsp;</p><p> &nbsp; - AWS Organizations allows consolidated billing, making it easier to track spending across all accounts. &nbsp;</p><p> &nbsp; - Enabling all features allows the use of Service Control Policies (SCPs) to enforce compliance. &nbsp;</p><p>2. D: Create an OU with an SCP that denies resource creation outside US Regions &nbsp;</p><p> &nbsp; - SCPs applied at the OU level prevent resources from being created in non-US Regions, ensuring compliance. &nbsp;</p><p> &nbsp; - A deny-based SCP is more secure than an allow-based one (Option C) because it blocks unwanted actions while allowing everything else. &nbsp;</p><p>3. E: Create an IAM role in the management account for the finance team &nbsp;</p><p> &nbsp; - The finance team can assume this role to access AWS Cost Explorer and the Billing Dashboard in the management account, which provides consolidated cost data. &nbsp;</p><p> &nbsp; - This is more efficient than creating roles in every account (Option F). &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: AWS Cost and Usage Reports (CUR) are useful but not necessary since AWS Organizations already provides consolidated billing. &nbsp;</p><p>- C: An allow-based SCP is less secure than a deny-based SCP (Option D). &nbsp;</p><p>- F: Creating IAM roles in every account is less efficient than using a single role in the management account (Option E). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7d36f2fce1eb452c9f8a424980f82987",
            "questionNumber": 246,
            "type": "single",
            "content": "<p>Question #246</p><p>A company needs to create and manage multiple AWS accounts for a number of departments from a central location. The security team requires read-only access to all accounts from its own AWS account. The company is using AWS Organizations and created an account for the security team.</p><p><br></p><p>How should a solutions architect meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use the OrganizationAccountAccessRole IAM role to create a new IAM policy with read-only access in each member account. Establish a trust relationship between the IAM policy in each member account and the security account. Ask the security team to use the IAM policy to gain access."
                },
                {
                    "label": "B",
                    "content": "Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access."
                },
                {
                    "label": "C",
                    "content": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the management account from the security account. Use the generated temporary credentials to gain access."
                },
                {
                    "label": "D",
                    "content": "Ask the security team to use AWS Security Token Service (AWS STS) to call the AssumeRole API for the OrganizationAccountAccessRole IAM role in the member account from the security account. Use the generated temporary credentials to gain access."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use the OrganizationAccountAccessRole IAM role to create a new IAM role with read-only access in each member account. Establish a trust relationship between the IAM role in each member account and the security account. Ask the security team to use the IAM role to gain access.</p><p>This solution meets the requirements for the following reasons:</p><p>Principle of Least Privilege: By creating a specific read-only role in each member account, the security team is granted exactly the level of access they need (read-only) rather than the broader permissions that typically come with the OrganizationAccountAccessRole.</p><p>Direct Access Path: This approach creates a direct trust relationship between the member accounts and the security team's account, allowing the security team to assume the read-only role directly without going through the management account.</p><p>Centralized Management: The initial setup can be done using the OrganizationAccountAccessRole, which allows administrators to access member accounts to create the necessary read-only roles. This leverages the existing AWS Organizations structure.</p><p>Scalability: While this approach requires creating roles in each member account, it can be automated using AWS CloudFormation StackSets to deploy consistent roles across all accounts in the organization.</p><p>Security Best Practices: This approach follows AWS security best practices by establishing cross-account access through role assumption rather than creating IAM users in multiple accounts.</p><p>Option C (using AWS STS to assume the OrganizationAccountAccessRole in the management account) is not appropriate because:</p><p>The OrganizationAccountAccessRole typically has administrative permissions, which violates the principle of least privilege for the security team that only needs read-only access.</p><p>This approach would require the security team to first access the management account and then access each member account, creating an unnecessary additional step.</p><p>It doesn't provide a direct path for the security team to access member accounts from their own account, which is a requirement.</p><p>The correct solution (Option B) provides a secure, scalable approach that follows AWS best practices for cross-account access while meeting the specific requirement of providing read-only access to the security team across all accounts from their own AWS account.</p><p>Sources</p><p>Using AWS Organizations for security - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/organizations-security.html）</p><p>Manage Organization Member Accounts via AWS CLI | AWS re:Post （https://repost.aws/questions/QUDltwquvCQtOrRIJUF6yreg/manage-organization-member-accounts-via-aws-cli）</p><p>SEC03-BP08 Share resources securely within your organization - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2025-02-25/framework/sec_permissions_share_securely.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "633e0b9fb02a4dfebff306ac014c6abb",
            "questionNumber": 247,
            "type": "single",
            "content": "<p>Question #247</p><p>A large company runs workloads in VPCs that are deployed across hundreds of AWS accounts. Each VPC consists of public subnets and private subnets that span across multiple Availability Zones. NAT gateways are deployed in the public subnets and allow outbound connectivity to the internet from the private subnets.</p><p><br></p><p>A solutions architect is working on a hub-and-spoke design. All private subnets in the spoke VPCs must route traffic to the internet through an egress VPC. The solutions architect already has deployed a NAT gateway in an egress VPC in a central AWS account.</p><p><br></p><p>Which set of additional steps should the solutions architect take to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create peering connections between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet."
                },
                {
                    "label": "B",
                    "content": "Create a transit gateway, and share it with the existing AWS accounts. Attach existing VPCs to the transit gateway. Configure the required routing to allow access to the internet."
                },
                {
                    "label": "C",
                    "content": "Create a transit gateway in every account. Attach the NAT gateway to the transit gateways. Configure the required routing to allow access to the internet."
                },
                {
                    "label": "D",
                    "content": "Create an AWS PrivateLink connection between the egress VPC and the spoke VPCs. Configure the required routing to allow access to the internet."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. A transit gateway can be used as a hub for network traffic between VPCs and on-premises networks. By creating a transit gateway and sharing it with existing AWS accounts, the architect can attach the VPCs to the transit gateway. This allows for centralized management of routing and ensures that all traffic from spoke VPCs is routed through the egress VPC with the NAT gateway, thus providing internet access in a hub-and-spoke architecture.</p><p>1. Hub-and-Spoke Design Requirement: The solution requires centralized internet egress through an egress VPC (hub) for all spoke VPCs across multiple AWS accounts.</p><p>2. Transit Gateway (TGW) Benefits:</p><p> &nbsp; - Centralized Routing: TGW allows you to connect hundreds of VPCs (spokes) to a central egress VPC (hub) efficiently.</p><p> &nbsp; - Cross-Account Sharing: You can create the TGW in one account (central account) and share it with other AWS accounts using AWS Resource Access Manager (RAM).</p><p> &nbsp; - Simplified Peering: Instead of managing individual VPC peering connections (which don’t scale well and have routing limitations), TGW provides a scalable and manageable solution.</p><p>3. Steps:</p><p> &nbsp; - Deploy a Transit Gateway in the central account (where the egress VPC is located).</p><p> &nbsp; - Share the TGW with spoke accounts using AWS RAM.</p><p> &nbsp; - Attach all spoke VPCs and the egress VPC to the TGW.</p><p> &nbsp; - Configure routing:</p><p> &nbsp; &nbsp; - Spoke VPCs: Route internet-bound traffic (0.0.0.0/0) to the TGW attachment.</p><p> &nbsp; &nbsp; - TGW Route Table: Route internet-bound traffic to the egress VPC attachment.</p><p> &nbsp; &nbsp; - Egress VPC: Ensure the NAT gateway routes traffic to the internet.</p><p>4. Why Not Other Options?:</p><p> &nbsp; - A (VPC Peering): Doesn’t scale well for hundreds of VPCs, requires complex routing, and lacks transitive routing.</p><p> &nbsp; - C (TGW in Every Account): Overly complex and unnecessary; a single shared TGW is sufficient.</p><p> &nbsp; - D (AWS PrivateLink): PrivateLink is for private connectivity to services, not for internet egress.</p><p> Key AWS Services Used:</p><p>- Transit Gateway: For scalable hub-and-spoke connectivity.</p><p>- AWS RAM (Resource Access Manager): To share the TGW across accounts.</p><p>- NAT Gateway: For outbound internet access from the egress VPC.</p><p>This approach ensures scalability, centralized management, and cost efficiency for internet egress across hundreds of accounts.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "93fb5dc9803c4d62a98591b803b3f77a",
            "questionNumber": 248,
            "type": "single",
            "content": "<p>Question #248</p><p>An education company is running a web application used by college students around the world. The application runs in an Amazon Elastic Container Service (Amazon ECS) cluster in an Auto Scaling group behind an Application Load Balancer (ALB). A system administrator detects a weekly spike in the number of failed login attempts, which overwhelm the application's authentication service. All the failed login attempts originate from about 500 different IP addresses that change each week. A solutions architect must prevent the failed login attempts from overwhelming the authentication service.</p><p><br></p><p>Which solution meets these requirements with the MOST operational efficiency?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Firewall Manager to create a security group and security group policy to deny access from the IP addresses."
                },
                {
                    "label": "B",
                    "content": "Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB."
                },
                {
                    "label": "C",
                    "content": "Use AWS Firewall Manager to create a security group and security group policy to allow access only to specific CIDR ranges."
                },
                {
                    "label": "D",
                    "content": "Create an AWS WAF web ACL with an IP set match rule, and set the rule action to Block. Connect the web ACL to the ALB."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS WAF web ACL with a rate-based rule, and set the rule action to Block. Connect the web ACL to the ALB.</p><p>This solution meets the requirements with the most operational efficiency for the following reasons:</p><p>Dynamic Protection Against Changing IP Addresses: The scenario specifically mentions that the 500 IP addresses change each week. AWS WAF rate-based rules automatically identify and block IP addresses that exceed a specified request rate, regardless of what those IP addresses are. This eliminates the need to manually update IP address lists weekly.</p><p>Automatic Adaptation: Rate-based rules automatically adapt to new threat sources without manual intervention. When the weekly spike occurs with new IP addresses, the rate-based rule will detect and block excessive requests from those IPs without requiring updates to the rule.</p><p>Targeted Protection: AWS WAF can be configured to specifically protect the authentication endpoint that's being overwhelmed, rather than blocking access to the entire application.</p><p>Operational Efficiency: Once configured, the rate-based rule requires minimal ongoing maintenance. It will continue to function effectively even as the attacking IP addresses change each week, making it highly operationally efficient.</p><p>Integration with ALB: AWS WAF integrates directly with Application Load Balancers, making implementation straightforward.</p><p>Option A (using AWS Firewall Manager to create security groups) would be less operationally efficient because:</p><p>It would require weekly updates to the security group rules to block the new set of 500 IP addresses.</p><p>Managing a large and constantly changing list of blocked IP addresses would create significant operational overhead.</p><p>Security groups are not designed for this type of dynamic threat mitigation where the source IPs change frequently.</p><p>This approach would require creating automation to detect and update the blocked IP addresses, adding complexity.</p><p>The AWS WAF rate-based rule solution provides an elegant, low-maintenance approach that automatically adapts to the changing threat landscape without requiring weekly manual updates or complex automation. This makes it the most operationally efficient solution for protecting the authentication service from the weekly spikes in failed login attempts.</p><p>Sources</p><p>AWS WAF Rate-Based Rule - How to permanently block offending IP addresses? | AWS re:Post （https://repost.aws/questions/QUyq6dArvCRECG10yKcg6erg/aws-waf-rate-based-rule-how-to-permanently-block-offending-ip-addresses）</p><p>Allow good bots in WAF rate-based rule | AWS re:Post （https://repost.aws/questions/QUNcT9XrO8RaWpMT8RAtO4Vg/allow-good-bots-in-waf-rate-based-rule）</p><p>AWS WAF – Rate-based rules - AWS Best Practices for DDoS Resiliency （https://docs.aws.amazon.com/whitepapers/latest/aws-best-practices-ddos-resiliency/aws-waf-rate-based-rules.html）</p><p>Static controls for managing bots - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/bot-control/static-controls.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "cd813e9bffa747909d389a1688dae1bb",
            "questionNumber": 249,
            "type": "single",
            "content": "<p>Question #249</p><p>A company operates an on-premises software-as-a-service (SaaS) solution that ingests several files daily. The company provides multiple public SFTP endpoints to its customers to facilitate the file transfers. The customers add the SFTP endpoint IP addresses to their firewall allow list for outbound traffic. Changes to the SFTP endpoint IP addresses are not permitted.</p><p><br></p><p>The company wants to migrate the SaaS solution to AWS and decrease the operational overhead of the file transfer service.</p><p><br></p><p>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Register the customer-owned block of IP addresses in the company&#39;s AWS account. Create Elastic IP addresses from the address pool and assign them to an AWS Transfer for SFTP endpoint. Use AWS Transfer to store the files in Amazon S3."
                },
                {
                    "label": "B",
                    "content": "Add a subnet containing the customer-owned block of IP addresses to a VPC. Create Elastic IP addresses from the address pool and assign them to an Application Load Balancer (ALB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the ALB. Store the files in attached Amazon Elastic Block Store (Amazon EBS) volumes."
                },
                {
                    "label": "C",
                    "content": "Register the customer-owned block of IP addresses with Amazon Route 53. Create alias records in Route 53 that point to a Network Load Balancer (NLB). Launch EC2 instances hosting FTP services in an Auto Scaling group behind the NLB. Store the files in Amazon S3."
                },
                {
                    "label": "D",
                    "content": "Register the customer-owned block of IP addresses in the company&rsquo;s AWS account. Create Elastic IP addresses from the address pool and assign them to an Amazon S3 VPC endpoint. Enable SFTP support on the S3 bucket."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The correct answer is A. By using AWS Transfer for SFTP, the company can maintain the existing IP addresses by registering them with AWS and assigning Elastic IPs to the SFTP endpoint. This approach allows customers to continue using the same IP addresses in their firewall allow list, satisfying the requirement that changes to the SFTP endpoint IP addresses are not permitted. Additionally, by using AWS Transfer for SFTP, the company can offload the operational overhead of managing an SFTP server to AWS, and leverage Amazon S3 for scalable file storage.</p><p>1. SFTP endpoints must retain the same IP addresses (changes are not permitted).</p><p>2. Migrate to AWS and reduce operational overhead.</p><p>3. Store files in Amazon S3 (implied by the need to migrate SaaS to AWS).</p><p> Analysis of Options:</p><p>- Option A (✅ Correct):</p><p> &nbsp;- Uses AWS Transfer for SFTP, a fully managed SFTP service.</p><p> &nbsp;- Allows assigning Elastic IPs from a customer-owned IP block (ensuring no IP changes).</p><p> &nbsp;- Stores files directly in Amazon S3, reducing operational overhead.</p><p> &nbsp;- Fully managed, so no need to maintain EC2 instances or scaling.</p><p>- Option B (❌ Incorrect):</p><p> &nbsp;- Uses EC2 instances with an ALB, which does not support SFTP (ALB works at Layer 7, not SFTP).</p><p> &nbsp;- Stores files on EBS volumes, which is not scalable and increases management overhead.</p><p> &nbsp;- ALB does not support static IP assignment (unlike AWS Transfer for SFTP or NLB).</p><p>- Option C (❌ Incorrect):</p><p> &nbsp;- Uses Route 53 and NLB, but NLBs do not support SFTP (they operate at Layer 4).</p><p> &nbsp;- Requires managing EC2 instances, increasing operational overhead.</p><p> &nbsp;- Route 53 does not solve the IP requirement (customers need static IPs, not DNS).</p><p>- Option D (❌ Incorrect):</p><p> &nbsp;- S3 VPC endpoints do not support SFTP (AWS Transfer for SFTP is needed).</p><p> &nbsp;- S3 does not natively support SFTP (this is a distractor).</p><p> Conclusion:</p><p>AWS Transfer for SFTP (Option A) is the only fully managed solution that allows:</p><p>- Static IP assignment (from customer-owned IP block).</p><p>- Direct S3 storage (reducing operational overhead).</p><p>- Native SFTP support without managing servers.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a8d335626ccf43c1ad8c684964d1972d",
            "questionNumber": 250,
            "type": "single",
            "content": "<p>Question #250</p><p>A company has a new application that needs to run on five Amazon EC2 instances in a single AWS Region. The application requires high throughput, low-latency network connections between all of the EC2 instances where the application will run. There is no requirement for the application to be fault tolerant.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking."
                },
                {
                    "label": "B",
                    "content": "Launch five new EC2 instances into an Auto Scaling group in the same Availability Zone. Attach an extra elastic network interface to each EC2 instance."
                },
                {
                    "label": "C",
                    "content": "Launch five new EC2 instances into a partition placement group. Ensure that the EC2 instance type supports enhanced networking."
                },
                {
                    "label": "D",
                    "content": "Launch five new EC2 instances into a spread placement group. Attach an extra elastic network interface to each EC2 instance."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Launch five new EC2 instances into a cluster placement group. Ensure that the EC2 instance type supports enhanced networking.</p><p>This solution will meet the requirements for the following reasons:</p><p>Cluster Placement Group: A cluster placement group is specifically designed to pack instances close together within a single Availability Zone, providing the lowest possible network latency and highest network throughput between instances. This directly addresses the requirement for \"high-throughput, low-latency network connections between all of the EC2 instances.\"</p><p>Enhanced Networking: Enhanced networking uses single root I/O virtualization (SR-IOV) to provide higher I/O performance and lower CPU utilization compared to traditional virtualized network interfaces. This further improves the network performance between instances, supporting the high-throughput, low-latency requirement.</p><p>Single Region Deployment: The solution keeps all instances within a single region as specified in the requirements.</p><p>No Fault Tolerance Requirement: The scenario explicitly states there is no requirement for the application to be fault tolerant. A cluster placement group places all instances in a single Availability Zone, which reduces fault tolerance but maximizes network performance - perfectly aligned with the stated requirements.</p><p>The other options would not be optimal:</p><p>B. Auto Scaling group in the same AZ with extra elastic network interfaces - While this keeps instances in the same AZ, it doesn't provide the same level of network optimization as a cluster placement group. Adding extra network interfaces is primarily useful for connecting to different networks, not for improving performance between instances in the same application.</p><p>C. Partition placement group with enhanced networking - Partition placement groups are designed to spread instances across distinct racks (partitions), which is useful for distributed applications that need to reduce correlated failures. This doesn't optimize for the lowest possible latency between all instances.</p><p>D. Spread placement group with extra network interfaces - Spread placement groups place instances on distinct underlying hardware to reduce the risk of simultaneous failures. This actually increases the physical distance between instances, which would likely increase latency rather than decrease it.</p><p>Option A provides the optimal solution for maximizing network performance between the five EC2 instances without concern for fault tolerance.</p><p>Sources</p><p>PERF04-BP06 Choose your workload's location based on network requirements - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2024-06-27/framework/perf_networking_choose_workload_location_network_requirements.html）</p><p>Placement Groups Latency | AWS re:Post （https://repost.aws/questions/QUz8Qi26ZfSZi6oeRtwRKfgQ/placement-groups-latency）</p><p>Optimize network performance on EC2 Windows instances - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/enhanced-networking-os.html）</p><p>Benchmark network throughput between Linux instances in the same VPC | AWS re:Post （https://repost.aws/knowledge-center/network-throughput-benchmark-linux-ec2）</p><p>PERF04-BP06 Choose your workload's location based on network requirements - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/perf_networking_choose_workload_location_network_requirements.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "17b9512fb2494231bd13f5380151fd83",
            "questionNumber": 251,
            "type": "single",
            "content": "<p>Question #251</p><p>A company is creating a REST API to share information with six of its partners based in the United States. The company has created an Amazon API Gateway Regional endpoint. Each of the six partners will access the API once per day to post daily sales figures.</p><p><br></p><p>After initial deployment, the company observes 1,000 requests per second originating from 500 different IP addresses around the world. The company believes this traffic is originating from a botnet and wants to secure its API while minimizing cost.</p><p><br></p><p>Which approach should the company take to secure its API?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Configure CloudFront with an origin access identity (OAI) and associate it with the distribution. Configure API Gateway to ensure only the OAI can run the POST method."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon CloudFront distribution with the API as the origin. Create an AWS WAF web ACL with a rule to block clients that submit more than five requests per day. Associate the web ACL with the CloudFront distribution. Add a custom header to the CloudFront distribution populated with an API key. Configure the API to require an API key on the POST method."
                },
                {
                    "label": "C",
                    "content": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a resource policy with a request limit and associate it with the API. Configure the API to require an API key on the POST method."
                },
                {
                    "label": "D",
                    "content": "Create an AWS WAF web ACL with a rule to allow access to the IP addresses used by the six partners. Associate the web ACL with the API. Create a usage plan with a request limit and associate it with the API. Create an API key and add it to the usage plan."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The correct answer is D. To secure the API from the botnet traffic, the company can use AWS WAF to create a web ACL that allows only the known IP addresses of the six partners. Additionally, by creating a usage plan with a request limit in API Gateway and requiring an API key for the POST method, the company can ensure that only authorized requests are processed, and it can limit the rate of requests to prevent the botnet traffic from overwhelming the API.</p><p>1. Requirements:</p><p> &nbsp; - The company has a Regional API Gateway endpoint.</p><p> &nbsp; - Only 6 partners (with known IPs) should access the API once per day.</p><p> &nbsp; - There is malicious traffic (botnet) from 500+ IPs worldwide making 1000 requests/sec.</p><p> &nbsp; - The goal is to secure the API while minimizing cost.</p><p>2. Key Security Measures Needed:</p><p> &nbsp; - Restrict access to only the 6 partners' IPs (whitelisting).</p><p> &nbsp; - Limit request rates to prevent abuse.</p><p> &nbsp; - Use API keys for additional authentication.</p><p> Why Option D is Correct:</p><p>- AWS WAF Web ACL with IP Allow List:</p><p> &nbsp;- Blocks all traffic except from the 6 partners' IPs.</p><p> &nbsp;- Effectively stops botnet traffic.</p><p>- Usage Plan with Request Limit:</p><p> &nbsp;- Enforces a rate limit (e.g., 1 request/day per partner).</p><p> &nbsp;- Prevents abuse even if IP whitelisting fails.</p><p>- API Key:</p><p> &nbsp;- Adds an extra layer of authentication.</p><p> &nbsp;- The key is added to the usage plan, ensuring only authorized clients can call the API.</p><p> Why Other Options Are Incorrect:</p><p>- Option A & B (CloudFront + WAF + OAI/API Key):</p><p> &nbsp;- CloudFront is unnecessary since the API is Regional (not global).</p><p> &nbsp;- Overcomplicates the solution and increases cost.</p><p>- Option C (WAF + Resource Policy + API Key):</p><p> &nbsp;- Resource policies are for cross-account access or IP-based restrictions, but WAF already handles IP whitelisting.</p><p> &nbsp;- A usage plan (Option D) is better for rate-limiting than a resource policy.</p><p> Conclusion:</p><p>Option D is the most cost-effective and secure solution, combining IP whitelisting, rate limiting, and API key authentication.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3fb015fc0b614020a249c6c74a32808d",
            "questionNumber": 252,
            "type": "single",
            "content": "<p>Question #252</p><p>A company uses an Amazon Aurora PostgreSQL DB cluster for applications in a single AWS Region. The company's database team must monitor all data activity on all the databases.</p><p><br></p><p>Which solution will achieve this goal?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon OpenSearch Service cluster for further analysis."
                },
                {
                    "label": "B",
                    "content": "Start a database activity stream on the Aurora DB cluster to capture the activity stream in Amazon EventBridge. Define an AWS Lambda function as a target for EventBridge. Program the Lambda function to decrypt the messages from EventBridge and to publish all database activity to Amazon S3 for further analysis."
                },
                {
                    "label": "C",
                    "content": "Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis."
                },
                {
                    "label": "D",
                    "content": "Set up an AWS Database Migration Service (AWS DMS) change data capture (CDC) task. Specify the Aurora DB cluster as the source. Specify Amazon Kinesis Data Firehose as the target. Use Kinesis Data Firehose to upload the data into an Amazon Redshift cluster. Run queries on the Amazon Redshift data to determine database activities on the Aurora database."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Start a database activity stream on the Aurora DB cluster to push the activity stream to an Amazon Kinesis data stream. Configure Amazon Kinesis Data Firehose to consume the Kinesis data stream and to deliver the data to Amazon S3 for further analysis.</p><p>This solution will achieve the goal of monitoring all data activity on all the databases in the Aurora PostgreSQL DB cluster for the following reasons:</p><p>Comprehensive Monitoring: Database Activity Streams is specifically designed for database auditing and monitoring purposes. It captures a wide range of database activities including data manipulation language (DML) operations, data definition language (DDL) operations, and system events. This provides complete visibility into all database activity, which is the primary requirement.</p><p>Real-time Monitoring: Database Activity Streams pushes activities to an Amazon Kinesis data stream in near real-time, allowing the database team to have immediate visibility into database actions as they occur.</p><p>Complete Audit Trail: The solution captures all database activity including SQL statements, users, application contexts, and timestamps, providing a comprehensive audit trail for the database team.</p><p>Scalable Architecture: Using Kinesis Data Streams with Kinesis Data Firehose creates a scalable pipeline that can handle the volume of activity from all databases in the cluster.</p><p>Persistent Storage: Delivering the data to Amazon S3 provides durable, long-term storage of all database activity, enabling historical analysis and compliance reporting.</p><p>Analysis Capabilities: With the data in S3, the database team can perform further analysis using various AWS analytics services like Amazon Athena, Amazon QuickSight, or Amazon EMR.</p><p>Option A (using AWS DMS with CDC) is not optimal because:</p><p>AWS DMS with CDC is primarily designed for data replication and migration, not comprehensive database activity monitoring.</p><p>It focuses mainly on capturing data changes (inserts, updates, deletes) rather than all database activity including queries, schema changes, and system events.</p><p>It may not capture all the information needed for complete database activity monitoring, such as user session details, SQL statements that don't modify data, and system events.</p><p>The database activity streams solution (Option C) provides a purpose-built, comprehensive approach to monitoring all database activity, which directly addresses the company's requirement to monitor all data activity on all databases in their Aurora PostgreSQL cluster.</p><p>Sources</p><p>Replace Amazon QLDB with Amazon Aurora PostgreSQL for audit use cases | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/replace-amazon-qldb-with-amazon-aurora-postgresql-for-audit-use-cases/）</p><p>Monitoring your Amazon RDS DB instance - Amazon Relational Database Service (https://docs.aws.amazon.com/AmazonRDS/latest/gettingstartedguide/managing-monitoring-perf.html)</p><p>Part 1: Audit Aurora PostgreSQL databases using Database Activity Streams and pgAudit | AWS Database Blog (https://aws.amazon.com/cn/blogs/database/part-1-audit-aurora-postgresql-databases-using-database-activity-streams-and-pgaudit/)</p><p>Monitoring tools for Amazon Aurora - Amazon Aurora (https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/MonitoringOverview.html)</p><p>Monitoring events, logs, and streams in an Amazon Aurora DB cluster - Amazon Aurora (https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Monitor_Logs_Events.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "95a35a8fc7bc49b99bf58950f696b981",
            "questionNumber": 253,
            "type": "single",
            "content": "<p>Question #253</p><p>An entertainment company recently launched a new game. To ensure a good experience for players during the launch period, the company deployed a static quantity of 12 r6g.16xlarge (memory optimized) Amazon EC2 instances behind a Network Load Balancer. The company's operations team used the Amazon CloudWatch agent and a custom metric to include memory utilization in its monitoring strategy.</p><p><br></p><p>Analysis of the CloudWatch metrics from the launch period showed consumption at about one quarter of the CPU and memory that the company expected. Initial demand for the game has subsided and has become more variable. The company decides to use an Auto Scaling group that monitors the CPU and memory consumption to dynamically scale the instance fleet. A solutions architect needs to configure the Auto Scaling group to meet demand in the most cost-effective way.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the Auto Scaling group to deploy c6g.4xlarge (compute optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12."
                },
                {
                    "label": "B",
                    "content": "Configure the Auto Scaling group to deploy m6g.4xlarge (general purpose) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12."
                },
                {
                    "label": "C",
                    "content": "Configure the Auto Scaling group to deploy r6g.4xlarge (memory optimized) instances. Configure a minimum capacity of 3, a desired capacity of 3, and a maximum capacity of 12."
                },
                {
                    "label": "D",
                    "content": "Configure the Auto Scaling group to deploy r6g.8xlarge (memory optimized) instances. Configure a minimum capacity of 2, a desired capacity of 2, and a maximum capacity of 6."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. Given that the company's analysis showed that the initially deployed instances were only utilizing a quarter of their CPU and memory capacity, it would be cost-inefficient to maintain the same instance size. By choosing option C, the company can downscale to a more appropriate instance size that matches the current demand while still providing memory-optimized performance. The Auto Scaling group can then adjust the number of instances between the minimum of 3 and the maximum of 12 based on actual CPU and memory consumption, which aligns with the company's need for a cost-effective and responsive scaling strategy.The original workload ran on memory-optimized instances with low utilization, so downsizing to `r6g.4xlarge` is appropriate. Auto Scaling with a minimum of 3 instances ensures baseline capacity, while scaling up to 12 handles spikes cost-effectively.</p><p>Option A: `c6g.4xlarge` (Compute Optimized)</p><p> &nbsp; - Problem: The workload was originally on memory-optimized instances, and memory utilization was low but still a factor. Switching to compute-optimized instances may not be appropriate if memory is still a consideration. &nbsp;</p><p> &nbsp; - Not the best choice. &nbsp;</p><p> Option B: `m6g.4xlarge` (General Purpose)</p><p> &nbsp; - Problem: While general-purpose instances balance CPU and memory, the workload was initially on memory-optimized instances. If memory is a key factor, general-purpose may not be optimal. &nbsp;</p><p> &nbsp; - Not the best choice. &nbsp;</p><p> Option C: `r6g.4xlarge` (Memory Optimized)</p><p> &nbsp; - Pros: &nbsp;</p><p> &nbsp; &nbsp; - Memory-optimized, matching the original instance family. &nbsp;</p><p> &nbsp; &nbsp; - Smaller size (`4xlarge` vs `16xlarge`) reduces cost while still providing sufficient memory. &nbsp;</p><p> &nbsp; &nbsp; - Scaling (3–12 instances) allows flexibility for variable demand. &nbsp;</p><p> &nbsp; - Best choice if memory is still a key requirement. &nbsp;</p><p> Option D: `r6g.8xlarge` (Memory Optimized)</p><p> &nbsp; - Pros: &nbsp;</p><p> &nbsp; &nbsp; - Still memory-optimized. &nbsp;</p><p> &nbsp; - Cons: &nbsp;</p><p> &nbsp; &nbsp; - Larger instance size (`8xlarge`) may still be over-provisioned given the previous low utilization. &nbsp;</p><p> &nbsp; &nbsp; - Fewer instances (2–6) means less granular scaling, potentially leading to less cost efficiency. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "bf00e8f3c1044a8bbe566e457c813a0b",
            "questionNumber": 254,
            "type": "single",
            "content": "<p>Question #254</p><p>A financial services company loaded millions of historical stock trades into an Amazon DynamoDB table. The table uses on-demand capacity mode. Once each day at midnight, a few million new records are loaded into the table. Application read activity against the table happens in bursts throughout the day, and a limited set of keys are repeatedly looked up. The company needs to reduce costs associated with DynamoDB.</p><p><br></p><p>Which strategy should a solutions architect recommend to meet this requirement?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy an Amazon ElastiCache cluster in front of the DynamoDB table"
                },
                {
                    "label": "B",
                    "content": "Deploy DynamoDB Accelerator (DAX). Configure DynamoDB auto scaling. Purchase Savings Plans in Cost Explorer."
                },
                {
                    "label": "C",
                    "content": "Use provisioned capacity mode. Purchase Savings Plans in Cost Explorer."
                },
                {
                    "label": "D",
                    "content": "Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Deploy DynamoDB Accelerator (DAX). Use provisioned capacity mode. Configure DynamoDB auto scaling.</p><p>This strategy will best reduce costs associated with DynamoDB for the financial services company's specific usage pattern for the following reasons:</p><p>DynamoDB Accelerator (DAX):</p><p>The company's workload has \"bursts throughout the day, and a limited set of keys are repeatedly looked up.\" This is an ideal use case for DAX, which provides in-memory caching specifically designed for DynamoDB.</p><p>DAX can significantly reduce the number of read requests that hit the DynamoDB table by caching frequently accessed items, which directly reduces costs since DynamoDB charges per request.</p><p>For the \"limited set of keys\" that are \"repeatedly looked up,\" DAX will provide microsecond response times while dramatically reducing the read capacity required from the main DynamoDB table.</p><p>Provisioned Capacity Mode:</p><p>The company's usage pattern is relatively predictable: millions of historical records with a few million new records loaded once daily at midnight, and bursty but predictable read patterns.</p><p>Provisioned capacity mode is more cost-effective than on-demand capacity mode for predictable workloads, as it allows you to pay for reserved capacity at a lower rate than on-demand pricing.</p><p>Since the company knows approximately when and how much data will be loaded (at midnight each day), they can provision appropriate write capacity for this predictable spike.</p><p>DynamoDB Auto Scaling:</p><p>Auto scaling allows the provisioned capacity to adjust automatically based on actual usage, which addresses the \"bursts throughout the day\" in read activity.</p><p>This prevents over-provisioning during low-usage periods while ensuring sufficient capacity during peak times.</p><p>Auto scaling helps optimize costs by matching provisioned capacity to actual demand patterns.</p><p>The other options are less optimal:</p><p>A. ElastiCache alone would add complexity and management overhead without the DynamoDB-specific optimizations that DAX provides.</p><p>B. Combining DAX and auto scaling with on-demand capacity mode would be more expensive than using provisioned capacity for this predictable workload. Savings Plans wouldn't offset this inefficiency.</p><p>C. Provisioned capacity mode with Savings Plans but without DAX wouldn't address the repeated lookups of a limited set of keys, missing a significant cost optimization opportunity.</p><p>Option D provides the most comprehensive cost optimization strategy by addressing all aspects of the company's usage pattern: caching for repeated reads, provisioned capacity for predictable workloads, and auto scaling for handling bursts efficiently.</p><p>Sources</p><p>Optimizing costs on DynamoDB tables - Amazon DynamoDB (https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-cost-optimization.html)</p><p>In-memory acceleration with DynamoDB Accelerator (DAX) - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.html）</p><p>Delhivery Case Study | AWS Cloud Financial Management （https://aws.amazon.com/cn/aws-cost-management/）</p><p>Use caching to reduce database demand - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/net-caching.html）</p><p>Is there a free tier for Dynamo DB On-Demand | AWS re:Post （https://repost.aws/questions/QULYTitlzWR3yD1fZ7OAjrtQ/is-there-a-free-tier-for-dynamo-db-on-demand）</p><p>Reduce latency and cost in read-heavy applications using Amazon DynamoDB Accelerator | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/reduce-latency-and-cost-in-read-heavy-applications-using-amazon-dynamodb-accelerator/）</p><p>How Samsung Cloud optimized Amazon DynamoDB costs | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/how-samsung-cloud-optimized-amazon-dynamodb-costs/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6c7795f24ce74287ba857e08f7ece53f",
            "questionNumber": 255,
            "type": "multiple",
            "content": "<p>Question #255</p><p>A company is creating a centralized logging service running on Amazon EC2 that will receive and analyze logs from hundreds of AWS accounts. AWS PrivateLink is being used to provide connectivity between the client services and the logging service. In each AWS account with a client, an interface endpoint has been created for the logging service and is available. The logging service running on EC2 instances with a Network Load Balancer (NLB) are deployed in different subnets. The clients are unable to submit logs using the VPC endpoint.</p><p>Which combination of steps should a solutions architect take to resolve this issue? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Check that the NACL is attached to the logging service subnet to allow communications to and from the NLB subnets. Check that the NACL is attached to the NLB subnet to allow communications to and from the logging service subnets running on EC2 instances."
                },
                {
                    "label": "B",
                    "content": "Check that the NACL is attached to the logging service subnets to allow communications to and from the interface endpoint subnets. Check that the NACL is attached to the interface endpoint subnet to allow communications to and from the logging service subnets running on EC2 instances."
                },
                {
                    "label": "C",
                    "content": "Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the NLB subnets."
                },
                {
                    "label": "D",
                    "content": "Check the security group for the logging service running on the EC2 instances to ensure it allows ingress from the clients."
                },
                {
                    "label": "E",
                    "content": "Check the security group for the NLB to ensure it allows ingress from the interface endpoint subnets."
                }
            ],
            "correctAnswer": "AC",
            "explanation": "<p>The correct answer is A and C. </p><p>To resolve the issue of clients being unable to submit logs using the VPC endpoint, a solutions architect should take the following steps:</p><p><br></p><p>Ensure that the Network Load Balancer (NLB) is configured correctly:</p><p><br></p><p>The NLB should be in the same VPC as the EC2 instances running the logging service.</p><p>Verify that the NLB's target group includes the EC2 instances running the logging service.</p><p>Check that the health checks for the target group are passing.</p><p>Configure the VPC endpoint service to use the Network Load Balancer:</p><p><br></p><p>In the VPC console, go to \"Endpoint Services\" and create a new endpoint service.</p><p>Select the Network Load Balancer as the network load balancer for this endpoint service.</p><p>Make sure to add the AWS account IDs of the client accounts to the \"Allowed principals\" list.</p><p>These two steps are crucial because:</p><p><br></p><p>The NLB configuration ensures that the logging service is properly load-balanced and accessible.</p><p>Creating and configuring the VPC endpoint service to use the NLB establishes the necessary link between the PrivateLink interface endpoints in the client accounts and the logging service in the central account.</p><p>Additional considerations:</p><p><br></p><p>Ensure that the security groups associated with the EC2 instances allow incoming traffic from the NLB.</p><p>Verify that the NLB listener is configured to forward traffic to the correct port on the EC2 instances.</p><p>Check that the VPC endpoint service is in an \"Available\" state.</p><p>In the client accounts, ensure that the VPC endpoint is associated with the correct subnets and security groups.</p><p>By implementing these steps, you should resolve the connectivity issue and allow clients to submit logs using the VPC endpoint through AWS PrivateLink.</p><p>Why Not Other Options?</p><p>- Option B: Incorrect because NACLs should focus on NLB ↔ EC2 communication, not the interface endpoint (PrivateLink handles this).</p><p>- Option D: Incorrect because clients connect via the interface endpoint, not directly to EC2 instances. The NLB acts as the intermediary.</p><p>- Option E: Incorrect because the NLB’s security group should allow traffic from clients (via the interface endpoint), not just the endpoint subnets. However, NLBs don’t use security groups (they rely on NACLs).</p><p><br></p><p>https://repost.aws/knowledge-center/security-network-acl-vpc-endpoint </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f3849f116ccb43758b0c67ff24394302",
            "questionNumber": 256,
            "type": "single",
            "content": "<p>Question #256</p><p>A company has millions of objects in an Amazon S3 bucket. The objects are in the S3 Standard storage class. All the S3 objects are accessed frequently. The number of users and applications that access the objects is increasing rapidly. The objects are encrypted with server-side encryption with AWS KMS keys (SSE-KMS).</p><p><br></p><p>A solutions architect reviews the company’s monthly AWS invoice and notices that AWS KMS costs are increasing because of the high number of requests from Amazon S3. The solutions architect needs to optimize costs with minimal changes to the application.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new S3 bucket that has server-side encryption with customer-provided keys (SSE-C) as the encryption type. Copy the existing objects to the new S3 bucket. Specify SSE-C."
                },
                {
                    "label": "B",
                    "content": "Create a new S3 bucket that has server-side encryption with Amazon S3 managed keys (SSE-S3) as the encryption type. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Specify SSE-S3."
                },
                {
                    "label": "C",
                    "content": "Use AWS CloudHSM to store the encryption keys. Create a new S3 bucket. Use S3 Batch Operations to copy the existing objects to the new S3 bucket. Encrypt the objects by using the keys from CloudHSM."
                },
                {
                    "label": "D",
                    "content": "Use the S3 Intelligent-Tiering storage class for the S3 bucket. Create an S3 Intelligent-Tiering archive configuration to transition objects that are not accessed for 90 days to S3 Glacier Deep Archive."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. To optimize costs with minimal operational overhead, the solutions architect can switch the encryption method from SSE-KMS to SSE-S3. This change reduces costs because SSE-S3 does not incur the per-request charges that AWS KMS does. Using S3 Batch Operations to copy the existing objects to a new bucket with SSE-S3 encryption requires minimal changes to the application and can be done without significant operational overhead.</p><p>The primary issue is the high AWS KMS costs due to frequent encryption/decryption requests for objects in the S3 bucket. The requirement is to reduce costs with minimal changes to the application. &nbsp;</p><p>- Option A (SSE-C) requires managing customer-provided keys, which introduces operational overhead (key management, rotation, etc.). This is not minimal effort. &nbsp;</p><p>- Option B (SSE-S3) replaces KMS encryption with S3-managed keys, which eliminates KMS request costs while maintaining server-side encryption. S3 Batch Operations automates the migration, minimizing manual effort. &nbsp;</p><p>- Option C (CloudHSM) is overly complex and expensive for this use case, requiring key management in a dedicated HSM. &nbsp;</p><p>- Option D (Intelligent-Tiering) does not address the KMS cost issue—it only optimizes storage costs. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "152ef4da609e4d1f9b2b41518f92ad90",
            "questionNumber": 257,
            "type": "multiple",
            "content": "<p>Question #257</p><p>A media storage application uploads user photos to Amazon S3 for processing by AWS Lambda functions. Application state is stored in Amazon DynamoDB tables. Users are reporting that some uploaded photos are not being processed properly. The application developers trace the logs and find that Lambda is experiencing photo processing issues when thousands of users upload photos simultaneously. The issues are the result of Lambda concurrency limits and the performance of DynamoDB when data is saved.</p><p>Which combination of actions should a solutions architect take to increase the performance and reliability of the application? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Evaluate and adjust the RCUs for the DynamoDB tables."
                },
                {
                    "label": "B",
                    "content": "Evaluate and adjust the WCUs for the DynamoDB tables."
                },
                {
                    "label": "C",
                    "content": "Add an Amazon ElastiCache layer to increase the performance of Lambda functions."
                },
                {
                    "label": "D",
                    "content": "Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions."
                },
                {
                    "label": "E",
                    "content": "<p>Use S3 Transfer Acceleration to provide lower latency to users.</p>"
                }
            ],
            "correctAnswer": "BD",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>B. Evaluate and adjust the WCUs for the DynamoDB tables. </p><p>D. Add an Amazon Simple Queue Service (Amazon SQS) queue and reprocessing logic between Amazon S3 and the Lambda functions.</p><p>These two actions together will address the performance and reliability issues in the application for the following reasons:</p><p>Evaluate and adjust the WCUs for the DynamoDB tables:</p><p>The scenario specifically mentions that there are \"performance issues with DynamoDB when data is saved,\" which directly points to write capacity limitations.</p><p>When thousands of users upload photos simultaneously, the Lambda functions are likely trying to write processing results and application state to DynamoDB at the same time, potentially exceeding the provisioned write capacity.</p><p>Increasing the Write Capacity Units (WCUs) will allow DynamoDB to handle more concurrent write operations, reducing throttling and improving the reliability of data storage during high-volume periods.</p><p>This addresses the DynamoDB performance bottleneck mentioned in the scenario.</p><p>Add an Amazon SQS queue and reprocessing logic between Amazon S3 and the Lambda functions:</p><p>The scenario mentions issues with \"Lambda concurrency limits\" when thousands of users upload photos simultaneously.</p><p>Adding an SQS queue decouples the S3 upload events from the Lambda processing, allowing the system to buffer incoming requests during traffic spikes.</p><p>Instead of trying to process all uploads immediately (which hits concurrency limits), Lambda functions can process items from the queue at a controlled rate.</p><p>This pattern provides built-in retry capabilities for failed processing attempts, improving reliability.</p><p>The queue acts as a buffer that smooths out the processing workload, preventing the Lambda concurrency limits from being reached during upload spikes.</p><p>The other options would not effectively address the specific issues mentioned:</p><p>A. Evaluate and adjust the RCUs for the DynamoDB tables - The problem is specifically with saving data (writes), not reading data, so adjusting Read Capacity Units would not address the issue.</p><p>C. Add an Amazon ElastiCache layer - While this might improve some aspects of application performance, it wouldn't address the core issues of Lambda concurrency limits during upload spikes or DynamoDB write performance.</p><p>The combination of adjusting DynamoDB WCUs and implementing an SQS queue provides a comprehensive solution that addresses both the Lambda concurrency limitations and the DynamoDB write performance issues identified in the scenario.</p><p>Sources</p><p>DynamoDB burst and adaptive capacity - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/burst-adaptive-capacity.html）</p><p>Distributing write activity efficiently during data upload in DynamoDB - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-partition-key-data-upload.html）</p><p>Performance guidelines for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance-guidelines.html）</p><p>Understanding serverless data processing - Serverless （https://docs.aws.amazon.com/serverless/latest/devguide/serverless-usecases.html）</p><p><br></p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b6d35526f8bb4199b2448a8beed7f8d9",
            "questionNumber": 258,
            "type": "single",
            "content": "<p>Question #258</p><p>A company runs an application in an on-premises data center. The application gives users the ability to upload media files. The files persist in a file server. The web application has many users. The application server is overutilized, which causes data uploads to fail occasionally. The company frequently adds new storage to the file server. The company wants to resolve these challenges by migrating the application to AWS.</p><p><br></p><p>Users from across the United States and Canada access the application. Only authenticated users should have the ability to access the application to upload files. The company will consider a solution that refactors the application, and the company needs to accelerate application development.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Modify the application to use Amazon S3 to persist the files. Use Amazon Cognito to authenticate users."
                },
                {
                    "label": "B",
                    "content": "Use AWS Application Migration Service to migrate the application server to Amazon EC2 instances. Create an Auto Scaling group for the EC2 instances. Use an Application Load Balancer to distribute the requests. Set up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to the application. Modify the application to use Amazon S3 to persist the files."
                },
                {
                    "label": "C",
                    "content": "Create a static website for uploads of media files. Store the static assets in Amazon S3. Use AWS AppSync to create an API. Use AWS Lambda resolvers to upload the media files to Amazon S3. Use Amazon Cognito to authenticate users."
                },
                {
                    "label": "D",
                    "content": "Use AWS Amplify to create a static website for uploads of media files. Use Amplify Hosting to serve the website through Amazon CloudFront. Use Amazon S3 to store the uploaded media files. Use Amazon Cognito to authenticate users."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The correct answer is D. AWS Amplify provides a solution that can create a static website for media file uploads with the least operational overhead. It simplifies the process of building and deploying web applications and integrates with Amazon CloudFront for fast content delivery. By using Amazon S3 for file storage and Amazon Cognito for user authentication, the company can meet its requirements without the need for managing additional infrastructure.</p><p>1. Least Operational Overhead &nbsp;</p><p> &nbsp; - AWS Amplify is a fully managed service that simplifies building, deploying, and hosting web applications. It handles scaling, CI/CD, and infrastructure management automatically.</p><p> &nbsp; - Amazon Cognito provides seamless user authentication without requiring manual setup.</p><p> &nbsp; - CloudFront ensures low-latency access for users across the US and Canada.</p><p>2. Scalability & Performance &nbsp;</p><p> &nbsp; - Amplify + CloudFront automatically scales to handle traffic spikes, solving the overutilization issue.</p><p> &nbsp; - S3 is ideal for storing media files with high durability and scalability.</p><p>3. Security & Authentication &nbsp;</p><p> &nbsp; - Amazon Cognito ensures only authenticated users can access the application.</p><p>4. Accelerated Development &nbsp;</p><p> &nbsp; - Amplify streamlines frontend and backend development, reducing time-to-market.</p><p> Why Not the Other Options?</p><p>- A & B require refactoring to use EC2 + Auto Scaling + ALB, which introduces more operational complexity than a serverless approach.</p><p>- C involves AppSync (GraphQL) and Lambda, which is more complex than needed for a simple file upload application.</p><p>- D is the most streamlined, fully managed solution.</p><p> Conclusion: &nbsp;</p><p>Option D is the best choice because it minimizes operational overhead, accelerates development, and meets all requirements effectively.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1d8a29bb8211493ea39934e451a17a94",
            "questionNumber": 259,
            "type": "single",
            "content": "<p>Question #259</p><p>A company has an application that is deployed on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are part of an Auto Scaling group. The application has unpredictable workloads and frequently scales out and in. The company’s development team wants to analyze application logs to find ways to improve the application's performance. However, the logs are no longer available after instances scale in.</p><p><br></p><p>Which solution will give the development team the ability to view the application logs after a scale-in event?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Enable access logs for the ALB. Store the logs in an Amazon S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent."
                },
                {
                    "label": "C",
                    "content": "Modify the Auto Scaling group to use a step scaling policy."
                },
                {
                    "label": "D",
                    "content": "Instrument the application with AWS X-Ray tracing."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Configure the EC2 instances to publish logs to Amazon CloudWatch Logs by using the unified CloudWatch agent.</p><p>This solution will give the development team the ability to view application logs after a scale-in event for the following reasons:</p><p>Persistent Log Storage: When EC2 instances publish logs to CloudWatch Logs using the CloudWatch agent, the logs are stored in a centralized location that persists independently of the EC2 instances. This means that even after instances are terminated during scale-in events, the logs remain accessible.</p><p>Real-time Log Streaming: The CloudWatch agent streams logs to CloudWatch Logs in near real-time, ensuring that logs are captured before instances are terminated during scale-in events.</p><p>Application-specific Logs: Unlike ALB access logs which only capture HTTP request information, the CloudWatch agent can be configured to collect the actual application logs that contain detailed information about the application's behavior and performance, which is what the development team needs to improve the application.</p><p>Analysis Capabilities: CloudWatch Logs provides powerful querying and analysis tools through CloudWatch Logs Insights, allowing the development team to effectively analyze the logs to find ways to improve application performance.</p><p>The other options would not effectively solve the problem:</p><p>A. Enable access logs for the ALB - While this would preserve logs about HTTP requests to the ALB, it wouldn't capture the application-specific logs needed to analyze and improve application performance. ALB access logs only contain information about external requests, not internal application behavior.</p><p>C. Modify the Auto Scaling group to use a step scaling policy - This would only change how the Auto Scaling group scales, not how logs are preserved. The logs would still be lost when instances are terminated.</p><p>D. Instrument the application with AWS X-Ray tracing - While X-Ray provides valuable insights into application performance through distributed tracing, it's not primarily a log storage solution. It captures traces of requests through the application but doesn't preserve the detailed application logs needed for comprehensive analysis.</p><p>Configuring the CloudWatch agent to publish logs to CloudWatch Logs is the most appropriate solution as it directly addresses the need to preserve application logs after instances are terminated during scale-in events.</p><p>Sources</p><p>SEC04-BP01 Configure service and application logging - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2025-02-25/framework/sec_detect_investigate_events_app_service_logging.html）</p><p>SEC04-BP01 Configure service and application logging - AWS Well-Architected Framework (2022-03-31)（https://docs.aws.amazon.com/wellarchitected/2022-03-31/framework/sec_detect_investigate_events_app_service_logging.html） </p><p>SEC04-BP01 Configure service and application logging - AWS Well-Architected Framework (2023-04-10) （https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/sec_detect_investigate_events_app_service_logging.html）</p><p>Viewing logs from Amazon EC2 instances in your Elastic Beanstalk environment - AWS Elastic Beanstalk（https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.logging.html） </p><p>EC2 Auto-Scaling Instances Retaining Old Data After Termination | AWS re:Post （https://repost.aws/questions/QUJetklujSTha-jtw3BPWtiw/ec2-auto-scaling-instances-retaining-old-data-after-termination）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3320f2ef146e49b7a76568edf6383ead",
            "questionNumber": 260,
            "type": "single",
            "content": "<p>Question #260</p><p>A company runs an unauthenticated static website (www.example.com) that includes a registration form for users. The website uses Amazon S3 for hosting and uses Amazon CloudFront as the content delivery network with AWS WAF configured. When the registration form is submitted, the website calls an Amazon API Gateway API endpoint that invokes an AWS Lambda function to process the payload and forward the payload to an external API call.</p><p><br></p><p>During testing, a solutions architect encounters a cross-origin resource sharing (CORS) error. The solutions architect confirms that the CloudFront distribution origin has the Access-Control-Allow-Origin header set to www.example.com.</p><p><br></p><p>What should the solutions architect do to resolve the error?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Change the CORS configuration on the S3 bucket. Add rules for CORS to the AllowedOrigin element for www.example.com."
                },
                {
                    "label": "B",
                    "content": "Enable the CORS setting in AWS WAF. Create a web ACL rule in which the Access-Control-Allow-Origin header is set to www.example.com."
                },
                {
                    "label": "C",
                    "content": "Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com."
                },
                {
                    "label": "D",
                    "content": "Enable the CORS setting on the Lambda function. Ensure that the return code of the function has the Access-Control-Allow-Origin header set to www.example.com."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. The CORS error is likely due to the API Gateway endpoint not being configured to allow the origin www.example.com. By enabling the CORS setting on the API Gateway, the architect can specify that the Access-Control-Allow-Origin header should be set to www.example.com for all responses. This will allow the JavaScript code running on the static website to make successful requests to the API Gateway endpoint.The correct answer is C. Enable the CORS setting on the API Gateway API endpoint. Ensure that the API endpoint is configured to return all responses that have the Access-Control-Allow-Origin header set to www.example.com.</p><p> Explanation:</p><p>The CORS error occurs when a web application (hosted at `www.example.com`) makes a cross-origin request (to API Gateway) but the API endpoint does not explicitly allow requests from `www.example.com` by including the correct CORS headers (`Access-Control-Allow-Origin`) in its response.</p><p>Here’s why the other options are incorrect:</p><p>- A: The S3 bucket's CORS configuration is irrelevant here because the error occurs when calling the API Gateway endpoint, not when accessing static content from S3.</p><p>- B: AWS WAF does not handle CORS headers. CORS is enforced at the application layer (API Gateway in this case), not the WAF layer.</p><p>- D: While the Lambda function could manually set CORS headers, the proper way to handle CORS for API Gateway is to enable CORS at the API Gateway level, which automatically adds the required headers to responses.</p><p> Correct Approach:</p><p>1. Enable CORS in API Gateway:</p><p> &nbsp; - Go to the API Gateway console, select the API, and enable CORS for the relevant endpoint.</p><p> &nbsp; - Specify `www.example.com` as an allowed origin.</p><p> &nbsp; - This ensures API Gateway includes the `Access-Control-Allow-Origin: www.example.com` header in responses.</p><p>2. Redeploy the API:</p><p> &nbsp; - After enabling CORS, redeploy the API to apply the changes.</p><p>This will resolve the CORS error when the static website (`www.example.com`) makes requests to the API Gateway endpoint.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6f0d7f20ce18485eb74b296284a8888b",
            "questionNumber": 261,
            "type": "multiple",
            "content": "<p>Question #261</p><p>A company has many separate AWS accounts and uses no central billing or management. Each AWS account hosts services for different departments in the company. The company has a Microsoft Azure Active Directory that is deployed.</p><p>A solutions architect needs to centralize billing and management of the company’s AWS accounts. The company wants to start using identity federation instead of manual user management. The company also wants to use temporary credentials instead of long-lived access keys.</p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS account to join the organization. Ensure that each account accepts the invitation."
                },
                {
                    "label": "B",
                    "content": "Configure each AWS account&#39;s email address to be aws+@example.com so that account management email messages and invoices are sent to the same place."
                },
                {
                    "label": "C",
                    "content": "Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronization of users and groups."
                },
                {
                    "label": "D",
                    "content": "Deploy an AWS Managed Microsoft AD directory in the management account. Share the directory with all other accounts in the organization by using AWS Resource Access Manager (AWS RAM)."
                },
                {
                    "label": "E",
                    "content": "Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts."
                },
                {
                    "label": "F",
                    "content": "Configure AWS Identity and Access Management (IAM) in each AWS account to use AWS Managed Microsoft AD for authentication and authorization."
                }
            ],
            "correctAnswer": "ACE",
            "explanation": "<p>The correct answers are A, C, and E. To centralize billing and management, a new AWS account can be created as a management account and an organization in AWS Organizations can be deployed (A). Identity federation can be achieved by deploying AWS IAM Identity Center (AWS Single Sign-On) and connecting it to the existing Azure Active Directory, allowing for the use of temporary credentials (C). Permission sets in AWS Single Sign-On can be created and attached to the appropriate groups and accounts to manage access (E). These steps will allow for centralized control over billing and user management while leveraging the company's existing Azure Active Directory for authentication.</p><p>1. A. Create a new AWS account to serve as a management account. Deploy an organization in AWS Organizations. Invite each existing AWS account to join the organization. Ensure that each account accepts the invitation. &nbsp;</p><p> &nbsp; - This centralizes billing and management by setting up AWS Organizations, which allows consolidated billing and account management under a single \"management account.\"</p><p>2. C. Deploy AWS IAM Identity Center (AWS Single Sign-On) in the management account. Connect IAM Identity Center to the Azure Active Directory. Configure IAM Identity Center for automatic synchronization of users and groups. &nbsp;</p><p> &nbsp; - This enables identity federation by integrating AWS IAM Identity Center (successor to AWS SSO) with the existing Azure Active Directory, allowing users to log in using their corporate credentials. Automatic synchronization ensures users and groups stay up to date.</p><p>3. E. Create AWS IAM Identity Center (AWS Single Sign-On) permission sets. Attach the permission sets to the appropriate IAM Identity Center groups and AWS accounts. &nbsp;</p><p> &nbsp; - Permission sets define access levels (like IAM policies) and can be assigned to users/groups from Azure AD, enabling temporary credentials (via AWS SSO) instead of long-lived access keys.</p><p> Why not the others?</p><p>- B. Changing email addresses for invoices is not necessary for centralizing billing (AWS Organizations already consolidates billing under the management account). This is not a required step.</p><p>- D. Deploying AWS Managed Microsoft AD is unnecessary because the company already has Azure AD, which can be federated directly via IAM Identity Center.</p><p>- F. Configuring IAM in each account to use AWS Managed Microsoft AD is redundant since IAM Identity Center already handles federation with Azure AD.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "636e5e3dc4914b28af1abb8ce5b121a6",
            "questionNumber": 262,
            "type": "single",
            "content": "<p>Question #262</p><p>A company wants to manage the costs associated with a group of 20 applications that are infrequently used, but are still business-critical, by migrating to AWS. The applications are a mix of Java and Node.js spread across different instance clusters. The company wants to minimize costs while standardizing by using a single deployment methodology. </p><p><br></p><p>Most of the applications are part of month-end processing routines with a small number of concurrent users, but they are occasionally run at other times. Average application memory consumption is less than 1 GB, though some applications use as much as 2.5 GB of memory during peak processing. The most important application in the group is a billing report written in Java that accesses multiple data sources and often runs for several hours.</p><p><br></p><p>Which is the MOST cost-effective solution?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy a separate AWS Lambda function for each application. Use AWS CloudTrail logs and Amazon CloudWatch alarms to verify completion of critical jobs."
                },
                {
                    "label": "B",
                    "content": "Deploy Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization of 75%. Deploy an ECS task for each application being migrated with ECS task scaling. Monitor services and hosts by using Amazon CloudWatch."
                },
                {
                    "label": "C",
                    "content": "Deploy AWS Elastic Beanstalk for each application with Auto Scaling to ensure that all requests have sufficient resources. Monitor each AWS Elastic Beanstalk deployment by using CloudWatch alarms."
                },
                {
                    "label": "D",
                    "content": "Deploy a new Amazon EC2 instance cluster that co-hosts all applications by using EC2 Auto Scaling and Application Load Balancers. Scale cluster size based on a custom metric set on instance memory utilization. Purchase 3-year Reserved Instance reservations equal to the GroupMaxSize parameter of the Auto Scaling group."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. Deploying Amazon ECS containers on Amazon EC2 with Auto Scaling configured for memory utilization is a cost-effective solution because it allows the applications to share resources and only use what they need. This approach is more cost-efficient than deploying separate Lambda functions (Option A), which might be overprovisioned for infrequent use, or using Elastic Beanstalk (Option C), which could also lead to overprovisioning. Option D, deploying a new EC2 instance cluster, would likely be more expensive due to the need for potentially larger, reserved instances.</p><p>The company needs to manage 20 infrequently used but business-critical applications with varying memory requirements (up to 2.5 GB). The key requirements are: &nbsp;</p><p>- Cost efficiency (since applications are used sporadically). &nbsp;</p><p>- Standardized deployment methodology (all applications should follow the same approach). &nbsp;</p><p>- Handling long-running processes (e.g., the Java billing report that runs for hours). &nbsp;</p><p>- Scaling based on demand (month-end processing with occasional spikes). &nbsp;</p><p> Why Option B is the Best Choice? &nbsp;</p><p>- Amazon ECS on EC2 with Auto Scaling allows running containers efficiently, scaling based on memory utilization (75% threshold ensures optimal resource usage). &nbsp;</p><p>- Task-level scaling ensures that only necessary resources are allocated per application. &nbsp;</p><p>- Cost-effective since EC2 instances can be stopped or scaled down when not in use, unlike always-on solutions like Option D. &nbsp;</p><p>- CloudWatch monitoring provides visibility into application performance and resource usage. &nbsp;</p><p> Why Other Options Are Less Suitable? &nbsp;</p><p>- A (Lambda): Lambda has a 15-minute execution limit, making it unsuitable for long-running jobs like the billing report. &nbsp;</p><p>- C (Elastic Beanstalk): Overkill for infrequently used apps; incurs higher baseline costs due to always-on resources. &nbsp;</p><p>- D (EC2 Reserved Instances): Not cost-effective for sporadic workloads; Reserved Instances require long-term commitment for underutilized apps. &nbsp;</p><p>Thus, Option B (Amazon ECS on EC2 with Auto Scaling) provides the best balance of cost efficiency, scalability, and standardization. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c9d9c29054f9450a974fcd8e10a5bbae",
            "questionNumber": 263,
            "type": "single",
            "content": "<p>Question #263</p><p>A solutions architect needs to review the design of an Amazon EMR cluster that is using the EMR File System (EMRFS). The cluster performs tasks that are critical to business needs. The cluster is running Amazon EC2 On-Demand Instances at all times for all task, primary, and core nodes. The EMR tasks run each morning, starting at 1:00 AM, and take 6 hours to finish running. The amount of time to complete the processing is not a priority because the data is not referenced until late in the day.</p><p><br></p><p>The solutions architect must review the architecture and suggest a solution to minimize the compute costs.</p><p><br></p><p>Which solution should the solutions architect recommend to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Launch all task, primary, and core nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed."
                },
                {
                    "label": "B",
                    "content": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage."
                },
                {
                    "label": "C",
                    "content": "Continue to launch all nodes on On-Demand Instances. Terminate the cluster, including all instances, when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage."
                },
                {
                    "label": "D",
                    "content": "Launch the primary and core nodes on On-Demand Instances. Launch the task nodes on Spot Instances in an instance fleet. Terminate only the task node instances when the processing is completed. Purchase Compute Savings Plans to cover the On-Demand Instance usage."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>To minimize compute costs for the Amazon EMR cluster while meeting the business requirements, I recommend the following solution:</p><p>1. Use a combination of On-Demand and Spot Instances:</p><p> &nbsp; - Keep the primary node and core nodes as On-Demand Instances to ensure stability and data integrity.</p><p> &nbsp; - Convert task nodes to Spot Instances to reduce costs significantly.</p><p>2. Implement an instance fleet configuration:</p><p> &nbsp; - This allows you to specify a mix of instance types and purchasing options (On-Demand and Spot) for each node type.</p><p> &nbsp; - Set up multiple instance types for task nodes to increase the likelihood of obtaining Spot Instances.</p><p>3. Schedule cluster creation and termination:</p><p> &nbsp; - Create the cluster at 12:45 AM, just before the tasks start running at 1:00 AM.</p><p> &nbsp; - Terminate the cluster after the tasks complete (around 7:00 AM).</p><p> &nbsp; - Use AWS Step Functions or AWS Lambda with Amazon EventBridge to automate this process.</p><p>4. Optimize cluster size:</p><p> &nbsp; - Analyze the current workload and adjust the number of core and task nodes to match the processing requirements.</p><p> &nbsp; - Consider using larger instance types if it reduces the overall number of instances needed.</p><p>5. Utilize Amazon EMR managed scaling:</p><p> &nbsp; - Enable managed scaling to automatically adjust the number of instances based on workload.</p><p> &nbsp; - This ensures you have enough capacity during peak processing times and scales down when not needed.</p><p>6. Leverage EMR on Amazon EC2 Capacity Reservations:</p><p> &nbsp; - For the critical On-Demand Instances (primary and core nodes), consider using Capacity Reservations to ensure availability and potentially benefit from discounted pricing.</p><p>7. Use Amazon S3 for data storage:</p><p> &nbsp; - Store input and output data in Amazon S3 instead of HDFS to reduce the need for large, persistent core nodes.</p><p>8. Optimize EMR configurations:</p><p> &nbsp; - Fine-tune the EMR application settings to improve job efficiency and potentially reduce processing time.</p><p>9. Monitor and analyze cluster performance:</p><p> &nbsp; - Use Amazon CloudWatch and EMR built-in monitoring tools to identify optimization opportunities.</p><p>10. Consider using EMR Notebooks or EMR Studio:</p><p> &nbsp; &nbsp;- For any interactive analysis needs, use these services instead of keeping a cluster running continuously.</p><p> Why Not Other Options? &nbsp;</p><p>- A: Using only Spot Instances for all nodes (including primary/core) risks cluster instability if Spot interruptions occur. &nbsp;</p><p>- C: Keeping all nodes On-Demand is expensive, even with Savings Plans (Spot is cheaper for task nodes). &nbsp;</p><p>- D: Not terminating primary/core nodes after processing leads to unnecessary costs (they should be shut down when not in use). &nbsp;</p><p>By implementing these recommendations, you can significantly reduce compute costs while ensuring that the critical business tasks are completed reliably each day. Remember to test these changes in a non-production environment first and monitor the impact on performance and reliability.</p><p>Sources</p><p>[1] [Amazon EMR architecture and service layers - Amazon EMR] (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-overview-arch.html)</p><p>[3] [Understand node types in Amazon EMR: primary, core, and task nodes - Amazon EMR] (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html)</p><p>[6] [Planning and configuring instance fleets for your Amazon EMR cluster - Amazon EMR] (https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-fleet.html)</p><p> </p><p>此题建议选择B，投票多数的答案是D，但真理可能掌握在少数人手里。</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "eb59dafc587440968c41a56ded0673dc",
            "questionNumber": 264,
            "type": "single",
            "content": "<p>Question #264</p><p>A company has migrated a legacy application to the AWS Cloud. The application runs on three Amazon EC2 instances that are spread across three Availability Zones. One EC2 instance is in each Availability Zone. The EC2 instances are running in three private subnets of the VPC and are set up as targets for an Application Load Balancer (ALB) that is associated with three public subnets.</p><p><br></p><p>The application needs to communicate with on-premises systems. Only traffic from IP addresses in the company's IP address range are allowed to access the on-premises systems. The company’s security team is bringing only one IP address from its internal IP address range to the cloud. The company has added this IP address to the allow list for the company firewall. The company also has created an Elastic IP address for this IP address.</p><p><br></p><p>A solutions architect needs to create a solution that gives the application the ability to communicate with the on-premises systems. The solution also must be able to mitigate failures automatically.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy three NAT gateways, one in each public subnet. Assign the Elastic IP address to the NAT gateways. Turn on health checks for the NAT gateways. If a NAT gateway fails a health check, recreate the NAT gateway and assign the Elastic IP address to the new NAT gateway."
                },
                {
                    "label": "B",
                    "content": "Replace the ALB with a Network Load Balancer (NLB). Assign the Elastic IP address to the NLB. Turn on health checks for the NLB. In the case of a failed health check, redeploy the NLB in different subnets."
                },
                {
                    "label": "C",
                    "content": "Deploy a single NAT gateway in a public subnet. Assign the Elastic IP address to the NAT gateway. Use Amazon CloudWatch with a custom metric to monitor the NAT gateway. If the NAT gateway is unhealthy, invoke an AWS Lambda function to create a new NAT gateway in a different subnet. Assign the Elastic IP address to the new NAT gateway."
                },
                {
                    "label": "D",
                    "content": "Assign the Elastic IP address to the ALB. Create an Amazon Route 53 simple record with the Elastic IP address as the value. Create a Route 53 health check. In the case of a failed health check, recreate the ALB in different subnets."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. By deploying a single NAT gateway with an Elastic IP address, the company can ensure that all outbound traffic from the EC2 instances appears to come from the whitelisted IP address, allowing communication with on-premises systems. Using Amazon CloudWatch with a custom metric to monitor the NAT gateway and invoking an AWS Lambda function to create a new NAT gateway in a different subnet if the current one becomes unhealthy provides a way to automatically mitigate failures.</p><p>Requirements:</p><p>1. Communication with On-Premises Systems: &nbsp;</p><p> &nbsp; - The application (running on EC2 instances in private subnets) needs to communicate with on-premises systems.</p><p> &nbsp; - The on-premises firewall only allows traffic from a single allowed IP address (the Elastic IP address).</p><p>2. High Availability & Automatic Failure Mitigation: &nbsp;</p><p> &nbsp; - The solution must handle failures automatically without manual intervention.</p><p> &nbsp; - Since only one Elastic IP is available, it must be reassigned dynamically if a failure occurs.</p><p> Why Option C is Correct:</p><p>- NAT Gateway with Elastic IP: &nbsp;</p><p> &nbsp;- A NAT Gateway allows instances in private subnets to communicate with on-premises systems while using the Elastic IP as the source IP.</p><p> &nbsp;- Only one NAT Gateway is needed since only one Elastic IP is available.</p><p> &nbsp;- NAT Gateway is highly available within a single Availability Zone (AZ) but not across AZs.</p><p>- Failure Mitigation with CloudWatch & Lambda: &nbsp;</p><p> &nbsp;- CloudWatch can monitor the NAT Gateway’s health (e.g., via custom metrics like connection failures).</p><p> &nbsp;- If the NAT Gateway fails, a Lambda function can:</p><p> &nbsp; &nbsp;- Create a new NAT Gateway in a different subnet (different AZ).</p><p> &nbsp; &nbsp;- Reassign the Elastic IP to the new NAT Gateway.</p><p> &nbsp;- This ensures automatic failover without manual intervention.</p><p> Why Other Options Are Incorrect:</p><p>- Option A: &nbsp;</p><p> &nbsp;- Deploying three NAT Gateways (one per AZ) is unnecessary because only one Elastic IP is available.</p><p> &nbsp;- NAT Gateway health checks are not natively supported by AWS, and manual recreation is not scalable.</p><p>- Option B: &nbsp;</p><p> &nbsp;- Replacing the ALB with an NLB does not solve the outbound communication problem (NLB is for inbound traffic).</p><p> &nbsp;- NLBs do not provide NAT functionality for private instances.</p><p>- Option D: &nbsp;</p><p> &nbsp;- Assigning the Elastic IP to the ALB is irrelevant because the ALB handles inbound traffic, not outbound traffic to on-premises systems.</p><p> &nbsp;- Route 53 health checks cannot fix NAT Gateway failures.</p><p> Conclusion: &nbsp;</p><p>Option C is the best solution because it uses a NAT Gateway for outbound traffic, assigns the Elastic IP, and implements automated failover using CloudWatch and Lambda.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "52d728e7c49a4706bfb40362db43e5a9",
            "questionNumber": 265,
            "type": "multiple",
            "content": "<p>Question #265</p><p>A company uses AWS Organizations to manage more than 1,000 AWS accounts. The company has created a new developer organization. There are 540 developer member accounts that must be moved to the new developer organization. All accounts are set up with all the required information so that each account can be operated as a standalone account.</p><p>Which combination of steps should a solutions architect take to move all of the developer accounts to the new developer organization? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Call the MoveAccount operation in the Organizations API from the old organization&#39;s management account to migrate the developer accounts to the new developer organization."
                },
                {
                    "label": "B",
                    "content": "From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API."
                },
                {
                    "label": "C",
                    "content": "From each developer account, remove the account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API."
                },
                {
                    "label": "D",
                    "content": "Sign in to the new developer organization&#39;s management account and create a placeholder member account that acts as a target for the developer account migration."
                },
                {
                    "label": "E",
                    "content": "Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization&#39;s management account to send invitations to the developer accounts."
                },
                {
                    "label": "F",
                    "content": "Have each developer sign in to their account and confirm to join the new developer organization."
                }
            ],
            "correctAnswer": "BEF",
            "explanation": "<p>The correct combination of steps is BEF. First, the management account of the current organization should remove each developer account using the RemoveAccountFromOrganization operation (B). Then, the new developer organization's management account should send an invitation to each developer account using the InviteAccountToOrganization operation (E). Finally, each developer must sign in to their account and accept the invitation to officially join the new organization (F).</p><p>Based on the information provided, the correct answers are:</p><p>B. From the management account, remove each developer account from the old organization using the RemoveAccountFromOrganization operation in the Organizations API.</p><p>E. Call the InviteAccountToOrganization operation in the Organizations API from the new developer organization's management account to send invitations to the developer accounts.</p><p>F. Have each developer sign in to their account and confirm to join the new developer organization.</p><p>These three steps represent the correct process for moving AWS accounts from one organization to another:</p><p>First, you must remove the accounts from their current organization. This is done from the management account of the old organization using the RemoveAccountFromOrganization API operation (option B). This step is necessary because an AWS account can only be a member of one organization at a time.</p><p>Next, the management account of the new developer organization needs to invite the standalone accounts to join. This is done by calling the InviteAccountToOrganization API operation (option E). This sends formal invitations to each of the developer accounts.</p><p>Finally, each invitation must be accepted. Since the accounts are now standalone (as mentioned in the scenario: \"All accounts are set up with all the required information so that each account can be operated as a standalone account\"), the account owners need to sign in to their respective accounts and accept the invitation to join the new organization (option F).</p><p>This three-step process (remove from old organization, invite to new organization, accept invitation) is the standard procedure for moving AWS accounts between organizations. The scenario specifically mentions that there are 540 developer accounts to move, so this process would need to be repeated or automated for each account.</p><p>The other options that weren't selected likely contained incorrect steps or operations that don't align with the proper AWS Organizations account migration process. The selected options represent the complete end-to-end process required to successfully move accounts from one organization to another.</p><p>Sources</p><p>Move an account to AWS Org from another Org | AWS re:Post （https://repost.aws/questions/QUd8oGp5STRgWveFxfolWV_w/move-an-account-to-aws-org-from-another-org）</p><p>Move accounts between organizations in AWS Organizations | AWS re:Post（https://repost.aws/knowledge-center/organizations-move-accounts） </p><p>Migrating AWS accounts between organizations | AWS re:Post（https://repost.aws/articles/ARf43Hri2LQmGARxerfK9B6Q/migrating-aws-accounts-between-organizations） </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2756eebc41094d4193ac7163f1522df1",
            "questionNumber": 266,
            "type": "single",
            "content": "<p>Question #266</p><p>A company’s interactive web application uses an Amazon CloudFront distribution to serve images from an Amazon S3 bucket. Occasionally, third-party tools ingest corrupted images into the S3 bucket. This image corruption causes a poor user experience in the application later. The company has successfully implemented and tested Python logic to detect corrupt images.</p><p><br></p><p>A solutions architect must recommend a solution to integrate the detection logic with minimal latency between the ingestion and serving.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use a Lambda@Edge function that is invoked by a viewer-response event."
                },
                {
                    "label": "B",
                    "content": "Use a Lambda@Edge function that is invoked by an origin-response event."
                },
                {
                    "label": "C",
                    "content": "Use an S3 event notification that invokes an AWS Lambda function."
                },
                {
                    "label": "D",
                    "content": "Use an S3 event notification that invokes an AWS Step Functions state machine."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use an S3 event notification that invokes an AWS Lambda function.</p><p>This solution will meet the requirements for the following reasons:</p><p>Timing of Detection: S3 event notifications trigger immediately when new objects are uploaded to the bucket. This means the corruption detection logic will run as soon as images are ingested, before they are served to users through CloudFront. This provides the minimal latency between ingestion and serving that the requirements specify.</p><p>Prevention Rather Than Detection: By running the detection logic at ingestion time, corrupted images can be identified and handled before they ever reach the CloudFront distribution. This is more efficient than detecting corruption after the images have already been requested by users.</p><p>Seamless Integration with Python Logic: The company has already implemented and tested Python logic to detect corrupt images. AWS Lambda natively supports Python, making it straightforward to integrate the existing detection code without significant modifications.</p><p>Asynchronous Processing: The Lambda function triggered by the S3 event notification can process the image in the background without blocking the ingestion process, maintaining system performance.</p><p>Scalability: This solution automatically scales to handle varying volumes of image uploads without manual intervention.</p><p>The other options have significant drawbacks:</p><p>A. Lambda@Edge with viewer-response event - This would detect corruption too late, as it runs after CloudFront has already retrieved the image and is about to send it to the viewer. This adds unnecessary latency and doesn't prevent corrupted images from being served.</p><p>B. Lambda@Edge with origin-response event - Similar to option A, this detects corruption too late in the process, after the image has been retrieved from S3 but before CloudFront caches it. This still doesn't prevent the initial serving of corrupted images.</p><p>D. S3 event notification with Step Functions - While this would work similarly to option C, it adds unnecessary complexity for this use case. Step Functions are beneficial for orchestrating complex workflows with multiple steps, but the scenario only mentions a single Python-based detection logic, making Lambda alone sufficient and more efficient.</p><p>Option C provides the optimal balance of minimal latency and effective implementation of the corruption detection logic by processing images immediately upon upload, before they are served to users through CloudFront.</p><p>Sources</p><p>How Lambda@Edge works with requests and responses - Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-event-request-response.html）</p><p>create_distribution - Boto3 1.38.12 documentation （https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/cloudfront/client/create_distribution.html）</p><p>Work with requests and responses - Amazon CloudFront（https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-generating-http-responses.html） </p><p>AWS services in this solution - Dynamic Image Transformation for Amazon CloudFront (Formerly known as Serverless Image Handler) （https://docs.aws.amazon.com/solutions/latest/serverless-image-handler/aws-services-in-this-solution.html）</p><p>Process Objects on Amazon S3 with Precision File Type Detection | AWS re:Post （https://repost.aws/articles/ARDCY-_jnOTbS_OOk8F4kLVA/process-objects-on-amazon-s3-with-precision-file-type-detection）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5535b002431c47b8b1a9e0271e3a06fd",
            "questionNumber": 267,
            "type": "single",
            "content": "<p>Question #267</p><p>A company has an application that runs on Amazon EC2 instances in an Amazon EC2 Auto Scaling group. The company uses AWS CodePipeline to deploy the application. The instances that run in the Auto Scaling group are constantly changing because of scaling events.</p><p><br></p><p>When the company deploys new application code versions, the company installs the AWS CodeDeploy agent on any new target EC2 instances and associates the instances with the CodeDeploy deployment group. The application is set to go live within the next 24 hours.</p><p><br></p><p>What should a solutions architect recommend to automate the application deployment process with the LEAST amount of operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure Amazon EventBridge to invoke an AWS Lambda function when a new EC2 instance is launched into the Auto Scaling group. Code the Lambda function to associate the EC2 instances with the CodeDeploy deployment group."
                },
                {
                    "label": "B",
                    "content": "Write a script to suspend Amazon EC2 Auto Scaling operations before the deployment of new code. When the deployment is complete, create a new AMI and configure the Auto Scaling group&#39;s launch template to use the new AMI for new launches. Resume Amazon EC2 Auto Scaling operations."
                },
                {
                    "label": "C",
                    "content": "Create a new AWS CodeBuild project that creates a new AMI that contains the new code. Configure CodeBuild to update the Auto Scaling group&rsquo;s launch template to the new AMI. Run an Amazon EC2 Auto Scaling instance refresh operation."
                },
                {
                    "label": "D",
                    "content": "Create a new AMI that has the CodeDeploy agent installed. Configure the Auto Scaling group&rsquo;s launch template to use the new AMI. Associate the CodeDeploy deployment group with the Auto Scaling group instead of the EC2 instances."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The correct answer is D. By creating a new AMI with the CodeDeploy agent pre-installed and configuring the Auto Scaling group to use this AMI, new instances launched by the Auto Scaling group will automatically have the necessary agent. Associating the CodeDeploy deployment group with the Auto Scaling group ensures that new instances are automatically included in the deployment process, reducing the operational overhead as it eliminates the need for manual installation of the agent or manual association with the deployment group.</p><p>1. Least Operational Overhead: </p><p> &nbsp; - By creating an AMI with the CodeDeploy agent pre-installed and configuring the Auto Scaling group's launch template to use this AMI, you ensure that every new EC2 instance launched by Auto Scaling will automatically have the CodeDeploy agent ready.</p><p> &nbsp; - Associating the CodeDeploy deployment group with the Auto Scaling group (instead of individual instances) ensures that any new instances launched will automatically be part of the deployment group without manual intervention.</p><p>2. Fully Automated:</p><p> &nbsp; - No need for additional Lambda functions (Option A) or manual suspension of scaling operations (Option B).</p><p> &nbsp; - No dependency on CodeBuild (Option C) to rebuild AMIs and trigger refreshes—this approach is simpler and more direct.</p><p>3. AWS Best Practice:</p><p> &nbsp; - Using launch templates with pre-configured AMIs is a recommended way to ensure consistency in Auto Scaling deployments.</p><p> &nbsp; - CodeDeploy natively supports Auto Scaling group integrations, eliminating the need to manage instances individually.</p><p> Why Not Other Options?</p><p>- Option A: While it works, it introduces unnecessary complexity with EventBridge and Lambda when CodeDeploy already supports Auto Scaling group integration.</p><p>- Option B: Suspending Auto Scaling operations is disruptive and not scalable for frequent deployments.</p><p>- Option C: While feasible, it requires additional steps (CodeBuild, AMI updates, instance refresh) when a simpler solution exists.</p><p> Conclusion</p><p>Option D provides the most automated and least operationally heavy solution by leveraging native AWS integrations (AMI + Auto Scaling group association with CodeDeploy). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "466016f5ff4645b992d2632f40ba8777",
            "questionNumber": 268,
            "type": "single",
            "content": "<p>Question #268</p><p>A company has a website that runs on four Amazon EC2 instances that are behind an Application Load Balancer (ALB). When the ALB detects that an EC2 instance is no longer available, an Amazon CloudWatch alarm enters the ALARM state. A member of the company's operations team then manually adds a new EC2 instance behind the ALB.</p><p><br></p><p>A solutions architect needs to design a highly available solution that automatically handles the replacement of EC2 instances. The company needs to minimize downtime during the switch to the new solution.</p><p><br></p><p>Which set of steps should the solutions architect take to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Delete the existing ALB. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Attach the existing EC2 instances to the Auto Scaling group."
                },
                {
                    "label": "B",
                    "content": "Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Attach the existing EC2 instances to the Auto Scaling group."
                },
                {
                    "label": "C",
                    "content": "Delete the existing ALB and the EC2 instances. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Create a new ALB. Attach the Auto Scaling group to the new ALB. Wait for the Auto Scaling group to launch the minimum number of EC2 instances."
                },
                {
                    "label": "D",
                    "content": "Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Wait for the existing ALB to register the existing EC2 instances with the Auto Scaling group."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. By creating an Auto Scaling group and attaching it to the existing ALB, the architect can ensure that new instances are automatically registered with the load balancer. Attaching the existing EC2 instances to the new Auto Scaling group allows for management of all instances, both new and old, through the same scaling policy. This approach minimizes downtime and operational overhead as it does not require the deletion of the existing ALB or the creation of a new one.</p><p> Requirements:</p><p>1. Highly available solution that automatically replaces EC2 instances.</p><p>2. Minimize downtime during the switch to the new solution.</p><p> Analysis of Options:</p><p>- Option A: Deleting the existing ALB first would cause downtime. Also, manually attaching existing EC2 instances to an Auto Scaling group is not a best practice.</p><p>- Option B: This is the correct approach because:</p><p> &nbsp;- It keeps the existing ALB, avoiding downtime.</p><p> &nbsp;- It creates an Auto Scaling group with a launch template (ensuring new instances match the desired configuration).</p><p> &nbsp;- It attaches the Auto Scaling group to the existing ALB, maintaining traffic flow.</p><p> &nbsp;- It registers existing EC2 instances with the Auto Scaling group, allowing Auto Scaling to manage them.</p><p> &nbsp;- This method ensures no downtime and provides automatic instance recovery.</p><p>- Option C: Deleting the existing ALB and EC2 instances first would cause significant downtime.</p><p>- Option D: The ALB does not \"register instances with the Auto Scaling group\"—this is misleading. Instances must be explicitly attached to the Auto Scaling group.</p><p> Why Option B is Best:</p><p>- No downtime: The existing ALB and instances remain operational.</p><p>- Auto Scaling integration: The Auto Scaling group takes over instance management, automatically replacing unhealthy instances.</p><p>- Seamless transition: Existing instances are registered with the Auto Scaling group, ensuring continuity.</p><p> Final Answer:</p><p>B. Create an Auto Scaling group that is configured to handle the web application traffic. Attach a new launch template to the Auto Scaling group. Attach the Auto Scaling group to the existing ALB. Attach the existing EC2 instances to the Auto Scaling group.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "932c80c045e54af6a7564c6856291cc2",
            "questionNumber": 269,
            "type": "single",
            "content": "<p>Question #269</p><p>A company wants to optimize AWS data-transfer costs and compute costs across developer accounts within the company's organization in AWS Organizations. Developers can configure VPCs and launch Amazon EC2 instances in a single AWS Region. The EC2 instances retrieve approximately 1 TB of data each day from Amazon S3. <br><br>The developer activity leads to excessive monthly data-transfer charges and NAT gateway processing charges between EC2 instances and S3 buckets, along with high compute costs. The company wants to proactively enforce approved architectural patterns for any EC2 instance and VPC infrastructure that developers deploy within the AWS accounts. The company does not want this enforcement to negatively affect the speed at which developers can perform their tasks.<br><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create SCPs to prevent developers from launching unapproved EC2 instance types. Provide the developers with an AWS CloudFormation template to deploy an approved VPC configuration with S3 interface endpoints. Scope the developers&#39; IAM permissions so that the developers can launch VPC resources only with CloudFormation."
                },
                {
                    "label": "B",
                    "content": "Create a daily forecasted budget with AWS Budgets to monitor EC2 compute costs and S3 data-transfer costs across the developer accounts. When the forecasted cost is 75% of the actual budget cost, send an alert to the developer teams. If the actual budget cost is 100%, create a budget action to terminate the developers&#39; EC2 instances and VPC infrastructure."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Service Catalog portfolio that users can use to create an approved VPC configuration with S3 gateway endpoints and approved EC2 instances. Share the portfolio with the developer accounts. Configure an AWS Service Catalog launch constraint to use an approved IAM role. Scope the developers&#39; IAM permissions to allow access only to AWS Service Catalog."
                },
                {
                    "label": "D",
                    "content": "Create and deploy AWS Config rules to monitor the compliance of EC2 and VPC resources in the developer AWS accounts. If developers launch unapproved EC2 instances or if developers create VPCs without S3 gateway endpoints, perform a remediation action to terminate the unapproved resources."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. By creating an AWS Service Catalog portfolio with approved VPC configurations and S3 gateway endpoints, the company can provide developers with a simple method to deploy resources that adhere to the architectural patterns. This approach automates the enforcement of best practices and reduces the operational overhead for developers. Additionally, by configuring launch constraints and scoping IAM permissions, the company ensures that developers only use approved resources, which can help to control costs.</p><p> Why Option C is the Best Choice? &nbsp;</p><p>1. Proactive Enforcement of Approved Architecture: &nbsp;</p><p> &nbsp; - AWS Service Catalog allows the company to predefine approved VPC configurations (with S3 gateway endpoints) and approved EC2 instance types, ensuring developers deploy only compliant resources. &nbsp;</p><p> &nbsp; - This prevents excessive data-transfer costs (by enforcing S3 gateway endpoints instead of NAT gateways) and unnecessary compute costs (by restricting EC2 instance types). &nbsp;</p><p>2. No Negative Impact on Developer Speed: &nbsp;</p><p> &nbsp; - Developers can still quickly deploy resources using pre-approved templates, without waiting for manual reviews or remediation. &nbsp;</p><p>3. IAM Permissions Restriction: &nbsp;</p><p> &nbsp; - By scoping developers' IAM permissions to only AWS Service Catalog, they cannot bypass the approved configurations. &nbsp;</p><p>4. Cost Optimization: &nbsp;</p><p> &nbsp; - S3 gateway endpoints eliminate NAT gateway charges for S3 traffic. &nbsp;</p><p> &nbsp; - Approved EC2 instance types prevent over-provisioning, reducing compute costs. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- Option A: SCPs can block unapproved instances, but they don’t automate deployment of compliant VPCs/S3 endpoints. Relying solely on CloudFormation templates still allows manual misconfigurations. &nbsp;</p><p>- Option B: Reactive budgeting doesn’t enforce architecture and terminating resources disrupts developers. &nbsp;</p><p>- Option D: AWS Config is reactive (detects & remediates after deployment), leading to potential cost spikes before fixes. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option C (AWS Service Catalog) is the most proactive, cost-effective, and developer-friendly solution. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4c262625b28c4cd1a4fe1078013ee00d",
            "questionNumber": 270,
            "type": "single",
            "content": "<p>Question #270</p><p>A company is expanding. The company plans to separate its resources into hundreds of different AWS accounts in multiple AWS Regions. A solutions architect must recommend a solution that denies access to any operations outside of specifically designated Regions.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create IAM roles for each account. Create IAM policies with conditional allow permissions that include only approved Regions for the accounts."
                },
                {
                    "label": "B",
                    "content": "Create an organization in AWS Organizations. Create IAM users for each account. Attach a policy to each user to block access to Regions where an account cannot deploy infrastructure."
                },
                {
                    "label": "C",
                    "content": "Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions."
                },
                {
                    "label": "D",
                    "content": "Enable AWS Security Hub in each account. Create controls to specify the Regions where an account can deploy infrastructure."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Launch an AWS Control Tower landing zone. Create OUs and attach SCPs that deny access to run services outside of the approved Regions.</p><p>This solution will meet the requirements for the following reasons:</p><p>Scalability: The scenario involves \"hundreds of different AWS accounts in multiple AWS Regions.\" AWS Control Tower is specifically designed to manage multi-account environments at scale. It provides a streamlined way to set up and govern a secure, multi-account AWS environment based on best practices.</p><p>Centralized Policy Management: Service Control Policies (SCPs) in AWS Organizations (which is part of Control Tower) allow for centralized policy management across all accounts. This makes it much more efficient to implement and maintain region restrictions across hundreds of accounts.</p><p>Preventive Controls: SCPs act as guardrails that cannot be overridden by account administrators. They provide a true preventive control that denies access to operations outside of designated regions, regardless of the permissions granted within individual accounts.</p><p>Hierarchical Structure: Organizational Units (OUs) in Control Tower allow for grouping accounts logically, making it possible to apply different region restrictions to different groups of accounts if needed.</p><p>Inheritance: Policies applied at the OU level are automatically inherited by all accounts within that OU, ensuring consistent enforcement without having to configure each account individually.</p><p>The other options have significant limitations:</p><p>A. Creating IAM roles with conditional allow permissions for each account would be extremely difficult to manage across hundreds of accounts. It would be prone to inconsistencies and human error.</p><p>B. Creating IAM users for each account and attaching policies to block access would be even more unmanageable than option A, as it would require maintaining individual users across hundreds of accounts. Additionally, IAM best practices recommend using roles rather than users for cross-account access.</p><p>D. AWS Security Hub is primarily a security monitoring and compliance service. While it can detect non-compliant resources, it doesn't provide preventive controls to deny access to operations. It would identify violations after they occur rather than preventing them.</p><p>AWS Control Tower with SCPs provides the most comprehensive, scalable, and manageable solution for enforcing region restrictions across hundreds of AWS accounts.</p><p>Sources</p><p>Defining Boundaries for Regional Services Access - Navigating GDPR Compliance on AWS （https://docs.aws.amazon.com/whitepapers/latest/navigating-gdpr-compliance/defining-boundaries-for-regional-services-access.html）</p><p>Using AWS Organizations for security - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/organizations-security.html）</p><p>AWS account management and separation - Security Pillar（https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/aws-account-management-and-separation.html） </p><p>Protect your AWS resources from unauthorized access using AWS Organizations integrations | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/protect-your-aws-resources-from-unauthorized-access-using-aws-organizations-integrations/）</p><p>Enforcing enterprise-wide preventive controls with AWS Organizations | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/enforcing-enterprise-wide-preventive-controls-with-aws-organizations/）</p><p>Effectively implementing resource controls policies in a multi-account environment | AWS Security Blog （https://aws.amazon.com/cn/blogs/security/effectively-implementing-resource-controls-policies-in-a-multi-account-environment/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b4165bb08aa94fe8bc099e94f1182c2c",
            "questionNumber": 271,
            "type": "single",
            "content": "<p>Question #271</p><p>A company wants to refactor its retail ordering web application that currently has a load-balanced Amazon EC2 instance fleet for web hosting, database API services, and business logic. The company needs to create a decoupled, scalable architecture with a mechanism for retaining failed orders while also minimizing operational costs.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Amazon S3 for web hosting with Amazon API Gateway for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use Amazon Elastic Container Service (Amazon ECS) for business logic with Amazon SQS long polling for retaining failed orders."
                },
                {
                    "label": "B",
                    "content": "Use AWS Elastic Beanstalk for web hosting with Amazon API Gateway for database API services. Use Amazon MQ for order queuing. Use AWS Step Functions for business logic with Amazon S3 Glacier Deep Archive for retaining failed orders."
                },
                {
                    "label": "C",
                    "content": "Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an Amazon SQS dead-letter queue for retaining failed orders."
                },
                {
                    "label": "D",
                    "content": "Use Amazon Lightsail for web hosting with AWS AppSync for database API services. Use Amazon Simple Email Service (Amazon SES) for order queuing. Use Amazon Elastic Kubernetes Service (Amazon EKS) for business logic with Amazon OpenSearch Service for retaining failed orders."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an Amazon SQS dead-letter queue for retaining failed orders.</p><p>This solution will meet the company's requirements for the following reasons:</p><p>Decoupled Architecture:</p><p>S3 for web hosting separates the frontend from the backend services</p><p>AppSync provides a managed GraphQL API layer that decouples the database interactions</p><p>SQS creates a buffer between order submission and processing</p><p>Lambda functions handle business logic independently from other components</p><p>This complete separation of concerns creates a truly decoupled system</p><p>Scalability:</p><p>All components (S3, AppSync, SQS, Lambda) are fully managed serverless services that scale automatically</p><p>S3 can handle virtually unlimited web traffic</p><p>SQS can scale to handle high volumes of orders during peak periods</p><p>Lambda functions automatically scale to process orders based on queue depth</p><p>This architecture eliminates the need to manage scaling for any component</p><p>Mechanism for Retaining Failed Orders:</p><p>SQS dead-letter queues provide a built-in mechanism to capture and retain failed orders</p><p>Failed orders can be easily monitored, analyzed, and reprocessed when issues are resolved</p><p>The retention period can be configured based on business needs</p><p>Minimizing Operational Costs:</p><p>All components follow a pay-for-what-you-use pricing model</p><p>No need to pay for idle resources during low-traffic periods</p><p>No server management or patching required</p><p>Reduced operational overhead for monitoring and maintaining infrastructure</p><p>SQS dead-letter queues are more cost-effective than Glacier Deep Archive for the use case of retaining failed orders that need to be accessible</p><p>Option B (Elastic Beanstalk, API Gateway, Amazon MQ, Step Functions, S3 Glacier Deep Archive) has several disadvantages:</p><p>Elastic Beanstalk still requires managing EC2 instances, which increases operational costs</p><p>Amazon MQ is more complex and expensive to operate than SQS for simple order queuing</p><p>S3 Glacier Deep Archive has extremely long retrieval times (12+ hours), making it impractical for retaining failed orders that may need to be reprocessed quickly</p><p>This solution has higher fixed costs regardless of usage levels</p><p>The serverless architecture in Option C provides the optimal balance of decoupling, scalability, effective handling of failed orders, and minimal operational costs, making it the best solution for the company's requirements.</p><p>Sources</p><p>Architecture details - Enhanced Document Understanding on AWS （https://docs.aws.amazon.com/solutions/latest/enhanced-document-understanding-on-aws/architecture-details.html）</p><p>Connect to Amazon EventBridge to send and receive events - AWS Amplify Gen 2 Documentation （https://docs.amplify.aws/react/build-a-backend/data/custom-business-logic/connect-eventbridge-datasource/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8216f7fc571a45f59e5dbc3e46923fb9",
            "questionNumber": 272,
            "type": "single",
            "content": "<p>Question #272</p><p>A company hosts a web application on AWS in the us-east-1 Region. The application servers are distributed across three Availability Zones behind an Application Load Balancer. The database is hosted in a MySQL database on an Amazon EC2 instance. A solutions architect needs to design a cross-Region data recovery solution using AWS services with an RTO of less than 5 minutes and an RPO of less than 1 minute. The solutions architect is deploying application servers in us-west-2, and has configured Amazon Route 53 health checks and DNS failover to us-west-2.</p><p><br></p><p>Which additional step should the solutions architect take?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica in us-west-2."
                },
                {
                    "label": "B",
                    "content": "Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2."
                },
                {
                    "label": "C",
                    "content": "Migrate the database to an Amazon RDS for MySQL instance with a Multi-AZ deployment."
                },
                {
                    "label": "D",
                    "content": "Create a MySQL standby database on an Amazon EC2 instance in us-west-2."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Migrate the database to an Amazon Aurora global database with the primary in us-east-1 and the secondary in us-west-2.</p><p>This solution will meet the required RTO of less than 5 minutes and RPO of less than 1 minute for the following reasons:</p><p>Recovery Point Objective (RPO) &lt; 1 minute:</p><p>Amazon Aurora Global Database uses a purpose-built replication technology that typically maintains replication lag of less than 1 second between the primary and secondary regions.</p><p>This ensures that in the event of a disaster, data loss would be minimal (measured in seconds), well within the RPO requirement of less than 1 minute.</p><p>The storage-based replication used by Aurora Global Database is more efficient than the binary log replication used by standard MySQL, resulting in lower replication lag.</p><p>Recovery Time Objective (RTO) &lt; 5 minutes:</p><p>Aurora Global Database allows for managed failover to the secondary region, which can be completed in less than 1 minute.</p><p>When combined with the already configured Route 53 health checks and DNS failover to us-west-2, the entire application stack can be failed over within the 5-minute RTO requirement.</p><p>The secondary region database is already warm and ready to accept write operations immediately after promotion.</p><p>Option A (Migrate the database to an Amazon RDS for MySQL instance with a cross-Region read replica) would not meet the requirements because:</p><p>RDS for MySQL cross-region read replicas use asynchronous replication based on binary logs, which can introduce higher replication lag that might exceed the RPO requirement of less than 1 minute, especially during periods of high write activity.</p><p>The failover process for RDS MySQL read replicas is manual and involves several steps:</p><p>Promoting the read replica to a standalone instance</p><p>Updating application connection strings</p><p>Waiting for DNS propagation</p><p>This manual process would likely exceed the RTO requirement of less than 5 minutes.</p><p>Amazon Aurora Global Database is specifically designed for cross-region disaster recovery scenarios with stringent RTO and RPO requirements. It provides the necessary infrastructure to meet the company's recovery objectives while minimizing operational complexity during a failover event.</p><p>Sources</p><p>Disaster recovery options in the cloud - Disaster Recovery of Workloads on AWS: Recovery in the Cloud（https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html） </p><p>Comparing Amazon Aurora replication solutions - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/aurora-replication-options/compare-solutions.html）</p><p>Cost-effective disaster recovery for Amazon Aurora databases using AWS Backup | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/cost-effective-disaster-recovery-for-amazon-aurora-databases-using-aws-backup/）</p><p>MySQL PostgreSQL Relational Database – Amazon Aurora Global Database – AWS （https://aws.amazon.com/cn/rds/aurora/global-database/）</p><p>Implement cross-Region disaster recovery with AWS DMS and Amazon Aurora - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/implement-cross-region-disaster-recovery-with-aws-dms-and-amazon-aurora.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8b5489c83a2447cba7316fc4ac2d4b79",
            "questionNumber": 273,
            "type": "single",
            "content": "<p>Question #273</p><p>A company is using AWS Organizations to manage multiple accounts. Due to regulatory requirements, the company wants to restrict specific member accounts to certain AWS Regions, where they are permitted to deploy resources. The resources in the accounts must be tagged, enforced based on a group standard, and centrally managed with minimal configuration.</p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Config rule in the specific member accounts to limit Regions and apply a tag policy."
                },
                {
                    "label": "B",
                    "content": "From the AWS Billing and Cost Management console, in the management account, disable Regions for the specific member accounts and apply a tag policy on the root."
                },
                {
                    "label": "C",
                    "content": "Associate the specific member accounts with the root. Apply a tag policy and an SCP using conditions to limit Regions."
                },
                {
                    "label": "D",
                    "content": "Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Associate the specific member accounts with a new OU. Apply a tag policy and an SCP using conditions to limit Regions.</p><p>This solution will meet the requirements for the following reasons:</p><p>Centralized Management: By creating a dedicated Organizational Unit (OU) for the specific member accounts that need region restrictions, you can centrally manage policies from the AWS Organizations management console. This provides a single point of control for both region restrictions and tagging requirements.</p><p>Minimal Configuration: Using OUs with Service Control Policies (SCPs) and tag policies is more efficient than configuring individual accounts. You only need to set up the policies once at the OU level, and they automatically apply to all accounts within that OU. When new accounts need the same restrictions, you simply move them into the OU.</p><p>Region Restriction: SCPs are preventive guardrails that can deny access to AWS services in specific regions. By using conditions in the SCP, you can precisely control which regions are allowed or denied for the accounts in the OU. This is a preventive control that blocks non-compliant actions before they occur.</p><p>Tag Enforcement: Tag policies in AWS Organizations allow you to define tagging rules and enforce them across multiple accounts. When applied at the OU level, these policies ensure consistent tagging across all resources in the member accounts, meeting the regulatory requirement for standardized tagging.</p><p>Flexibility: This approach allows for different policies to be applied to different groups of accounts by organizing them into appropriate OUs, which is ideal for environments with varying regulatory requirements.</p><p>The other options have significant limitations:</p><p>A. Creating AWS Config rules in specific member accounts would be detective rather than preventive, meaning non-compliant resources could still be created and would only be flagged after the fact. This approach also requires configuration in each account, which doesn't meet the \"centrally managed with minimal configuration\" requirement.</p><p>B. Disabling regions from the Billing console is not a comprehensive solution for restricting resource deployment. It primarily affects the console experience but doesn't prevent API or CLI access to those regions. Additionally, this approach doesn't provide robust tag enforcement capabilities.</p><p>C. Applying policies at the root level would affect all accounts in the organization, which may be too broad if only specific member accounts need these restrictions. This doesn't allow for the targeted application of policies that the scenario requires.</p><p>Option D provides the most effective, centralized, and least-effort approach to meeting the regulatory requirements for both region restrictions and tag enforcement on specific member accounts.</p><p>Sources</p><p>Tag Policies vs required-tags rule | AWS re:Post （https://repost.aws/questions/QUJVliDkVfQ6iJD0aUy0EU2A/tag-policies-vs-required-tags-rule）</p><p>AWS account management and separation - Security Pillar (https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/aws-account-management-and-separation.html)</p><p>Defining Boundaries for Regional Services Access - Navigating GDPR Compliance on AWS (https://docs.aws.amazon.com/whitepapers/latest/navigating-gdpr-compliance/defining-boundaries-for-regional-services-access.html)</p><p>Delegated Administrators Guide to Effective Controls in AWS Organizations | AWS Cloud Operations Blog (https://aws.amazon.com/cn/blogs/mt/delegated-administrators-guide-to-effective-controls-in-aws-organizations/)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5265f7fa58e94a449a3573d88176d0ff",
            "questionNumber": 274,
            "type": "single",
            "content": "<p>Question #274</p><p>A company has an application that generates reports and stores them in an Amazon S3 bucket. When a user accesses their report, the application generates a signed URL to allow the user to download the report. The company's security team has discovered that the files are public and that anyone can download them without authentication. The company has suspended the generation of new reports until the problem is resolved.<br><br></p><p>Which set of actions will immediately remediate the security issue without impacting the application's normal workflow?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Lambda function that applies a deny all policy for users who are not authenticated. Create a scheduled event to invoke the Lambda function."
                },
                {
                    "label": "B",
                    "content": "Review the AWS Trusted Advisor bucket permissions check and implement the recommended actions."
                },
                {
                    "label": "C",
                    "content": "Run a script that puts a private ACL on all of the objects in the bucket."
                },
                {
                    "label": "D",
                    "content": "Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcls option to TRUE on the bucket."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Use the Block Public Access feature in Amazon S3 to set the IgnorePublicAcls option to TRUE on the bucket.</p><p>This solution will immediately remediate the security issue without impacting the application's normal workflow for the following reasons:</p><p>Immediate Effect: The Block Public Access feature takes effect immediately when enabled, instantly blocking any public access to the bucket and its objects regardless of existing permissions.</p><p>Preserves Signed URL Functionality: Setting IgnorePublicAcls to TRUE specifically targets and blocks public access through ACLs, but it doesn't interfere with pre-signed URL functionality. Pre-signed URLs work through temporary credentials and don't rely on public ACLs, so the application can continue generating signed URLs for authenticated users to download their reports.</p><p>Non-Disruptive: This approach doesn't require modifying individual objects or changing the application's code. It works at the bucket level to override any public permissions without altering the underlying objects or their ACLs.</p><p>Comprehensive Protection: The IgnorePublicAcls setting ensures that even if objects in the bucket have public ACLs applied to them (which is likely the cause of the current issue), those ACLs will be ignored, effectively making the objects private.</p><p>The other options have significant limitations:</p><p>B. Reviewing the AWS Trusted Advisor bucket permissions check is only an advisory step that identifies potential issues but doesn't actually implement any changes to remediate the problem. This would not immediately fix the security issue.</p><p>C. Running a script to put a private ACL on all objects would be time-consuming for a large number of objects and might not address bucket-level permissions that could be causing the public access. Additionally, this approach could potentially disrupt the application if it relies on specific ACL settings for certain functionality.</p><p>Option D provides the most efficient and effective solution that immediately addresses the security concern while maintaining the application's ability to generate and use signed URLs for authorized access to the reports.</p><p>Sources</p><p>ACCT.08 Prevent public access to private S3 buckets - AWS Prescriptive Guidance (https://docs.aws.amazon.com/prescriptive-guidance/latest/aws-startup-security-baseline/acct-08.html)</p><p>Blocking public access to your Amazon S3 storage - Amazon Simple Storage Service (https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html)</p><p>Amazon S3 Block Public Access - AWS (https://aws.amazon.com/cn/s3/features/block-public-access/)</p><p>S3: is public access possible when Block all public access is on and object ownership is bucket owner enforced | AWS re:Post (https://repost.aws/questions/QUPKYOp6dZS-GDAtT84cbUjg/s3-is-public-access-possible-when-block-all-public-access-is-on-and-object-ownership-is-bucket-owner-enforced)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b7badbf7c2474110accff5a80431a016",
            "questionNumber": 275,
            "type": "multiple",
            "content": "<p>Question #275</p><p>A company is planning to migrate an Amazon RDS for Oracle database to an RDS for PostgreSQL DB instance in another AWS account. A solutions architect needs to design a migration strategy that will require no downtime and that will minimize the amount of time necessary to complete the migration. The migration strategy must replicate all existing data and any new data that is created during the migration. The target database must be identical to the source database at completion of the migration process.</p><p>All applications currently use an Amazon Route 53 CNAME record as their endpoint for communication with the RDS for Oracle DB instance. The RDS for Oracle DB instance is in a private subnet.</p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database."
                },
                {
                    "label": "B",
                    "content": "Use the AWS Schema Conversion Tool (AWS SCT) to create a new RDS for PostgreSQL DB instance in the target account with the schema and initial data from the source database."
                },
                {
                    "label": "C",
                    "content": "Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account."
                },
                {
                    "label": "D",
                    "content": "Temporarily allow the source DB instance to be publicly accessible to provide connectivity from the VPC in the target account."
                },
                {
                    "label": "E",
                    "content": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint."
                },
                {
                    "label": "F",
                    "content": "Use AWS Database Migration Service (AWS DMS) in the target account to perform a change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint."
                }
            ],
            "correctAnswer": "ACE",
            "explanation": "<p>The correct combination of steps is ACE. First, the AWS Schema Conversion Tool (AWS SCT) can be used to create a new RDS for PostgreSQL DB instance with the schema and initial data from the source Oracle database (A and B). Next, AWS DMS can be used to perform a full load of the data followed by change data capture (CDC) to replicate ongoing changes during the migration (E). Finally, once the migration is complete, the CNAME record in Route 53 can be updated to point to the new PostgreSQL DB instance endpoint, ensuring no downtime (E). Configuring VPC peering (C) is necessary to enable connectivity between the source and target environments, facilitating the migration process.</p><p>1. A. Create a new RDS for PostgreSQL DB instance in the target account. Use the AWS Schema Conversion Tool (AWS SCT) to migrate the database schema from the source database to the target database. &nbsp;</p><p> &nbsp; - AWS SCT is used to convert the Oracle schema to PostgreSQL schema, including stored procedures, functions, and other database objects. This is necessary because Oracle and PostgreSQL have different syntax and features. &nbsp;</p><p> &nbsp; - The initial data load is not handled by SCT; that is done by AWS DMS.</p><p>2. C. Configure VPC peering between the VPCs in the two AWS accounts to provide connectivity to both DB instances from the target account. Configure the security groups to allow traffic on the database port from the VPC in the target account. &nbsp;</p><p> &nbsp; - Since both databases are in private subnets, VPC peering is required to establish connectivity between them securely. &nbsp;</p><p> &nbsp; - Security groups must be configured to allow traffic between the source and target databases.</p><p>3. E. Use AWS Database Migration Service (AWS DMS) in the target account to perform a full load plus change data capture (CDC) migration from the source database to the target database. When the migration is complete, change the CNAME record to point to the target DB instance endpoint. &nbsp;</p><p> &nbsp; - AWS DMS performs the full load (initial data migration) and CDC (ongoing replication of changes during migration). &nbsp;</p><p> &nbsp; - Once the databases are in sync, the Route 53 CNAME is updated to point to the new PostgreSQL database, completing the cutover with minimal downtime.</p><p> Why not the other options?</p><p>- B: AWS SCT does not create a new RDS instance or migrate data; it only converts the schema.</p><p>- D: Making the source database publicly accessible is not secure and violates the requirement of keeping it in a private subnet.</p><p>- F: CDC alone is insufficient; a full load + CDC is required to migrate all existing data and replicate changes.</p><p>Thus, the correct answers are A, C, and E.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4b0f23b0f5fe4261a9d19098b903462a",
            "questionNumber": 276,
            "type": "single",
            "content": "<p>Question #276</p><p>A company has implemented an ordering system using an event-driven architecture. During initial testing, the system stopped processing orders. Further log analysis revealed that one order message in an Amazon Simple Queue Service (Amazon SQS) standard queue was causing an error on the backend and blocking all subsequent order messages. The visibility timeout of the queue is set to 30 seconds, and the backend processing timeout is set to 10 seconds. A solutions architect needs to analyze faulty order messages and ensure that the system continues to process subsequent messages.</p><p><br></p><p>Which step should the solutions architect take to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Increase the backend processing timeout to 30 seconds to match the visibility timeout."
                },
                {
                    "label": "B",
                    "content": "Reduce the visibility timeout of the queue to automatically remove the faulty message."
                },
                {
                    "label": "C",
                    "content": "Configure a new SQS FIFO queue as a dead-letter queue to isolate the faulty messages."
                },
                {
                    "label": "D",
                    "content": "Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Configure a new SQS standard queue as a dead-letter queue to isolate the faulty messages.</p><p>This solution will meet the requirements for the following reasons:</p><p>Continued Processing of Subsequent Messages:</p><p>By implementing a dead-letter queue (DLQ), the system can move the problematic message out of the main processing queue after a defined number of failed processing attempts.</p><p>Once the faulty message is moved to the DLQ, the system will automatically continue processing the next messages in the queue, preventing the blockage that's currently occurring.</p><p>This ensures business continuity while still preserving the problematic message for analysis.</p><p>Analysis of Faulty Messages:</p><p>The dead-letter queue preserves the faulty messages in a separate queue where they can be examined without affecting the main processing flow.</p><p>This allows the solutions architect to analyze the problematic order message in detail to understand what's causing the error.</p><p>The original message content and metadata are maintained, providing valuable diagnostic information.</p><p>Proper Error Handling:</p><p>Using a DLQ is a standard pattern for handling poison messages (messages that cannot be processed successfully) in event-driven architectures.</p><p>It provides a systematic way to deal with errors rather than letting them repeatedly block the system.</p><p>The other options have significant limitations:</p><p>A. Increasing the backend processing timeout to 30 seconds would not solve the problem. If the message is causing an error, giving it more processing time won't help - it will still fail and continue to block subsequent messages.</p><p>B. Reducing the visibility timeout would not automatically remove the faulty message. It would only make the message visible again sooner, causing it to be picked up for processing again, likely resulting in the same error. This could create an infinite loop of failed processing attempts.</p><p>Option D provides the most effective solution by isolating the problematic message while allowing the system to continue processing other orders. It also preserves the faulty message for analysis, which is crucial for identifying and fixing the underlying issue.</p><p>Sources</p><p>Amazon SQS error handling and problematic messages - Amazon Simple Queue Service (https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/best-practices-error-handling.html)</p><p>Using dead-letter queues in Amazon SQS - Amazon Simple Queue Service (https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-dead-letter-queues.html)</p><p>Amazon SQS best practices - Amazon Simple Queue Service (https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-best-practices.html)</p><p>Regarding SQS dead-letter queue (DLQ) | AWS re:Post (https://repost.aws/questions/QU6S7Kzhe4T3yWAQbyuHufow/regarding-sqs-dead-letter-queue-dlq)</p><p>Infinite retries due to exceeded SQS visibility timeout | AWS re:Post(https://repost.aws/questions/QUOGzGulGSS1-wtgDIj22USw/infinite-retries-due-to-exceeded-sqs-visibility-timeout) </p><p>Capturing problematic messages in Amazon SQS - Amazon Simple Queue Service (https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/capturing-problematic-messages.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "cad6a302894643838f069df251a7423f",
            "questionNumber": 277,
            "type": "multiple",
            "content": "<p>Question #277</p><p>A company has automated the nightly retraining of its machine learning models by using AWS Step Functions. The workflow consists of multiple steps that use AWS Lambda. Each step can fail for various reasons, and any failure causes a failure of the overall workflow. A review reveals that the retraining has failed multiple nights in a row without the company noticing the failure. A solutions architect needs to improve the workflow so that notifications are sent for all types of failures in the retraining process.</p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon Simple Notification Service (Amazon SNS) topic with a subscription of type &quot;Email&quot; that targets the team&#39;s mailing list."
                },
                {
                    "label": "B",
                    "content": "Create a task named &quot;Email&quot; that forwards the input arguments to the SNS topic."
                },
                {
                    "label": "C",
                    "content": "Add a Catch field to all Task, Map, and Parallel states that have a statement of &quot;ErrorEquals&quot;: [&quot;States.ALL&quot;] and &quot;Next&quot;: &quot;Email&quot;."
                },
                {
                    "label": "D",
                    "content": "Add a new email address to Amazon Simple Email Service (Amazon SES). Verify the email address."
                },
                {
                    "label": "E",
                    "content": "Create a task named &quot;Email&quot; that forwards the input arguments to the SES email address."
                },
                {
                    "label": "F",
                    "content": "Add a Catch field to all Task, Map, and Parallel states that have a statement of &quot;ErrorEquals&quot;: [&quot;States.Runtime&quot;] and &quot;Next&quot;: &quot;Email&quot;."
                }
            ],
            "correctAnswer": "ABC",
            "explanation": "<p>The correct combination of steps is ABC. To ensure that the team is notified of any failures in the retraining process, the architect should create an SNS topic and subscribe the team's email to it (A). Then, a task named \"Email\" should be created within the Step Functions workflow to publish messages to the SNS topic when a failure occurs (B). To catch all types of failures, a Catch block should be added to all states with a condition that listens for any error (\"States.ALL\") and then transitions to the \"Email\" task (C). This setup will capture any failure, regardless of its type, and trigger the notification process.</p><p> </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "31c965e4f01343e59de57b9c52df864e",
            "questionNumber": 278,
            "type": "single",
            "content": "<p>Question #278</p><p>A company plans to deploy a new private intranet service on Amazon EC2 instances inside a VPC. An AWS Site-to-Site VPN connects the VPC to the company's on-premises network. The new service must communicate with existing on-premises services. The on-premises services are accessible through the use of hostnames that reside in the company.example DNS zone. This DNS zone is wholly hosted on premises and is available only on the company's private network.</p><p><br></p><p>A solutions architect must ensure that the new service can resolve hostnames on the company.example domain to integrate with existing services.</p><p><br></p><p>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an empty private zone in Amazon Route 53 for company.example. Add an additional NS record to the company&#39;s on-premises company.example zone that points to the authoritative name servers for the new private zone in Route 53."
                },
                {
                    "label": "B",
                    "content": "Turn on DNS hostnames for the VPC. Configure a new outbound endpoint with Amazon Route 53 Resolver. Create a Resolver rule to forward requests for company.example to the on-premises name servers."
                },
                {
                    "label": "C",
                    "content": "Turn on DNS hostnames for the VPC. Configure a new inbound resolver endpoint with Amazon Route 53 Resolver. Configure the on-premises DNS server to forward requests for company.example to the new resolver."
                },
                {
                    "label": "D",
                    "content": "Use AWS Systems Manager to configure a run document that will install a hosts file that contains any required hostnames. Use an Amazon EventBridge rule to run the document when an instance is entering the running state."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. By enabling DNS hostnames for the VPC and configuring a new outbound endpoint with Amazon Route 53 Resolver, the architect can create a Resolver rule that forwards DNS queries for the company.example domain to the on-premises DNS server. This setup allows the EC2 instances within the VPC to resolve the hostnames of the on-premises services, thus integrating with the existing services.</p><p>The requirement is for the EC2 instances in the VPC to resolve hostnames in the company.example domain, which is hosted on-premises. Since the DNS zone is only available on the company's private network, we need to forward DNS queries for company.example to the on-premises DNS servers. &nbsp;</p><p> Why Option B is Correct: &nbsp;</p><p>1. Turn on DNS hostnames for the VPC – Ensures that instances in the VPC can resolve DNS hostnames. &nbsp;</p><p>2. Configure an outbound endpoint with Route 53 Resolver – Allows the VPC to send DNS queries to external (on-premises) DNS servers. &nbsp;</p><p>3. Create a Resolver rule to forward requests for company.example to on-premises name servers – This ensures that any DNS queries for company.example are forwarded to the on-premises DNS servers, while all other queries go to the default AWS resolver. &nbsp;</p><p> Why Other Options Are Incorrect: &nbsp;</p><p>- A: Creating an empty private zone in Route 53 and modifying the on-premises DNS to point to it would break resolution since the zone is hosted on-premises. &nbsp;</p><p>- C: Configuring an inbound resolver would allow on-premises systems to query AWS DNS, but we need the opposite (VPC to query on-premises DNS). &nbsp;</p><p>- D: Using a hosts file is not scalable and would require manual updates whenever hostnames change. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the correct solution because it forwards DNS queries for company.example to the on-premises DNS servers while allowing all other queries to resolve normally. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "90b685956f5e40ac97f06d9e2e5d9b19",
            "questionNumber": 279,
            "type": "single",
            "content": "<p>Question #279</p><p>A company uses AWS CloudFormation to deploy applications within multiple VPCs that are all attached to a transit gateway. Each VPC that sends traffic to the public internet must send the traffic through a shared services VPC. Each subnet within a VPC uses the default VPC route table, and the traffic is routed to the transit gateway. The transit gateway uses its default route table for any VPC attachment.</p><p><br></p><p>A security audit reveals that an Amazon EC2 instance that is deployed within a VPC can communicate with an EC2 instance that is deployed in any of the company's other VPCs. A solutions architect needs to limit the traffic between the VPCs. Each VPC must be able to communicate only with a pre-defined, limited set of authorized VPCs.</p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Update the network ACL of each subnet within a VPC to allow outbound traffic only to the authorized VPCs. Remove all deny rules except the default deny rule."
                },
                {
                    "label": "B",
                    "content": "Update all the security groups that are used within a VPC to deny outbound traffic to security groups that are used within the unauthorized VPCs."
                },
                {
                    "label": "C",
                    "content": "Create a dedicated transit gateway route table for each VPC attachment. Route traffic only to the authorized VPCs."
                },
                {
                    "label": "D",
                    "content": "Update the main route table of each VPC to route traffic only to the authorized VPCs through the transit gateway."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. To restrict traffic between VPCs and ensure that each VPC can only communicate with a specific set of authorized VPCs, the solutions architect should create a dedicated transit gateway route table for each VPC attachment. By doing so, the architect can define explicit routes within these dedicated route tables to only allow traffic to the authorized VPCs. This approach provides granular control over the routing of traffic between VPCs and ensures that the communication is limited as per the security requirements.</p><p>The issue is that all VPCs are attached to the transit gateway and using its default route table, which allows unrestricted communication between all attached VPCs. To restrict traffic between VPCs while still allowing communication only with predefined authorized VPCs, the best approach is to:</p><p>1. Create dedicated transit gateway route tables for each VPC attachment instead of using the default route table.</p><p>2. Configure explicit routes in each transit gateway route table to allow traffic only to the authorized VPCs.</p><p>This ensures that VPCs can communicate only with the explicitly allowed VPCs while still maintaining the shared internet egress through the shared services VPC.</p><p> Why Not the Other Options?</p><p>- A. Network ACLs are stateless and apply broadly to subnets, making them inefficient for fine-grained VPC-to-VPC control. They also don’t scale well for dynamic environments.</p><p>- B. Security Groups are instance-level controls and would require manual updates for every instance, which is impractical.</p><p>- D. Updating VPC route tables alone won’t restrict traffic at the transit gateway level, as the transit gateway’s default route table still allows all VPCs to communicate.</p><p> Key Takeaway:</p><p>Transit Gateway route tables provide the most scalable and centralized way to control inter-VPC routing while maintaining the required connectivity. </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "eed222a6764f43e18dfd5c17d480db16",
            "questionNumber": 280,
            "type": "single",
            "content": "<p>Question #280</p><p>A company has a Windows-based desktop application that is packaged and deployed to the users' Windows machines. The company recently acquired another company that has employees who primarily use machines with a Linux operating system. The acquiring company has decided to migrate and rehost the Windows-based desktop application to AWS.</p><p><br></p><p>All employees must be authenticated before they use the application. The acquiring company uses Active Directory on premises but wants a simplified way to manage access to the application on AWS for all the employees.</p><p><br></p><p>Which solution will rehost the application on AWS with the LEAST development effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up and provision an Amazon Workspaces virtual desktop for every employee. Implement authentication by using Amazon Cognito identity pools. Instruct employees to run the application from their provisioned Workspaces virtual desktops."
                },
                {
                    "label": "B",
                    "content": "Create an Auto Scaling group of Windows-based Amazon EC2 instances. Join each EC2 instance to the company&rsquo;s Active Directory domain. Implement authentication by using the Active Directory that is running on premises. Instruct employees to run the application by using a Windows remote desktop."
                },
                {
                    "label": "C",
                    "content": "Use an Amazon AppStream 2.0 image builder to create an image that includes the application and the required configurations. Provision an AppStream 2.0 On-Demand fleet with dynamic Fleet Auto Scaling policies for running the image. Implement authentication by using AppStream 2.0 user pools. Instruct the employees to access the application by starting browser-based AppStream 2.0 streaming sessions."
                },
                {
                    "label": "D",
                    "content": "Refactor and containerize the application to run as a web-based application. Run the application in Amazon Elastic Container Service (Amazon ECS) on AWS Fargate with step scaling policies. Implement authentication by using Amazon Cognito user pools. Instruct the employees to run the application from their browsers."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. Amazon AppStream 2.0 allows for the streaming of desktop applications through a web browser, which requires minimal to no changes to the existing application. By creating an AppStream 2.0 image with the application and configuring it to run on an On-Demand fleet with Auto Scaling, the company can provide access to the application across different operating systems with the least development effort. Additionally, using AppStream 2.0 user pools for authentication simplifies access management for the AWS environment.</p><p> Option C: Amazon AppStream 2.0</p><p>- Pros: </p><p> &nbsp;- Streams the Windows application to any device (including Linux) via a browser.</p><p> &nbsp;- No need to refactor the application.</p><p> &nbsp;- AppStream 2.0 User Pools provide simple authentication (or can integrate with Active Directory).</p><p> &nbsp;- Dynamic scaling ensures cost efficiency.</p><p>- Cons: None significant for this use case.</p><p>- Effort: Least (no refactoring, minimal setup, supports Linux users seamlessly).</p><p> Option D: Refactor to Web App + ECS/Fargate + Cognito</p><p>- Pros: Modern approach, scalable.</p><p>- Cons: </p><p> &nbsp;- Requires refactoring the Windows desktop app into a web app (high development effort).</p><p> &nbsp;- Not a rehosting solution (this is re-architecting).</p><p>- Effort: Highest (complete rewrite of the application).</p><p> Conclusion</p><p>Option C (Amazon AppStream 2.0) is the best choice because:</p><p>- It rehosts the Windows app without modification.</p><p>- Supports Linux users via browser streaming.</p><p>- Provides simple authentication (User Pools or AD integration).</p><p>- Requires the least development effort.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d72923c43fa7439a8683ce1287cfccfc",
            "questionNumber": 281,
            "type": "single",
            "content": "<p>Question #281</p><p>A company is collecting a large amount of data from a fleet of IoT devices. Data is stored as Optimized Row Columnar (ORC) files in the Hadoop Distributed File System (HDFS) on a persistent Amazon EMR cluster. The company's data analytics team queries the data by using SQL in Apache Presto deployed on the same EMR cluster. Queries scan large amounts of data, always run for less than 15 minutes, and run only between 5 PM and 10 PM.</p><p><br></p><p>The company is concerned about the high cost associated with the current solution. A solutions architect must propose the most cost-effective solution that will allow SQL data queries.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Store data in Amazon S3. Use Amazon Redshift Spectrum to query data."
                },
                {
                    "label": "B",
                    "content": "Store data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data."
                },
                {
                    "label": "C",
                    "content": "Store data in EMR File System (EMRFS). Use Presto in Amazon EMR to query data."
                },
                {
                    "label": "D",
                    "content": "Store data in Amazon Redshift. Use Amazon Redshift to query data."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Store data in Amazon S3. Use the AWS Glue Data Catalog and Amazon Athena to query data.</p><p>This solution will be the most cost-effective for the given scenario for the following reasons:</p><p>Serverless Architecture: Amazon Athena is a serverless query service, meaning you don't pay for any idle resources. Since the queries only run between 5 PM and 10 PM, a serverless solution eliminates the cost of maintaining infrastructure during the 19 hours per day when no queries are running.</p><p>Pay-Per-Query Pricing: With Athena, you only pay for the data scanned during query execution. This is ideal for the intermittent query pattern described (only 5 hours per day), as you're not paying for unused capacity.</p><p>Optimized for ORC Files: Athena works efficiently with columnar file formats like ORC (which the company is already using). ORC files in S3 can be queried directly by Athena, providing good performance while minimizing the amount of data scanned, which further reduces costs.</p><p>AWS Glue Data Catalog Integration: The AWS Glue Data Catalog provides a persistent metadata store that makes the data easily discoverable and queryable without requiring constant cluster resources.</p><p>SQL Interface: Athena provides a standard SQL interface, which meets the requirement for SQL data queries and would be familiar to users currently using Presto.</p><p>The other options have significant cost disadvantages:</p><p>A. Amazon Redshift Spectrum with data in S3 would require maintaining a Redshift cluster even during the 19 hours per day when no queries are running, which would be less cost-effective than the serverless Athena approach.</p><p>D. Storing data in Amazon Redshift would be the most expensive option because:</p><p>It would require provisioning Redshift cluster capacity to handle the large data volumes</p><p>The cluster would be idle for 19 hours per day</p><p>There would be additional costs for loading data from IoT devices into Redshift</p><p>Redshift is optimized for complex analytical queries rather than simple queries that run for less than 15 minutes</p><p>The combination of S3 storage (which is very cost-effective for large data volumes) and Athena's serverless query capability provides the most economical solution for the described workload pattern, while still meeting the requirement for SQL-based data analysis.</p><p>Sources</p><p>Amazon Redshift Spectrum overview - Amazon Redshift (https://docs.aws.amazon.com/redshift/latest/dg/c-spectrum-overview.html)</p><p>When should I use Athena? - Amazon Athena (https://docs.aws.amazon.com/athena/latest/ug/when-should-i-use-ate.html)</p><p>In-place querying - Storage Best Practices for Data and Analytics Applications (https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/in-place-querying.html)</p><p>Querying your data lake - Amazon Redshift (https://docs.aws.amazon.com/redshift/latest/gsg/data-lake.html)</p><p>Architecture patterns to optimize Amazon Redshift performance at scale | AWS Big Data Blog(https://aws.amazon.com/cn/blogs/big-data/architecture-patterns-to-optimize-amazon-redshift-performance-at-scale/) </p><p>Amazon Redshift deep dive - Data Warehousing on AWS (https://docs.aws.amazon.com/whitepapers/latest/data-warehousing-on-aws/amazon-redshift-deep-dive.html)</p><p>Accelerate Amazon Redshift Data Lake queries with AWS Glue Data Catalog Column Statistics | AWS Big Data Blog (https://aws.amazon.com/cn/blogs/big-data/accelerate-amazon-redshift-data-lake-queries-with-column-level-statistics/)</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "34db74aa8fb2416fb87b296c6043f438",
            "questionNumber": 282,
            "type": "single",
            "content": "<p>Question #282</p><p>A large company recently experienced an unexpected increase in Amazon RDS and Amazon DynamoDB costs. The company needs to increase visibility into details of AWS Billing and Cost Management. There are various accounts associated with AWS Organizations, including many development and production accounts. There is no consistent tagging strategy across the organization, but there are guidelines in place that require all infrastructure to be deployed using AWS CloudFormation with consistent tagging. Management requires cost center numbers and project ID numbers for all existing and future DynamoDB tables and RDS instances.</p><p><br></p><p>Which strategy should the solutions architect provide to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources."
                },
                {
                    "label": "B",
                    "content": "Use an AWS Config rule to alert the finance team of untagged resources. Create a centralized AWS Lambda based solution to tag untagged RDS databases and DynamoDB resources every hour using a cross-account role."
                },
                {
                    "label": "C",
                    "content": "Use Tag Editor to tag existing resources. Create cost allocation tags to define the cost center and project ID. Use SCPs to restrict resource creation that do not have the cost center and project ID on the resource."
                },
                {
                    "label": "D",
                    "content": "Create cost allocation tags to define the cost center and project ID and allow 24 hours for tags to propagate to existing resources. Update existing federated roles to restrict privileges to provision resources that do not include the cost center and project ID on the resource."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. The solutions architect should use the Tag Editor to apply tags to existing resources, ensuring that cost allocation tags for the cost center and project ID are in place. Additionally, creating SCPs (Service Control Policies) within AWS Organizations can enforce tagging requirements for all resources created moving forward, ensuring that new DynamoDB tables and RDS instances cannot be provisioned without the necessary tags. This approach not only addresses the immediate need to tag existing resources but also prevents future un-tagged resources from being created.</p><p> Requirements Summary:</p><p>1. Increase visibility into AWS Billing and Cost Management (requires proper tagging).</p><p>2. No consistent tagging strategy, but CloudFormation mandates consistent tagging for new deployments.</p><p>3. Management requires cost center and project ID tags for all existing and future RDS and DynamoDB resources.</p><p>4. Multiple accounts under AWS Organizations (centralized control needed).</p><p> Analysis of Options:</p><p> Option A:</p><p>- Tag Editor can manually tag existing resources (partially meets the requirement).</p><p>- Cost allocation tags help in billing visibility.</p><p>- But: Waiting 24 hours for tag propagation doesn’t enforce future compliance. No mechanism prevents untagged resource creation.</p><p> Option B:</p><p>- AWS Config rule can detect untagged resources (reactive, not proactive).</p><p>- Lambda-based tagging solution (complex, requires maintenance, and may lag in real-time enforcement).</p><p>- Does not prevent untagged resource creation, only reacts afterward.</p><p> Option C:</p><p>- Tag Editor ensures existing resources are tagged.</p><p>- Cost allocation tags enable billing visibility.</p><p>- SCPs (Service Control Policies) enforce compliance proactively by blocking resource creation without required tags.</p><p>- CloudFormation ensures new deployments comply with tagging guidelines.</p><p>- Best meets all requirements: Existing tagging, billing visibility, and future enforcement.</p><p> Option D:</p><p>- Cost allocation tags help with billing.</p><p>- Federated role updates restrict untagged resource creation, but this is less scalable and harder to manage than SCPs.</p><p>- No mention of tagging existing resources (only propagation, which doesn’t guarantee coverage).</p><p> Why Option C is Best:</p><p>- Proactive enforcement via SCPs (blocks non-compliant resource creation).</p><p>- Tag Editor covers existing resources.</p><p>- CloudFormation ensures future deployments comply (since SCPs enforce tagging).</p><p>- Cost allocation tags provide billing visibility.</p><p> Conclusion:</p><p>Option C is the most comprehensive and scalable solution, addressing both existing and future tagging requirements while enforcing compliance via SCPs.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5626670137eb424fad74fa125210998a",
            "questionNumber": 283,
            "type": "multiple",
            "content": "<p>Question #283</p><p>A company wants to send data from its on-premises systems to Amazon S3 buckets. The company created the S3 buckets in three different accounts. The company must send the data privately without the data traveling across the internet. The company has no existing dedicated connectivity to AWS.</p><p>Which combination of steps should a solutions architect take to meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a private VIF between the on-premises environment and the private VPC."
                },
                {
                    "label": "B",
                    "content": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a public VIF between the on-premises environment and the private VPC."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon S3 interface endpoint in the networking account."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon S3 gateway endpoint in the networking account."
                },
                {
                    "label": "E",
                    "content": "Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Peer VPCs from the &nbsp;accounts that host the S3 buckets with the VPC in the network account</span></p>"
                }
            ],
            "correctAnswer": "AD",
            "explanation": "<p>To send data from on-premises systems to Amazon S3 buckets in three different accounts privately without traversing the internet, a solutions architect should take the following combination of steps:</p><p><br></p><p>A. Establish a networking account in the AWS Cloud. Create a private VPC in the networking account. Set up an AWS Direct Connect connection with a private VIF between the on-premises environment and the private VPC.</p><p><br></p><p>D. Create an Amazon S3 gateway endpoint in the networking account.</p><p><br></p><p>Here's why this combination is the correct solution:</p><p><br></p><p>AWS Direct Connect with a private VIF (Virtual Interface) provides a dedicated, private network connection between the on-premises environment and AWS. This satisfies the requirement for private data transfer without using the internet. The private VIF allows access to resources within a VPC using private IP addresses.</p><p><br></p><p>An S3 gateway endpoint in the networking account enables private connectivity to S3 from the VPC without requiring public IP addresses or internet access. This endpoint allows traffic to flow directly from the VPC to S3 through the AWS private network.</p><p><br></p><p>With this architecture:</p><p><br></p><p>Data flows from on-premises systems to AWS via the Direct Connect private connection</p><p>From the networking account VPC, data can be securely transferred to S3 buckets in all three accounts through the S3 gateway endpoint</p><p>The entire data path remains private and never traverses the public internet</p><p>The other options would not be suitable because:</p><p><br></p><p>Option B (Direct Connect with public VIF) would route traffic over the internet, violating the requirement for private data transfer</p><p>Option C (S3 interface endpoint) would work but is less cost-effective than a gateway endpoint for S3 access and doesn't provide any additional benefits in this scenario</p><p>This solution provides a secure, private, and efficient way to transfer data from on-premises systems to S3 buckets across multiple AWS accounts.</p><p><br></p><p>Sources</p><p>Create a server in a virtual private cloud - AWS Transfer Family (https://docs.aws.amazon.com/transfer/latest/userguide/create-server-in-vpc.html)</p><p>AWS Direct Connect - Amazon Virtual Private Cloud Connectivity Options (https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect.html)</p><p>AWS Direct Connect &nbsp;- Building a Scalable and Secure Multi-VPC AWS Network Infrastructure (https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html)</p><p>Set up private access to an Amazon S3 bucket through a VPC endpoint - AWS Prescriptive Guidance (https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/set-up-private-access-to-an-amazon-s3-bucket-through-a-vpc-endpoint.html)</p><p>AWS PrivateLink for Amazon S3 - Amazon Simple Storage Service (https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html)</p><p>Migrate small sets of data from on premises to Amazon S3 using AWS SFTP - AWS Prescriptive Guidance (https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-small-sets-of-data-from-on-premises-to-amazon-s3-using-aws-sftp.html)</p><p>Transfer S3 object via Direct Connect - Data Transfer Hub (https://docs.aws.amazon.com/solutions/latest/data-transfer-hub/transfer-s3-object-via-direct-connect.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d1b73beaf3894913aa8b3b557d40f0fd",
            "questionNumber": 284,
            "type": "single",
            "content": "<p>Question #284</p><p>A company operates quick-service restaurants. The restaurants follow a predictable model with high sales traffic for 4 hours daily. Sales traffic is lower outside of those peak hours.</p><p><br></p><p>The point of sale and management platform is deployed in the AWS Cloud and has a backend that is based on Amazon DynamoDB. The database table uses provisioned throughput mode with 100,000 RCUs and 80,000 WCUs to match known peak resource consumption.</p><p><br></p><p>The company wants to reduce its DynamoDB cost and minimize the operational overhead for the IT staff.</p><p><br></p><p>Which solution meets these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Reduce the provisioned RCUs and WCUs."
                },
                {
                    "label": "B",
                    "content": "Change the DynamoDB table to use on-demand capacity."
                },
                {
                    "label": "C",
                    "content": "Enable Dynamo DB auto scaling for the table."
                },
                {
                    "label": "D",
                    "content": "Purchase 1-year reserved capacity that is sufficient to cover the peak load for 4 hours each day."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Enable DynamoDB auto scaling for the table.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>Predictable Traffic Pattern: The restaurant has a predictable model with high sales traffic for 4 hours daily and lower traffic outside those peak hours. This is an ideal scenario for DynamoDB auto scaling, which can automatically adjust capacity based on actual usage patterns.</p><p>Cost Optimization: With auto scaling, the provisioned capacity will automatically scale up during the 4-hour peak periods and scale down during the remaining 20 hours of lower traffic. This means you'll only pay for the capacity you actually need at any given time, rather than constantly paying for peak capacity (100,000 RCUs and 80,000 WCUs) throughout the entire day.</p><p>Minimal Operational Overhead: Once configured, auto scaling requires minimal intervention from the IT staff. The system will automatically adjust capacity based on defined utilization targets, eliminating the need for manual capacity adjustments throughout the day.</p><p>Balance of Performance and Cost: Auto scaling ensures that the application maintains performance during peak hours while optimizing costs during off-peak hours.</p><p>The other options have significant limitations:</p><p>A. Reducing the provisioned RCUs and WCUs would lower costs but could lead to throttling during peak hours, potentially affecting customer experience. This approach doesn't address the variable nature of the workload.</p><p>B. Changing to on-demand capacity could work but would likely be more expensive for this use case. On-demand pricing is generally higher than provisioned capacity pricing for predictable workloads with high utilization during specific periods.</p><p>D. Purchasing 1-year reserved capacity for peak load would be wasteful since the peak load only occurs for 4 hours each day. You would be paying for unused capacity during the remaining 20 hours, which is not cost-effective.</p><p>DynamoDB auto scaling provides the optimal balance between cost savings and operational simplicity for this specific use case with predictable traffic patterns. It automatically adjusts capacity to match the restaurant's daily traffic patterns while minimizing both costs and operational overhead.</p><p>Sources</p><p>Reduce latency and cost in read-heavy applications using Amazon DynamoDB Accelerator | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/reduce-latency-and-cost-in-read-heavy-applications-using-amazon-dynamodb-accelerator/）</p><p>Choose the right throughput strategy for Amazon DynamoDB applications | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/choose-the-right-throughput-strategy-for-amazon-dynamodb-applications/）</p><p>Amazon DynamoDB Reserved Capacity – Fast NoSQL Key-Value Database – Amazon Web Services （https://aws.amazon.com/cn/dynamodb/reserved-capacity/）</p><p>Evaluate your DynamoDB table's capacity mode - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/CostOptimization_TableCapacityMode.html）</p><p>Minimize DynamoDB costs | AWS re:Post （https://repost.aws/knowledge-center/dynamodb-minimize-costs）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "66bf56065abd4c32bcf266360744373d",
            "questionNumber": 285,
            "type": "single",
            "content": "<p>Question #285</p><p>A company hosts a blog post application on AWS using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application currently does not use API keys to authorize requests. The API model is as follows:<br><br></p><p>GET /posts/{postId}: to get post details</p><p>GET /users/{userId}: to get user details</p><p>GET /comments/{commentId}: to get comments details<br><br></p><p>The company has noticed users are actively discussing topics in the comments section, and the company wants to increase user engagement by making the comments appear in real time.<br><br></p><p>Which design should be used to reduce comment latency and improve user experience?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use edge-optimized API with Amazon CloudFront to cache API responses."
                },
                {
                    "label": "B",
                    "content": "Modify the blog application code to request GET/comments/{commentId} every 10 seconds."
                },
                {
                    "label": "C",
                    "content": "Use AWS AppSync and leverage WebSockets to deliver comments."
                },
                {
                    "label": "D",
                    "content": "Change the concurrency limit of the Lambda functions to lower the API response time."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use AWS AppSync and leverage WebSockets to deliver comments.</p><p>This solution will best reduce comment latency and improve user experience for the following reasons:</p><p>Real-time Communication: WebSockets provide a persistent, bidirectional connection between the client and server, allowing comments to be pushed to users instantly as they are created. This is the only solution that enables true real-time updates without requiring users to take any action.</p><p>Reduced Latency: With WebSockets, new comments are delivered to users immediately when they're posted, eliminating the delay that would occur with traditional request-response patterns. This directly addresses the requirement to \"reduce comment latency.\"</p><p>Enhanced User Experience: Real-time comment updates create a more engaging and interactive experience, as users can see conversations develop naturally without having to refresh the page or wait for periodic updates. This directly addresses the requirement to \"improve user experience.\"</p><p>Efficient Resource Usage: WebSockets are more efficient than polling approaches, as they only transmit data when there's new information, rather than making repeated requests that often return no new data.</p><p>Scalability: AWS AppSync is designed to handle large numbers of concurrent WebSocket connections, making it suitable for a growing blog application with active user discussions.</p><p>The other options have significant limitations:</p><p>A. Using edge-optimized API with CloudFront to cache API responses would actually be counterproductive for real-time comments. Caching would delay the delivery of new comments, as users might see cached (older) versions of the comments section rather than the latest updates.</p><p>B. Polling the comments endpoint every 10 seconds creates unnecessary load on the backend and still introduces a delay of up to 10 seconds before new comments appear. This approach is inefficient and provides a suboptimal user experience compared to real-time updates.</p><p>D. Changing the concurrency limit of Lambda functions might improve the overall throughput of the API but doesn't address the fundamental issue of delivering comments in real time. It would still rely on clients repeatedly requesting data rather than receiving updates as they occur.</p><p>AWS AppSync with WebSockets is the only solution that fundamentally changes the communication pattern to enable true real-time delivery of comments, which is essential for creating the engaging user experience the company is looking for.</p><p>Sources</p><p>Amazon API Gateway - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/amazon-api-gateway.html）</p><p>Amazon ElastiCache vs API gateway edge-optimized endpoint | AWS re:Post （https://repost.aws/questions/QU4xZPFTZ3TASRtyDteJBM7Q/amazon-elasticache-vs-api-gateway-edge-optimized-endpoint）</p><p>Scale lambdas to tens of thousands concurrent runnings | AWS re:Post （https://repost.aws/questions/QUjv5PAE_dQ56EXvEpkJ1WTQ/scale-lambdas-to-tens-of-thousands-concurrent-runnings）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "46a5bb2f4df34a2ebb4930064a87b440",
            "questionNumber": 286,
            "type": "single",
            "content": "<p>Question #286</p><p>A company manages hundreds of AWS accounts centrally in an organization in AWS Organizations. The company recently started to allow product teams to create and manage their own S3 access points in their accounts. The S3 access points can be accessed only within VPCs, not on the internet.<br><br></p><p>What is the MOST operationally efficient way to enforce this requirement?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set the S3 access point resource policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
                },
                {
                    "label": "B",
                    "content": "Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
                },
                {
                    "label": "C",
                    "content": "Use AWS CloudFormation StackSets to create a new IAM policy in each AWS account that allows the s3:CreateAccessPoint action only if the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
                },
                {
                    "label": "D",
                    "content": "Set the S3 bucket policy to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an SCP at the root level in the organization to deny the s3:CreateAccessPoint action unless the s3:AccessPointNetworkOrigin condition key evaluates to VPC.</p><p>This solution is the MOST operationally efficient way to enforce the requirement that S3 access points can be accessed only within VPCs for the following reasons:</p><p>Centralized Management: With hundreds of AWS accounts to manage, implementing a single Service Control Policy (SCP) at the organization's root level provides a centralized approach to enforce this requirement across all accounts. This eliminates the need to configure and maintain separate policies in each individual account.</p><p>Preventative Control: SCPs act as guardrails that cannot be overridden by account administrators. By denying the creation of access points unless they're VPC-only, this approach prevents non-compliant access points from being created in the first place, rather than detecting them after creation.</p><p>Minimal Maintenance: Once the SCP is deployed at the root level, it automatically applies to all existing and future accounts in the organization without requiring additional configuration or deployment steps as new accounts are added.</p><p>Consistent Enforcement: The SCP ensures uniform enforcement of the VPC-only requirement across all accounts, eliminating potential gaps or inconsistencies that might occur with decentralized approaches.</p><p>The other options have significant limitations:</p><p>A. Setting the S3 access point resource policy would need to be done for each access point individually after it's created. This is not operationally efficient for hundreds of accounts and doesn't prevent the initial creation of non-compliant access points.</p><p>C. Using AWS CloudFormation StackSets to create IAM policies in each account would require more complex setup and ongoing maintenance. While this approach could work, it's less efficient than using an SCP because:</p><p>It requires deploying and maintaining StackSets</p><p>IAM policies can potentially be modified by account administrators</p><p>It adds unnecessary complexity compared to the simpler SCP approach</p><p>Option B provides the most straightforward, centralized, and efficient way to enforce the requirement across the entire organization with minimal operational overhead, making it the best choice for this scenario.</p><p>Sources</p><p>Creating access points restricted to a virtual private cloud - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-vpc.html）</p><p>Configuring IAM policies for using access points for directory buckets - Amazon Simple Storage Service（https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points-directory-buckets-policies.html） </p><p>Access control in Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-management.html）</p><p>SEC03-BP08 Share resources securely within your organization - AWS Well-Architected Framework (2023-04-10) （https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/sec_permissions_share_securely.html）</p><p>Controlling access from VPC endpoints with bucket policies - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies-vpc-endpoint.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b6c40a390f63448ba0d6802dc73e1fb7",
            "questionNumber": 287,
            "type": "single",
            "content": "<p>Question #287</p><p>A solutions architect must update an application environment within AWS Elastic Beanstalk using a blue/green deployment methodology. The solutions architect creates an environment that is identical to the existing application environment and deploys the application to the new environment.<br><br></p><p>What should be done next to complete the update?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Redirect to the new environment using Amazon Route 53."
                },
                {
                    "label": "B",
                    "content": "Select the Swap Environment URLs option."
                },
                {
                    "label": "C",
                    "content": "Replace the Auto Scaling launch configuration."
                },
                {
                    "label": "D",
                    "content": "Update the DNS records to point to the green environment."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Select the Swap Environment URLs option.</p><p>This is the correct next step in completing a blue/green deployment in AWS Elastic Beanstalk for the following reasons:</p><p>Seamless Transition: The \"Swap Environment URLs\" feature is specifically designed for blue/green deployments in Elastic Beanstalk. It allows for an instantaneous switch of the CNAME records between the two environments, directing traffic from the old (blue) environment to the new (green) environment without downtime.</p><p>Built-in Functionality: This is a native feature of Elastic Beanstalk that's purpose-built for this exact scenario. It's the standard and recommended approach for completing blue/green deployments within the Elastic Beanstalk service.</p><p>Minimal DNS Propagation: When you swap environment URLs in Elastic Beanstalk, the change happens almost immediately because it's simply swapping the CNAME records within AWS's DNS infrastructure. This results in minimal or no downtime for users.</p><p>Easy Rollback: If any issues are discovered after the swap, you can easily roll back by performing another swap to return traffic to the original environment.</p><p>The other options have significant limitations:</p><p>A. Redirecting using Amazon Route 53 would require additional configuration and wouldn't be the most direct method when using Elastic Beanstalk's built-in functionality. While Route 53 can be used for more complex routing scenarios, it's unnecessary for a standard Elastic Beanstalk blue/green deployment.</p><p>C. Replacing the Auto Scaling launch configuration wouldn't redirect traffic to the new environment. Launch configurations define how new instances are created within an Auto Scaling group, but they don't affect routing of user traffic.</p><p>D. Manually updating DNS records to point to the green environment would be more complex and time-consuming than using the built-in \"Swap Environment URLs\" feature. It would also likely result in longer propagation times, potentially causing inconsistent user experiences during the transition.</p><p>The \"Swap Environment URLs\" option is the most efficient and reliable method to complete a blue/green deployment in AWS Elastic Beanstalk after creating and deploying to a new environment.</p><p>Sources</p><p>Swap the Environment of an Elastic Beanstalk Application - Blue/Green Deployments on AWS （https://docs.aws.amazon.com/whitepapers/latest/blue-green-deployments/swap-the-environment-of-an-elastic-beanstalk-application.html）</p><p>Blue/Green deployments with Elastic Beanstalk - AWS Elastic Beanstalk （https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.CNAMESwap.html）</p><p>Blue/Green deployment | AWS re:Post （https://repost.aws/questions/QU39OiY-naQZ--BOKDcRkzTw/blue-green-deployment）</p><p>Seamless Production Deployment with Elastic Beanstalk | .NET on AWS Blog （https://aws.amazon.com/cn/blogs/dotnet/seamless-production-deployment-with-elastic-beanstalk/）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ae1d3226990e4fe895ae831122b180a0",
            "questionNumber": 288,
            "type": "single",
            "content": "<p>Question #288</p><p>A company is building an image service on the web that will allow users to upload and search random photos. At peak usage, up to 10,000 users worldwide will upload their images. They will then overlay text on the uploaded images, which will then be published on the company website.</p><p>Which design should a solutions architect implement?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Store the uploaded images in Amazon Elastic File System (Amazon EFS). Send application log information about each image to Amazon CloudWatch Logs. Create a fleet of Amazon EC2 instances that use CloudWatch Logs to determine which images need to be processed. Place processed images in another directory in Amazon EFS. Enable Amazon CloudFront and configure the origin to be the one of the EC2 instances in the fleet."
                },
                {
                    "label": "B",
                    "content": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to Amazon Simple Notification Service (Amazon SNS). Create a fleet of Amazon EC2 instances behind an Application Load Balancer (ALB) to pull messages from Amazon SNS to process the images and place them in Amazon Elastic File System (Amazon EFS). Use Amazon CloudWatch metrics for the SNS message volume to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the ALB in front of the EC2 instances."
                },
                {
                    "label": "C",
                    "content": "Store the uploaded images in an Amazon S3 bucket and configure an S3 bucket event notification to send a message to the Amazon Simple Queue Service (Amazon SQS) queue. Create a fleet of Amazon EC2 instances to pull messages from the SQS queue to process the images and place them in another S3 bucket. Use Amazon CloudWatch metrics for queue depth to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to be the S3 bucket that contains the processed images."
                },
                {
                    "label": "D",
                    "content": "Store the uploaded images on a shared Amazon Elastic Block Store (Amazon EBS) volume mounted to a fleet of Amazon EC2 Spot instances. Create an Amazon DynamoDB table that contains information about each uploaded image and whether it has been processed. Use an Amazon EventBridge rule to scale out EC2 instances. Enable Amazon CloudFront and configure the origin to reference an Elastic Load Balancer in front of the fleet of EC2 instances."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. This solution involves using Amazon S3 for storing uploaded images, which is a scalable and durable storage solution. By configuring S3 bucket event notifications to send messages to an SQS queue, the system can decouple image processing from the upload process. A fleet of EC2 instances can then process the images as they are received from the SQS queue. Using CloudWatch metrics to monitor the SQS queue depth allows for automatic scaling of EC2 instances to handle the workload. Finally, enabling Amazon CloudFront with the S3 bucket of processed images as the origin ensures fast content delivery to users worldwide.</p><p>1. Uploaded images stored in Amazon S3: &nbsp;</p><p> &nbsp; - S3 is highly scalable, durable, and cost-effective for storing large volumes of images. &nbsp;</p><p> &nbsp; - It supports event notifications when new objects are uploaded, triggering downstream processing.</p><p>2. S3 event notification → Amazon SQS: &nbsp;</p><p> &nbsp; - SQS decouples the upload event from processing, ensuring reliability and fault tolerance. &nbsp;</p><p> &nbsp; - If processing fails, messages remain in the queue for retry.</p><p>3. EC2 instances process messages from SQS: &nbsp;</p><p> &nbsp; - Workers pull messages from SQS, process images (overlaying text), and store results in another S3 bucket. &nbsp;</p><p> &nbsp; - Auto-scaling based on queue depth (CloudWatch metrics) ensures efficient scaling during peak loads.</p><p>4. Processed images in another S3 bucket + CloudFront: &nbsp;</p><p> &nbsp; - S3 is ideal for serving static content (processed images). &nbsp;</p><p> &nbsp; - CloudFront (CDN) improves global access speed by caching images at edge locations. &nbsp;</p><p> Why the other options are suboptimal:</p><p>- A: Uses EFS for storage, which is expensive for large-scale object storage (better for shared file systems). Also, relying on CloudWatch Logs for processing logic is inefficient. &nbsp;</p><p>- B: Uses SNS (pub/sub) instead of SQS (queue), which lacks message retention if workers fail. Also, EFS is unnecessary for processed images (S3 is better). &nbsp;</p><p>- D: Uses shared EBS, which doesn’t scale well globally and introduces single-point failure risks. Spot Instances are risky for critical workloads, and DynamoDB adds unnecessary complexity.</p><p> Key AWS Services in C:</p><p>✅ Amazon S3 (storage) &nbsp;</p><p>✅ SQS (decoupled processing) &nbsp;</p><p>✅ EC2 + Auto Scaling (processing) &nbsp;</p><p>✅ CloudFront (fast global delivery) &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ce95ce5ba7a6468094f47acf22682578",
            "questionNumber": 289,
            "type": "single",
            "content": "<p>Question #289</p><p>A company has deployed its database on an Amazon RDS for MySQL DB instance in the us-east-1 Region. The company needs to make its data available to customers in Europe. The customers in Europe must have access to the same data as customers in the United States (US) and will not tolerate high application latency or stale data. The customers in Europe and the customers in the US need to write to the database. Both groups of customers need to see updates from the other group in real time.<br><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon Aurora MySQL replica of the RDS for MySQL DB instance. Pause application writes to the RDS DB instance. Promote the Aurora Replica to a standalone DB cluster. Reconfigure the application to use the Aurora database and resume writes. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1."
                },
                {
                    "label": "B",
                    "content": "Add a cross-Region replica in eu-west-1 for the RDS for MySQL DB instance. Configure the replica to replicate write queries back to the primary DB instance. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1."
                },
                {
                    "label": "C",
                    "content": "Copy the most recent snapshot from the RDS for MySQL DB instance to eu-west-1. Create a new RDS for MySQL DB instance in eu-west-1 from the snapshot. Configure MySQL logical replication from us-east-1 to eu-west-1. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the RDS for MySQL endpoint in eu-west-1."
                },
                {
                    "label": "D",
                    "content": "Convert the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster. Add eu-west-1 as a secondary Region to the DB cluster. Enable write forwarding on the DB cluster. Deploy the application in eu-west-1. Configure the application to use the Aurora MySQL endpoint in eu-west-1."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The correct answer is D. Amazon Aurora global databases allow for a single database cluster that spans multiple AWS Regions, providing fast local reads with low latency in each region. By converting the RDS for MySQL DB instance to an Amazon Aurora MySQL DB cluster and adding eu-west-1 as a secondary Region, the company can meet the requirement for real-time updates and low latency for customers in Europe. Write forwarding ensures that write operations are forwarded from the secondary region to the primary region, maintaining data consistency across regions.</p><p> Requirements:</p><p>1. Global Read/Write Access: Customers in both the US (us-east-1) and Europe (eu-west-1) need to write to the database.</p><p>2. Real-Time Updates: Both regions must see updates from each other in real time.</p><p>3. Low Latency: High application latency is unacceptable.</p><p>4. No Stale Data: Customers cannot tolerate stale data.</p><p> Why Option D is Correct:</p><p>- Amazon Aurora Global Database with Write Forwarding is the best solution because:</p><p> &nbsp;- It allows multiple regions to read and write while maintaining low-latency performance.</p><p> &nbsp;- Write Forwarding enables writes in the secondary region (eu-west-1) to be forwarded to the primary region (us-east-1), ensuring real-time consistency.</p><p> &nbsp;- Aurora Global Database provides fast replication (typically &lt; 1 second) across regions.</p><p> &nbsp;- It eliminates the need for complex replication setups or manual failovers.</p><p> Why Other Options Are Incorrect:</p><p>- Option A: Promotes an Aurora Replica to standalone, which breaks replication and doesn't provide real-time sync between regions.</p><p>- Option B: Cross-Region replicas in RDS for MySQL are read-only, so writes in eu-west-1 would not be possible.</p><p>- Option C: Logical replication introduces lag and complexity, and RDS for MySQL does not natively support write forwarding.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c31b22c8db2645c68fec50e1ab736dd6",
            "questionNumber": 290,
            "type": "single",
            "content": "<p>Question #290</p><p>A company is serving files to its customers through an SFTP server that is accessible over the internet. The SFTP server is running on a single Amazon EC2 instance with an Elastic IP address attached. Customers connect to the SFTP server through its Elastic IP address and use SSH for authentication. The EC2 instance also has an attached security group that allows access from all customer IP addresses. </p><p><br></p><p>A solutions architect must implement a solution to improve availability, minimize the complexity of infrastructure management, and minimize the disruption to customers who access files. The solution must not change the way customers connect.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a publicly accessible endpoint. Associate the SFTP Elastic IP address with the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Disassociate the Elastic IP address from the EC2 instance. Create an Amazon S3 bucket to be used for SFTP file hosting. Create an AWS Transfer Family server. Configure the Transfer Family server with a VPC-hosted, internet-facing endpoint. Associate the SFTP Elastic IP address with the new endpoint. Attach the security group with customer IP addresses to the new endpoint. Point the Transfer Family server to the S3 bucket. Sync all files from the SFTP server to the S3 bucket."
                },
                {
                    "label": "C",
                    "content": "Disassociate the Elastic IP address from the EC2 instance. Create a new Amazon Elastic File System (Amazon EFS) file system to be used for SFTP file hosting. Create an AWS Fargate task definition to run an SFTP server. Specify the EFS file system as a mount in the task definition. Create a Fargate service by using the task definition, and place a Network Load Balancer (NLB) in front of the service. When configuring the service, attach the security group with customer IP addresses to the tasks that run the SFTP server. Associate the Elastic IP address with the NLB. Sync all files from the SFTP server to the S3 bucket."
                },
                {
                    "label": "D",
                    "content": "Disassociate the Elastic IP address from the EC2 instance. Create a multi-attach Amazon Elastic Block Store (Amazon EBS) volume to be used for SFTP file hosting. Create a Network Load Balancer (NLB) with the Elastic IP address attached. Create an Auto Scaling group with EC2 instances that run an SFTP server. Define in the Auto Scaling group that instances that are launched should attach the new multi-attach EBS volume. Configure the Auto Scaling group to automatically add instances behind the NLB. Configure the Auto Scaling group to use the security group that allows customer IP addresses for the EC2 instances that the Auto Scaling group launches. Sync all files from the SFTP server to the new multi-attach EBS volume."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. By disassociating the Elastic IP from the EC2 instance and creating an AWS Transfer Family server with a VPC-hosted, internet-facing endpoint, the architect can maintain the same IP address for customer connections while improving availability and reducing management complexity. The security group can be attached to the new endpoint to maintain the existing access controls. Pointing the Transfer Family server to an S3 bucket allows for scalable file storage and synchronization from the existing SFTP server.</p><p> Requirements:</p><p>1. Improve availability: The current single EC2 instance is a single point of failure. The solution must provide high availability.</p><p>2. Minimize infrastructure management complexity: Moving to a managed service (like AWS Transfer Family) reduces operational overhead.</p><p>3. Minimize disruption to customers: Customers must continue connecting the same way (using the same Elastic IP and SSH authentication).</p><p>4. No change to customer connection method: The Elastic IP and security group rules must remain intact.</p><p> Why Option B?</p><p>- AWS Transfer Family is a managed SFTP service that provides high availability and scalability without managing servers.</p><p>- By configuring a VPC-hosted, internet-facing endpoint, the Transfer Family server can reuse the existing Elastic IP and security group (allowing customer IPs), ensuring no changes for customers.</p><p>- Amazon S3 is a durable and highly available storage backend for the files.</p><p>- The solution syncs files from the old SFTP server to S3, ensuring continuity.</p><p> Why Not Other Options?</p><p>- Option A: Uses a publicly accessible endpoint for Transfer Family, which doesn’t allow attaching the existing security group (required for customer IP restrictions).</p><p>- Option C: Uses Fargate + EFS + NLB, which introduces unnecessary complexity (managing containers, load balancers, and file sync) compared to the managed AWS Transfer Family.</p><p>- Option D: Uses multi-attach EBS + Auto Scaling + NLB, which is overly complex and doesn’t leverage managed services effectively.</p><p> Key Points:</p><p>- AWS Transfer Family is the best-managed solution for SFTP with built-in HA.</p><p>- Reusing the Elastic IP and security group ensures no customer disruption.</p><p>- S3 is the most scalable and durable storage option.</p><p>Thus, B is the correct answer. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3992c45fb4d44dee98199288e5a1d80d",
            "questionNumber": 291,
            "type": "single",
            "content": "<p>Question #291</p><p>A company ingests and processes streaming market data. The data rate is constant. A nightly process that calculates aggregate statistics takes 4 hours to complete. The statistical analysis is not critical to the business, and data points are processed during the next iteration if a particular run fails.</p><p><br></p><p>The current architecture uses a pool of Amazon EC2 Reserved Instances with 1-year reservations. These EC2 instances run full time to ingest and store the streaming data in attached Amazon Elastic Block Store (Amazon EBS) volumes. A scheduled script launches EC2 On-Demand Instances each night to perform the nightly processing. The instances access the stored data from NFS shares on the ingestion servers. The script terminates the instances when the processing is complete.</p><p><br></p><p>The Reserved Instance reservations are expiring. The company needs to determine whether to purchase new reservations or implement a new design.</p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use a scheduled script to launch a fleet of EC2 On-Demand Instances each night to perform the batch processing of the S3 data. Configure the script to terminate the instances when the processing is complete."
                },
                {
                    "label": "B",
                    "content": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price."
                },
                {
                    "label": "C",
                    "content": "Update the ingestion process to use a fleet of EC2 Reserved Instances with 3-year reservations behind a Network Load Balancer. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price."
                },
                {
                    "label": "D",
                    "content": "Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon Redshift. Use Amazon EventBridge to schedule an AWS Lambda function to run nightly to query Amazon Redshift to generate the daily statistics."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. Using Amazon Kinesis Data Firehose to save data to Amazon S3 is a cost-effective way to handle streaming data. By using AWS Batch with Spot Instances for the nightly processing, the company can take advantage of lower Spot Instance prices while maintaining the flexibility to handle the processing if it fails. Setting a maximum Spot price that is 50% of the On-Demand price provides a buffer against market price fluctuations. This approach is more cost-effective than maintaining a fleet of Reserved Instances or using only On-Demand Instances, as it optimizes for both storage and compute costs.</p><p>Update the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3. Use AWS Batch with Spot Instances to perform nightly processing with a maximum Spot price that is 50% of the On-Demand price. &nbsp;</p><p> Why? &nbsp;</p><p>1. Kinesis Data Firehose + S3: &nbsp;</p><p> &nbsp; - Replacing the EC2 ingestion servers with Kinesis Data Firehose eliminates the need for Reserved Instances, reducing costs significantly. &nbsp;</p><p> &nbsp; - Firehose automatically streams data to Amazon S3, which is highly durable, scalable, and cost-efficient for storage. &nbsp;</p><p>2. AWS Batch with Spot Instances: &nbsp;</p><p> &nbsp; - Since the nightly processing is non-critical and can be retried, Spot Instances (at 50% of On-Demand price) are ideal for cost savings. &nbsp;</p><p> &nbsp; - AWS Batch simplifies job scheduling and automatically manages Spot Instance fleets. &nbsp;</p><p>3. Cost Optimization: &nbsp;</p><p> &nbsp; - No upfront Reserved Instance commitments. &nbsp;</p><p> &nbsp; - Spot Instances provide the lowest compute cost for batch workloads. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option A: Uses On-Demand Instances (more expensive than Spot). &nbsp;</p><p>- Option C: Still relies on Reserved Instances (unnecessary cost since Firehose is serverless). &nbsp;</p><p>- Option D: Amazon Redshift is overkill for this use case (expensive for just nightly batch processing). &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the most cost-effective, leveraging serverless ingestion (Firehose + S3) and cheap compute (Spot Instances via AWS Batch). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "386885b95ddf4b8daecacebbb14f765b",
            "questionNumber": 292,
            "type": "single",
            "content": "<p>Question #292</p><p>A company needs to migrate an on-premises SFTP site to AWS. The SFTP site currently runs on a Linux VM. Uploaded files are made available to downstream applications through an NFS share. </p><p><br></p><p>As part of the migration to AWS, a solutions architect must implement high availability. The solution must provide external vendors with a set of static public IP addresses that the vendors can allow. The company has set up an AWS Direct Connect connection between its on-premises data center and its VPC.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Transfer Family server. Configure an internet-facing VPC endpoint for the Transfer Family server. Specify an Elastic IP address for each subnet. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon Elastic File System (Amazon EFS) file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead."
                },
                {
                    "label": "C",
                    "content": "Use AWS Application Migration Service to migrate the existing Linux VM to an Amazon EC2 instance. Assign an Elastic IP address to the EC2 instance. Mount an Amazon Elastic File System (Amazon EFS) file system to the EC2 instance. Configure the SFTP server to place files in the EFS file system. Modify the configuration on the downstream applications that access the existing NFS share to mount the EFS endpoint instead."
                },
                {
                    "label": "D",
                    "content": "Use AWS Application Migration Service to migrate the existing Linux VM to an AWS Transfer Family server. Configure a publicly accessible endpoint for the Transfer Family server. Configure the Transfer Family server to place files into an Amazon FSx for Lustre file system that is deployed across multiple Availability Zones. Modify the configuration on the downstream applications that access the existing NFS share to mount the FSx for Lustre endpoint instead."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The correct answer is A. By creating an AWS Transfer Family server and configuring it with an internet-facing VPC endpoint, the company can provide a secure and scalable SFTP solution. Assigning an Elastic IP address to each subnet ensures that external vendors have a set of static public IP addresses to allow. Using Amazon EFS for file storage provides high availability and is accessible across multiple Availability Zones. This approach requires minimal operational overhead as it leverages managed services and does not require managing the underlying EC2 instances or network infrastructure.</p><p>The question outlines the following requirements: &nbsp;</p><p>1. Migrate an on-premises SFTP site to AWS with high availability. &nbsp;</p><p>2. Provide external vendors with static public IP addresses (Elastic IPs) for allowlisting. &nbsp;</p><p>3. Use AWS Direct Connect for connectivity between on-premises and AWS. &nbsp;</p><p>4. Replace the NFS share with a highly available AWS storage solution accessible by downstream applications. &nbsp;</p><p>5. Minimize operational overhead. &nbsp;</p><p> Why Option A is Correct: &nbsp;</p><p>- AWS Transfer Family is a managed SFTP service that eliminates the need to manage EC2 instances. &nbsp;</p><p>- Internet-facing VPC endpoint allows external vendors to connect via static Elastic IPs (required for allowlisting). &nbsp;</p><p>- Amazon EFS is a fully managed, multi-AZ file system that replaces the NFS share and provides high availability. &nbsp;</p><p>- Least operational overhead because AWS manages the SFTP server (Transfer Family) and EFS, reducing maintenance. &nbsp;</p><p> Why Other Options Are Incorrect: &nbsp;</p><p>- Option B: Lacks Elastic IPs, which are required for vendors to allowlist static IPs. &nbsp;</p><p>- Option C: Uses EC2 instead of AWS Transfer Family, increasing operational overhead (manual patching, scaling, HA setup). &nbsp;</p><p>- Option D: Uses FSx for Lustre, which is not a direct replacement for NFS (EFS is better suited). Also, migrating a VM to Transfer Family is not a standard approach. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option A is the best solution because it uses AWS Transfer Family (managed SFTP) with Elastic IPs and EFS (highly available NFS replacement), meeting all requirements with minimal operational effort. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "74ebc85d78c54c1a9736d569adc248ec",
            "questionNumber": 293,
            "type": "single",
            "content": "<p>Question #293</p><p>A solutions architect has an operational workload deployed on Amazon EC2 instances in an Auto Scaling group. The VPC architecture spans two Availability Zones (AZ) with a subnet in each that the Auto Scaling group is targeting. The VPC is connected to an on-premises environment and connectivity cannot be interrupted. The maximum size of the Auto Scaling group is 20 instances in service. The VPC IPv4 addressing is as follows:</p><p><br></p><p>VPC CIDR: 10.0.0.0/23</p><p><br></p><p>AZ1 subnet CIDR: 10.0.0.0/24</p><p><br></p><p>AZ2 subnet CIDR: 10.0.1.0/24</p><p><br></p><p>Since deployment, a third AZ has become available in the Region. The solutions architect wants to adopt the new AZ without adding additional IPv4 address space and without service downtime.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Update the Auto Scaling group to use the AZ2 subnet only. Delete and re-create the AZ1 subnet using half the previous address space. Adjust the Auto Scaling group to also use the new AZ1 subnet. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Remove the current AZ2 subnet. Create a new AZ2 subnet using the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets."
                },
                {
                    "label": "B",
                    "content": "Terminate the EC2 instances in the AZ1 subnet. Delete and re-create the AZ1 subnet using half the address space. Update the Auto Scaling group to use this new subnet. Repeat this for the second AZ. Define a new subnet in AZ3, then update the Auto Scaling group to target all three new subnets."
                },
                {
                    "label": "C",
                    "content": "Create a new VPC with the same IPv4 address space and define three subnets, with one for each AZ. Update the existing Auto Scaling group to target the new subnets in the new VPC."
                },
                {
                    "label": "D",
                    "content": "Update the Auto Scaling group to use the AZ2 subnet only. Update the AZ1 subnet to have half the previous address space. Adjust the Auto Scaling group to also use the AZ1 subnet again. When the instances are healthy, adjust the Auto Scaling group to use the AZ1 subnet only. Update the current AZ2 subnet and assign the second half of the address space from the original AZ1 subnet. Create a new AZ3 subnet using half the original AZ2 subnet address space, then update the Auto Scaling group to target all three new subnets."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The correct answer is A. This solution allows the architect to adopt the new AZ without needing additional IPv4 address space or causing downtime. By first using the AZ2 subnet, modifying the AZ1 subnet to half its size, and then reintroducing it, the architect can create space for a new subnet in the AZ3. This approach maintains connectivity and service availability while efficiently utilizing the existing address space to accommodate the new AZ.</p><p>The requirements are:</p><p>1. Adopt a third AZ without adding additional IPv4 address space.</p><p>2. No service downtime (connectivity cannot be interrupted).</p><p>3. Maximize address space usage within the existing VPC CIDR (`10.0.0.0/23`).</p><p> Why A is correct:</p><p>- Step 1: Shift all instances to AZ2 (using `10.0.1.0/24`) to free up AZ1 (`10.0.0.0/24`).</p><p>- Step 2: Delete and re-create AZ1 subnet with half the original space (`10.0.0.0/25`), then adjust the Auto Scaling group to include it again.</p><p>- Step 3: Once instances are healthy, shift all traffic to AZ1 (`10.0.0.0/25`) and free up AZ2 (`10.0.1.0/24`).</p><p>- Step 4: Reconfigure AZ2 subnet to use the second half of the original AZ1 space (`10.0.0.128/25`).</p><p>- Step 5: Create a new AZ3 subnet using half of the original AZ2 space (`10.0.1.0/25`).</p><p>- Step 6: Update the Auto Scaling group to use all three subnets (`10.0.0.0/25`, `10.0.0.128/25`, and `10.0.1.0/25`).</p><p>This approach:</p><p>- Avoids downtime by gradually migrating instances.</p><p>- Uses the same VPC CIDR without expansion.</p><p>- Evenly distributes addresses across three AZs.</p><p> Why D is incorrect:</p><p>- It suggests updating the AZ1 subnet to half its size without deleting it first, which is not possible in AWS. Subnet CIDRs cannot be modified after creation; they must be deleted and recreated.</p><p> Why B and C are incorrect:</p><p>- B: Terminating instances causes downtime.</p><p>- C: Creating a new VPC would require reconfiguring connectivity (VPN/Direct Connect), causing interruption.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a53ca2ad295349e880198e68d46a7721",
            "questionNumber": 294,
            "type": "single",
            "content": "<p>Question #294</p><p>A company uses an organization in AWS Organizations to manage the company's AWS accounts. The company uses AWS CloudFormation to deploy all infrastructure. A finance team wants to build a chargeback model. The finance team asked each business unit to tag resources by using a predefined list of project values.</p><p><br></p><p>When the finance team used the AWS Cost and Usage Report in AWS Cost Explorer and filtered based on project, the team noticed noncompliant project values. The company wants to enforce the use of project tags for new resources.</p><p><br></p><p>Which solution will meet these requirements with the LEAST effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a tag policy that contains the allowed project tag values in the organization&#39;s management account. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU."
                },
                {
                    "label": "B",
                    "content": "Create a tag policy that contains the allowed project tag values in each OU. Create an SCP that denies the cloudformation:CreateStack API operation unless a project tag is added. Attach the SCP to each OU."
                },
                {
                    "label": "C",
                    "content": "Create a tag policy that contains the allowed project tag values in the AWS management account. Create an IAM policy that denies the cloudformation:CreateStack API operation unless a project tag is added. Assign the policy to each user."
                },
                {
                    "label": "D",
                    "content": "Use AWS Service Catalog to manage the CloudFormation stacks as products. Use a TagOptions library to control project tag values. Share the portfolio with all OUs that are in the organization."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The correct answer is A. By creating a tag policy in the organization's management account, the company can define the allowed project tag values at the organizational level. Then, creating an SCP (Service Control Policy) that denies the creation of CloudFormation stacks without a project tag ensures compliance across all accounts within the organization. Attaching the SCP to each OU (Organizational Unit) simplifies the enforcement process and requires minimal effort compared to creating policies in each individual account or OU.</p><p>The requirements are: &nbsp;</p><p>1. Enforce the use of project tags for new resources. &nbsp;</p><p>2. Ensure only predefined project tag values are used. &nbsp;</p><p>3. Minimize effort in implementation. &nbsp;</p><p>Option A is the best solution because: &nbsp;</p><p>- Tag Policy (AWS Organizations feature) enforces allowed tag values across all accounts in the organization. &nbsp;</p><p>- Service Control Policy (SCP) prevents CloudFormation stack creation unless the required `project` tag is included. &nbsp;</p><p>- SCP applies to all OUs, ensuring consistent enforcement without per-account or per-user configuration. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- B: Tag policies are set at the organization level, not per OU. This approach is redundant and less efficient. &nbsp;</p><p>- C: IAM policies require manual assignment to each user, which is more effort than using SCPs. &nbsp;</p><p>- D: AWS Service Catalog is a valid approach but requires more setup effort (creating portfolios, products, and sharing them) compared to tag policies + SCPs. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the most efficient solution because it leverages AWS Organizations features (Tag Policies + SCPs) to enforce tagging compliance at scale. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "44f010e322b14b63a96637ad1248f908",
            "questionNumber": 295,
            "type": "single",
            "content": "<p>Question #295</p><p>An application is deployed on Amazon EC2 instances that run in an Auto Scaling group. The Auto Scaling group configuration uses only one type of instance. <br><br>CPU and memory utilization metrics show that the instances are underutilized. A solutions architect needs to implement a solution to permanently reduce the EC2 cost and increase the utilization.<br><br></p><p>Which solution will meet these requirements with the LEAST number of configuration changes in the future?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "List instance types that have properties that are similar to the properties that the current instances have. Modify the Auto Scaling group&#39;s launch template configuration to use multiple instance types from the list."
                },
                {
                    "label": "B",
                    "content": "Use the information about the application&#39;s CPU and memory utilization to select an instance type that matches the requirements. Modify the Auto Scaling group&#39;s configuration by adding the new instance type. Remove the current instance type from the configuration."
                },
                {
                    "label": "C",
                    "content": "Use the information about the application&#39;s CPU and memory utilization to specify CPU and memory requirements in a new revision of the Auto Scaling group&#39;s launch template. Remove the current instance type from the configuration."
                },
                {
                    "label": "D",
                    "content": "Create a script that selects the appropriate instance types from the AWS Price List Bulk API. Use the selected instance types to create a new revision of the Auto Scaling group&#39;s launch template."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use the information about the application's CPU and memory utilization to specify CPU and memory requirements in a new revision of the Auto Scaling group's launch template. Remove the current instance type from the configuration.</p><p>This solution will meet the requirements with the LEAST number of configuration changes in the future for the following reasons:</p><p>Automatic Instance Selection: By specifying CPU and memory requirements rather than specific instance types, you allow AWS to automatically select the most cost-effective instances that meet your application's needs. This approach leverages AWS's built-in instance selection capabilities.</p><p>Future-Proofing: As AWS introduces new instance types with better price-performance ratios, your Auto Scaling group will automatically be able to use these new instances without requiring any configuration changes. The system will continuously select the most cost-effective option that meets your specified requirements.</p><p>Adaptability to Price Changes: When instance pricing changes over time, the Auto Scaling group will automatically adjust to use the most cost-effective instances that meet your CPU and memory requirements without requiring manual updates.</p><p>Right-Sizing Based on Actual Usage: Using the actual CPU and memory utilization data ensures that you're selecting instance sizes that match your application's real needs, addressing the underutilization issue directly.</p><p>Minimal Maintenance: Once configured, this solution requires virtually no ongoing maintenance or updates to continue optimizing costs as AWS's instance offerings evolve.</p><p>Option D (creating a script that selects instance types from the AWS Price List Bulk API) would require significantly more configuration changes in the future because:</p><p>The script would need to be regularly updated and maintained as AWS introduces new instance types or changes pricing.</p><p>Each time the script identifies better instance options, you would need to create a new revision of the launch template.</p><p>This approach introduces additional complexity and potential points of failure with the custom script.</p><p>It requires ongoing monitoring and manual intervention to ensure the script continues to function correctly.</p><p>By specifying CPU and memory requirements directly in the launch template, you create a more automated, self-maintaining solution that will continue to optimize costs with minimal future configuration changes, making Option C the best answer.</p><p>Sources</p><p>EC2 Instance upgrade or auto scale? | AWS re:Post （https://repost.aws/questions/QUKa8gFPuzTs2kC31vM5OiKA/ec2-instance-upgrade-or-auto-scale）</p><p>Migrate your Auto Scaling groups to launch templates - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/migrate-to-launch-templates.html）</p><p>Autoscaling group cannot update capacity despite sufficient service quota being available. | AWS re:Post （https://repost.aws/questions/QUUx0Nl_XBTdiHpXthkeJvgA/autoscaling-group-cannot-update-capacity-despite-sufficient-service-quota-being-available）</p><p>COST09-BP03 Supply resources dynamically - Cost Optimization Pillar （https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/cost_manage_demand_resources_dynamic.html）</p><p>Use multiple launch templates - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-mixed-instances-groups-launch-template-overrides.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ca285e31483a453cb4271219f6020c3f",
            "questionNumber": 296,
            "type": "single",
            "content": "Question #296<p>A company implements a containerized application by using Amazon Elastic Container Service (Amazon ECS) and Amazon API Gateway. The application data is stored in Amazon Aurora databases and Amazon DynamoDB databases. The company automates infrastructure provisioning by using AWS CloudFormation. The company automates application deployment by using AWS CodePipeline.<br><br></p><p>A solutions architect needs to implement a disaster recovery (DR) strategy that meets an RPO of 2 hours and an RTO of 4 hours.<br><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon CloudFront with origin failover to route traffic to the secondary Region during a DR scenario."
                },
                {
                    "label": "B",
                    "content": "Use AWS Database Migration Service (AWS DMS), Amazon EventBridge, and AWS Lambda to replicate the Aurora databases to a secondary AWS Region. Use DynamoDB Streams, EventBridge, and Lambda to replicate the DynamoDB databases to the secondary Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."
                },
                {
                    "label": "C",
                    "content": "Use AWS Backup to create backups of the Aurora databases and the DynamoDB databases in a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."
                },
                {
                    "label": "D",
                    "content": "Set up an Aurora global database and DynamoDB global tables to replicate the databases to a secondary AWS Region. In the primary Region and in the secondary Region, configure an API Gateway API with a Regional endpoint. Implement Amazon Route 53 failover routing to switch traffic from the primary Region to the secondary Region."
                }
            ],
            "correctAnswer": "C",
            "explanation": "Correct C. https://docs.aws.amazon.com/aws-backup/latest/devguide/cross-region-backup.html",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "9c403eae24844e7e9dfdd96f25a2bf15",
            "questionNumber": 297,
            "type": "single",
            "content": "<p>Question #297</p><p>A company has a complex web application that leverages Amazon CloudFront for global scalability and performance. Over time, users report that the web application is slowing down. <br><br>The company's operations team reports that the CloudFront cache hit ratio has been dropping steadily. The cache metrics report indicates that query strings on some URLs are inconsistently ordered and are specified sometimes in mixed-case letters and sometimes in lowercase letters.<br><br></p><p>Which set of actions should the solutions architect take to increase the cache hit ratio as quickly as possible?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger to invoke the function."
                },
                {
                    "label": "B",
                    "content": "Update the CloudFront distribution to disable caching based on query string parameters."
                },
                {
                    "label": "C",
                    "content": "Deploy a reverse proxy after the load balancer to post-process the emitted URLs in the application to force the URL strings to be lowercase."
                },
                {
                    "label": "D",
                    "content": "Update the CloudFront distribution to specify casing-insensitive query string processing."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Deploy a Lambda@Edge function to sort parameters by name and force them to be lowercase. Select the CloudFront viewer request trigger to invoke the function.</p><p>This solution will increase the cache hit ratio as quickly as possible for the following reasons:</p><p>Root Cause Targeting: The problem is specifically identified as inconsistently ordered query strings and mixed-case letters in the URLs. Lambda@Edge with a viewer request trigger directly addresses both of these issues by normalizing the query parameters before CloudFront processes the request.</p><p>Complete Solution: This approach solves both problems simultaneously:</p><p>It sorts the parameters by name, ensuring consistent ordering regardless of how they're submitted</p><p>It forces all parameters to lowercase, eliminating case sensitivity issues</p><p>Preserves Functionality: Unlike disabling query string caching altogether (option B), this solution maintains the ability to cache based on query parameters, which is likely important for the application's functionality.</p><p>Immediate Effect: Once deployed, the Lambda@Edge function will immediately begin normalizing all incoming requests, quickly improving the cache hit ratio without requiring changes to the application code.</p><p>Flexibility: If additional normalization is needed in the future, the Lambda function can be updated to handle new requirements.</p><p>The other options have significant limitations:</p><p>B. Updating the CloudFront distribution to disable caching based on query string parameters would likely break application functionality if those parameters are meaningful for content differentiation. This is a blunt approach that doesn't address the actual problem and could negatively impact the application.</p><p>D. There is no built-in CloudFront setting for \"casing-insensitive query string processing.\" CloudFront treats query strings as case-sensitive by default, and there isn't a simple configuration option to change this behavior. This option doesn't exist as described and wouldn't address the ordering issue even if it did.</p><p>Lambda@Edge is specifically designed for this type of request manipulation and is the most appropriate solution for normalizing query strings before they reach the CloudFront cache. By implementing option A, the company can quickly improve the cache hit ratio while maintaining all the functionality of their web application.</p><p>Sources</p><p>Increase the proportion of requests that are served directly from the CloudFront caches (cache hit ratio) - Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/cache-hit-ratio.html）</p><p>Lambda@Edge example functions - Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-examples.html）</p><p>Cloudfront CDN is not working with Videos | AWS re:Post （https://repost.aws/questions/QUZrZsoVf9SwuBr3loEgCk-Q/cloudfront-cdn-is-not-working-with-videos）</p><p>CloudFront will miss cache if request is from CURL | AWS re:Post （https://repost.aws/questions/QUqHQyupGdTgqf6vAqGle1Lg/cloudfront-will-miss-cache-if-request-is-from-curl）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1293fbac672f4515841d9ee48f3a6bae",
            "questionNumber": 298,
            "type": "single",
            "content": "<p>Question #298</p><p>A company runs an ecommerce application in a single AWS Region. The application uses a five-node Amazon Aurora MySQL DB cluster to store information about customers and their recent orders. The DB cluster experiences a large number of write transactions throughout the day. <br><br>The company needs to replicate the data in the Aurora database to another Region to meet disaster recovery requirements. The company has an RPO of 1 hour.</p><p><br>Which solution will meet these requirements with the LOWEST cost?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Modify the Aurora database to be an Aurora global database. Create a second Aurora database in another Region."
                },
                {
                    "label": "B",
                    "content": "Enable the Backtrack feature for the Aurora database. Create an AWS Lambda function that runs daily to copy the snapshots of the database to a backup Region."
                },
                {
                    "label": "C",
                    "content": "Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region."
                },
                {
                    "label": "D",
                    "content": "Turn off automated Aurora backups. Configure Aurora backups with a backup frequency of 1 hour. Specify another Region as the destination Region. Select the Aurora database as the resource assignment."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use AWS Database Migration Service (AWS DMS). Create a DMS change data capture (CDC) task that replicates the ongoing changes from the Aurora database to an Amazon S3 bucket in another Region.</p><p>This solution will meet the RPO requirement of 1 hour with the LOWEST cost for the following reasons:</p><p>Cost-Effectiveness: Among the options presented, AWS DMS with CDC to S3 is the most cost-effective solution for cross-region replication. S3 storage is significantly less expensive than maintaining a full Aurora database replica in another region. You only pay for:</p><p>The DMS replication instance (which can be sized appropriately)</p><p>S3 storage costs (which are much lower than Aurora storage)</p><p>Data transfer costs (which would be incurred with any solution)</p><p>RPO Compliance: AWS DMS with CDC can easily meet the 1-hour RPO requirement. CDC captures changes continuously and can be configured to replicate these changes to the target S3 bucket at frequencies well under 1 hour.</p><p>Appropriate for Write-Heavy Workloads: The scenario mentions \"a large number of write transactions throughout the day.\" DMS with CDC is designed specifically to efficiently capture and replicate ongoing changes, making it suitable for write-intensive workloads.</p><p>The other options have significant cost disadvantages:</p><p>A. Aurora Global Database would provide the lowest RPO (typically less than 1 second), but at a much higher cost. It requires maintaining a full secondary Aurora cluster in the destination region, including compute instances and storage. This is overkill for an RPO requirement of 1 hour and would be significantly more expensive than the DMS to S3 approach.</p><p>B. Using Aurora Backtrack with Lambda to copy snapshots daily would not meet the 1-hour RPO requirement. Daily snapshots would result in an RPO of up to 24 hours, which exceeds the specified requirement. Even if modified to run hourly, this approach would be less efficient and potentially more costly than DMS with CDC for a database with frequent write transactions.</p><p>While the DMS to S3 solution may require additional steps during recovery (restoring from S3 to a new Aurora cluster), this trade-off is acceptable given the significant cost savings and the ability to meet the 1-hour RPO requirement. The company can restore from S3 to a new Aurora cluster in the DR region only when needed, avoiding the ongoing costs of maintaining a full secondary database.</p><p>Sources</p><p>Comparing Amazon Aurora Replicas, Aurora cross-Region Replicas, and Aurora global databases - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/aurora-replication-options/introduction.html）</p><p>Disaster recovery options in the cloud - Disaster Recovery of Workloads on AWS: Recovery in the Cloud （https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html）</p><p>Automated Backup and Restore Solution for Cross-Region Aurora MySQL Databases | AWS re:Post （https://repost.aws/questions/QUhC-a3j4USkKFB7-_MCi--A/automated-backup-and-restore-solution-for-cross-region-aurora-mysql-databases）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "aaf8e7aad148408bae683bc0e6bb0330",
            "questionNumber": 299,
            "type": "single",
            "content": "<p>Question #299</p><p>A company's solutions architect is evaluating an AWS workload that was deployed several years ago. The application tier is stateless and runs on a single large Amazon EC2 instance that was launched from an AMI. The application stores data in a MySQL database that runs on a single EC2 instance. <br><br>The CPU utilization on the application server EC2 instance often reaches 100% and causes the application to stop responding. The company manually installs patches on the instances. Patching has caused downtime in the past. The company needs to make the application highly available. <br><br>Which solution will meet these requirements with the LEAST development effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Move the application tier to AWS Lambda functions in the existing VPC. Create an Application Load Balancer to distribute traffic across the Lambda functions. Use Amazon GuardDuty to scan the Lambda functions. Migrate the database to Amazon DocumentDB (with MongoDB compatibility)."
                },
                {
                    "label": "B",
                    "content": "Change the EC2 instance type to a smaller Graviton powered instance type. Use the existing AMI to create a launch template for an Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon DynamoDB."
                },
                {
                    "label": "C",
                    "content": "Move the application tier to containers by using Docker. Run the containers on Amazon Elastic Container Service (Amazon ECS) with EC2 instances. Create an Application Load Balancer to distribute traffic across the ECS cluster. Configure the ECS cluster to scale based on CPU utilization. Migrate the database to Amazon Neptune."
                },
                {
                    "label": "D",
                    "content": "Create a new AMI that is configured with AWS Systems Manager Agent (SSM Agent). Use the new AMI to create a launch template for an Auto Scaling group. Use smaller instances in the Auto Scaling group. Create an Application Load Balancer to distribute traffic across the instances in the Auto Scaling group. Set the Auto Scaling group to scale based on CPU utilization. Migrate the database to Amazon Aurora MySQL."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer because it involves the least development effort by creating a new AMI with the SSM Agent for automated patching and setting up an Auto Scaling group with smaller instances for high availability. Additionally, migrating the database to Amazon Aurora MySQL is less effort than the other database migration options since it's a compatible choice with the existing MySQL database.</p><p> Key Requirements:</p><p>1. High Availability: The current setup has a single EC2 instance for both the application and database, which is a single point of failure.</p><p>2. Performance Issues: The application server CPU often reaches 100%, causing downtime.</p><p>3. Patching Issues: Manual patching causes downtime.</p><p>4. Least Development Effort: The solution should minimize code changes or rearchitecting.</p><p> Why Option D is Best:</p><p>- Auto Scaling Group (ASG): Replaces the single large EC2 instance with multiple smaller instances behind a load balancer, improving scalability and availability.</p><p>- AWS Systems Manager (SSM): Automates patching without downtime (replacing manual patching).</p><p>- Amazon Aurora MySQL: Provides a highly available, scalable, and managed database solution (replacing the single MySQL instance). Aurora is MySQL-compatible, minimizing migration effort.</p><p>- Least Development Effort: Uses AMI-based deployment (no need to refactor the app for containers or serverless).</p><p> Why Other Options Are Less Ideal:</p><p>- A (Lambda + DocumentDB): Requires rewriting the app for serverless and migrating to a non-relational database (high development effort).</p><p>- B (Graviton + DynamoDB): Switching to Graviton may require AMI/application changes, and DynamoDB is non-relational (may not fit the data model).</p><p>- C (ECS + Neptune): Containerization requires significant rework, and Neptune is a graph database (not a drop-in replacement for MySQL).</p><p> Summary:</p><p>Option D meets all requirements with minimal changes by leveraging Auto Scaling, SSM, and Aurora MySQL. </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "aaef0bba648a447f91c8e42b1a026908",
            "questionNumber": 300,
            "type": "single",
            "content": "<p>Question #300</p><p>A company is planning to migrate several applications to AWS. The company does not have a good understanding of its entire application estate. The estate consists of a mixture of physical machines and VMs. <br><br>One application that the company will migrate has many dependencies that are sensitive to latency. The company is unsure what all the dependencies are. However, the company knows that the low-latency communications use a custom IP-based protocol that runs on port 1000. The company wants to migrate the application and these dependencies together to move all the low-latency interfaces to AWS at the same time. <br><br>The company has installed the AWS Application Discovery Agent and has been collecting data for several months. </p><p><br></p><p>What should the company do to identify the dependencies that need to be migrated in the same phase as the application?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Migration Hub and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Turn on data exploration in Amazon Athena. Query the data that is transferred between the servers to identify the servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries."
                },
                {
                    "label": "B",
                    "content": "Use AWS Application Migration Service and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Configure Application Migration Service to launch test instances for all the servers that interact with the application. Perform acceptance tests on the test instances. If no issues are identified, create a move group that is based on the tested servers."
                },
                {
                    "label": "C",
                    "content": "Use AWS Migration Hub and select the servers that host the application. Turn on data exploration in Network Access Analyzer. Use the Network Access Analyzer console to select the servers that host the application. Select a Network Access Scope of port 1000 and note the matching servers. Return to Migration Hub. Create a move group that is based on the findings from Network Access Analyzer."
                },
                {
                    "label": "D",
                    "content": "Use AWS Migration Hub and select the servers that host the application. Push the Amazon CloudWatch agent to the identified servers by using the AWS Application Discovery Agent. Export the CloudWatch logs that the agents collect to Amazon S3. Use Amazon Athena to query the logs to find servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The correct answer is: &nbsp;</p><p>A. Use AWS Migration Hub and select the servers that host the application. Visualize the network graph to find servers that interact with the application. Turn on data exploration in Amazon Athena. Query the data that is transferred between the servers to identify the servers that communicate on port 1000. Return to Migration Hub. Create a move group that is based on the findings from the Athena queries. &nbsp;</p><p> Explanation: &nbsp;</p><p>1. AWS Migration Hub is the central tool for tracking migrations, and it integrates with AWS Application Discovery Service (which has been collecting data for months). &nbsp;</p><p>2. The network graph visualization in Migration Hub helps identify dependencies by showing server interactions. &nbsp;</p><p>3. Amazon Athena can query the collected discovery data to filter servers communicating on port 1000 (the custom protocol). &nbsp;</p><p>4. Finally, a move group can be created in Migration Hub to migrate the application and its identified dependencies together. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: AWS Application Migration Service (MGN) is for rehosting (lift-and-shift), not dependency discovery. &nbsp;</p><p>- C: Network Access Analyzer checks security policies, not dependency mapping. &nbsp;</p><p>- D: CloudWatch logs are not the right tool for discovering network dependencies (Athena queries on Discovery data are more efficient). &nbsp;</p><p>Thus, Option A is the best approach.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        }
    ],
    "totalQuestions": 529,
    "hasNextPage": true,
    "page": 3,
    "pageSize": 100
}

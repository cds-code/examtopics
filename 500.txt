{
    "questions": [
        {
            "id": "e8ab5d3dc9f24f3cb06f45d9facff681",
            "questionNumber": 401,
            "type": "single",
            "content": "Question #401<p>A company wants to design a disaster recovery (DR) solution for an application that runs in the company’s data center. The application writes to an SMB file share and creates a copy on a second file share. Both file shares are in the data center. The application uses two types of files: metadata files and image files.</p><p><br></p><p>The company wants to store the copy on AWS. The company needs the ability to use SMB to access the data from either the data center or AWS if a disaster occurs. The copy of the data is rarely accessed but must be available within 5 minutes.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy AWS Outposts with Amazon S3 storage. Configure a Windows Amazon EC2 instance on Outposts as a file server."
                },
                {
                    "label": "B",
                    "content": "Deploy an Amazon FSx File Gateway. Configure an Amazon FSx for Windows File Server Multi-AZ file system that uses SSD storage."
                },
                {
                    "label": "C",
                    "content": "Deploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and to use S3 Glacier Deep Archive for the image files."
                },
                {
                    "label": "D",
                    "content": "Deploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and image files."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Deploy an Amazon S3 File Gateway. Configure the S3 File Gateway to use Amazon S3 Standard-Infrequent Access (S3 Standard-IA) for the metadata files and image files.</p><p>This solution meets all the requirements for the following reasons:</p><p>SMB Protocol Access: S3 File Gateway supports the SMB protocol, allowing the company to access the data using the same protocol they currently use in their data center. This ensures compatibility with their existing application.</p><p>Availability within 5 minutes: S3 Standard-IA provides millisecond retrieval times, easily meeting the requirement that data must be available within 5 minutes. This storage class is designed for data that is accessed infrequently but requires rapid access when needed.</p><p>Cost Optimization: Since the copy of the data is \"rarely accessed,\" using S3 Standard-IA is cost-effective as it offers a lower storage cost compared to S3 Standard while still maintaining rapid retrieval capabilities.</p><p>Disaster Recovery: This solution enables the company to access their data from either the data center or AWS in the event of a disaster, providing the flexibility needed for their DR strategy.</p><p>Option C (using S3 Glacier Deep Archive for image files) would not meet the requirements because:</p><p>S3 Glacier Deep Archive has a retrieval time of 12+ hours, which far exceeds the 5-minute availability requirement specified by the company.</p><p>S3 Glacier Deep Archive is designed for long-term archival of data that may be accessed once or twice a year, not for disaster recovery scenarios where data needs to be quickly accessible.</p><p>S3 File Gateway does not support direct access to S3 Glacier Deep Archive storage class. Files stored in Glacier Deep Archive would need to be restored to S3 before they could be accessed through the gateway, adding complexity and delays to the recovery process.</p><p>By using S3 Standard-IA for both metadata files and image files, the company ensures that all their data is readily available within the required timeframe while still optimizing costs for infrequently accessed data. This approach provides a simple, consistent solution that meets all the specified requirements for their disaster recovery strategy.</p><p>Sources</p><p>Secure archive storage – Amazon S3 Glacier storage classes – AWS （https://aws.amazon.com/s3/storage-classes/glacier/?ams%23interactive-card-vertical%23pattern-data.filter=%257B%2522filters%2522%253A%255B%255D%257D）</p><p>AWS Storage Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/optimize-costs-microsoft-workloads/storage-main-storage-gateway.html）</p><p>Backup and recovery from on-premises infrastructure to AWS - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/on-premises-to-aws.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d20e89f71570432ab369524d1283b497",
            "questionNumber": 402,
            "type": "single",
            "content": "<p>Question #402</p><p>A company is creating a solution that can move 400 employees into a remote working environment in the event of an unexpected disaster. The user desktops have a mix of Windows and Linux operating systems. Multiple types of software, such as web browsers and mail clients, are installed on each desktop.</p><p><br></p><p>A solutions architect needs to implement a solution that can be integrated with the company’s on-premises Active Directory to allow employees to use their existing identity credentials. The solution must provide multifactor authentication (MFA) and must replicate the user experience from the existing desktops.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Activate MFA for Amazon WorkSpaces by using the AWS Management Console."
                },
                {
                    "label": "B",
                    "content": "Use Amazon AppStream 2.0 as an application streaming service. Configure Desktop View for the employees. Set up a VPN connection to the on-premises network. Set up Active Directory Federation Services (AD FS) on premises. Connect the VPC network to AD FS through the VPN connection."
                },
                {
                    "label": "C",
                    "content": "Use Amazon WorkSpaces for the cloud desktop service. Set up a VPN connection to the on-premises network. Create an AD Connector, and connect to the on-premises Active Directory. Configure a RADIUS server for MFA."
                },
                {
                    "label": "D",
                    "content": "Use Amazon AppStream 2.0 as an application streaming service. Set up Active Directory Federation Services on premises. Configure MFA to grant users access on AppStream 2.0."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct solution. Amazon WorkSpaces provides a cloud-based desktop service that can be integrated with an on-premises Active Directory through an AD Connector. This allows employees to use their existing identity credentials. By setting up a VPN connection, the company can ensure a secure connection to the on-premises network. Configuring a RADIUS server for MFA adds an additional layer of security and meets the requirement for multifactor authentication, thus replicating a secure user experience from the existing desktops.</p><p>The requirements are: &nbsp;</p><p>1. Support for both Windows and Linux desktops → Amazon WorkSpaces supports both, while AppStream 2.0 is primarily for application streaming (not full desktops). &nbsp;</p><p>2. Integration with on-premises Active Directory → AD Connector allows WorkSpaces to authenticate against on-premises AD. &nbsp;</p><p>3. Multifactor Authentication (MFA) → WorkSpaces can integrate with a RADIUS server (e.g., AWS MFA or third-party MFA solutions). &nbsp;</p><p>4. Replicate the existing user experience → WorkSpaces provides full virtual desktops, matching the employees' current setup. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: Incorrect because MFA for WorkSpaces via the AWS Management Console is not the best practice; a RADIUS server is preferred for on-prem AD integration. &nbsp;</p><p>- B: Incorrect because AppStream 2.0 does not provide full desktop replication (only application streaming). &nbsp;</p><p>- D: Incorrect because AppStream 2.0 does not fully replicate the desktop experience and is not ideal for mixed OS environments. &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the best solution as it meets all requirements by leveraging Amazon WorkSpaces with AD Connector and RADIUS-based MFA. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "06f941e8bad14565a5f8ef56ea5cbda9",
            "questionNumber": 403,
            "type": "single",
            "content": "Question #403<p>A company has deployed an Amazon Connect contact center. Contact center agents are reporting large numbers of computer-generated calls. The company is concerned about the cost and productivity effects of these calls. The company wants a solution that will allow agents to flag the call as spam and automatically block the numbers from going to an agent in the future.</p><p><br></p><p>What is the MOST operationally efficient solution to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Customize the Contact Control Panel (CCP) by adding a flag call button that will invoke an AWS Lambda function that calls the UpdateContactAttributes API. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the updated attribute and to use a Lambda function to read and write to the DynamoDB table."
                },
                {
                    "label": "B",
                    "content": "Use a Contact Lens for Amazon Connect rule that will look for spam calls. Use an Amazon DynamoDB table to store the spam numbers. Modify the contact flows to look for the rule and to invoke an AWS Lambda function to read and write to the DynamoDB table."
                },
                {
                    "label": "C",
                    "content": "Use an Amazon DynamoDB table to store the spam numbers. Create a quick connect that the agents can transfer the spam call to from the Contact Control Panel (CCP). Modify the quick connect contact flow to invoke an AWS Lambda function to write to the DynamoDB table."
                },
                {
                    "label": "D",
                    "content": "Modify the initial contact flow to ask for caller input. If the agent does not receive input, the agent should mark the caller as spam. Use an Amazon DynamoDB table to store the spam numbers. Use an AWS Lambda function to read and write to the DynamoDB table."
                }
            ],
            "correctAnswer": "A",
            "explanation": "To address the issue of computer-generated calls in the Amazon Connect contact center while allowing agents to flag spam calls and automatically block these numbers in the future, the most operationally efficient solution would be:<p>Use Amazon Connect Contact Lens:</p><p>Enable Contact Lens for Amazon Connect, which provides real-time analytics and post-call analytics.</p><p>Implement a custom Contact Flow:</p><p>Create a custom contact flow that includes a \"Flag as Spam\" option for agents.</p><p>Utilize AWS Lambda integration:</p><p>Integrate an AWS Lambda function within the contact flow.</p><p>When an agent flags a call as spam, the Lambda function will: a. Record the caller's number in an Amazon DynamoDB table. b. Add the number to an Amazon Connect block list.</p><p>Modify the initial Contact Flow:</p><p>At the beginning of each call, check the caller's number against the DynamoDB table of spam numbers.</p><p>If the number is found in the table, automatically end the call or route it to a pre-recorded message.</p><p>Use Amazon Connect Streams API:</p><p>Implement a custom agent interface using the Amazon Connect Streams API.</p><p>Add a \"Flag as Spam\" button in the agent's interface for easy reporting.</p><p>Set up Amazon CloudWatch Alarms:</p><p>Create alarms to monitor the rate of spam calls and alert administrators if it exceeds a certain threshold.</p><p>Implement Amazon QuickSight dashboard:</p><p><br></p><p>Create a dashboard to visualize spam call trends and the effectiveness of the blocking mechanism.</p><p>This solution is operationally efficient because:</p><p><br></p><p>It leverages existing Amazon Connect features and integrations.</p><p>It automates the process of identifying and blocking spam calls.</p><p>It requires minimal ongoing maintenance once set up.</p><p>It provides agents with an easy way to flag spam calls.</p><p>It allows for continuous monitoring and improvement of the spam detection process.</p><p>By implementing this solution, the company can effectively reduce the number of computer-generated calls reaching agents, thereby improving productivity and reducing costs associated with handling these calls. The solution also provides flexibility for future enhancements and analytics to further optimize the contact center operations.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8348e5f5c6bb444584ae482c936e2cbc",
            "questionNumber": 404,
            "type": "single",
            "content": "<p>Question #404</p><p>A company has mounted sensors to collect information about environmental parameters such as humidity and light throughout all the company's factories. The company needs to stream and analyze the data in the AWS Cloud in real time. If any of the parameters fall out of acceptable ranges, the factory operations team must receive a notification immediately.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Stream the data to an Amazon Kinesis Data Firehose delivery stream. Use AWS Step Functions to consume and analyze the data in the Kinesis Data Firehose delivery stream. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team."
                },
                {
                    "label": "B",
                    "content": "Stream the data to an Amazon Managed Streaming for Apache Kafka (Amazon MSK) cluster. Set up a trigger in Amazon MSK to invoke an AWS Fargate task to analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team."
                },
                {
                    "label": "C",
                    "content": "Stream the data to an Amazon Kinesis data stream. Create an AWS Lambda function to consume the Kinesis data stream and to analyze the data. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team."
                },
                {
                    "label": "D",
                    "content": "Stream the data to an Amazon Kinesis Data Analytics application. Use an automatically scaled and containerized service in Amazon Elastic Container Service (Amazon ECS) to consume and analyze the data. Use Amazon Simple Email Service (Amazon SES) to notify the operations team."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Stream the data to an Amazon Kinesis data stream. Create an AWS Lambda function to consume the Kinesis data stream and to analyze the data. Use Amazon Simple Notification Service (Amazon SNS) to notify the operations team.</p><p>This solution meets the requirements for the following reasons:</p><p>Real-time processing: The company needs to analyze the data in real time and send immediate notifications when parameters fall out of acceptable ranges. Kinesis Data Streams provides true real-time processing with sub-second latency, which is essential for monitoring critical factory environmental parameters.</p><p>Immediate analysis: Lambda functions can be triggered by new records in a Kinesis data stream, allowing for immediate analysis of the incoming sensor data. Lambda can execute custom code to check if humidity, light, or other parameters are outside acceptable ranges.</p><p>Immediate notifications: When Lambda detects out-of-range parameters, it can immediately trigger Amazon SNS to send notifications to the factory operations team. This ensures the team is alerted without delay, as required.</p><p>Scalability: This architecture can easily scale to handle data from multiple factories and thousands of sensors, automatically adjusting to varying data volumes.</p><p>Option A (using Kinesis Data Firehose with Step Functions) is less suitable because:</p><p>Kinesis Data Firehose is designed primarily for data delivery to destinations like S3, Redshift, or Elasticsearch, not for real-time analysis. It typically has a minimum delivery time of 60 seconds, which doesn't meet the requirement for immediate notifications.</p><p>AWS Step Functions is an orchestration service designed for complex workflows, not for continuous real-time data processing. It would introduce unnecessary complexity and potential delays in the notification process.</p><p>This architecture would likely result in higher latency between when a parameter falls out of range and when the operations team receives a notification.</p><p>The combination of Kinesis Data Streams, Lambda, and SNS creates a streamlined, event-driven architecture that can process sensor data in real time, analyze it immediately, and send notifications without delay when issues are detected. This makes it the optimal solution for the company's requirements of real-time analysis and immediate notifications.</p><p>Sources</p><p>Unlocking Scalable IoT Analytics on AWS | The Internet of Things on AWS – Official Blog （https://aws.amazon.com/cn/blogs/iot/unlocking-scalable-iot-analytics-on-aws/）</p><p>Data processing - Data Warehousing on AWS （https://docs.aws.amazon.com/whitepapers/latest/data-warehousing-on-aws/data-processing.html）</p><p>Step 1: Redirect ongoing data ingestion - AWS IoT Analytics （https://docs.aws.amazon.com/iotanalytics/latest/userguide/redirect-ongoing-data.html）</p><p>Messaging and streaming layer - Serverless Applications Lens （https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/messaging-and-streaming-layer.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3ad5039286494149b493b96e78381eef",
            "questionNumber": 405,
            "type": "single",
            "content": "<p>Question #405</p><p>A company is preparing to deploy an Amazon Elastic Kubernetes Service (Amazon EKS) cluster for a workload. The company expects the cluster to support an unpredictable number of stateless pods. Many of the pods will be created during a short time period as the workload automatically scales the number of replicas that the workload uses.</p><p><br></p><p>Which solution will MAXIMIZE node resilience?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use a separate launch template to deploy the EKS control plane into a second cluster that is separate from the workload node groups."
                },
                {
                    "label": "B",
                    "content": "Update the workload node groups. Use a smaller number of node groups and larger instances in the node groups."
                },
                {
                    "label": "C",
                    "content": "Configure the Kubernetes Cluster Autoscaler to ensure that the compute capacity of the workload node groups stays underprovisioned."
                },
                {
                    "label": "D",
                    "content": "Configure the workload to use topology spread constraints that are based on Availability Zone."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Configure the workload to use topology spread constraints that are based on Availability Zone.</p><p>This solution will maximize node resilience for the Amazon EKS cluster for the following reasons:</p><p>High Availability Across Multiple Zones: Topology spread constraints allow you to distribute pods evenly across different Availability Zones (AZs). This ensures that if an entire AZ experiences an outage or degradation, your application will continue to run in other AZs with minimal impact.</p><p>Fault Isolation: By spreading pods across multiple AZs, you create natural fault boundaries. This means that infrastructure failures in one zone won't affect the entire workload, significantly improving overall resilience.</p><p>Balanced Resource Utilization: Topology spread constraints help the Kubernetes scheduler make intelligent decisions about pod placement, ensuring that resources are utilized evenly across the cluster. This prevents overloading specific nodes or zones.</p><p>Compatibility with Autoscaling: This approach works well with the unpredictable scaling requirements mentioned in the scenario. As new pods are created during rapid scaling events, they will be distributed according to the topology constraints, maintaining resilience even during dynamic scaling.</p><p>Stateless Workload Advantage: Since the pods are stateless, they can be easily redistributed across zones without concerns about data persistence, making topology spread constraints particularly effective.</p><p>Let's examine why the other options are less effective:</p><p>Option A (separate launch template for EKS control plane in a second cluster) doesn't directly address node resilience for the workload. The EKS control plane is already managed by AWS across multiple AZs, so this approach adds unnecessary complexity without improving workload resilience.</p><p>Option B (smaller number of node groups with larger instances) could actually reduce resilience by creating fewer points of failure. If a large instance fails, it would affect more pods than if the same workload were spread across more numerous, smaller instances.</p><p>Option C (configuring Cluster Autoscaler to keep compute capacity underprovisioned) would likely lead to resource shortages during rapid scaling events, potentially causing pod scheduling delays or failures. Underprovisioning works against resilience by limiting available resources when they're needed most.</p><p>Topology spread constraints based on Availability Zone provide the most effective way to maximize node resilience for this unpredictable, rapidly scaling stateless workload by ensuring pods are distributed across multiple failure domains.</p><p>Sources</p><p>Scaling - Containers on AWS （https://docs.aws.amazon.com/whitepapers/latest/containers-on-aws/scaling.html）</p><p>Running highly-available applications - Amazon EKS （https://docs.aws.amazon.com/eks/latest/best-practices/application.html）</p><p>EKS Data Plane - Amazon EKS （https://docs.aws.amazon.com/eks/latest/best-practices/data-plane.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "cf0a3100b81a47399ae13bf618697d5c",
            "questionNumber": 406,
            "type": "single",
            "content": "<p>Question #406</p><p>A company needs to implement a disaster recovery (DR) plan for a web application. The application runs in a single AWS Region.</p><p><br></p><p>The application uses microservices that run in containers. The containers are hosted on AWS Fargate in Amazon Elastic Container Service (Amazon ECS). The application has an Amazon RDS for MySQL DB instance as its data layer and uses Amazon Route 53 for DNS resolution. An Amazon CloudWatch alarm invokes an Amazon EventBridge rule if the application experiences a failure.</p><p><br></p><p>A solutions architect must design a DR solution to provide application recovery to a separate Region. The solution must minimize the time necessary to recover from a failure.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create an AWS Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Lambda function that creates a second ECS cluster and ECS service in the separate Region. Configure the Lambda function to perform the following actions: take a snapshot of the RDS DB instance, copy the snapshot to the separate Region, create a new RDS DB instance from the snapshot, and update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
                },
                {
                    "label": "C",
                    "content": "Setup a second ECS cluster and ECS service on Fargate in the separate Region. Create a cross-Region read replica of the RDS DB instance in the separate Region. Create an AWS Lambda function to promote the read replica to the primary database. Configure the Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
                },
                {
                    "label": "D",
                    "content": "Setup a second ECS cluster and ECS service on Fargate in the separate Region. Take a snapshot of the RDS DB instance. Convert the snapshot to an Amazon DynamoDB global table. Create an AWS Lambda function to update Route 53 to route traffic to the second ECS cluster. Update the EventBridge rule to add a target that will invoke the Lambda function."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the solution that will meet the requirements for a disaster recovery plan with minimal recovery time. By setting up a second ECS cluster and ECS service on Fargate in a separate Region, the company can ensure that the application can be quickly switched over to the backup Region in the event of a failure. Creating a cross-Region read replica of the RDS DB instance allows for data redundancy and the ability to promote the read replica to primary quickly, minimizing data loss. Additionally, configuring an AWS Lambda function to update Route 53 ensures that traffic is automatically routed to the healthy ECS cluster, reducing downtime.</p><p>The requirements are: &nbsp;</p><p>1. Disaster Recovery (DR) in a separate Region → Need a standby setup in another AWS Region. &nbsp;</p><p>2. Minimize recovery time (RTO) → Using a cross-Region RDS read replica allows faster promotion to primary compared to snapshot-based recovery. &nbsp;</p><p>3. Microservices on ECS Fargate → A pre-configured ECS cluster in the DR Region ensures quick failover. &nbsp;</p><p>4. Automated DNS failover with Route 53 → Lambda can update DNS to redirect traffic to the DR Region. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- Cross-Region RDS Read Replica: Enables near-instant promotion to primary (much faster than snapshot restores). &nbsp;</p><p>- Pre-Provisioned ECS Cluster: Eliminates deployment delays during failover. &nbsp;</p><p>- Automated Lambda Failover: Triggered by EventBridge, updates Route 53 for seamless traffic redirection. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A & B: Rely on snapshot-based recovery, which is slower than promoting a read replica. &nbsp;</p><p>- D: Incorrectly suggests converting RDS MySQL to DynamoDB (not a valid DR strategy for relational data). &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the most efficient solution, leveraging cross-Region RDS read replicas and pre-deployed ECS for minimal downtime. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "29d7c36131c049a6a2a3135dba0d854a",
            "questionNumber": 407,
            "type": "single",
            "content": "<p>Question #407</p><p>A company has AWS accounts that are in an organization in AWS Organizations. The company wants to track Amazon EC2 usage as a metric. The company’s architecture team must receive a daily alert if the EC2 usage is more than 10% higher than the average EC2 usage from the last 30 days.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure AWS Budgets in the organization&#39;s management account. Specify a usage type of EC2 running hours. Specify a daily period. Set the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer. Configure an alert to notify the architecture team if the usage threshold is met."
                },
                {
                    "label": "B",
                    "content": "Configure AWS Cost Anomaly Detection in the organization&#39;s management account. Configure a monitor type of AWS Service. Apply a filter of Amazon EC2. Configure an alert subscription to notify the architecture team if the usage is 10% more than the average usage for the last 30 days."
                },
                {
                    "label": "C",
                    "content": "Enable AWS Trusted Advisor in the organization&#39;s management account. Configure a cost optimization advisory alert to notify the architecture team if the EC2 usage is 10% more than the reported average usage for the last 30 days."
                },
                {
                    "label": "D",
                    "content": "Configure Amazon Detective in the organization&#39;s management account. Configure an EC2 usage anomaly alert to notify the architecture team if Detective identifies a usage anomaly of more than 10%."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Configure AWS Budgets in the organization's management account. Specify a usage type of EC2 running hours. Specify a daily period. Set the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer. Configure an alert to notify the architecture team if the usage threshold is met.</p><p>This solution meets the requirements for the following reasons:</p><p>Precise metric tracking: AWS Budgets allows you to specifically track EC2 running hours as a usage metric, which directly aligns with the requirement to track EC2 usage.</p><p>Daily monitoring period: The solution specifies a daily period for the budget, ensuring that the architecture team receives alerts on a daily basis as required.</p><p>Exact threshold calculation: By setting the budget amount to be 10% more than the reported average usage for the last 30 days from AWS Cost Explorer, the solution precisely implements the requirement to alert when usage is more than 10% higher than the average from the last 30 days.</p><p>Appropriate notification mechanism: The solution includes configuring an alert to notify the architecture team when the usage threshold is met, fulfilling the notification requirement.</p><p>Organization-wide visibility: By configuring AWS Budgets in the organization's management account, the solution can track EC2 usage across all AWS accounts in the organization.</p><p>Option B (using AWS Cost Anomaly Detection) is less suitable for this specific requirement because:</p><p>AWS Cost Anomaly Detection is designed to detect unusual spending patterns using machine learning algorithms, rather than comparing against a specific, predefined threshold like \"10% above the 30-day average.\"</p><p>Cost Anomaly Detection doesn't guarantee daily alerts. It may detect anomalies less frequently or combine multiple days into a single alert, which doesn't satisfy the requirement for daily alerts.</p><p>While Cost Anomaly Detection can monitor EC2 usage, it's primarily designed to detect unexpected cost patterns rather than tracking usage against a specific historical average.</p><p>The configuration described in option B doesn't specify how the 10% threshold would be implemented within Cost Anomaly Detection, as this service typically uses machine learning to determine what constitutes an anomaly rather than fixed percentage thresholds.</p><p>AWS Budgets provides the precise control needed to implement the specific requirement of daily alerts when EC2 usage exceeds the 30-day average by 10%, making option A the correct solution.</p><p>Sources</p><p>COST02-BP05 Implement cost controls - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2024-06-27/framework/cost_govern_usage_controls.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "56a2784b5a404a1ca82023dd85b4f420",
            "questionNumber": 408,
            "type": "single",
            "content": "<p>Question #408</p><p>An e-commerce company is revamping its IT infrastructure and is planning to use AWS services. The company’s CIO has asked a solutions architect to design a simple, highly available, and loosely coupled order processing application. The application is responsible for receiving and processing orders before storing them in an Amazon DynamoDB table. The application has a sporadic traffic pattern and should be able to scale during marketing campaigns to process the orders with minimal delays.</p><p><br></p><p>Which of the following is the MOST reliable approach to meet the requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Receive the orders in an Amazon EC2-hosted database and use EC2 instances to process them."
                },
                {
                    "label": "B",
                    "content": "Receive the orders in an Amazon SQS queue and invoke an AWS Lambda function to process them."
                },
                {
                    "label": "C",
                    "content": "Receive the orders using the AWS Step Functions program and launch an Amazon ECS container to process them."
                },
                {
                    "label": "D",
                    "content": "Receive the orders in Amazon Kinesis Data Streams and use Amazon EC2 instances to process them."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Receive the orders in an Amazon SQS queue and invoke an AWS Lambda function to process them.</p><p>This approach is the MOST reliable solution for meeting the e-commerce company's requirements for the following reasons:</p><p>High Availability: Amazon SQS is a fully managed message queuing service that provides high availability by default. It stores messages across multiple servers and data centers, ensuring that orders are not lost even if some AWS infrastructure components fail.</p><p>Loose Coupling: Using SQS creates a buffer between the components that receive orders and those that process them. This decoupling is essential for building resilient systems where failures in one component don't affect others.</p><p>Scalability for Sporadic Traffic: The SQS queue can handle varying loads of incoming orders without any configuration changes. During marketing campaigns when order volume spikes, the queue will simply accumulate messages without any performance degradation.</p><p>Automatic Scaling: AWS Lambda functions automatically scale based on the number of messages in the SQS queue. As more orders come in, more Lambda function instances are automatically created to process them, without any manual intervention or pre-provisioning.</p><p>Cost Efficiency: With this serverless approach, the company only pays for actual usage. During periods of low activity, costs are minimal, and during high-traffic marketing campaigns, resources scale up as needed.</p><p>Minimal Delays: The combination of SQS and Lambda ensures that orders are processed with minimal delays, even during traffic spikes. Lambda functions can be configured to process multiple messages in parallel.</p><p>Simplified Operations: This architecture eliminates the need to manage servers, databases, or scaling policies, reducing operational complexity.</p><p>Option A (using EC2 instances and an EC2-hosted database) is less reliable because:</p><p>It requires manual scaling or complex auto-scaling configurations to handle sporadic traffic patterns.</p><p>The EC2-hosted database could become a bottleneck during high-traffic periods.</p><p>It introduces more potential points of failure with multiple EC2 instances and a self-managed database.</p><p>It requires more operational overhead to ensure high availability.</p><p>It's not as cost-effective for sporadic workloads since EC2 instances would need to be running even during low-traffic periods.</p><p>The SQS and Lambda approach provides a simple, highly available, loosely coupled, and automatically scaling solution that meets all the requirements specified by the company's CIO, making it the most reliable choice for this e-commerce order processing application.</p><p>Sources</p><p>Scaling and event processing with SNS and SQS | AWS re:Post （https://repost.aws/questions/QUcnYW9ZAkSA-Px9tyHm_sYw/scaling-and-event-processing-with-sns-and-sqs）</p><p>Amazon SQS FIFO queue and Lambda concurrency behavior - Amazon Simple Queue Service （https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/fifo-queue-lambda-behavior.html）</p><p>Building resilient applications: design patterns for handling database outages | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/building-resilient-applications-design-patterns-for-handling-database-outages/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d6003f9ce14d4e01b5edbabd2908b9ff",
            "questionNumber": 409,
            "type": "single",
            "content": "<p>Question #409</p><p>A company is deploying AWS Lambda functions that access an Amazon RDS for PostgreSQL database. The company needs to launch the Lambda functions in a QA environment and in a production environment. <br><br>The company must not expose credentials within application code and must rotate passwords automatically.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Store the database credentials for both environments in AWS Systems Manager Parameter Store. Encrypt the credentials by using an AWS Key Management Service (AWS KMS) key. Within the application code of the Lambda functions, pull the credentials from the Parameter Store parameter by using the AWS SDK for Python (Boto3). Add a role to the Lambda functions to provide access to the Parameter Store parameter."
                },
                {
                    "label": "B",
                    "content": "Store the database credentials for both environments in AWS Secrets Manager with distinct key entries for the QA environment and the production environment. Turn on rotation. Provide a reference to the Secrets Manager key as an environment variable for the Lambda functions."
                },
                {
                    "label": "C",
                    "content": "Store the database credentials for both environments in AWS Key Management Service (AWS KMS). Turn on rotation. Provide a reference to the credentials that are stored in AWS KMS as an environment variable for the Lambda functions."
                },
                {
                    "label": "D",
                    "content": "Create separate S3 buckets for the QA environment and the production environment. Turn on server-side encryption with AWS KMS keys (SSE-KMS) for the S3 buckets. Use an object naming pattern that gives each Lambda function&rsquo;s application code the ability to pull the correct credentials for the function&#39;s corresponding environment. Grant each Lambda function&#39;s execution role access to Amazon S3."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct solution to meet the requirements of not exposing credentials within application code and automating password rotation. AWS Secrets Manager is designed to securely store, manage, and rotate database credentials, along with other types of secrets. By storing the credentials in Secrets Manager with distinct key entries for each environment (QA and production), the company ensures that sensitive information is not hard-coded in the application code. The rotation feature of Secrets Manager allows for automatic password rotation, which enhances security by regularly updating credentials. Providing a reference to the Secrets Manager key as an environment variable for the Lambda functions enables the Lambda functions to access the credentials without including them in the code.</p><p>The requirements are: &nbsp;</p><p>1. No credentials in application code → Secrets must be stored externally. &nbsp;</p><p>2. Automatic password rotation → Requires a service that supports rotation. &nbsp;</p><p>3. Environment separation (QA & Production) → Must manage credentials per environment securely. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- AWS Secrets Manager: &nbsp;</p><p> &nbsp;- Securely stores credentials (encrypted at rest). &nbsp;</p><p> &nbsp;- Supports automatic rotation (key requirement). &nbsp;</p><p> &nbsp;- Allows environment separation (distinct keys for QA/prod). &nbsp;</p><p>- Lambda Integration: &nbsp;</p><p> &nbsp;- Secrets can be referenced via environment variables (no hardcoding). &nbsp;</p><p> &nbsp;- IAM roles grant Lambda access to Secrets Manager. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Parameter Store): Does not support automatic rotation (only Secrets Manager does). &nbsp;</p><p>- C (AWS KMS): KMS encrypts but does not store or rotate secrets. &nbsp;</p><p>- D (S3): Not secure for credential storage (S3 is not designed for secrets management). &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the only solution that meets all requirements (secure storage, rotation, and environment separation). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1e4589ccf66c47ce99e96fe02424399a",
            "questionNumber": 410,
            "type": "single",
            "content": "<p>Question #410</p><p>A company is using AWS Control Tower to manage AWS accounts in an organization in AWS Organizations. The company has an OU that contains accounts. The company must prevent any new or existing Amazon EC2 instances in the OU's accounts from gaining a public IP address.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure all instances in each account in the OU to use AWS Systems Manager. Use a Systems Manager Automation runbook to prevent public IP addresses from being attached to the instances."
                },
                {
                    "label": "B",
                    "content": "Implement the AWS Control Tower proactive control to check whether instances in the OU&#39;s accounts have a public IP address. Set the AssociatePublicIpAddress property to False. Attach the proactive control to the OU."
                },
                {
                    "label": "C",
                    "content": "Create an SCP that prevents the launch of instances that have a public IP address. Additionally, configure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP to the OU."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Config custom rule that detects instances that have a public IP address. Configure a remediation action that uses an AWS Lambda function to detach the public IP addresses from the instances."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an SCP that prevents the launch of instances that have a public IP address. Additionally, configure the SCP to prevent the attachment of a public IP address to existing instances. Attach the SCP to the OU.</p><p>This solution will meet the requirements for the following reasons:</p><p>Comprehensive Prevention: Service Control Policies (SCPs) act as permission guardrails that apply to all accounts within an OU. By creating an SCP that specifically prevents both the launch of instances with public IP addresses and the attachment of public IPs to existing instances, the company ensures complete coverage of the requirement.</p><p>Preventative Control: SCPs work as preventative controls that deny actions before they can occur, rather than detecting and remediating after the fact. This ensures that no EC2 instance in the affected accounts can ever gain a public IP address.</p><p>Organization-Wide Enforcement: By attaching the SCP to the OU, the policy automatically applies to all accounts within that OU, including any new accounts that might be added in the future, providing consistent enforcement.</p><p>No Operational Overhead: Once implemented, the SCP requires no ongoing maintenance or monitoring to continue enforcing the policy.</p><p>Option A (using Systems Manager Automation runbooks) is less effective because:</p><p>It's a reactive approach that would need to continuously monitor and remediate instances</p><p>It requires all instances to have Systems Manager agents installed and properly configured</p><p>It introduces operational complexity and potential points of failure</p><p>It might not prevent the initial assignment of public IPs during instance launch</p><p>Option B (implementing AWS Control Tower proactive controls) is incorrect because:</p><p>AWS Control Tower proactive controls (also known as preventive guardrails) don't have a specific built-in control for the \"AssociatePublicIpAddress\" property</p><p>Proactive controls in Control Tower are typically implemented using SCPs or AWS CloudFormation hooks, not as a direct property setting</p><p>This approach as described wouldn't address existing instances that might already have public IPs or prevent other methods of attaching public IPs</p><p>The SCP approach (Option C) provides the most comprehensive, preventative, and low-maintenance solution to ensure that no EC2 instances in the OU's accounts can gain public IP addresses, making it the correct answer.</p><p>Sources</p><p>ec2-instance-no-public-ip - AWS Config （https://docs.aws.amazon.com/config/latest/developerguide/ec2-instance-no-public-ip.html）</p><p>Data residency controls with detective behavior - AWS Control Tower （https://docs.aws.amazon.com/controltower/latest/controlreference/data-residency-detective-controls.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "246734ca3142497290f3a2b9d8d7a415",
            "questionNumber": 411,
            "type": "single",
            "content": "<p>Question #411</p><p>A company is deploying a third-party web application on AWS. The application is packaged as a Docker image. The company has deployed the Docker image as an AWS Fargate service in Amazon Elastic Container Service (Amazon ECS). An Application Load Balancer (ALB) directs traffic to the application.<br><br></p><p>The company needs to give only a specific list of users the ability to access the application from the internet. The company cannot change the application and cannot integrate the application with an identity provider. All users must be authenticated through multi-factor authentication (MFA).<br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a user pool in Amazon Cognito. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFA. Configure a listener rule on the ALB to require authentication through the Amazon Cognito hosted UI."
                },
                {
                    "label": "B",
                    "content": "Configure the users in AWS Identity and Access Management (IAM). Attach a resource policy to the Fargate service to require users to use MFA. Configure a listener rule on the ALB to require authentication through IAM."
                },
                {
                    "label": "C",
                    "content": "Configure the users in AWS Identity and Access Management (IAM). Enable AWS IAM Identity Center (AWS Single Sign-On). Configure resource protection for the ALB. Create a resource protection rule to require users to use MFA."
                },
                {
                    "label": "D",
                    "content": "Create a user pool in AWS Amplify. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFA. Configure a listener rule on the ALB to require authentication through the Amplify hosted UI."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a user pool in Amazon Cognito. Configure the pool for the application. Populate the pool with the required users. Configure the pool to require MFA. Configure a listener rule on the ALB to require authentication through the Amazon Cognito hosted UI.</p><p>This solution meets all the requirements for the following reasons:</p><p>Restricted Access: By creating a user pool in Amazon Cognito and populating it with only the specific list of users who should have access, the company can control exactly who can access the application.</p><p>No Application Changes: This approach doesn't require any modifications to the third-party web application itself. The authentication happens at the ALB level, before traffic even reaches the application running in the Fargate containers.</p><p>No Identity Provider Integration: The solution doesn't require integrating the application with an identity provider. Instead, it leverages Amazon Cognito's hosted UI for authentication, which sits in front of the application.</p><p>Multi-Factor Authentication (MFA): Amazon Cognito supports MFA, and the solution specifically mentions configuring the user pool to require MFA, satisfying this security requirement.</p><p>Internet Access: The solution maintains the ability for authorized users to access the application from the internet while preventing unauthorized access.</p><p>Works with Containerized Applications: This approach is compatible with the Docker image running as a Fargate service in Amazon ECS.</p><p>The implementation works as follows:</p><p>When a user attempts to access the application, the ALB listener rule intercepts the request.</p><p>The ALB redirects the user to the Amazon Cognito hosted UI for authentication.</p><p>The user provides their credentials and completes the MFA challenge.</p><p>Upon successful authentication, Amazon Cognito redirects the user back to the ALB with authentication information.</p><p>The ALB validates the authentication and forwards the request to the Fargate service running the application.</p><p>Users not in the Cognito user pool cannot authenticate and therefore cannot access the application.</p><p>This solution provides a secure authentication layer in front of the application without requiring any changes to the application itself, making it the correct answer for the given requirements.</p><p>Sources</p><p>ECS Task with public ip connect permanent | AWS re:Post （https://repost.aws/questions/QUYfik8-3vSqO8fLIq2Z4GeQ/ecs-task-with-public-ip-connect-permanent）</p><p>Customer identity management - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/customer-identity-management.html）</p><p>Control Access to Web Applications and Mobile Apps - Navigating GDPR Compliance on AWS （https://docs.aws.amazon.com/whitepapers/latest/navigating-gdpr-compliance/control-access-to-web-applications-and-mobile-apps.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1e9c965184124385abec15b5cc0b5933",
            "questionNumber": 412,
            "type": "single",
            "content": "<p>Question #412</p><p>A solutions architect is preparing to deploy a new security tool into several previously unused AWS Regions. The solutions architect will deploy the tool by using an AWS CloudFormation stack set. The stack set's template contains an IAM role that has a custom name. Upon creation of the stack set, no stack instances are created successfully.<br><br>What should the solutions architect do to deploy the stacks successfully? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Enable the new Regions in all relevant accounts. Specify the CAPABILITY_NAMED_IAM capability during the creation of the stack set."
                },
                {
                    "label": "B",
                    "content": "Use the Service Quotas console to request a quota increase for the number of CloudFormation stacks in each new Region in all relevant accounts. Specify the CAPABILITY_IAM capability during the creation of the stack set."
                },
                {
                    "label": "C",
                    "content": "Specify the CAPABILITY_NAMED_IAM capability and the SELF_MANAGED permissions model during the creation of the stack set."
                },
                {
                    "label": "D",
                    "content": "Specify an administration role ARN and the CAPABILITY_IAM capability during the creation of the stack set."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Enable the new Regions in all relevant accounts. Specify the CAPABILITY_NAMED_IAM capability during the creation of the stack set.</p><p>This solution addresses both key issues preventing the successful deployment of the stack instances:</p><p>Region Enablement:</p><p>The problem mentions \"previously unused AWS Regions,\" which indicates that these Regions may not be enabled in the AWS accounts.</p><p>By default, some AWS Regions are not enabled in AWS accounts and must be explicitly activated before resources can be deployed there.</p><p>Enabling the new Regions in all relevant accounts is a necessary first step to allow CloudFormation to deploy resources in those Regions.</p><p>IAM Permissions:</p><p>The CloudFormation template contains \"an IAM role that has a custom name.\"</p><p>When creating CloudFormation stacks or stack sets that include IAM resources with custom names, you must explicitly acknowledge this capability.</p><p>The CAPABILITY_NAMED_IAM capability is specifically required when creating or updating stacks that contain custom-named IAM resources.</p><p>Without this capability specified, CloudFormation will refuse to create the stack instances as a safety measure to prevent unintended IAM changes.</p><p>Option B is incorrect for the following reasons:</p><p>Requesting a quota increase for the number of CloudFormation stacks would not resolve the issue, as the problem is not related to reaching a stack limit.</p><p>The CAPABILITY_IAM capability is insufficient for templates that include IAM resources with custom names. This capability only covers IAM resources with auto-generated names, not custom-named resources.</p><p>The issue is related to permissions and Region enablement, not service quotas.</p><p>By enabling the previously unused Regions and specifying the CAPABILITY_NAMED_IAM capability during stack set creation, the solutions architect will address both barriers preventing the successful deployment of the security tool across the new Regions.</p><p>Sources</p><p>Control CloudFormation access with AWS Identity and Access Management - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/control-access-with-iam.html）</p><p>CreateStackSet - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/API_CreateStackSet.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "681151266ed94b3580b06e66cf555ffb",
            "questionNumber": 413,
            "type": "single",
            "content": "<p>Question #413</p><p>A company has an application that uses an Amazon Aurora PostgreSQL DB cluster for the application's database. The DB cluster contains one small primary instance and three larger replica instances. The application runs on an AWS Lambda function. The application makes many short-lived connections to the database's replica instances to perform read-only operations.<br><br></p><p>During periods of high traffic, the application becomes unreliable and the database reports that too many connections are being established. The frequency of high-traffic periods is unpredictable.</p><p><br>Which solution will improve the reliability of the application?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Amazon RDS Proxy to create a proxy for the DB cluster. Configure a read-only endpoint for the proxy. Update the Lambda function to connect to the proxy endpoint."
                },
                {
                    "label": "B",
                    "content": "Increase the max_connections setting on the DB cluster&#39;s parameter group. Reboot all the instances in the DB cluster. Update the Lambda function to connect to the DB cluster endpoint."
                },
                {
                    "label": "C",
                    "content": "Configure instance scaling for the DB cluster to occur when the DatabaseConnections metric is close to the max connections setting. Update the Lambda function to connect to the Aurora reader endpoint."
                },
                {
                    "label": "D",
                    "content": "Use Amazon RDS Proxy to create a proxy for the DB cluster. Configure a read-only endpoint for the Aurora Data API on the proxy. Update the Lambda function to connect to the proxy endpoint."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer. Amazon RDS Proxy can be used to manage database connections efficiently, especially when dealing with numerous short-lived connections from AWS Lambda functions. By configuring a read-only endpoint for the proxy, the Lambda function can connect to the Aurora replica instances through the proxy, which will help manage the connection pool and reduce the number of concurrent connections to the database.</p><p>The problem stems from too many short-lived connections from Lambda to Aurora PostgreSQL replicas during high traffic, overwhelming the database. &nbsp;</p><p> Why Option A is Best? &nbsp;</p><p>- Amazon RDS Proxy is designed to manage connection pooling, reducing the overhead of frequent connections. &nbsp;</p><p> &nbsp;- It sits between the Lambda function and the Aurora cluster, reusing connections efficiently. &nbsp;</p><p> &nbsp;- The read-only endpoint ensures read traffic is distributed across replicas. &nbsp;</p><p>- Lambda Integration: &nbsp;</p><p> &nbsp;- Updating the Lambda function to use the RDS Proxy endpoint (instead of direct DB connections) solves the connection exhaustion issue. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Increasing `max_connections`): &nbsp;</p><p> &nbsp;- Only a temporary fix; does not address connection churn (short-lived connections still cause overhead). &nbsp;</p><p> &nbsp;- Replicas may still get overwhelmed during unpredictable spikes. &nbsp;</p><p>- C (Instance Scaling): &nbsp;</p><p> &nbsp;- Scaling Aurora instances does not solve connection pooling issues. &nbsp;</p><p> &nbsp;- The Aurora reader endpoint alone doesn’t manage connections efficiently. &nbsp;</p><p>- D (Aurora Data API): &nbsp;</p><p> &nbsp;- The Aurora Data API is for serverless apps (not connection pooling). &nbsp;</p><p> &nbsp;- Misleading—RDS Proxy itself is the solution, not the Data API. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the most reliable solution because RDS Proxy directly addresses the connection management problem without requiring manual scaling or configuration changes. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7da7494ad63e4f96a708d5c9d1d0d104",
            "questionNumber": 414,
            "type": "single",
            "content": "<p>Question #414</p><p>A retail company is mounting IoT sensors in all of its stores worldwide. During the manufacturing of each sensor, the company’s private certificate authority (CA) issues an X.509 certificate that contains a unique serial number. The company then deploys each certificate to its respective sensor.</p><p><br></p><p>A solutions architect needs to give the sensors the ability to send data to AWS after they are installed. Sensors must not be able to send data to AWS until they are installed.</p><p><br>Which solution will meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. During manufacturing, call the RegisterThing API operation and specify the template and parameters."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Step Functions state machine that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Specify the Step Functions state machine to validate parameters. Call the StartThingRegistrationTask API operation during installation."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. Register the CA with AWS IoT Core, specify the provisioning template, and set the allow-auto-registration parameter."
                },
                {
                    "label": "D",
                    "content": "Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Include parameter validation in the template. Provision a claim certificate and a private key for each device that uses the CA. Grant AWS IoT Core service permissions to update AWS IoT things during provisioning."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an AWS Lambda function that can validate the serial number. Create an AWS IoT Core provisioning template. Include the SerialNumber parameter in the Parameters section. Add the Lambda function as a pre-provisioning hook. Register the CA with AWS IoT Core, specify the provisioning template, and set the allow-auto-registration parameter.</p><p>This solution meets the requirements for the following reasons:</p><p>Security Control: The solution ensures that sensors can only send data to AWS after they are installed by implementing a validation step through a Lambda function that acts as a pre-provisioning hook.</p><p>Certificate Utilization: The solution leverages the X.509 certificates that are already being issued by the company's private CA during manufacturing, which contain unique serial numbers.</p><p>Automated Provisioning: By registering the CA with AWS IoT Core and enabling auto-registration, the solution allows for a streamlined process where devices can be automatically recognized when they connect.</p><p>Installation Validation: The Lambda function can validate the serial number against a database of installed sensors, ensuring that only sensors that have been properly installed are allowed to send data.</p><p>Scalability: This approach works well for a global deployment across all stores worldwide, as it doesn't require manual intervention for each sensor installation.</p><p>The workflow of this solution is as follows:</p><p>During manufacturing, each sensor receives an X.509 certificate with a unique serial number from the company's private CA.</p><p>The company registers their private CA with AWS IoT Core and configures it to use the provisioning template with auto-registration enabled.</p><p>When a sensor is installed in a store, it attempts to connect to AWS IoT Core using its certificate.</p><p>AWS IoT Core recognizes the certificate as being issued by the registered CA and initiates the provisioning process using the specified template.</p><p>The provisioning template extracts the SerialNumber parameter and passes it to the Lambda function pre-provisioning hook.</p><p>The Lambda function validates the serial number against a database or list of sensors that have been officially installed.</p><p>If the validation is successful, the Lambda function allows the provisioning to continue, and the sensor is fully registered with AWS IoT Core, enabling it to send data.</p><p>If the validation fails (e.g., for a sensor that hasn't been installed yet), the provisioning is rejected, and the sensor cannot send data to AWS.</p><p>This approach provides a secure, automated way to ensure that only properly installed sensors can communicate with AWS, meeting the company's requirements.</p><p>Sources</p><p>Provisioning identity in AWS IoT Core for device connections - Device Manufacturing and Provisioning with X.509 Certificates in AWS IoT Core （https://docs.aws.amazon.com/whitepapers/latest/device-manufacturing-provisioning/provisioning-identity-in-aws-iot-core-for-device-connections.html）</p><p>Device provisioning - IoT Lens （https://docs.aws.amazon.com/wellarchitected/latest/iot-lens/device-provisioning.html）</p><p>Create your own client certificates - AWS IoT Core （https://docs.aws.amazon.com/iot/latest/developerguide/device-certs-your-own.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "bd65643adbe948c9a3366c3a0d1a0d33",
            "questionNumber": 415,
            "type": "single",
            "content": "<p>Question #415</p><p>A startup company recently migrated a large ecommerce website to AWS. The website has experienced a 70% increase in sales. Software engineers are using a private GitHub repository to manage code. The DevOps team is using Jenkins for builds and unit testing. The engineers need to receive notifications for bad builds and zero downtime during deployments. The engineers also need to ensure any changes to production are seamless for users and can be rolled back in the event of a major issue.</p><p><br></p><p>The software engineers have decided to use AWS CodePipeline to manage their build and deployment process.<br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use GitHub websockets to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy."
                },
                {
                    "label": "B",
                    "content": "Use GitHub webhooks to trigger the CodePipeline pipeline. Use the Jenkins plugin for AWS CodeBuild to conduct unit testing. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy."
                },
                {
                    "label": "C",
                    "content": "Use GitHub websockets to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in a blue/green deployment using AWS CodeDeploy."
                },
                {
                    "label": "D",
                    "content": "Use GitHub webhooks to trigger the CodePipeline pipeline. Use AWS X-Ray for unit testing and static code analysis. Send alerts to an Amazon SNS topic for any bad builds. Deploy in an in-place, all-at-once deployment configuration using AWS CodeDeploy."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. Using GitHub webhooks to trigger the AWS CodePipeline ensures a seamless integration between code commits and the start of the build process. Utilizing the Jenkins plugin for AWS CodeBuild allows the team to continue using their existing Jenkins configurations for unit testing within the AWS ecosystem. Sending alerts to an Amazon SNS topic for bad builds provides a notification mechanism for the team. Finally, deploying using a blue/green deployment strategy with AWS CodeDeploy helps achieve zero downtime and seamless user experience during deployments, with the ability to roll back to the previous version in case of any issues.</p><p>The requirements are: &nbsp;</p><p>1. GitHub integration → Webhooks (not websockets) are the correct way to trigger CodePipeline from GitHub. &nbsp;</p><p>2. Jenkins for builds/unit testing → The Jenkins plugin for AWS CodeBuild allows integration with existing Jenkins workflows. &nbsp;</p><p>3. Notifications for bad builds → Amazon SNS can send alerts for failed pipeline stages. &nbsp;</p><p>4. Zero downtime deployments & rollback capability → Blue/Green deployment (via AWS CodeDeploy) ensures seamless updates and easy rollback. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- GitHub Webhooks: Properly triggers CodePipeline on code changes. &nbsp;</p><p>- Jenkins + CodeBuild: Maintains existing CI workflows while integrating with AWS. &nbsp;</p><p>- SNS Alerts: Ensures engineers are notified of build failures. &nbsp;</p><p>- Blue/Green Deployment: Guarantees zero downtime and safe rollback. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A & D: Use in-place deployments, which cause downtime and lack rollback safety. &nbsp;</p><p>- C & D: Incorrectly suggest X-Ray for unit testing (X-Ray is for distributed tracing, not testing). &nbsp;</p><p>- A & C: Incorrectly mention websockets (GitHub uses webhooks for pipeline triggers). &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the only solution that meets all requirements (GitHub webhooks, Jenkins integration, SNS alerts, and blue/green deployments). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "cd9ab881103f48e58f2728165ec9a5ce",
            "questionNumber": 416,
            "type": "single",
            "content": "<p>Question #416</p><p>A software as a service (SaaS) company has developed a multi-tenant environment. The company uses Amazon DynamoDB tables that the tenants share for the storage layer. The company uses AWS Lambda functions for the application services.<br><br></p><p>The company wants to offer a tiered subscription model that is based on resource consumption by each tenant. Each tenant is identified by a unique tenant ID that is sent as part of each request to the Lambda functions. The company has created an AWS Cost and Usage Report (AWS CUR) in an AWS account. The company wants to allocate the DynamoDB costs to each tenant to match that tenant's resource consumption.<br><br>Which solution will provide a granular view of the DynamoDB cost for each tenant with the LEAST operational effort? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Associate a new tag that is named tenant ID with each table in DynamoDB. Activate the tag as a cost allocation tag in the AWS Billing and Cost Management console. Deploy new Lambda function code to log the tenant ID in Amazon CloudWatch Logs. Use the AWS CUR to separate DynamoDB consumption cost for each tenant ID."
                },
                {
                    "label": "B",
                    "content": "Configure the Lambda functions to log the tenant ID and the number of RCUs and WCUs consumed from DynamoDB for each transaction to Amazon CloudWatch Logs. Deploy another Lambda function to calculate the tenant costs by using the logged capacity units and the overall DynamoDB cost from the AWS Cost Explorer API. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule."
                },
                {
                    "label": "C",
                    "content": "Create a new partition key that associates DynamoDB items with individual tenants. Deploy a Lambda function to populate the new column as part of each transaction. Deploy another Lambda function to calculate the tenant costs by using Amazon Athena to calculate the number of tenant items from DynamoDB and the overall DynamoDB cost from the AWS CUR. Create an Amazon EventBridge rule to invoke the calculation Lambda function on a schedule."
                },
                {
                    "label": "D",
                    "content": "Deploy a Lambda function to log the tenant ID, the size of each response, and the duration of the transaction call as custom metrics to Amazon CloudWatch Logs. Use CloudWatch Logs Insights to query the custom metrics for each tenant. Use AWS Pricing Calculator to obtain the overall DynamoDB costs and to calculate the tenant costs."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct answer. By configuring Lambda functions to log the tenant ID along with the consumed read and write capacity units (RCUs and WCUs) for each transaction, the company can track individual tenant resource consumption. Utilizing the AWS Cost Explorer API to calculate tenant costs based on these logged metrics allows for a dynamic and automated approach to cost allocation. Scheduling the invocation of the calculation Lambda function using Amazon EventBridge ensures that the cost allocation is performed regularly and stays up-to-date.</p><p>The requirements are: &nbsp;</p><p>1. Granular DynamoDB cost allocation per tenant → Need to track Read Capacity Units (RCUs) and Write Capacity Units (WCUs) per tenant. &nbsp;</p><p>2. Least operational effort → Avoid schema changes or complex ETL processes. &nbsp;</p><p>3. Tiered subscription model → Must accurately measure each tenant's resource usage. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- Log RCUs/WCUs per Tenant: &nbsp;</p><p> &nbsp;- Lambda functions can log tenant_id + consumed capacity units to CloudWatch. &nbsp;</p><p> &nbsp;- This provides direct visibility into DynamoDB usage per tenant. &nbsp;</p><p>- Automated Cost Calculation: &nbsp;</p><p> &nbsp;- A scheduled Lambda function can aggregate usage data and calculate costs using the AWS Cost Explorer API. &nbsp;</p><p> &nbsp;- This avoids manual analysis and provides near-real-time cost allocation. &nbsp;</p><p>- No Schema Changes: &nbsp;</p><p> &nbsp;- Unlike Option C, this does not require modifying DynamoDB tables. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Cost Allocation Tags): &nbsp;</p><p> &nbsp;- DynamoDB does not support per-request tagging, so tags cannot accurately track tenant-level usage in shared tables. &nbsp;</p><p> &nbsp;- Only works for table-level costs, not tenant-level consumption. &nbsp;</p><p>- C (Partition Key + Athena): &nbsp;</p><p> &nbsp;- Requires schema changes (operational overhead). &nbsp;</p><p> &nbsp;- Athena queries add complexity and latency. &nbsp;</p><p>- D (Custom Metrics + Pricing Calculator): &nbsp;</p><p> &nbsp;- Response size/duration ≠ RCU/WCU consumption (inaccurate for billing). &nbsp;</p><p> &nbsp;- Pricing Calculator is manual and not automated. &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the most accurate and least operationally intensive solution, as it: &nbsp;</p><p>1. Directly measures RCU/WCU consumption per tenant. &nbsp;</p><p>2. Automates cost allocation without schema changes. &nbsp;</p><p>3. Aligns with AWS-native tools (CloudWatch, Cost Explorer). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "cabf10b5b2b14e5a935e20965e839f79",
            "questionNumber": 417,
            "type": "single",
            "content": "<p>Question #417</p><p>A company has an application that stores data in a single Amazon S3 bucket. The company must keep all data for 1 year. The company’s security team is concerned that an attacker could gain access to the AWS account through leaked long-term credentials.<br><br>Which solution will ensure that existing and future objects in the S3 bucket are protected? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new AWS account that is accessible only to the security team through an assumed role. Create an S3 bucket in the new account. Enable S3 Versioning and S3 Object Lock. Configure a default retention period of 1 year. Set up replication from the existing S3 bucket to the new S3 bucket. Create an S3 Batch Replication job to copy all existing data."
                },
                {
                    "label": "B",
                    "content": "Use the s3-bucket-versioning-enabled AWS Config managed rule. Configure an automatic remediation action that uses an AWS Lambda function to enable S3 Versioning and MFA Delete on noncompliant resources. Add an S3 Lifecycle rule to delete objects after 1 year."
                },
                {
                    "label": "C",
                    "content": "Explicitly deny bucket creation from all users and roles except for an AWS Service Catalog launch constraint role. Define a Service Catalog product for the creation of the S3 bucket to force S3 Versioning and MFA Delete to be enabled. Authorize users to launch the product when they need to create an S3 bucket."
                },
                {
                    "label": "D",
                    "content": "Enable Amazon GuardDuty with the S3 protection feature for the account and the AWS Region. Add an S3 Lifecycle rule to delete objects after 1 year."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a new AWS account that is accessible only to the security team through an assumed role. Create an S3 bucket in the new account. Enable S3 Versioning and S3 Object Lock. Configure a default retention period of 1 year. Set up replication from the existing S3 bucket to the new S3 bucket. Create an S3 Batch Replication job to copy all existing data.</p><p>This solution will ensure that existing and future objects in the S3 bucket are protected for the following reasons:</p><p>Account Isolation: Creating a separate AWS account that's only accessible to the security team provides strong isolation from the primary account. If an attacker gains access to the primary account through leaked credentials, they won't have access to the security account where the protected data is stored.</p><p>Immutable Storage: S3 Object Lock with a 1-year retention period makes the data immutable - it cannot be deleted or modified by anyone, including administrators and even AWS, until the retention period expires. This is critical for protecting against attackers who might try to delete or corrupt data.</p><p>Data Versioning: S3 Versioning ensures that multiple versions of objects are maintained, protecting against accidental or malicious overwrites.</p><p>Complete Data Protection: The combination of replication for new objects and batch replication for existing data ensures that all data (both current and future) is protected in the secure secondary account.</p><p>Least Privilege Access: Using assumed roles for the security team follows the principle of least privilege, further reducing the attack surface.</p><p>Option B (using AWS Config with automatic remediation) is less effective because:</p><p>Same Account Vulnerability: It keeps all data in the same AWS account. If an attacker gains access to the account through leaked credentials, they could potentially disable the AWS Config rules or modify the Lambda functions used for remediation.</p><p>No True Immutability: While MFA Delete adds a layer of protection, it doesn't provide the same level of immutability as Object Lock. An attacker with sufficient permissions could still disable MFA Delete.</p><p>No Data Isolation: Without cross-account replication, there's no isolation between the production data and its protected copies.</p><p>Lifecycle Rules Only: The lifecycle rule would only delete objects after 1 year but doesn't prevent deletion or modification before that time.</p><p>The solution in option A creates a comprehensive defense-in-depth strategy by combining account isolation, immutable storage, versioning, and replication to ensure that data is protected even if the primary account is compromised. This approach aligns with AWS security best practices for protecting critical data against threats from compromised credentials.</p><p>Sources</p><p>Data protection in Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html）</p><p>Remediating exposures for Amazon S3 buckets - AWS Security Hub （https://docs.aws.amazon.com/securityhub/latest/userguide/exposure-s3-bucket.html）</p><p>Data protection in Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/data-protection.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b917ff9b07824c1aa7c9d3f632a3a967",
            "questionNumber": 418,
            "type": "single",
            "content": "<p>Question #418</p><p>A company needs to improve the security of its web-based application on AWS. The application uses Amazon CloudFront with two custom origins. The first custom origin routes requests to an Amazon API Gateway HTTP API. The second custom origin routes traffic to an Application Load Balancer (ALB). The application integrates with an OpenID Connect (OIDC) identity provider (IdP) for user management.<br><br></p><p>A security audit shows that a JSON Web Token (JWT) authorizer provides access to the API. The security audit also shows that the ALB accepts requests from unauthenticated users.<br><br></p><p>A solutions architect must design a solution to ensure that all backend services respond to only authenticated users. <br><br></p><p>Which solution will meet this requirement? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the ALB to enforce authentication and authorization by integrating the ALB with the IdP. Allow only authenticated users to access the backend services."
                },
                {
                    "label": "B",
                    "content": "Modify the CloudFront configuration to use signed URLs. Implement a permissive signing policy that allows any request to access the backend services."
                },
                {
                    "label": "C",
                    "content": "Create an AWS WAF web ACL that filters out unauthenticated requests at the ALB level. Allow only authenticated traffic to reach the backend services."
                },
                {
                    "label": "D",
                    "content": "Enable AWS CloudTrail to log all requests that come to the ALB. Create an AWS Lambda function to analyze the logs and block any requests that come from unauthenticated users."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Option A is the correct answer. By configuring the ALB to work with the OIDC identity provider, the company can ensure that only authenticated users are able to access the backend services. This integration will allow the ALB to validate the JWT tokens provided by the IdP, thereby preventing unauthenticated requests from reaching the backend.</p><p>The key requirements are:</p><p>1. All backend services must only respond to authenticated users - Currently the ALB accepts unauthenticated requests</p><p>2. Application already uses OIDC for authentication - The solution should leverage the existing identity provider</p><p>3. Two backend origins need protection - API Gateway (already secured with JWT) and ALB (needs securing)</p><p> Why Option A is Correct:</p><p>- ALB Native OIDC Integration: </p><p> &nbsp;- ALB can directly integrate with OIDC identity providers for authentication</p><p> &nbsp;- This provides the same authentication flow as the API Gateway's JWT authorizer</p><p> &nbsp;- Only authenticated users will reach backend services</p><p>- Consistent Security Model:</p><p> &nbsp;- Both API Gateway and ALB will use the same OIDC provider</p><p> &nbsp;- Provides uniform authentication across all backend services</p><p>- Minimal Configuration:</p><p> &nbsp;- Uses existing infrastructure (ALB and OIDC provider)</p><p> &nbsp;- No additional services needed</p><p> Why Other Options Are Incorrect:</p><p>B (CloudFront Signed URLs):</p><p>- Signed URLs don't integrate with OIDC</p><p>- Would require completely separate authentication system</p><p>- \"Permissive signing policy\" contradicts security requirements</p><p>C (AWS WAF):</p><p>- WAF can't natively authenticate users</p><p>- Would require complex rules to inspect authentication tokens</p><p>- Not the proper tool for authentication</p><p>D (CloudTrail + Lambda):</p><p>- Reactive rather than preventive approach</p><p>- Significant latency in blocking unauthenticated requests</p><p>- Overly complex solution for the requirement</p><p> Conclusion:</p><p>Option A provides the most secure, efficient, and maintainable solution by:</p><p>1. Using native ALB-OIDC integration</p><p>2. Providing consistent authentication across all services</p><p>3. Requiring minimal additional configuration</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6035bff5721849a7b09f7be7fd637907",
            "questionNumber": 419,
            "type": "single",
            "content": "<p>Question #419</p><p>A company creates an AWS Control Tower landing zone to manage and govern a multi-account AWS environment. The company's security team will deploy preventive controls and detective controls to monitor AWS services across all the accounts. The security team needs a centralized view of the security state of all the accounts.</p><p><br>Which solution will meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "From the AWS Control Tower management account, use AWS CloudFormation StackSets to deploy an AWS Config conformance pack to all accounts in the organization."
                },
                {
                    "label": "B",
                    "content": "Enable Amazon Detective for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Detective."
                },
                {
                    "label": "C",
                    "content": "From the AWS Control Tower management account, deploy an AWS CloudFormation stack set that uses the automatic deployment option to enable Amazon Detective for the organization."
                },
                {
                    "label": "D",
                    "content": "Enable AWS Security Hub for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Security Hub."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Enable AWS Security Hub for the organization in AWS Organizations. Designate one AWS account as the delegated administrator for Security Hub.</p><p>This solution will meet the requirements for providing a centralized view of the security state across all accounts in the AWS Control Tower landing zone for the following reasons:</p><p>Comprehensive Security Visibility: AWS Security Hub provides a comprehensive view of security alerts and compliance status across all AWS accounts in an organization. It aggregates, organizes, and prioritizes security findings from multiple AWS services (such as Amazon GuardDuty, Amazon Inspector, and Amazon Macie) and AWS Partner solutions.</p><p>Centralized Management: By enabling Security Hub at the organization level and designating a delegated administrator account, the security team can manage security findings, conduct investigations, and take remediation actions from a single account rather than having to switch between multiple accounts.</p><p>Integration with Control Tower: Security Hub integrates seamlessly with AWS Control Tower and complements its governance capabilities. While Control Tower provides the landing zone and guardrails, Security Hub provides the ongoing security monitoring and compliance checking.</p><p>Support for Both Preventive and Detective Controls: Security Hub supports both types of controls mentioned in the requirements:</p><p>It can monitor preventive controls implemented through AWS Organizations SCPs and Control Tower guardrails</p><p>It provides detective controls through its security standards and checks</p><p>Automated Compliance Checking: Security Hub automatically checks resources against industry standards and best practices (such as CIS AWS Foundations Benchmark, AWS Foundational Security Best Practices, and PCI DSS).</p><p>Option C (deploying Amazon Detective via CloudFormation stack set) is less suitable because:</p><p>Narrower Focus: While Detective is powerful for investigating security findings, it's primarily designed for deep analysis of specific security issues rather than providing a comprehensive overview of security posture across accounts.</p><p>Limited Integration: Detective doesn't provide the same breadth of integration with security standards and compliance frameworks as Security Hub does.</p><p>Different Purpose: Detective is more focused on investigation after a security finding has been identified, rather than providing an overall security posture assessment and continuous monitoring.</p><p>Complementary Service: Detective works best as a complementary service to Security Hub rather than as the primary tool for centralized security monitoring.</p><p>By enabling AWS Security Hub for the organization and designating a delegated administrator account, the security team will have the centralized view they need to effectively monitor and manage security across all accounts in their AWS Control Tower landing zone.</p><p>Sources</p><p>Control services - Management and Governance Cloud Environment Guide （https://docs.aws.amazon.com/wellarchitected/latest/management-and-governance-guide/aws-controls-and-guardrails-services.html）</p><p>Security incident response for a multi-account architecture - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/transitioning-to-multiple-aws-accounts/security-incident-response.html）</p><p>Managing the multi-account environment using AWS Organizations and AWS Control Tower | AWS Cloud Operations & Migrations Blog （https://aws.amazon.com/cn/blogs/mt/managing-the-multi-account-environment-using-aws-organizations-and-aws-control-tower/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2b22056ddeaf4e78b89dc518de848880",
            "questionNumber": 420,
            "type": "single",
            "content": "<p>Question #420</p><p>A company that develops consumer electronics with offices in Europe and Asia has 60 TB of software images stored on premises in Europe. The company wants to transfer the images to an Amazon S3 bucket in the ap-northeast-1 Region. New software images are created daily and must be encrypted in transit. The company needs a solution that does not require custom development to automatically transfer all existing and new software images to Amazon S3.<br><br>Which solution will meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy an AWS DataSync agent and configure a task to transfer the images to the S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Configure Amazon Kinesis Data Firehose to transfer the images using S3 Transfer Acceleration."
                },
                {
                    "label": "C",
                    "content": "Use an AWS Snowball device to transfer the images with the S3 bucket as the target."
                },
                {
                    "label": "D",
                    "content": "Transfer the images over a Site-to-Site VPN connection using the S3 API with multipart upload."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Deploy an AWS DataSync agent and configure a task to transfer the images to the S3 bucket.</p><p>This solution is the most appropriate next step in the transfer process for the following reasons:</p><p>Volume of Data: With 60 TB of software images to transfer, AWS DataSync is designed to handle large-scale data transfers efficiently between on-premises storage and AWS services.</p><p>Encryption in Transit: DataSync automatically encrypts data in transit using TLS 1.2, meeting the company's requirement for encryption during transfer.</p><p>No Custom Development: DataSync is a fully managed service with a straightforward setup process that doesn't require any custom code development. It provides a graphical interface and CLI for configuration.</p><p>Automatic Transfer of New Files: DataSync can be scheduled to run on a recurring basis, which will automatically transfer new software images as they are created daily. It can detect and transfer only the new or changed files.</p><p>Cross-Region Capability: DataSync can efficiently transfer data from Europe (on-premises) to the ap-northeast-1 Region (Asia Pacific - Tokyo) as specified in the requirements.</p><p>Optimized for WAN: DataSync uses a protocol that's optimized for WAN transfers, which is important for transferring large amounts of data across continents (Europe to Asia).</p><p>Let's examine why the other options are less suitable:</p><p>Option B (Kinesis Data Firehose with S3 Transfer Acceleration) is not appropriate because:</p><p>Kinesis Data Firehose is designed for streaming data, not for bulk file transfers</p><p>It would require custom development to integrate with the company's systems</p><p>Not ideal for transferring large files like software images</p><p>Option C (AWS Snowball device) is not the best choice because:</p><p>While good for the initial transfer, it doesn't provide an automated solution for transferring new images created daily</p><p>It's a manual process requiring physical device handling</p><p>Not suitable for ongoing, regular transfers of new data</p><p>Option D (Site-to-Site VPN with S3 API) is not optimal because:</p><p>It would require setting up and managing VPN infrastructure</p><p>Would likely need custom scripts to automate the transfer process</p><p>Less efficient for large-scale transfers compared to DataSync</p><p>AWS DataSync provides the most comprehensive solution that meets all the stated requirements: handling the large volume of data, encrypting data in transit, requiring no custom development, and automatically transferring both existing and new software images to the S3 bucket in ap-northeast-1.</p><p>Sources</p><p>Using AWS DataSync to transfer differentials after Snowball device is transferred to S3 | AWS re:Post （https://repost.aws/questions/QUXziUu0T0QFKmQWu9N_lb4g/using-aws-datasync-to-transfer-differentials-after-snowball-device-is-transferred-to-s3）</p><p>How can we transfer TBs of postgresql from on prem to s3 | AWS re:Post （https://repost.aws/questions/QUHzSzZG16QjOAho5BZvDUfA/how-can-we-transfer-tbs-of-postgresql-from-on-prem-to-s3）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7fbf1c849b9b45cbae42d675bea0dc30",
            "questionNumber": 421,
            "type": "single",
            "content": "<p>Question #421</p><p>A company has a web application that uses Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. A recent marketing campaign has increased demand. Monitoring software reports that many requests have significantly longer response times than before the marketing campaign.<br><br>A solutions architect enabled Amazon CloudWatch Logs for API Gateway and noticed that errors are occurring on 20% of the requests. In CloudWatch, the Lambda function Throttles metric represents 1% of the requests and the Errors metric represents 10% of the requests. Application logs indicate that, when errors occur, there is a call to DynamoDB.<br><br>What change should the solutions architect make to improve the current response times as the web application becomes more popular? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Increase the concurrency limit of the Lambda function."
                },
                {
                    "label": "B",
                    "content": "Implement DynamoDB auto scaling on the table."
                },
                {
                    "label": "C",
                    "content": "Increase the API Gateway throttle limit."
                },
                {
                    "label": "D",
                    "content": "Re-create the DynamoDB table with a better-partitioned primary index."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Implement DynamoDB auto scaling on the table.</p><p>This solution will improve the current response times as the web application becomes more popular for the following reasons:</p><p>Root Cause Analysis: The monitoring data provides important clues about where the bottleneck is occurring:</p><p>20% of API Gateway requests have errors</p><p>Lambda function has 1% throttles and 10% errors</p><p>Application logs show that errors occur when there's a call to DynamoDB</p><p>This pattern strongly suggests that DynamoDB is the primary bottleneck in the system, not Lambda concurrency.</p><p>Error Distribution: While Lambda is experiencing some throttling (1%), the majority of Lambda errors (10%) are occurring during DynamoDB operations. This indicates that the Lambda function is failing when it tries to interact with DynamoDB, likely due to DynamoDB throttling or capacity limitations.</p><p>Scaling Solution: DynamoDB auto scaling automatically adjusts the provisioned throughput capacity of your table based on actual traffic patterns. When implemented:</p><p>It will increase read and write capacity units during the high-demand periods caused by the marketing campaign</p><p>It will decrease capacity during periods of lower usage to optimize costs</p><p>It will help prevent throttling of DynamoDB operations that are currently causing errors</p><p>End-to-End Improvement: By addressing the DynamoDB bottleneck, you'll see cascading improvements:</p><p>Fewer DynamoDB throttling errors</p><p>Reduced Lambda function errors (since most errors occur during DynamoDB calls)</p><p>Improved API Gateway response times and fewer errors returned to clients</p><p>Option A (increasing the Lambda function concurrency limit) would only address the 1% of requests that are being throttled at the Lambda level. This would not solve the larger issue where 10% of Lambda executions are failing during DynamoDB operations. Increasing Lambda concurrency might actually make the problem worse by allowing more concurrent requests to hit DynamoDB, potentially increasing the throttling at the database level.</p><p>By implementing DynamoDB auto scaling, the solutions architect will address the root cause of the performance issues, allowing the web application to handle the increased demand from the marketing campaign and scale appropriately as it becomes more popular.</p><p>Sources</p><p>AWS REST apigateway throws 502 even though integration timeout is set to 59000 ms | AWS re:Post （https://repost.aws/questions/QUNv9DIMb4Tv-ySS25pdvIaw/aws-rest-apigateway-throws-502-even-though-integration-timeout-is-set-to-59000-ms）</p><p>Lambda scaling behavior - AWS Lambda （https://docs.aws.amazon.com/lambda/latest/dg/scaling-behavior.html）</p><p>Scale lambdas to tens of thousands concurrent runnings | AWS re:Post （https://repost.aws/questions/QUjv5PAE_dQ56EXvEpkJ1WTQ/scale-lambdas-to-tens-of-thousands-concurrent-runnings）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "466dea7454eb4f08bdd33bdead35e527",
            "questionNumber": 422,
            "type": "single",
            "content": "<p>Question #422</p><p>A company has an application that has a web frontend. The application runs in the company's on-premises data center and requires access to file storage for critical data. The application runs on three Linux VMs for redundancy. The architecture includes a load balancer with HTTP request-based routing.<br><br></p><p>The company needs to migrate the application to AWS as quickly as possible. The architecture on AWS must be highly available.<br><br>Which solution will meet these requirements with the FEWEST changes to the architecture? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type in three Availability Zones. Use Amazon S3 to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers."
                },
                {
                    "label": "B",
                    "content": "Migrate the application to Amazon EC2 instances in three Availability Zones. Use Amazon Elastic File System (Amazon EFS) for file storage. Mount the file storage on all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances."
                },
                {
                    "label": "C",
                    "content": "Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use the Fargate launch type in three Availability Zones. Use Amazon FSx for Lustre to provide file storage for all three containers. Use a Network Load Balancer to direct traffic to the containers."
                },
                {
                    "label": "D",
                    "content": "Migrate the application to Amazon EC2 instances in three AWS Regions. Use Amazon Elastic Block Store (Amazon EBS) for file storage. Enable Cross-Region Replication (CRR) for all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Migrate the application to Amazon EC2 instances in three Availability Zones. Use Amazon Elastic File System (Amazon EFS) for file storage. Mount the file storage on all three EC2 instances. Use an Application Load Balancer to direct traffic to the EC2 instances.</p><p>This solution will meet the requirements with the fewest changes to the architecture for the following reasons:</p><p>Direct Mapping of Current Architecture:</p><p>The current architecture has three Linux VMs for redundancy, which maps directly to three EC2 instances in AWS.</p><p>The on-premises load balancer with HTTP request-based routing maps directly to an Application Load Balancer (ALB) in AWS, which specializes in HTTP/HTTPS traffic routing.</p><p>The shared file storage requirement is fulfilled by Amazon EFS, which can be mounted on all EC2 instances simultaneously.</p><p>High Availability:</p><p>Placing the EC2 instances across three Availability Zones provides high availability and fault tolerance.</p><p>Amazon EFS is designed to be highly available by default, with data stored redundantly across multiple Availability Zones.</p><p>The Application Load Balancer automatically distributes incoming traffic across multiple Availability Zones and only routes traffic to healthy instances.</p><p>Minimal Changes:</p><p>This approach maintains the same basic architecture: three servers behind a load balancer with shared file storage.</p><p>The application code likely requires no changes since it will still run on Linux instances with the same file system mount points.</p><p>The load balancing logic remains similar, with HTTP request-based routing capabilities preserved in the ALB.</p><p>Quick Migration Path:</p><p>EC2 instances can be created using AMIs that closely match the on-premises Linux environment.</p><p>Tools like AWS Application Migration Service (formerly CloudEndure Migration) can help migrate the existing VMs to EC2 with minimal downtime.</p><p>EFS provides a standard NFS interface that the application can use without code changes.</p><p>Operational Simplicity:</p><p>No need to refactor the application for containers or serverless architecture.</p><p>Familiar Linux administration and troubleshooting procedures can be maintained.</p><p>Standard mounting procedures for NFS file systems apply to EFS.</p><p>This solution provides a \"lift and shift\" approach that meets the requirement to migrate as quickly as possible while ensuring high availability in the AWS environment. It minimizes the architectural changes required, allowing the company to maintain a similar operational model while gaining the benefits of AWS's infrastructure reliability and scalability.</p><p>Sources</p><p>Moving to an AWS architecture - Hosting Static Websites on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-static-websites-aws/moving-to-an-aws-architecture.html）</p><p>Set up a highly available PeopleSoft architecture on AWS - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/set-up-a-highly-available-peoplesoft-architecture-on-aws.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "187b807aa1af4cb0a7d0e99358780c5d",
            "questionNumber": 423,
            "type": "single",
            "content": "<p>Question #423</p><p>A company is planning to migrate an on-premises data center to AWS. The company currently hosts the data center on Linux-based VMware VMs.A solutions architect must collect information about network dependencies between the VMs. The information must be in the form of a diagram that details host IP addresses, hostnames, and network connection information.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Application Discovery Service. Select an AWS Migration Hub home AWS Region. Install the AWS Application Discovery Agent on the on-premises servers for data collection. Grant permissions to Application Discovery Service to use the Migration Hub network diagrams."
                },
                {
                    "label": "B",
                    "content": "Use the AWS Application Discovery Service Agentless Collector for server data collection. Export the network diagrams from the AWS Migration Hub in .png format."
                },
                {
                    "label": "C",
                    "content": "Install the AWS Application Migration Service agent on the on-premises servers for data collection. Use AWS Migration Hub data in Workload Discovery on AWS to generate network diagrams."
                },
                {
                    "label": "D",
                    "content": "Install the AWS Application Migration Service agent on the on-premises servers for data collection. Export data from AWS Migration Hub in .csv format into an Amazon CloudWatch dashboard to generate network diagrams."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Use AWS Application Discovery Service. Select an AWS Migration Hub home AWS Region. Install the AWS Application Discovery Agent on the on-premises servers for data collection. Grant permissions to Application Discovery Service to use the Migration Hub network diagrams.</p><p>This solution will meet the requirements for collecting network dependency information between the Linux-based VMware VMs for the following reasons:</p><p>Comprehensive Data Collection: The AWS Application Discovery Agent provides detailed information about each server, including IP addresses, hostnames, and network connections. When installed directly on the Linux VMs, the agent can capture real-time network traffic and connection details that are essential for creating accurate dependency diagrams.</p><p>Detailed Network Dependency Mapping: The agent-based approach collects more granular data about network communications between servers, including connection frequency, ports used, and data transfer volumes. This level of detail is crucial for understanding the complete picture of how the VMs interact with each other.</p><p>Migration Hub Network Diagrams: By granting permissions to Application Discovery Service to use Migration Hub network diagrams, the company can visualize the collected data in diagram form, which directly meets the requirement for information \"in the form of a diagram that details host IP addresses, hostnames, and network connection information.\"</p><p>Accuracy for Linux-Based Systems: The Application Discovery Agent is specifically designed to work with Linux systems and can collect OS-specific information that might be missed by agentless approaches.</p><p>Option B (using the AWS Application Discovery Service Agentless Collector) is less suitable because:</p><p>Limited Network Visibility: The Agentless Collector primarily gathers information through VMware vCenter and doesn't have the same level of visibility into the actual network traffic between VMs as the agent-based approach does.</p><p>Less Detailed Connection Information: While the Agentless Collector can identify some connections between servers, it may not capture all the network dependency details required, especially for complex applications with dynamic communications.</p><p>Export Limitations: Simply exporting network diagrams in .png format doesn't ensure that all the required information (IP addresses, hostnames, and network connection details) will be included in a comprehensive and usable way.</p><p>The agent-based approach provides the most thorough and accurate network dependency information, which is critical for planning a successful migration from on-premises to AWS. By understanding these dependencies in detail, the solutions architect can ensure that all necessary connections are maintained when the workloads are moved to AWS, minimizing the risk of application failures after migration.</p><p>Sources</p><p>Welcome - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/APIReference/Welcome.html）</p><p>What is AWS Application Discovery Service? - AWS Application Discovery Service （https://docs.aws.amazon.com/application-discovery/latest/userguide/what-is-appdiscovery.html）</p><p>Viewing network connections in AWS Migration Hub - AWS Migration Hub （https://docs.aws.amazon.com/migrationhub/latest/ug/network-diagram.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "03decf19560b45fbacf781c56d6d95d0",
            "questionNumber": 424,
            "type": "single",
            "content": "<p>Question #424</p><p>A company runs a software-as-a-service (SaaS) application on AWS. The application consists of AWS Lambda functions and an Amazon RDS for MySQL Multi-AZ database. During market events, the application has a much higher workload than normal. Users notice slow response times during the peak periods because of many database connections. The company needs to improve the scalable performance and availability of the database.<br><br>Which solution meets these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon CloudWatch alarm action that triggers a Lambda function to add an Amazon RDS for MySQL read replica when resource utilization hits a threshold."
                },
                {
                    "label": "B",
                    "content": "Migrate the database to Amazon Aurora, and add a read replica. Add a database connection pool outside of the Lambda handler function."
                },
                {
                    "label": "C",
                    "content": "Migrate the database to Amazon Aurora, and add a read replica. Use Amazon Route 53 weighted records."
                },
                {
                    "label": "D",
                    "content": "Migrate the database to Amazon Aurora, and add an Aurora Replica. Configure Amazon RDS Proxy to manage database connection pools."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Migrate the database to Amazon Aurora, and add an Aurora Replica. Configure Amazon RDS Proxy to manage database connection pools.</p><p>This solution best meets the requirements for improving the scalable performance and availability of the database for the following reasons:</p><p>Connection Management: The primary issue identified is \"slow response times during the peak periods because of many database connections.\" RDS Proxy directly addresses this by efficiently managing database connection pools. It maintains a pool of established database connections that can be reused by Lambda functions, which significantly reduces the connection overhead that's causing the performance issues.</p><p>Serverless Integration: Since the application uses AWS Lambda functions, which can scale rapidly during market events, RDS Proxy provides an ideal solution for handling the connection surge. Lambda functions create and destroy connections frequently, which can overwhelm a database. RDS Proxy acts as a buffer between the Lambda functions and the database.</p><p>Improved Scalability: Amazon Aurora itself offers better performance and scalability compared to standard RDS for MySQL. Adding an Aurora Replica further enhances read scalability by distributing read operations across multiple instances.</p><p>Enhanced Availability: Aurora's architecture already provides better availability than standard RDS MySQL. The combination of Aurora's built-in replication, Aurora Replicas, and RDS Proxy's faster failover capabilities (reducing failover time by up to 66%) creates a highly available solution.</p><p>No Application Code Changes: This solution can be implemented without significant changes to the application code, making it easier and faster to deploy.</p><p>Option C (Migrating to Aurora with a read replica and using Route 53 weighted records) is less effective because:</p><p>It doesn't address the core issue of connection management. Even with weighted routing, the database would still face the same connection overhead problems during peak periods.</p><p>Using Route 53 weighted records for database connections would require application-level changes to handle different connection endpoints for reads and writes, adding complexity.</p><p>Route 53 weighted records are typically used for distributing traffic across different application instances, not for managing database connections within a single application.</p><p>By migrating to Amazon Aurora with an Aurora Replica and implementing RDS Proxy, the company will directly address the connection bottleneck that's causing slow response times during peak periods, while also improving the overall scalability and availability of their database infrastructure.</p><p>Sources</p><p>Amazon RDS Proxy - Oracle to Aurora MySQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/oracle-to-aurora-mysql-migration-playbook/chap-oracle-aurora-mysql.tools.rdsproxy.html）</p><p>Amazon RDS Proxy - Oracle to Aurora PostgreSQL Migration Playbook （https://docs.aws.amazon.com/dms/latest/oracle-to-aurora-postgresql-migration-playbook/chap-oracle-aurora-pg.tools.rdsproxy.html）</p><p>Highly Available Database Proxy – Amazon RDS Proxy – Amazon Web Services （https://aws.amazon.com/cn/rds/proxy/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "353f91ddda3f4efaa771037e62d8fed4",
            "questionNumber": 425,
            "type": "single",
            "content": "<p>Question #425</p><p>A company is planning to migrate an application from on premises to the AWS Cloud. The company will begin the migration by moving the application’s underlying data storage to AWS. The application data is stored on a shared file system on premises, and the application servers connect to the shared file system through SMB.</p><p><br></p><p>A solutions architect must implement a solution that uses an Amazon S3 bucket for shared storage. Until the application is fully migrated and code is rewritten to use native Amazon S3 APIs, the application must continue to have access to the data through SMB. The solutions architect must migrate the application data to AWS to its new location while still allowing the on-premises application to access the data.<br><br>Which solution will meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new Amazon FSx for Windows File Server file system. Configure AWS DataSync with one location for the on-premises file share and one location for the new Amazon FSx file system. Create a new DataSync task to copy the data from the on-premises file share location to the Amazon FSx file system."
                },
                {
                    "label": "B",
                    "content": "Create an S3 bucket for the application. Copy the data from the on-premises storage to the S3 bucket."
                },
                {
                    "label": "C",
                    "content": "Deploy an AWS Server Migration Service (AWS SMS) VM to the on-premises environment. Use AWS SMS to migrate the file storage server from on premises to an Amazon EC2 instance."
                },
                {
                    "label": "D",
                    "content": "Create an S3 bucket for the application. Deploy a new AWS Storage Gateway file gateway on an on-premises VM. Create a new file share that stores data in the S3 bucket and is associated with the file gateway. Copy the data from the on-premises storage to the new file gateway endpoint."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The requirements are:</p><p>1. Migrate application data to Amazon S3 - Need to move from on-premises file share to S3</p><p>2. Maintain SMB access during migration - Application still needs SMB protocol access until rewritten</p><p>3. Allow on-premises access during transition - Existing on-premises servers must continue accessing the data</p><p> Why Option D is Correct:</p><p>- Storage Gateway File Gateway:</p><p> &nbsp;- Provides SMB/NFS interface to on-premises applications</p><p> &nbsp;- Stores data natively in Amazon S3</p><p> &nbsp;- Allows seamless transition to cloud storage</p><p>- Data Migration Path:</p><p> &nbsp;- Files can be copied from existing on-premises share to the new gateway endpoint</p><p> &nbsp;- Applications continue using same SMB paths during migration</p><p>- Future Ready:</p><p> &nbsp;- Once application is rewritten, it can access S3 directly</p><p> &nbsp;- No need for further storage migration</p><p> Why Other Options Are Incorrect:</p><p>A (FSx + DataSync):</p><p>- Migrates to FSx instead of S3</p><p>- Doesn't meet requirement to use S3 as primary storage</p><p>- More expensive than needed solution</p><p>B (Direct to S3):</p><p>- Doesn't provide SMB access during transition</p><p>- Would break existing application functionality</p><p>C (AWS SMS):</p><p>- Migrates entire VM rather than just storage</p><p>- Doesn't achieve S3 storage goal</p><p>- More complex than necessary</p><p> Key Benefits of Solution D:</p><p>1. Meets immediate SMB access requirement</p><p>2. Achieves S3 storage target</p><p>3. Provides smooth migration path</p><p>4. Minimizes operational disruption</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "04de3187b49e4255bdffd5abba45f5b8",
            "questionNumber": 426,
            "type": "single",
            "content": "<p>Question #426</p><p>A global company has a mobile app that displays ticket barcodes. Customers use the tickets on the mobile app to attend live events. Event scanners read the ticket barcodes and call a backend API to validate the barcode data against data in a database. After the barcode is scanned, the backend logic writes to the database's single table to mark the barcode as used.</p><p><br></p><p>The company needs to deploy the app on AWS with a DNS name of api.example.com. The company will host the database in three AWS Regions around the world.<br><br>Which solution will meet these requirements with the LOWEST latency? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Host the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Container Service (Amazon ECS) clusters that are in the same Regions as the database. Create an accelerator in AWS Global Accelerator to route requests to the nearest ECS cluster. Create an Amazon Route 53 record that maps api.example.com to the accelerator endpoint."
                },
                {
                    "label": "B",
                    "content": "Host the database on Amazon Aurora global database clusters. Host the backend on three Amazon Elastic Kubernetes Service (Amazon EKS) clusters that are in the same Regions as the database. Create an Amazon CloudFront distribution with the three clusters as origins. Route requests to the nearest EKS cluster. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution."
                },
                {
                    "label": "C",
                    "content": "Host the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a CloudFront function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution."
                },
                {
                    "label": "D",
                    "content": "Host the database on Amazon DynamoDB global tables. Create an Amazon CloudFront distribution. Associate the CloudFront distribution with a Lambda@Edge function that contains the backend logic to validate the barcodes. Create an Amazon Route 53 record that maps api.example.com to the CloudFront distribution."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Option D is the correct answer. By hosting the database on Amazon DynamoDB global tables, the company can ensure that the database is replicated across multiple AWS Regions, providing low latency access to the data. Using Lambda@Edge, the backend logic can be executed close to the end-users in AWS Lambda's edge locations, which are integrated with CloudFront. This setup can offer the lowest latency for the mobile app users while also allowing the company to route requests to the nearest region using Amazon Route 53.</p><p>The requirements are:</p><p>1. Global low-latency access - Need to serve customers worldwide with minimal delay</p><p>2. Database in 3 Regions - Data must be globally distributed</p><p>3. Barcode validation API - Requires backend logic to validate and update ticket status</p><p>4. DNS name api.example.com - Needs proper routing</p><p> Why Option D is Correct:</p><p>- DynamoDB Global Tables:</p><p> &nbsp;- Provides multi-region, multi-active database</p><p> &nbsp;- Automatically syncs data across regions</p><p> &nbsp;- Single-digit millisecond latency for reads/writes</p><p>- Lambda@Edge:</p><p> &nbsp;- Runs validation logic at edge locations</p><p> &nbsp;- Processes requests closest to users</p><p> &nbsp;- Can read/write to DynamoDB global tables</p><p>- CloudFront + Route 53:</p><p> &nbsp;- Provides global distribution</p><p> &nbsp;- Routes to nearest edge location</p><p> &nbsp;- Simple DNS configuration</p><p> Why Other Options Are Incorrect:</p><p>A (Aurora Global + ECS + Global Accelerator):</p><p>- Aurora Global has higher replication lag than DynamoDB</p><p>- ECS clusters add management overhead</p><p>- Global Accelerator doesn't provide edge computing</p><p>B (Aurora Global + EKS + CloudFront):</p><p>- Same Aurora limitations as Option A</p><p>- EKS adds unnecessary complexity</p><p>- CloudFront can't route to EKS directly</p><p>C (CloudFront Functions):</p><p>- Functions have severe limitations (no DB access)</p><p>- Can't perform database writes</p><p>- Execution time limited to 1ms</p><p> Key Advantages of Solution D:</p><p>1. True global low-latency with edge processing</p><p>2. Fully serverless architecture</p><p>3. Simple data synchronization with DynamoDB</p><p>4. Automatic scaling for event spikes</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "0b668975484844199e699ab1e867b88a",
            "questionNumber": 427,
            "type": "single",
            "content": "<p>Question #427</p><p>A medical company is running a REST API on a set of Amazon EC2 instances. The EC2 instances run in an Auto Scaling group behind an Application Load Balancer (ALB). The ALB runs in three public subnets, and the EC2 instances run in three private subnets. The company has deployed an Amazon CloudFront distribution that has the ALB as the only origin.<br><br>Which solution should a solutions architect recommend to enhance the origin security? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Store a random string in AWS Secrets Manager. Create an AWS Lambda function for automatic secret rotation. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Create an AWS WAF web ACL rule with a string match rule for the custom header. Associate the web ACL with the ALB."
                },
                {
                    "label": "B",
                    "content": "Create an AWS WAF web ACL rule with an IP match condition of the CloudFront service IP address ranges. Associate the web ACL with the ALB. Move the ALB into the three private subnets."
                },
                {
                    "label": "C",
                    "content": "Store a random string in AWS Systems Manager Parameter Store. Configure Parameter Store automatic rotation for the string. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Inspect the value of the custom HTTP header, and block access in the ALB."
                },
                {
                    "label": "D",
                    "content": "Configure AWS Shield Advanced. Create a security group policy to allow connections from CloudFront service IP address ranges. Add the policy to AWS Shield Advanced, and attach the policy to the ALB."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Store a random string in AWS Secrets Manager. Create an AWS Lambda function for automatic secret rotation. Configure CloudFront to inject the random string as a custom HTTP header for the origin request. Create an AWS WAF web ACL rule with a string match rule for the custom header. Associate the web ACL with the ALB.</p><p>This solution enhances origin security through a custom header verification approach, which is considered a best practice for securing CloudFront to ALB communication. Here's why this is the better solution:</p><p>Custom Origin Header Authentication: By configuring CloudFront to add a secret custom header to requests sent to the ALB, you create a verification mechanism that ensures only requests coming through CloudFront are processed by the ALB. This prevents direct access to the ALB, even if someone discovers its DNS name.</p><p>Dynamic Secret Management: Storing the random string in AWS Secrets Manager with automatic rotation via Lambda ensures that the authentication mechanism remains secure over time. Even if the header value is somehow compromised, it will be automatically changed on a regular basis.</p><p>Defense in Depth: This approach implements multiple layers of security - the secret itself, the custom header mechanism, and the WAF rule that validates the header.</p><p>Maintains Existing Architecture: This solution doesn't require moving the ALB from public to private subnets, which could potentially require significant architectural changes.</p><p>Option B (creating a WAF rule based on CloudFront IP ranges and moving the ALB to private subnets) has several limitations:</p><p>CloudFront IP Ranges Change: CloudFront operates from many edge locations with IP addresses that can change over time. Maintaining an accurate list of all CloudFront IP addresses in WAF rules would require frequent updates.</p><p>Less Secure Authentication: IP-based filtering alone is generally considered less secure than cryptographic verification methods like secret headers.</p><p>Architectural Changes: Moving the ALB from public to private subnets would require additional components like NAT gateways for the EC2 instances to access the internet, potentially increasing complexity and cost.</p><p>The custom header approach (Option A) is a widely recommended best practice by AWS for securing communication between CloudFront and origin servers. It provides strong security through cryptographic verification rather than relying solely on network-level controls, making it the more robust solution for enhancing origin security in this scenario.</p><p>Sources</p><p>Custom origin with CloudFront - Secure Content Delivery with Amazon CloudFront （https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/SecurityAndPrivateContent.html）</p><p>WAF on ALB or CloudFront ? | AWS re:Post （https://repost.aws/questions/QUTY5hPVxgS6Caa3eZHX7-nQ/waf-on-alb-or-cloudfront）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3a400293bc29404db313525c643f9ce6",
            "questionNumber": 428,
            "type": "single",
            "content": "<p>Question #428</p><p>To abide by industry regulations, a solutions architect must design a solution that will store a company's critical data in multiple public AWS Regions, including in the United States, where the company's headquarters is located. The solutions architect is required to provide access to the data stored in AWS to the company’s global WAN network. The security team mandates that no traffic accessing this data should traverse the public internet.<br><br>How should the solutions architect design a highly available solution that meets the requirements and is cost-effective? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Establish AWS Direct Connect connections from the company headquarters to all AWS Regions in use. Use the company WAN to send traffic over to the headquarters and then to the respective DX connection to access the data."
                },
                {
                    "label": "B",
                    "content": "Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use inter-region VPC peering to access the data in other AWS Regions."
                },
                {
                    "label": "C",
                    "content": "Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use an AWS transit VPC solution to access data in other AWS Regions."
                },
                {
                    "label": "D",
                    "content": "Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use Direct Connect Gateway to access data in other AWS Regions."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Establish two AWS Direct Connect connections from the company headquarters to an AWS Region. Use the company WAN to send traffic over a DX connection. Use Direct Connect Gateway to access data in other AWS Regions.</p><p>This solution meets all the requirements for the following reasons:</p><p>Multi-Region Data Storage: The solution supports storing critical data in multiple public AWS Regions, including in the United States where the company's headquarters is located, which satisfies the industry regulation requirement.</p><p>Private Network Access: By using AWS Direct Connect, the company establishes a dedicated private connection between their headquarters and AWS, ensuring that no traffic traverses the public internet as mandated by the security team.</p><p>High Availability: The solution includes two Direct Connect connections from the company headquarters to an AWS Region, providing redundancy and high availability. If one connection fails, traffic can still flow through the second connection.</p><p>Global Access: The company can leverage their existing global WAN network to route traffic to the Direct Connect connections at headquarters. This allows offices worldwide to access the AWS resources through the company's private network.</p><p>Multi-Region Connectivity: Direct Connect Gateway is a global service that enables the company to access multiple AWS Regions through their Direct Connect connections. This means they can access data stored in any connected Region without having to establish separate Direct Connect connections to each Region.</p><p>Cost-Effectiveness: This solution is cost-effective because:</p><p>It leverages the existing company WAN infrastructure</p><p>It requires Direct Connect connections only at the headquarters location, not at every global office</p><p>Direct Connect Gateway eliminates the need for separate connections to each AWS Region</p><p>Traffic between Regions stays on AWS's private global network, avoiding data transfer charges associated with public internet traffic</p><p>The solution creates a secure, private network path from the company's global offices to multiple AWS Regions:</p><p>Global offices connect to the company headquarters via the existing global WAN</p><p>Headquarters connects to AWS via redundant Direct Connect connections</p><p>Direct Connect Gateway enables access to multiple AWS Regions</p><p>All traffic remains on private networks (company WAN and AWS global network), never traversing the public internet</p><p>This architecture satisfies the industry regulations for multi-region data storage while meeting the security team's requirement for private network access, all while maintaining high availability and cost-effectiveness.</p><p>Sources</p><p>AWS Direct Connect &nbsp;- Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html）</p><p>Direct Connect gateway attachments in AWS Cloud WAN - AWS Network Manager （https://docs.aws.amazon.com/network-manager/latest/cloudwan/cloudwan-dxattach-about.html）</p><p>How Salesforce Business Technology uses AWS Direct Connect SiteLink for reliable global connectivity | Networking & Content Delivery （https://aws.amazon.com/cn/blogs/networking-and-content-delivery/how-salesforce-business-technology-uses-aws-direct-connect-sitelink-for-reliable-global-connectivity/）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4a5a2d6c42af4ed282e87e4d212d8e5e",
            "questionNumber": 429,
            "type": "single",
            "content": "<p>Question #429</p><p>A company has developed an application that is running Windows Server on VMware vSphere VMs that the company hosts on premises. The application data is stored in a proprietary format that must be read through the application. The company manually provisioned the servers and the application.</p><p><br></p><p>As part of its disaster recovery plan, the company wants the ability to host its application on AWS temporarily if the company's on-premises environment becomes unavailable. The company wants the application to return to on-premises hosting after a disaster recovery event is complete. The RPO is 5 minutes.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure AWS DataSync. Replicate the data to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and attach the EBS volumes."
                },
                {
                    "label": "B",
                    "content": "Configure AWS Elastic Disaster Recovery. Replicate the data to replication Amazon EC2 instances that are attached to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use Elastic Disaster Recovery to launch EC2 instances that use the replicated volumes."
                },
                {
                    "label": "C",
                    "content": "Provision an AWS Storage Gateway file gateway. Replicate the data to an Amazon S3 bucket. When the on-premises environment is unavailable, use AWS Backup to restore the data to Amazon Elastic Block Store (Amazon EBS) volumes and launch Amazon EC2 instances from these EBS volumes."
                },
                {
                    "label": "D",
                    "content": "Provision an Amazon FSx for Windows File Server file system on AWS. Replicate the data to the file system. When the on-premises environment is unavailable, use AWS CloudFormation templates to provision Amazon EC2 instances and use AWS::CloudFormation::Init commands to mount the Amazon FSx file shares."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. Configure AWS Elastic Disaster Recovery. Replicate the data to replication Amazon EC2 instances that are attached to Amazon Elastic Block Store (Amazon EBS) volumes. When the on-premises environment is unavailable, use Elastic Disaster Recovery to launch EC2 instances that use the replicated volumes.</p><p> Explanation:</p><p>1. AWS Elastic Disaster Recovery (DRS) is designed specifically for this use case—disaster recovery with minimal operational overhead. It continuously replicates on-premises VMware vSphere VMs (or physical servers) to AWS, maintaining a ready-to-launch state in a low-cost staging area.</p><p>2. RPO of 5 minutes: Elastic Disaster Recovery supports frequent replication (as low as seconds), meeting the 5-minute RPO requirement.</p><p>3. Automated failover and recovery: When a disaster occurs, you can quickly launch fully provisioned EC2 instances from the replicated data with minimal manual intervention.</p><p>4. Return to on-premises: After the disaster is resolved, you can fail back to your on-premises environment.</p><p>5. Least operational overhead: Elastic Disaster Recovery handles the replication, orchestration, and recovery process automatically, reducing manual steps compared to other options.</p><p> Why not the other options?</p><p>- A (AWS DataSync + CloudFormation): DataSync is for data transfer but doesn’t automate VM replication or disaster recovery orchestration. Manual steps for provisioning EC2 instances and attaching EBS volumes increase overhead.</p><p>- C (Storage Gateway + AWS Backup): Storage Gateway is not designed for low-RPO disaster recovery. Restoring from S3 to EBS and launching EC2 instances manually is time-consuming and complex.</p><p>- D (Amazon FSx + CloudFormation): FSx is for file storage, not full VM replication. Mounting file shares doesn’t address the need for application server recovery, and manual steps add overhead.</p><p>B is the best solution because it provides automated, continuous replication and recovery with minimal operational effort.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b1930b7cdd9f42a69f1e4c3668ccd668",
            "questionNumber": 430,
            "type": "single",
            "content": "<p>Question #430</p><p>A company runs a highly available data collection application on Amazon EC2 in the eu-north-1 Region. The application collects data from end-user devices and writes records to an Amazon Kinesis data stream and a set of AWS Lambda functions that process the records. The company persists the output of the record processing to an Amazon S3 bucket in eu-north-1. The company uses the data in the S3 bucket as a data source for Amazon Athena.</p><p><br></p><p>The company wants to increase its global presence. A solutions architect must launch the data collection capabilities in the sa-east-1 and ap-northeast-1 Regions. The solutions architect deploys the application, the Kinesis data stream, and the Lambda functions in the two new Regions.The solutions architect keeps the S3 bucket in eu-north-1 to meet a requirement to centralize the data analysis. </p><p><br></p><p>During testing of the new setup, the solutions architect notices a significant lag on the arrival of data from the new Regions to the S3 bucket.<br><br>Which solution will improve this lag time the MOST?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "In each of the two new Regions, set up the Lambda functions to run in a VPC. Set up an S3 gateway endpoint in that VPC."
                },
                {
                    "label": "B",
                    "content": "Turn on S3 Transfer Acceleration on the S3 bucket in eu-north-1. Change the application to use the new S3 accelerated endpoint when the application uploads data to the S3 bucket."
                },
                {
                    "label": "C",
                    "content": "Create an S3 bucket in each of the two new Regions. Set the application in each new Region to upload to its respective S3 bucket. Set up S3 Cross-Region Replication to replicate data to the S3 bucket in eu-north-1."
                },
                {
                    "label": "D",
                    "content": "Increase the memory requirements of the Lambda functions to ensure that they have multiple cores available. Use the multipart upload feature when the application uploads data to Amazon S3 from Lambda."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Answer: C</p><p>The company is experiencing significant lag when data is uploaded from applications running in new AWS Regions (sa-east-1 and ap-northeast-1) to an S3 bucket in the eu-north-1 Region. The requirement is to minimize this lag while maintaining centralized storage for data analysis in the eu-north-1 S3 bucket.</p><p>Options Analysis:</p><ul><li>A: Setting up Lambda functions in a VPC with an S3 Gateway Endpoint does not address the lag issue. While this can optimize network traffic within a region, it does not improve cross-region data transfer latency.</li><li>B: Enabling S3 Transfer Acceleration in the eu-north-1 bucket can improve upload speed by utilizing Amazon’s globally distributed edge locations to accelerate data transfer. However, this may not fully eliminate the lag compared to replicating data across regions.</li><li>C: Creating an S3 bucket in each new Region (sa-east-1 and ap-northeast-1) allows the application to upload data locally, minimizing latency. Setting up S3 Cross-Region Replication ensures that data is automatically synchronized with the centralized S3 bucket in eu-north-1, maintaining the centralized data storage for analysis. This approach effectively reduces lag while meeting the requirement for centralization.</li><li>D: Increasing memory for Lambda functions or enabling the multipart upload feature may slightly improve upload performance but does not directly address the lag caused by cross-region data transfer.</li></ul><p>Key Benefits of Option C:</p><ul><li>Reduced Latency: Local S3 buckets in the same region as the application minimize the time taken to upload data.</li><li>Centralized Data Storage: S3 Cross-Region Replication ensures that all data is eventually centralized in the eu-north-1 bucket for analysis without manual intervention.</li><li>Scalability: This solution efficiently supports the company’s global expansion without requiring significant changes to existing workflows.</li></ul><p>Thus, Option C is the most effective solution for improving lag time while meeting the requirements.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f15703bfca404512b85487c15e76fcb2",
            "questionNumber": 431,
            "type": "single",
            "content": "<p>Question #431</p><p>A company provides a centralized Amazon EC2 application hosted in a single shared VPC. The centralized application must be accessible from client applications running in the VPCs of other business units. The centralized application front end is configured with a Network Load Balancer (NLB) for scalability.</p><p><br></p><p>Up to 10 business unit VPCs will need to be connected to the shared VPC. Some of the business unit VPC CIDR blocks overlap with the shared VPC, and some overlap with each other. Network connectivity to the centralized application in the shared VPC should be allowed from authorized business unit VPCs only.<br><br></p><p>Which network configuration should a solutions architect use to provide connectivity from the client applications in the business unit VPCs to the centralized application in the shared VPC?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Transit Gateway. Attach the shared VPC and the authorized business unit VPCs to the transit gateway. Create a single transit gateway route table and associate it with all of the attached VPCs. Allow automatic propagation of routes from the attachments into the route table. Configure VPC routing tables to send traffic to the transit gateway."
                },
                {
                    "label": "B",
                    "content": "Create a VPC endpoint service using the centralized application NLB and enable the option to require endpoint acceptance. Create a VPC endpoint in each of the business unit VPCs using the service name of the endpoint service. Accept authorized endpoint requests from the endpoint service console."
                },
                {
                    "label": "C",
                    "content": "Create a VPC peering connection from each business unit VPC to the shared VPC. Accept the VPC peering connections from the shared VPC console. Configure VPC routing tables to send traffic to the VPC peering connection."
                },
                {
                    "label": "D",
                    "content": "Configure a virtual private gateway for the shared VPC and create customer gateways for each of the authorized business unit VPCs. Establish a Site-to-Site VPN connection from the business unit VPCs to the shared VPC. Configure VPC routing tables to send traffic to the VPN connection."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Answer: B</p><p>The company hosts a centralized EC2 application in a shared VPC, fronted by a Network Load Balancer (NLB). Multiple business unit VPCs (up to 10) need connectivity to this centralized application. However, the following challenges must be addressed:</p><ol><li>Overlapping CIDR blocks: Some business unit VPC CIDR blocks overlap with the shared VPC or with each other.</li><li>Restrict access: Only authorized business unit VPCs should be able to connect to the centralized application.</li></ol><p>Options Analysis:</p><ul><li>A: Use AWS Transit Gateway (TGW):A Transit Gateway could connect multiple VPCs, but it does not solve the problem of overlapping CIDR blocks. Transit Gateway operates at the IP layer and cannot manage overlapping IP ranges efficiently.</li><li>B: Create a VPC Endpoint Service using the NLB:Creating a VPC Endpoint Service backed by the NLB allows authorized VPCs to connect to the centralized application without IP layer routing. This solves the overlapping CIDR issue because VPC endpoints use private DNS and do not rely on IP addressing.Requiring endpoint acceptance ensures that only authorized VPCs can connect to the centralized application. This satisfies the requirement to restrict access.This option is scalable and avoids the complexity of managing additional route tables or gateways.</li><li>C: VPC Peering Connections:VPC peering does not support overlapping CIDR blocks, making it unsuitable for this scenario.It would also require manual configuration and maintenance of individual peering connections, making it less scalable.</li><li>D: Site-to-Site VPN:Configuring a Site-to-Site VPN for each business unit VPC is unnecessarily complex and costly for this use case.VPN connections are better suited for on-premises connectivity rather than inter-VPC communication.</li></ul><p>Why Option B is Correct:</p><ol><li>Scalability: VPC Endpoint Services can scale easily to handle multiple client VPCs.</li><li>CIDR Overlap Resolution: VPC Endpoint Services operate at the DNS layer and do not depend on IP routing, which eliminates issues caused by overlapping CIDR blocks.</li><li>Access Control: Requiring endpoint acceptance ensures only authorized business units can connect to the application.</li></ol><p>Conclusion:</p><p>Option B is the most effective solution because it addresses overlapping CIDR blocks, supports access control, and provides a scalable way to connect business unit VPCs to the centralized application.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "e41e7a562c3c48618fa0bd27f421b7a4",
            "questionNumber": 432,
            "type": "single",
            "content": "<p>Question #432</p><p>A company wants to migrate its website to AWS. The website uses microservices and runs on containers that are deployed in an on-premises, self-managed Kubernetes cluster. All the manifests that define the deployments for the containers in the Kubernetes deployment are in source control.</p><p><br></p><p>All data for the website is stored in a PostgreSQL database. An open source container image repository runs alongside the on-premises environment.</p><p><br></p><p>A solutions architect needs to determine the architecture that the company will use for the website on AWS.</p><p><br></p><p>Which solution will meet these requirements with the LEAST effort to migrate?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS App Runner service. Connect the App Runner service to the open source container image repository. Deploy the manifests from on premises to the App Runner service. Create an Amazon RDS for PostgreSQL database."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the manifests from on premises to the EKS cluster. Create an Amazon Aurora PostgreSQL DB cluster."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon Elastic Container Service (Amazon ECS) cluster that has an Amazon EC2 capacity pool. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Register each container image as a new task definition. Configure ECS services for each task definition to match the original Kubernetes deployments. Create an Amazon Aurora PostgreSQL DB cluster."
                },
                {
                    "label": "D",
                    "content": "Rebuild the on-premises Kubernetes cluster by hosting the cluster on Amazon EC2 instances. Migrate the open source container image repository to the EC2 instances. Deploy the manifests from on premises to the new cluster on AWS. Deploy an open source PostgreSQL database on the new cluster."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster that has managed node groups. Copy the application containers to a new Amazon Elastic Container Registry (Amazon ECR) repository. Deploy the manifests from on premises to the EKS cluster. Create an Amazon Aurora PostgreSQL DB cluster.</p><p> Explanation:</p><p>1. Least Effort Migration: &nbsp;</p><p> &nbsp; - The company already uses Kubernetes (self-managed on-premises), so migrating to Amazon EKS (AWS-managed Kubernetes) requires minimal changes. &nbsp;</p><p> &nbsp; - Kubernetes manifests can be reused as-is (no need to rewrite deployments for ECS or App Runner). &nbsp;</p><p>2. Container Management: &nbsp;</p><p> &nbsp; - Moving containers to Amazon ECR (AWS-managed container registry) is straightforward and more secure than maintaining an open-source repository. &nbsp;</p><p>3. Database Migration: &nbsp;</p><p> &nbsp; - Amazon Aurora PostgreSQL is fully compatible with PostgreSQL and offers better performance, scalability, and managed operations compared to self-managed PostgreSQL. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (AWS App Runner): &nbsp;</p><p> &nbsp;- App Runner is a simpler PaaS service but does not support Kubernetes manifests, requiring a rewrite of deployment logic. &nbsp;</p><p> &nbsp;- Tying App Runner to an on-premises container registry is not ideal for AWS migration. &nbsp;</p><p>- C (Amazon ECS): &nbsp;</p><p> &nbsp;- ECS is a good alternative but requires converting Kubernetes manifests to ECS task definitions, increasing migration effort. &nbsp;</p><p>- D (Self-managed Kubernetes on EC2): &nbsp;</p><p> &nbsp;- This does not reduce operational overhead—it just moves the same complexity to AWS (still requires managing EC2, Kubernetes, and PostgreSQL manually). &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B (EKS + ECR + Aurora PostgreSQL) is the best choice because: &nbsp;</p><p>✔ Minimal changes (reuse existing Kubernetes manifests). &nbsp;</p><p>✔ Fully managed services (EKS, ECR, Aurora reduce operational burden). &nbsp;</p><p>✔ Seamless PostgreSQL migration (Aurora is PostgreSQL-compatible). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5437654e68f04a9e91b9f0dff05fb54e",
            "questionNumber": 433,
            "type": "single",
            "content": "<p>Question #433</p><p>A company uses a mobile app on AWS to run online contests. The company selects a winner at random at the end of each contest. The contests run for variable lengths of time. The company does not need to retain any data from a contest after the contest is finished.</p><p><br></p><p>The company uses custom code that is hosted on Amazon EC2 instances to process the contest data and select a winner. The EC2 instances run behind an Application Load Balancer and store contest entries on Amazon RDS DB instances. The company must design a new architecture to reduce the cost of running the contests.<br><br>Which solution will meet these requirements MOST cost-effectively? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate storage of the contest entries to Amazon DynamoDB. Create a DynamoDB Accelerator (DAX) cluster. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. At the end of the contest, delete the DynamoDB table."
                },
                {
                    "label": "B",
                    "content": "Migrate the storage of the contest entries to Amazon Redshift. Rewrite the code as AWS Lambda functions. At the end of the contest, delete the Redshift cluster."
                },
                {
                    "label": "C",
                    "content": "Add an Amazon ElastiCache for Redis cluster in front of the RDS DB instances to cache the contest entries. Rewrite the code to run as Amazon Elastic Container Service (Amazon ECS) containers that use the Fargate launch type. Set the ElastiCache TTL attribute on each entry to expire each entry at the end of the contest."
                },
                {
                    "label": "D",
                    "content": "Migrate the storage of the contest entries to Amazon DynamoDB. Rewrite the code as AWS Lambda functions. Set the DynamoDB TTL attribute on each entry to expire each entry at the end of the contest."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Migrate the storage of the contest entries to Amazon DynamoDB. Rewrite the code as AWS Lambda functions. Set the DynamoDB TTL attribute on each entry to expire each entry at the end of the contest.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>Pay-per-use Model: By migrating from EC2 instances and RDS databases to Lambda functions and DynamoDB, the company will shift from paying for continuously running resources to paying only for what they use. Since contests run for variable lengths of time, this approach eliminates costs during periods of inactivity.</p><p>Automatic Scaling: Lambda functions automatically scale based on the number of incoming requests, which is ideal for handling variable contest loads without overprovisioning resources. Similarly, DynamoDB can scale to accommodate any number of contest entries.</p><p>Automated Data Cleanup: The DynamoDB Time-to-Live (TTL) feature allows for automatic deletion of data after a specified time. This perfectly aligns with the requirement that \"the company does not need to retain any data from a contest after the contest is finished.\" The TTL attribute can be set to expire entries at the end of each contest, eliminating the need for manual cleanup processes.</p><p>Reduced Operational Overhead: Both Lambda and DynamoDB are fully managed services, which means AWS handles the underlying infrastructure, patching, and maintenance. This reduces the operational burden and associated costs.</p><p>No Idle Resources: With the serverless approach, there are no idle resources to pay for between contests or during periods of low activity. The company only pays for actual compute time used by Lambda and the actual storage and throughput consumed in DynamoDB.</p><p>Simplified Architecture: The serverless architecture eliminates the need to manage load balancers, EC2 instances, and RDS databases, resulting in a simpler, more cost-effective solution that still meets the functional requirements of running contests and selecting winners.</p><p>Efficient Processing: Lambda functions are well-suited for the task of randomly selecting a winner at the end of each contest, as this is a relatively short-lived, event-driven process.</p><p>This solution provides the most cost-effective approach by aligning the cost structure with the actual usage patterns of the contest application, automatically managing resources based on demand, and eliminating unnecessary costs for data retention after contests end.</p><p>Sources</p><p>AWS Well-Architected design considerations - Account Assessment for AWS Organizations （https://docs.aws.amazon.com/solutions/latest/account-assessment-for-aws-organizations/aws-well-architected.html）</p><p>How to Trigger RDS and EC2 to automatically start up | AWS re:Post （https://repost.aws/questions/QUv5qKD3q3QxOWoISBJiy3UA/how-to-trigger-rds-and-ec2-to-automatically-start-up）</p><p>Community | EC2 cost optimization:15 ways to save on Amazon EC2 （https://community.aws/content/2hQNGTh6vpwOHTgKELzL8mkqmKb/ec2-cost-optimization-15-ways-to-save-on-amazon-ec2）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "eb7d4a51c95f46bfbe524ac25b0e57d6",
            "questionNumber": 434,
            "type": "single",
            "content": "<p>Question #434</p><p>A company has implemented a new security requirement. According to the new requirement, the company must scan all traffic from corporate AWS instances in the company's VPC for violations of the company's security policies. As a result of these scans, the company can block access to and from specific IP addresses.</p><p><br></p><p>To meet the new requirement, the company deploys a set of Amazon EC2 instances in private subnets to serve as transparent proxies. The company installs approved proxy server software on these EC2 instances. The company modifies the route tables on all subnets to use the corresponding EC2 instances with proxy software as the default route. The company also creates security groups that are compliant with the security policies and assigns these security groups to the EC2 instances.</p><p><br></p><p>Despite these configurations, the traffic of the EC2 instances in their private subnets is not being properly forwarded to the internet.<br><br>What should a solutions architect do to resolve this issue? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Disable source/destination checks on the EC2 instances that run the proxy software."
                },
                {
                    "label": "B",
                    "content": "Add a rule to the security group that is assigned to the proxy EC2 instances to allow all traffic between instances that have this security group. Assign this security group to all EC2 instances in the VPC."
                },
                {
                    "label": "C",
                    "content": "Change the VPCs DHCP options set. Set the DNS server options to point to the addresses of the proxy EC2 instances."
                },
                {
                    "label": "D",
                    "content": "Assign one additional elastic network interface to each proxy EC2 instance. Ensure that one of these network interfaces has a route to the private subnets. Ensure that the other network interface has a route to the internet."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The correct answer is A. Disable source/destination checks on the EC2 instances that run the proxy software.</p><p> Explanation:</p><p>1. Problem Identification:</p><p> &nbsp; - The proxy EC2 instances are deployed as transparent proxies, meaning they intercept and forward traffic for other instances in the VPC.</p><p> &nbsp; - By default, EC2 instances perform source/destination checks, meaning they drop traffic that is not explicitly destined for or originating from their own IP addresses.</p><p> &nbsp; - Since the proxies are forwarding traffic on behalf of other instances, the source/destination checks must be disabled to allow this behavior.</p><p>2. Why Option A Works:</p><p> &nbsp; - Disabling source/destination checks allows the proxy instances to handle traffic that is not destined for their own IPs, which is necessary for transparent proxying.</p><p> &nbsp; - This is a well-documented requirement for EC2 instances acting as NAT instances, firewalls, or proxies.</p><p>3. Why Not the Other Options?</p><p> &nbsp; - B (Security group rules for all traffic): &nbsp;</p><p> &nbsp; &nbsp; - While security groups must allow traffic between instances, this alone does not solve the source/destination check issue.</p><p> &nbsp; - C (Modify DHCP options set for DNS): &nbsp;</p><p> &nbsp; &nbsp; - This is unrelated to the forwarding of internet-bound traffic through proxies.</p><p> &nbsp; - D (Add additional ENIs with specific routes): &nbsp;</p><p> &nbsp; &nbsp; - While multi-homed proxies can be used in some architectures, this is not necessary for basic transparent proxying and does not address the core issue (source/destination checks).</p><p> Conclusion:</p><p>Option A is the correct solution because it directly addresses the source/destination check issue, which is the primary reason traffic is not being forwarded properly. Disabling this feature allows the proxy instances to handle traffic for other instances in the VPC.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7dc9e972f6124922aef28e4d80b6d95f",
            "questionNumber": 435,
            "type": "single",
            "content": "<p>Question #435</p><p>A company is running its solution on AWS in a manually created VPC. The company is using AWS CloudFormation to provision other parts of the infrastructure. According to a new requirement, the company must manage all infrastructure in an automatic way.<br><br>What should the company do to meet this new requirement with the LEAST effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new AWS Cloud Development Kit (AWS CDK) stack that strictly provisions the existing VPC resources and configuration. Use AWS CDK to import the VPC into the stack and to manage the VPC."
                },
                {
                    "label": "B",
                    "content": "Create a CloudFormation stack set that creates the VPC. Use the stack set to import the VPC into the stack."
                },
                {
                    "label": "C",
                    "content": "Create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the Existing resources."
                },
                {
                    "label": "D",
                    "content": "Create a new CloudFormation template that creates the VPC. Use the AWS Serverless Application Model (AWS SAM) CLI to import the VPC."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create a new CloudFormation template that strictly provisions the existing VPC resources and configuration. From the CloudFormation console, create a new stack by importing the Existing resources.</p><p>This solution will meet the requirement to manage all infrastructure in an automatic way with the least effort for the following reasons:</p><p>Direct Import Capability: CloudFormation provides a built-in feature specifically designed for importing existing resources into a stack. This \"Import resources into stack\" feature allows you to bring manually created resources under CloudFormation management without having to recreate them.</p><p>No Additional Tools Required: Unlike option A (using AWS CDK), this approach doesn't require setting up and learning an additional tool or programming model. The company is already using CloudFormation for other parts of their infrastructure, so continuing with CloudFormation maintains consistency.</p><p>Simplified Process: The CloudFormation console provides a guided workflow for importing existing resources:</p><p>Create a template that matches the existing VPC configuration</p><p>Use the \"Create stack\" &gt; \"With existing resources (import resources)\" option</p><p>Select the resources to import</p><p>Complete the import process</p><p>Preservation of Existing Resources: This approach doesn't require creating new resources or making changes to the existing VPC. It simply brings the existing VPC under CloudFormation management.</p><p>Integration with Existing Infrastructure: Since the company is already using CloudFormation for other parts of their infrastructure, this approach allows for seamless integration between the newly imported VPC and the existing CloudFormation-managed resources.</p><p>Option A (using AWS CDK) would require additional effort to set up the CDK environment, learn the CDK programming model, and write code to define the infrastructure, which is more complex than using CloudFormation directly.</p><p>Option B (using CloudFormation stack sets) is not appropriate for this scenario. Stack sets are designed for deploying stacks across multiple accounts and regions, not for importing existing resources. This approach would likely create new resources rather than importing existing ones.</p><p>By creating a CloudFormation template that matches the existing VPC configuration and using the import feature, the company can bring their manually created VPC under automated management with minimal effort while preserving their existing infrastructure.</p><p>Sources</p><p>Import entire applications into AWS CloudFormation | AWS DevOps Blog （https://aws.amazon.com/cn/blogs/devops/import-entire-applications-into-aws-cloudformation/）</p><p>How can I move a serverless VPC endpoint to another CloudFormation stack? | AWS re:Post （https://repost.aws/questions/QUKDx-TuTYRXa1wzy_xOfMyA/how-can-i-move-a-serverless-vpc-endpoint-to-another-cloudformation-stack）</p><p>Import AWS resources into a CloudFormation stack - AWS CloudFormation （https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/import-resources.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2c2c8a7906f844b28d0a233dbb1da685",
            "questionNumber": 436,
            "type": "single",
            "content": "<p>Question #436</p><p>A company has developed a new release of a popular video game and wants to make it available for public download. The new release package is approximately 5 GB in size. The company provides downloads for existing releases from a Linux-based, publicly facing FTP site hosted in an on-premises data center. The company expects the new release will be downloaded by users worldwide. The company wants a solution that provides improved download performance and low transfer costs, regardless of a user's location.</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Store the game files on Amazon EBS volumes mounted on Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package."
                },
                {
                    "label": "B",
                    "content": "Store the game files on Amazon EFS volumes that are attached to Amazon EC2 instances within an Auto Scaling group. Configure an FTP service on each of the EC2 instances. Use an Application Load Balancer in front of the Auto Scaling group. Publish the game download URL for users to download the package."
                },
                {
                    "label": "C",
                    "content": "Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Use Amazon CloudFront for the website. Publish the game download URL for users to download the package."
                },
                {
                    "label": "D",
                    "content": "Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Set Requester Pays for the S3 bucket. Publish the game download URL for users to download the package."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Configure Amazon Route 53 and an Amazon S3 bucket for website hosting. Upload the game files to the S3 bucket. Use Amazon CloudFront for the website. Publish the game download URL for users to download the package.</p><p>This solution will provide improved download performance and low transfer costs for the video game release for the following reasons:</p><p>Global Content Delivery Network: Amazon CloudFront is a content delivery network (CDN) with a global network of edge locations. When users request the game download, CloudFront delivers it from the edge location that provides the lowest latency, significantly improving download performance regardless of the user's location.</p><p>Cost-Effective Data Transfer: CloudFront offers cost-effective data transfer rates, especially for large files like the 5GB game package. CloudFront's pricing model includes:</p><p>Reduced data transfer costs compared to serving directly from S3</p><p>Regional data transfer pricing that can be more economical for global distribution</p><p>Caching at edge locations that reduces origin fetches and associated costs</p><p>Scalability: This architecture can easily handle the high demand expected for a popular video game release. S3 provides virtually unlimited storage capacity, while CloudFront can scale to handle millions of concurrent users downloading the game.</p><p>Simple Implementation: The solution is straightforward to implement:</p><p>Amazon S3 provides reliable storage for the game files</p><p>Route 53 handles DNS resolution to direct users to the CloudFront distribution</p><p>CloudFront distributes the content globally with minimal configuration</p><p>Improved Reliability: Moving from an on-premises FTP server to AWS cloud services increases reliability and availability. The solution eliminates single points of failure that might exist in the current on-premises setup.</p><p>Support for Large Files: The solution is well-suited for distributing large files like the 5GB game package. CloudFront supports efficient delivery of large objects and can handle partial downloads if users need to resume interrupted downloads.</p><p>Reduced Origin Load: By caching content at edge locations, CloudFront reduces the load on the origin S3 bucket, which helps maintain consistent performance even during peak download periods.</p><p>This architecture represents a significant improvement over the current on-premises FTP solution, providing better global performance, lower costs, and improved reliability for distributing the new game release to users worldwide.</p><p>Sources</p><p>Speeding up your Amazon S3-based website using Amazon CloudFront - Hosting Static Websites on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-static-websites-aws/speeding-up-your-amazon-s3-based-website-using-amazon-cloudfront.html）</p><p>Cost-effective resources - Games Industry Lens （https://docs.aws.amazon.com/wellarchitected/latest/games-industry-lens/games-cost-cost-eff-resc.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7a4db8ec664741bc89f0b3c028d32e9a",
            "questionNumber": 437,
            "type": "multiple",
            "content": "<p>Question #437</p><p>A company runs an application in the cloud that consists of a database and a website. Users can post data to the website, have the data processed, and have the data sent back to them in an email. Data is stored in a MySQL database running on an Amazon EC2 instance. The database is running in a VPC with two private subnets. The website is running on Apache Tomcat in a single EC2 instance in a different VPC with one public subnet. There is a single VPC peering connection between the database and website VPC.</p><p><br></p><p>The website has suffered several outages during the last month due to high traffic.<br><br>Which actions should a solutions architect take to increase the reliability of the application? (Choose three.) </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Place the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an Application<p>&nbsp;Load Balancer.</p>"
                },
                {
                    "label": "B",
                    "content": "Provision an additional VPC peering connection."
                },
                {
                    "label": "C",
                    "content": "Migrate the MySQL database to Amazon Aurora with one Aurora Replica."
                },
                {
                    "label": "D",
                    "content": "Provision two NAT gateways in the database VPC."
                },
                {
                    "label": "E",
                    "content": "Move the Tomcat server to the database VPC."
                },
                {
                    "label": "F",
                    "content": "Create an additional public subnet in a different Availability Zone in the website VPC."
                }
            ],
            "correctAnswer": "ACF",
            "explanation": "<p>1. A. Place the Tomcat server in an Auto Scaling group with multiple EC2 instances behind an Application Load Balancer. &nbsp;</p><p> &nbsp; - Why? The website outages are caused by high traffic. Auto Scaling ensures the application can scale horizontally to handle increased load, while an ALB distributes traffic across instances. &nbsp;</p><p> &nbsp; - Impact: Improves fault tolerance and availability by preventing single-instance failures.</p><p>2. C. Migrate the MySQL database to Amazon Aurora with one Aurora Replica. &nbsp;</p><p> &nbsp; - Why? The current single EC2-based MySQL database is a single point of failure. Aurora provides high availability with automatic failover to a replica in another AZ. &nbsp;</p><p> &nbsp; - Impact: Increases database reliability and performance while reducing management overhead.</p><p>3. F. Create an additional public subnet in a different Availability Zone in the website VPC. &nbsp;</p><p> &nbsp; - Why? The website runs in a single public subnet, making it vulnerable to AZ failures. Adding a subnet in another AZ improves redundancy when combined with Auto Scaling. &nbsp;</p><p> &nbsp; - Impact: Ensures multi-AZ resilience for the web tier.</p><p>---</p><p> Why Not the Other Options?</p><p>- B (Additional VPC peering connection): &nbsp;</p><p> &nbsp;- VPC peering is not a bottleneck in this scenario (only one connection is needed). &nbsp;</p><p>- D (NAT gateways in the database VPC): &nbsp;</p><p> &nbsp;- The database is in private subnets and likely doesn’t need NAT for outbound internet access. This doesn’t address website outages. &nbsp;</p><p>- E (Move Tomcat to the database VPC): &nbsp;</p><p> &nbsp;- This doesn’t improve reliability—it just consolidates resources but doesn’t solve scaling or high availability issues. &nbsp;</p><p>---</p><p> Summary of Correct Actions: &nbsp;</p><p>✔ A (Auto Scaling + ALB) → Handles traffic spikes. &nbsp;</p><p>✔ C (Aurora with replica) → Eliminates database single point of failure. &nbsp;</p><p>✔ F (Multi-AZ subnets) → Ensures web tier redundancy. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "efb60c8400ab410dbd6d338efad1706b",
            "questionNumber": 438,
            "type": "multiple",
            "content": "<p>Question #438</p><p>A retail company is operating its ecommerce application on AWS. The application runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The company uses an Amazon RDS DB instance as the database backend. Amazon CloudFront is configured with one origin that points to the ALB. Static content is cached. Amazon Route 53 is used to host all public zones.</p><p><br></p><p>After an update of the application, the ALB occasionally returns a 502 status code (Bad Gateway) error. The root cause is malformed HTTP headers that are returned to the ALB. The webpage returns successfully when a solutions architect reloads the webpage immediately after the error occurs.</p><p><br></p><p>While the company is working on the problem, the solutions architect needs to provide a custom error page instead of the standard ALB error page to visitors.<br><br>Which combination of steps will meet this requirement with the LEAST amount of operational overhead? (Choose two.) </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Target.FailedHealthChecks is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a publicly accessible web server."
                },
                {
                    "label": "C",
                    "content": "Modify the existing Amazon Route 53 records by adding health checks. Configure a fallback target if the health check fails. Modify DNS records to point to a publicly accessible webpage."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon CloudWatch alarm to invoke an AWS Lambda function if the ALB health check response Elb.InternalError is greater than 0. Configure the Lambda function to modify the forwarding rule at the ALB to point to a public accessible web server."
                },
                {
                    "label": "E",
                    "content": "Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible webpage."
                }
            ],
            "correctAnswer": "AE",
            "explanation": "<p> A. Create an Amazon S3 bucket. Configure the S3 bucket to host a static webpage. Upload the custom error pages to Amazon S3.</p><p>- Why? &nbsp;</p><p> &nbsp;- S3 is a simple, cost-effective way to host static error pages (e.g., `502.html`). &nbsp;</p><p> &nbsp;- Requires minimal operational overhead—just upload the custom error page and enable static website hosting. &nbsp;</p><p> &nbsp;- Works seamlessly with CloudFront (next step). &nbsp;</p><p> E. Add a custom error response by configuring a CloudFront custom error page. Modify DNS records to point to a publicly accessible web page.</p><p>- Why? &nbsp;</p><p> &nbsp;- CloudFront allows custom error page configurations (e.g., redirecting `502` errors to a static S3 page). &nbsp;</p><p> &nbsp;- Instead of showing the default ALB error, CloudFront can serve a friendly error page from S3. &nbsp;</p><p> &nbsp;- No need to modify ALB or RDS—this solution works at the CDN layer. &nbsp;</p><p> Why Not the Other Options?</p><p>- B & D (CloudWatch + Lambda + ALB modification): &nbsp;</p><p> &nbsp;- Overly complex for this use case. &nbsp;</p><p> &nbsp;- Requires real-time ALB rule changes, which introduce latency and potential failures. &nbsp;</p><p> &nbsp;- Higher operational overhead than using S3 + CloudFront. &nbsp;</p><p>- C (Route 53 health checks + DNS failover): &nbsp;</p><p> &nbsp;- DNS changes take time to propagate, making this unsuitable for transient `502` errors. &nbsp;</p><p> &nbsp;- Health checks won’t react fast enough for intermittent issues. &nbsp;</p><p> Summary of Correct Steps: &nbsp;</p><p>1. Host the custom error page in S3 (A). &nbsp;</p><p>2. Configure CloudFront to serve the custom error page when ALB returns `502` (E). &nbsp;</p><p>This approach is simple, scalable, and low-maintenance, meeting the requirement with the least operational overhead. &nbsp;</p><p>Thus, A and E are the correct answers.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b04a0d7d9552483f955f4643485ee36a",
            "questionNumber": 439,
            "type": "multiple",
            "content": "<p>Question #439</p><p>A company wants to migrate an Amazon Aurora MySQL DB cluster from an existing AWS account to a new AWS account in the same AWS Region. Both accounts are members of the same organization in AWS Organizations.<br><br>The company must minimize database service interruption before the company performs DNS cutover to the new database.<br><br>Which migration strategy will meet this requirement? (Choose two.) </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Take a snapshot of the existing Aurora database. Share the snapshot with the new AWS account. Create an Aurora DB cluster in the new account from the snapshot."
                },
                {
                    "label": "B",
                    "content": "Create an Aurora DB cluster in the new AWS account. Use AWS Database Migration Service (AWS DMS) to migrate data between the two Aurora DB clusters."
                },
                {
                    "label": "C",
                    "content": "Use AWS Backup to share an Aurora database backup from the existing AWS account to the new AWS account. Create an Aurora DB cluster in the new AWS account from the snapshot."
                },
                {
                    "label": "D",
                    "content": "Create an Aurora DB cluster in the new AWS account. Use AWS Application Migration Service to migrate data between the two Aurora DB clusters."
                }
            ],
            "correctAnswer": "AB",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Take a snapshot of the existing Aurora database. Share the snapshot with the new AWS account. Create an Aurora DB cluster in the new account from the snapshot.</p><p>B. Create an Aurora DB cluster in the new AWS account. Use AWS Database Migration Service (AWS DMS) to migrate data between the two Aurora DB clusters.</p><p>Both of these migration strategies will help minimize database service interruption before performing DNS cutover to the new database. Let me explain why each is appropriate:</p><p>Option A: Snapshot-based migration</p><p>This approach allows you to create a point-in-time copy of your entire Aurora database</p><p>Since both accounts are in the same AWS Organization, you can easily share snapshots between accounts</p><p>The snapshot restore process creates an exact replica of your database structure and data</p><p>While there is some downtime required to take the final snapshot, this approach is relatively simple to implement</p><p>After restoring the snapshot in the new account, you can set up replication from the source database to catch any changes that occurred during the migration process</p><p>This method is particularly efficient for databases that aren't extremely large or don't have extremely high transaction rates</p><p>Option B: AWS DMS-based migration</p><p>AWS Database Migration Service allows for continuous data replication between the source and target databases</p><p>You can set up initial data load and ongoing change data capture (CDC) to keep the databases in sync</p><p>This approach can achieve near-zero downtime as it continuously replicates changes from the source to the target</p><p>DMS supports Aurora MySQL as both source and target endpoints</p><p>When it's time to cut over, the target database will already be in sync with the source, minimizing interruption</p><p>This method works well for databases of any size and is especially valuable for databases with high transaction volumes</p><p>Both strategies allow you to:</p><p>Prepare the target database in the new account while the source database continues to operate</p><p>Minimize the cutover window by having the data already migrated or continuously replicating</p><p>Perform testing on the new database before the final cutover</p><p>Execute a clean DNS cutover with minimal service interruption</p><p>The company could even use a hybrid approach, using the snapshot method for the initial bulk data transfer and then setting up DMS to replicate any changes that occur after the snapshot is taken. This would combine the simplicity of the snapshot approach with the continuous replication benefits of DMS.</p><p>Either way, both options A and B are valid strategies that would meet the requirement to minimize database service interruption before performing the DNS cutover to the new database.</p><p>Sources</p><p>Perform cross-account Amazon Aurora MySQL migration with minimal downtime | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/perform-cross-account-amazon-aurora-mysql-migration-with-minimal-downtime/）</p><p>Migrating from Amazon RDS for MySQL - Amazon Aurora MySQL Migration Handbook （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.AuroraMySQL.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3db3c16a1ae3412eaf4ae69182ef3967",
            "questionNumber": 440,
            "type": "multiple",
            "content": "<p>Question #440</p><p>A software as a service (SaaS) company provides a media software solution to customers. The solution is hosted on 50 VPCs across various AWS Regions and AWS accounts. One of the VPCs is designated as a management VPC. The compute resources in the VPCs work independently.</p><p><br></p><p>The company has developed a new feature that requires all 50 VPCs to be able to communicate with each other. The new feature also requires one-way access from each customer's VPC to the company's management VPC. The management VPC hosts a compute resource that validates licenses for the media software solution.</p><p><br></p><p>The number of VPCs that the company will use to host the solution will continue to increase as the solution grows.<br><br>Which combination of steps will provide the required VPC connectivity with the LEAST operational overhead? (Choose two.) </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a transit gateway. Attach all the company&#39;s VPCs and relevant subnets to the transit gateway."
                },
                {
                    "label": "B",
                    "content": "Create VPC peering connections between all the company&#39;s VPCs."
                },
                {
                    "label": "C",
                    "content": "Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer&#39;s VPC. Associate the endpoint service with the NLB."
                },
                {
                    "label": "D",
                    "content": "Create a VPN appliance in each customer&#39;s VPC. Connect the company&#39;s management VPC to each customer&#39;s VPC by using AWS Site-to-Site VPN."
                },
                {
                    "label": "E",
                    "content": "Create a VPC peering connection between the company&#39;s management VPC and each customer&#39;s VPC."
                }
            ],
            "correctAnswer": "AC",
            "explanation": "<p> A. Create a transit gateway. Attach all the company's VPCs and relevant subnets to the transit gateway.</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS Transit Gateway is designed for scalable, hub-and-spoke VPC connectivity across multiple accounts and regions. &nbsp;</p><p> &nbsp;- It eliminates the need for complex VPC peering meshes (which become unmanageable as the number of VPCs grows). &nbsp;</p><p> &nbsp;- Supports dynamic routing and simplifies network management as new VPCs are added. &nbsp;</p><p> C. Create a Network Load Balancer (NLB) that points to the compute resource for license validation. Create an AWS PrivateLink endpoint service that is available to each customer's VPC. Associate the endpoint service with the NLB.</p><p>- Why? &nbsp;</p><p> &nbsp;- PrivateLink provides secure, one-way access from customer VPCs to the management VPC (license validation). &nbsp;</p><p> &nbsp;- Avoids exposing the management VPC to the internet or requiring complex VPN setups. &nbsp;</p><p> &nbsp;- Scales automatically as new customers are onboarded. &nbsp;</p><p> Why Not the Other Options?</p><p>- B (VPC peering between all VPCs): &nbsp;</p><p> &nbsp;- Does not scale—50+ VPCs would require a full mesh of peering connections, which is operationally prohibitive. &nbsp;</p><p> &nbsp;- Manual management is required for each new VPC. &nbsp;</p><p>- D (Site-to-Site VPN per customer VPC): &nbsp;</p><p> &nbsp;- High operational overhead—each VPN requires manual setup and maintenance. &nbsp;</p><p> &nbsp;- Not scalable for a growing SaaS solution. &nbsp;</p><p>- E (VPC peering between management VPC and each customer VPC): &nbsp;</p><p> &nbsp;- Limited to 125 peering connections per VPC, which is insufficient for large-scale growth. &nbsp;</p><p> &nbsp;- Manual effort for each new customer VPC. &nbsp;</p><p> Summary of Correct Steps: &nbsp;</p><p>1. Use Transit Gateway (A) for scalable, centralized communication between all SaaS VPCs. &nbsp;</p><p>2. Use PrivateLink (C) for secure, one-way access to the management VPC (license validation). &nbsp;</p><p>This approach provides scalability, security, and minimal operational overhead as the SaaS solution grows. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "e066aaa8b163461683b24010be0e9ed9",
            "questionNumber": 441,
            "type": "multiple",
            "content": "<p>Question #441</p><p>A company has multiple lines of business (LOBs) that roll up to the parent company. The company has asked its solutions architect to develop a solution with the following requirements:</p><p><br></p><p>• Produce a single AWS invoice for all of the AWS accounts used by its LOBs.</p><p>• The costs for each LOB account should be broken out on the invoice.</p><p>• Provide the ability to restrict services and features in the LOB accounts, as defined by the company's governance policy.</p><p>• Each LOB account should be delegated full administrator permissions, regardless of the governance policy.<br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Organizations to create an organization in the parent account for each LOB. Then invite each LOB account to the appropriate organization."
                },
                {
                    "label": "B",
                    "content": "Use AWS Organizations to create a single organization in the parent account. Then, invite each LOB&#39;s AWS account to join the organization."
                },
                {
                    "label": "C",
                    "content": "Implement service quotas to define the services and features that are permitted and apply the quotas to each LOB, as appropriate."
                },
                {
                    "label": "D",
                    "content": "Create an SCP that allows only approved services and features, then apply the policy to the LOB accounts."
                },
                {
                    "label": "E",
                    "content": "Enable consolidated billing in the parent account&#39;s billing console and link the LOB accounts."
                }
            ],
            "correctAnswer": "BD",
            "explanation": "<p> B. Use AWS Organizations to create a single organization in the parent account. Then, invite each LOB's AWS account to join the organization.</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS Organizations allows the parent company to consolidate billing under a single invoice while breaking out costs per LOB. &nbsp;</p><p> &nbsp;- Provides a centralized way to manage multiple accounts under a single organization. &nbsp;</p><p> &nbsp;- Supports Service Control Policies (SCPs) for governance (next step). &nbsp;</p><p> D. Create an SCP that allows only approved services and features, then apply the policy to the LOB accounts.</p><p>- Why? &nbsp;</p><p> &nbsp;- Service Control Policies (SCPs) enforce governance by restricting services/features in LOB accounts while still allowing full admin permissions within allowed boundaries. &nbsp;</p><p> &nbsp;- Ensures compliance with company policies without limiting LOB admins' control over permitted services. &nbsp;</p><p> Why Not the Other Options?</p><p>- A (Multiple Organizations per LOB): &nbsp;</p><p> &nbsp;- Incorrect: AWS allows only one organization per account hierarchy. Creating multiple organizations is not feasible. &nbsp;</p><p>- C (Service Quotas): &nbsp;</p><p> &nbsp;- Service quotas limit usage (e.g., EC2 instance count) but do not restrict services/features like SCPs do. &nbsp;</p><p>- E (Consolidated Billing Only): &nbsp;</p><p> &nbsp;- Consolidated billing is outdated (replaced by AWS Organizations). &nbsp;</p><p> &nbsp;- Does not provide governance controls (SCPs are needed). &nbsp;</p><p> Summary of Correct Steps: &nbsp;</p><p>1. Set up AWS Organizations (B) to consolidate billing and manage accounts. &nbsp;</p><p>2. Apply SCPs (D) to enforce governance while allowing LOB admins full access within defined limits. &nbsp;</p><p>This approach meets all requirements with minimal operational overhead. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "aa7c27f1d53a45cdbbe16bebed9cd7b7",
            "questionNumber": 442,
            "type": "multiple",
            "content": "<p>Question #442</p><p>A solutions architect has deployed a web application that serves users across two AWS Regions under a custom domain. The application uses Amazon Route 53 latency-based routing. The solutions architect has associated weighted record sets with a pair of web servers in separate Availability Zones for each Region.</p><p><br></p><p>The solutions architect runs a disaster recovery scenario. When all the web servers in one Region are stopped, Route 53 does not automatically redirect users to the other Region.<br><br>Which of the following are possible root causes of this issue? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "The weight for the Region where the web servers were stopped is higher than the weight for the other Region."
                },
                {
                    "label": "B",
                    "content": "One of the web servers in the secondary Region did not pass its HTTP health check."
                },
                {
                    "label": "C",
                    "content": "Latency resource record sets cannot be used in combination with weighted resource record sets."
                },
                {
                    "label": "D",
                    "content": "The setting to evaluate target health is not turned on for the latency alias resource record set that is associated with the domain in the Region where the web servers were stopped."
                },
                {
                    "label": "E",
                    "content": "An HTTP health check has not been set up for one or more of the weighted resource record sets associated with the stopped web servers."
                }
            ],
            "correctAnswer": "DE",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>D. The setting to evaluate target health is not turned on for the latency alias resource record set that is associated with the domain in the Region where the web servers were stopped.</p><p>E. An HTTP health check has not been set up for one or more of the weighted resource record sets associated with the stopped web servers.</p><p>These are the two possible root causes of the issue where Route 53 is not automatically redirecting users to the other Region during the disaster recovery scenario. Let me explain why:</p><p>Evaluate Target Health Setting (Option D):</p><p>In a Route 53 configuration with latency-based routing, the \"Evaluate Target Health\" setting is crucial for failover functionality.</p><p>When this setting is enabled on a latency alias record, Route 53 checks the health of the underlying resources (in this case, the weighted record sets for the web servers).</p><p>If this setting is turned off, Route 53 will continue to route traffic based solely on latency calculations, regardless of whether the resources in that Region are actually available.</p><p>Without evaluating target health, Route 53 has no way to know that the web servers in one Region have been stopped, so it continues to direct some users to that Region based on latency.</p><p>Missing HTTP Health Checks (Option E):</p><p>For Route 53 to determine if resources are healthy, specific health checks must be configured and associated with the resource record sets.</p><p>In this architecture, the weighted record sets for individual web servers need HTTP health checks to monitor their availability.</p><p>Without these health checks, Route 53 cannot detect that the web servers have been stopped.</p><p>Even if \"Evaluate Target Health\" is enabled, without proper health checks, Route 53 has no data to evaluate and will assume the resources are healthy.</p><p>The combination of these two issues prevents the automatic failover mechanism from working properly. When the web servers in one Region are stopped:</p><p>Without health checks (Option E), Route 53 doesn't know the servers are down.</p><p>Without evaluating target health (Option D), Route 53 doesn't consider health status in its routing decisions.</p><p>To fix this issue, the solutions architect would need to:</p><p>Enable the \"Evaluate Target Health\" setting for the latency alias resource record sets</p><p>Set up HTTP health checks for all weighted resource record sets associated with the web servers</p><p>Associate these health checks with the appropriate record sets</p><p>With these configurations in place, Route 53 would detect when servers in one Region are unavailable and automatically redirect traffic to the healthy servers in the other Region, ensuring the disaster recovery mechanism works as expected.</p><p>Sources</p><p>How health checks work in complex Amazon Route 53 configurations - Amazon Route 53 （https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover-complex-configs.html）</p><p>Fast Failover using Active-Passive failover policy | AWS re:Post （https://repost.aws/questions/QUyGuTcle8Q1-GsnNKThE6zQ/fast-failover-using-active-passive-failover-policy）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7620d85946244bc982c2f2e472a4f282",
            "questionNumber": 443,
            "type": "single",
            "content": "<p>Question #443</p><p>A flood monitoring agency has deployed more than 10,000 water-level monitoring sensors. Sensors send continuous data updates, and each update is less than 1 MB in size. The agency has a fleet of on-premises application servers. These servers receive updates from the sensors, convert the raw data into a human-readable format, and write the results to an on-premises relational database server. Data analysts then use simple SQL queries to monitor the data.</p><p><br></p><p>The agency wants to increase overall application availability and reduce the effort that is required to perform maintenance tasks. These maintenance tasks, which include updates and patches to the application servers, cause downtime. While an application server is down, data is lost from sensors because the remaining servers cannot handle the entire workload.</p><p><br></p><p>The agency wants a solution that optimizes operational overhead and costs. A solutions architect recommends the use of AWS IoT Core to collect the sensor data.</p><p><br></p><p>What else should the solutions architect recommend to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to .csv format, and insert it into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance."
                },
                {
                    "label": "B",
                    "content": "Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to Apache Parquet format, and save it to an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena."
                },
                {
                    "label": "C",
                    "content": "Send the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to .csv format and store it in an Amazon S3 bucket. Import the data into an Amazon Aurora MySQL DB instance. Instruct the data analysts to query the data directly from the DB instance."
                },
                {
                    "label": "D",
                    "content": "Send the sensor data to an Amazon Managed Service for Apache Flink (previously known as Amazon Kinesis Data Analytics) application to convert the data to Apache Parquet format and store it in an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. Send the sensor data to Amazon Kinesis Data Firehose. Use an AWS Lambda function to read the Kinesis Data Firehose data, convert it to Apache Parquet format, and save it to an Amazon S3 bucket. Instruct the data analysts to query the data by using Amazon Athena.</p><p> Explanation:</p><p>1. Problem Requirements:</p><p> &nbsp; - High availability & reduced maintenance: Eliminate downtime caused by on-premises server maintenance.</p><p> &nbsp; - Handle 10,000+ sensors: Scalable ingestion of continuous, small (&lt;1 MB) data updates.</p><p> &nbsp; - Cost & operational efficiency: Minimize overhead while ensuring analysts can query data easily.</p><p>2. Why Option B Works Best:</p><p> &nbsp; - AWS IoT Core + Kinesis Data Firehose: &nbsp;</p><p> &nbsp; &nbsp; - IoT Core scales seamlessly for 10,000+ sensors. &nbsp;</p><p> &nbsp; &nbsp; - Firehose buffers and batches data efficiently, reducing costs. &nbsp;</p><p> &nbsp; - Lambda for transformation: &nbsp;</p><p> &nbsp; &nbsp; - Converts raw data to Parquet (columnar format), optimizing storage and query performance. &nbsp;</p><p> &nbsp; - Amazon S3 + Athena: &nbsp;</p><p> &nbsp; &nbsp; - Serverless architecture (no maintenance, high availability). &nbsp;</p><p> &nbsp; &nbsp; - Analysts use simple SQL queries (familiar, no changes needed). &nbsp;</p><p> &nbsp; &nbsp; - Cost-effective (pay per query, no database provisioning). &nbsp;</p><p>3. Why Not the Other Options?</p><p> &nbsp; - A (Aurora MySQL): &nbsp;</p><p> &nbsp; &nbsp; - Operational overhead (database management, scaling, patching). &nbsp;</p><p> &nbsp; &nbsp; - Higher cost for continuous writes from 10,000 sensors. &nbsp;</p><p> &nbsp; - C (Managed Flink → CSV → Aurora): &nbsp;</p><p> &nbsp; &nbsp; - Overkill for simple transformations (Flink is for complex streaming analytics). &nbsp;</p><p> &nbsp; &nbsp; - CSV is less efficient than Parquet for analytics. &nbsp;</p><p> &nbsp; &nbsp; - Aurora adds unnecessary cost/complexity. &nbsp;</p><p> &nbsp; - D (Managed Flink → Parquet + Athena): &nbsp;</p><p> &nbsp; &nbsp; - Flink is excessive for basic data conversion (Lambda is simpler/cheaper). &nbsp;</p><p> Key Benefits of Option B:</p><p>✔ Fully serverless (zero maintenance, scales automatically). &nbsp;</p><p>✔ Cost-optimized (Firehose batching, S3 storage, Athena pay-per-query). &nbsp;</p><p>✔ High performance (Parquet + Athena enables fast SQL queries). &nbsp;</p><p>✔ Meets all requirements with minimal operational effort. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ace000a65ad1462d95f5b9e5261e9e58",
            "questionNumber": 444,
            "type": "multiple",
            "content": "<p>Question #444</p><p>A public retail web application uses an Application Load Balancer (ALB) in front of Amazon EC2 instances running across multiple Availability Zones (AZs) in a Region backed by an Amazon RDS MySQL Multi-AZ deployment. Target group health checks are configured to use HTTP and pointed at the product catalog page. Auto Scaling is configured to maintain the web fleet size based on the ALB health check.</p><p><br></p><p>Recently, the application experienced an outage. Auto Scaling continuously replaced the instances during the outage. A subsequent investigation determined that the web server metrics were within the normal range, but the database tier was experiencing high load, resulting in severely elevated query response times.</p><p><br></p><p>Which of the following changes together would remediate these issues while improving monitoring capabilities for the availability and functionality of the entire application stack for future growth? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure read replicas for Amazon RDS MySQL and use the single reader endpoint in the web application to reduce the load on the backend database tier."
                },
                {
                    "label": "B",
                    "content": "Configure the target group health check to point at a simple HTML page instead of a product catalog page and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails."
                },
                {
                    "label": "C",
                    "content": "Configure the target group health check to use a TCP check of the Amazon EC2 web server and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails."
                },
                {
                    "label": "D",
                    "content": "Configure an Amazon CloudWatch alarm for Amazon RDS with an action to recover a high-load, impaired RDS instance in the database tier."
                },
                {
                    "label": "E",
                    "content": "Configure an Amazon ElastiCache cluster and place it between the web application and RDS MySQL instances to reduce the load on the backend database tier."
                }
            ],
            "correctAnswer": "BE",
            "explanation": "<p> B. Configure the target group health check to point at a simple HTML page instead of a product catalog page and the Amazon Route 53 health check against the product page to evaluate full application functionality. Configure Amazon CloudWatch alarms to notify administrators when the site fails.</p><p>- Why? &nbsp;</p><p> &nbsp;- The ALB health check was failing because the product catalog page depended on the database, which was overloaded. &nbsp;</p><p> &nbsp;- A simple HTML page ensures the web server itself is healthy, while Route 53 health checks monitor full application functionality (including DB-dependent pages). &nbsp;</p><p> &nbsp;- CloudWatch alarms provide proactive monitoring and alerting. &nbsp;</p><p> E. Configure an Amazon ElastiCache cluster and place it between the web application and RDS MySQL instances to reduce the load on the backend database tier.</p><p>- Why? &nbsp;</p><p> &nbsp;- ElastiCache (Redis/Memcached) caches frequent database queries, reducing RDS load and preventing high-latency issues. &nbsp;</p><p> &nbsp;- Scales read-heavy workloads (like product catalogs) efficiently. &nbsp;</p><p> Why Not the Other Options?</p><p>- A (RDS read replicas): &nbsp;</p><p> &nbsp;- Helps with read scaling, but does not cache results like ElastiCache (still hits the database). &nbsp;</p><p> &nbsp;- Not as effective for reducing query load compared to caching. &nbsp;</p><p>- C (TCP health check + Route 53): &nbsp;</p><p> &nbsp;- TCP checks only verify connectivity, not application logic. This doesn’t solve the root cause (DB dependency). &nbsp;</p><p>- D (CloudWatch RDS recovery): &nbsp;</p><p> &nbsp;- Reactive, not preventive—doesn’t reduce database load or improve health checks. &nbsp;</p><p> Summary of Correct Actions: &nbsp;</p><p>✔ B: Fix health checks (simple page for ALB, Route 53 for full app checks) + monitoring. &nbsp;</p><p>✔ E: Add ElastiCache to offload database queries and prevent future slowdowns. &nbsp;</p><p>This combination improves availability, reduces DB load, and enhances monitoring for future growth. &nbsp;</p><p>Thus, B and E are the correct answers.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8cbae6b9100247e6a07fc63de18b8d5b",
            "questionNumber": 445,
            "type": "single",
            "content": "<p>Question #445</p><p>A company has an on-premises data center and is using Kubernetes to develop a new solution on AWS. The company uses Amazon Elastic Kubernetes Service (Amazon EKS) clusters for its development and test environments.</p><p><br></p><p>The EKS control plane and data plane for production workloads must reside on-premises. The company needs an AWS managed solution for Kubernetes management.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using a local cluster configuration on the Outposts server for the production workloads."
                },
                {
                    "label": "B",
                    "content": "Install Amazon EKS Anywhere on the company&#39;s hardware in the on-premises data center. Deploy the production workloads on an EKS Anywhere cluster."
                },
                {
                    "label": "C",
                    "content": "Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using an extended cluster configuration on the Outposts server for the production workloads."
                },
                {
                    "label": "D",
                    "content": "Install an AWS Outposts server in the on-premises data center. Install Amazon EKS Anywhere on the Outposts server. Deploy the production workloads on an EKS Anywhere cluster."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Install an AWS Outposts server in the on-premises data center. Deploy Amazon EKS by using a local cluster configuration on the Outposts server for the production workloads.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>On-premises Control Plane and Data Plane: AWS Outposts with Amazon EKS local cluster configuration allows both the Kubernetes control plane and data plane to run entirely on-premises on the Outposts hardware, satisfying the requirement that \"EKS control plane and data plane for production workloads must reside on premises.\"</p><p>AWS Managed Solution: AWS Outposts is a fully managed service by AWS. With EKS on Outposts in local cluster mode, AWS manages the underlying infrastructure, hardware maintenance, and software updates. This provides an AWS managed solution for Kubernetes management as required.</p><p>Minimal Operational Overhead: With AWS Outposts:</p><p>AWS handles the maintenance and updates of the underlying hardware</p><p>AWS manages the EKS control plane software updates and patches</p><p>The solution integrates seamlessly with AWS services and management tools</p><p>AWS provides unified support for both hardware and software components</p><p>Consistent operational experience with the company's existing EKS development and test environments</p><p>Option B (Amazon EKS Anywhere) would require more operational overhead because:</p><p>The company would be responsible for managing their own hardware infrastructure</p><p>They would need to handle hardware failures, replacements, and capacity planning</p><p>While EKS Anywhere provides tools for Kubernetes management, it places more responsibility on the company for maintaining the overall environment</p><p>It would create a different operational model compared to their existing EKS development and test environments</p><p>AWS Outposts with EKS local cluster configuration provides a true extension of AWS into the on-premises environment, with the same managed experience the company is already using for development and testing. This creates a consistent operational model across all environments while meeting the requirement for on-premises control and data planes, all with the least operational overhead.</p><p>Sources</p><p>Amazon Elastic Kubernetes Service - Overview of Deployment Options on AWS （https://docs.aws.amazon.com/whitepapers/latest/overview-deployment-options/amazon-elastic-kubernetes-service.html）</p><p>AWS re:Invent 2024 - Amazon EKS for edge and hybrid use cases | AWS re:Post （https://repost.aws/articles/ARP74Xj00HRkGHaiN1HrI52Q/aws-re-invent-2024-amazon-eks-for-edge-and-hybrid-use-cases）</p><p>Managed Kubernetes Service - Amazon EKS Features - Amazon Web Services （https://aws.amazon.com/cn/eks/features/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a7639483ccea4b38a2faea75ae994bad",
            "questionNumber": 446,
            "type": "single",
            "content": "Question #446<p>A company uses AWS Organizations to manage its development environment. Each development team at the company has its own AWS account. Each account has a single VPC and CIDR blocks that do not overlap.</p><p><br></p><p>The company has an Amazon Aurora DB cluster in a shared services account. All the development teams need to work with live data from the DB cluster.</p><p><br></p><p>Which solution will provide the required connectivity to the DB cluster with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Resource Access Manager (AWS RAM) resource share for the DB cluster. Share the DB cluster with all the development accounts."
                },
                {
                    "label": "B",
                    "content": "Create a transit gateway in the shared services account. Create an AWS Resource Access Manager (AWS RAM) resource share for the transit gateway. Share the transit gateway with all the development accounts. Instruct the developers to accept the resource share and configure networking."
                },
                {
                    "label": "C",
                    "content": "Create an Application Load Balancer (ALB) that points to the IP address of the DB cluster. Create an AWS PrivateLink endpoint service that uses the ALB. Add permissions to allow each development account to connect to the endpoint service."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Site-to-Site VPN connection in the shared services account. Configure networking. Use AWS Marketplace VPN software in each development account to connect to the Site-to-Site VPN connection."
                }
            ],
            "correctAnswer": "B",
            "explanation": "Option B is correct as it allows for a scalable and manageable way to connect multiple development accounts to the shared DB cluster using transit gateway and AWS RAM, with minimal operational overhead. Option A is not correct because AWS RAM cannot directly share an Amazon Aurora DB cluster. Option C is incorrect because PrivateLink requires a Network Load Balancer (NLB), not an ALB. Option D is less efficient as it requires managing VPN connections for each development account.",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7e2951142649484eb891f3fbb67ecf3c",
            "questionNumber": 447,
            "type": "single",
            "content": "<p>Question #447</p><p>A company used AWS CloudFormation to create all new infrastructure in its AWS member accounts. The resources rarely change and are properly sized for the expected load. The monthly AWS bill is consistent.</p><p><br></p><p>Occasionally, a developer creates a new resource for testing and forgets to remove the resource when the test is complete. Most of these tests last a few days before the resources are no longer needed.</p><p><br></p><p>The company wants to automate the process of finding unused resources. A solutions architect needs to design a solution that determines whether the cost in the AWS bill is increasing. The solution must help identify resources that cause an increase in cost and must automatically notify the company's operations team.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Turn on billing alerts. Use AWS Cost Explorer to determine the costs for the past month. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached."
                },
                {
                    "label": "B",
                    "content": "Turn on billing alerts. Use AWS Cost Explorer to determine the average monthly costs for the past 3 months. Create an Amazon CloudWatch alarm for total estimated charges. Specify a cost threshold that is higher than the costs that Cost Explorer determined. Add a notification to alert the operations team if the alarm threshold is breached."
                },
                {
                    "label": "C",
                    "content": "Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of Linked account. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance."
                },
                {
                    "label": "D",
                    "content": "Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of AWS services. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The correct answer is D. Use AWS Cost Anomaly Detection to create a cost monitor that has a monitor type of AWS services. Create a subscription to send daily AWS cost summaries to the operations team. Specify a threshold for cost variance.</p><p> Explanation:</p><p>1. Problem Requirements:</p><p> &nbsp; - Detect unused resources (e.g., forgotten test instances).</p><p> &nbsp; - Monitor cost increases in the AWS bill.</p><p> &nbsp; - Automatically notify the operations team when anomalies occur.</p><p>2. Why Option D Works Best:</p><p> &nbsp; - AWS Cost Anomaly Detection:</p><p> &nbsp; &nbsp; - Automatically identifies unusual spending patterns (e.g., sudden spikes from forgotten test resources).</p><p> &nbsp; &nbsp; - Monitor type: AWS services → Tracks cost changes per service (e.g., EC2, S3), making it easy to pinpoint which resource caused the increase.</p><p> &nbsp; - Daily cost summaries + threshold alerts:</p><p> &nbsp; &nbsp; - Proactively notifies the operations team when costs deviate from expected baselines.</p><p> &nbsp; - Fully automated (no manual analysis needed).</p><p>3. Why Not the Other Options?</p><p> &nbsp; - A & B (Billing alerts + CloudWatch alarms):</p><p> &nbsp; &nbsp; - Reactive—only triggers after costs exceed a fixed threshold.</p><p> &nbsp; &nbsp; - Does not identify the root cause (e.g., which service/resource caused the spike).</p><p> &nbsp; - C (Cost Anomaly Detection with \"Linked account\" monitor type):</p><p> &nbsp; &nbsp; - Tracks anomalies per account, not per service → Less granular for identifying unused resources.</p><p> Key Benefits of Option D:</p><p>✔ Automatic anomaly detection (machine learning-based, no manual setup). &nbsp;</p><p>✔ Service-level granularity (identifies exactly which AWS service/resource is causing cost increases). &nbsp;</p><p>✔ Proactive alerts (notifies operations team before costs spiral). &nbsp;</p><p>https://aws.amazon.com/aws-cost-management/aws-cost-anomaly</p><p>detection/faqs/#:~:text=What%20is%20the%20difference%20between%20a%20linked%20account%20monitor%20in%20a%20 </p><p>payer%20account,%20and%20a%20services%20monitor%20in%20a%20linked%20account</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "548862b3765c45a5bab427f5407b9d29",
            "questionNumber": 448,
            "type": "single",
            "content": "<p>Question #448</p><p>A company is deploying a new web-based application and needs a storage solution for the Linux application servers. The company wants to create a single location for updates to application data for all instances. The active dataset will be up to 100 GB in size. A solutions architect has determined that peak operations will occur for 3 hours daily and will require a total of 225 MiBps of read throughput.</p><p><br></p><p>The solutions architect must design a Multi-AZ solution that makes a copy of the data available in another AWS Region for disaster recovery (DR).The DR copy has an RPO of less than 1 hour.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. Configure the file system for 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region."
                },
                {
                    "label": "B",
                    "content": "Deploy a new Amazon FSx for Lustre file system. Configure Bursting Throughput mode for the file system. Use AWS Backup to back up the file system to the DR Region."
                },
                {
                    "label": "C",
                    "content": "Deploy a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume with 225 MiBps of throughput. Enable Multi-Attach for the EBS volume. Use AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region."
                },
                {
                    "label": "D",
                    "content": "Deploy an Amazon FSx for OpenZFS file system in both the production Region and the DR Region. Create an AWS DataSync scheduled task to replicate the data from the production file system to the DR file system every 10 minutes."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Deploy a new Amazon ElasticFile System (Amazon EFS) Multi-AZ file system. Configure the file system for 75 MiBps of provisioned throughput. Implement replication to a file system in the DR Region.</p><p>This solution will meet all the requirements for the following reasons:</p><p>Single Shared Storage Location: Amazon EFS provides a fully managed file system that can be mounted on multiple Linux application servers simultaneously, creating a single location for updates to application data that all instances can access.</p><p>Sufficient Capacity: EFS can easily handle the 100 GB active dataset size, as it's designed to scale to petabytes of data without needing to provision storage in advance.</p><p>Required Throughput: The requirement states a need for 225 MiBps of read throughput during peak operations. EFS with 75 MiBps of provisioned throughput will meet this need because:</p><p>EFS provisioned throughput is measured per unit of storage (in MiB/s per TiB)</p><p>For a 100 GB dataset, 75 MiBps of provisioned throughput provides sufficient performance</p><p>EFS automatically scales throughput based on the size of your file system and can burst to higher throughput levels when needed</p><p>The 75 MiBps provisioned throughput ensures a baseline performance level, while EFS's bursting capability can handle the peak 225 MiBps read operations during the 3-hour daily peak</p><p>Multi-AZ Solution: EFS is designed to be Multi-AZ by default, storing data redundantly across multiple Availability Zones within a region, providing high availability and durability.</p><p>Disaster Recovery Requirements: EFS supports cross-region replication, which continuously replicates data to a file system in another AWS Region. This replication is asynchronous but typically achieves an RPO of minutes, well within the required RPO of less than 1 hour.</p><p>Operational Simplicity: As a fully managed service, EFS eliminates the operational overhead of setting up, patching, and maintaining file servers and storage volumes.</p><p>The EFS solution provides the perfect balance of:</p><p>Shared access for all application servers</p><p>Sufficient performance for the workload</p><p>Built-in Multi-AZ redundancy</p><p>Cross-region replication for disaster recovery</p><p>Managed service with minimal operational overhead</p><p>This makes it the ideal choice for meeting the stated requirements with the least complexity and management burden.</p><p>Sources</p><p>Plan your deployment - Simple File Manager for Amazon EFS （https://docs.aws.amazon.com/solutions/latest/simple-file-manager-for-amazon-efs/plan-your-deployment.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5988e2f1286d49d68fd21f9dd1b81382",
            "questionNumber": 449,
            "type": "single",
            "content": "<p>Question #449</p><p>A company needs to gather data from an experiment in a remote location that does not have internet connectivity. During the experiment, sensors that are connected to a local network will generate 6 TB of data in a proprietary format over the course of 1 week. The sensors can be configured to upload their data files to an FTP server periodically, but the sensors do not have their own FTP server. The sensors also do not support other protocols. The company needs to collect the data centrally and move the data to object storage in the AWS Cloud as soon as possible after the experiment.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Order an AWS Snowball Edge Compute Optimized device. Connect the device to the local network. Configure AWS DataSync with a target bucket name, and unload the data over NFS to the device. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3."
                },
                {
                    "label": "B",
                    "content": "Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Create a shell script that periodically downloads data from each sensor. After the experiment, return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume."
                },
                {
                    "label": "C",
                    "content": "Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Install and configure an FTP server on the EC2 instance. Configure the sensors to upload data to the EC2 instance. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3."
                },
                {
                    "label": "D",
                    "content": "Order an AWS Snowcone device. Connect the device to the local network. Configure the device to use Amazon FSx. Configure the sensors to upload data to the device. Configure AWS DataSync on the device to synchronize the uploaded data with an Amazon S3 bucket. Return the device to AWS so that the data can be loaded as an Amazon Elastic Block Store (Amazon EBS) volume.<p><br></p>"
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. Order an AWS Snowcone device, including an Amazon Linux 2 AMI. Connect the device to the local network. Launch an Amazon EC2 instance on the device. Install and configure an FTP server on the EC2 instance. Configure the sensors to upload data to the EC2 instance. After the experiment, return the device to AWS so that the data can be loaded into Amazon S3.</p><p> Explanation:</p><p>1. Problem Requirements:</p><p> &nbsp; - Remote location with no internet connectivity.</p><p> &nbsp; - 6 TB of data generated over 1 week (proprietary format).</p><p> &nbsp; - Sensors only support FTP uploads (no other protocols).</p><p> &nbsp; - Need to move data to AWS (Amazon S3) after the experiment.</p><p>2. Why Option C Works Best:</p><p> &nbsp; - AWS Snowcone (small, portable edge device) is ideal for offline data collection.</p><p> &nbsp; - Amazon Linux 2 AMI + EC2 instance allows running an FTP server (matching sensor requirements).</p><p> &nbsp; - Data is stored locally on Snowcone during the experiment.</p><p> &nbsp; - After the experiment, return Snowcone to AWS to automatically load data into Amazon S3.</p><p>3. Why Not the Other Options?</p><p> &nbsp; - A (Snowball Edge Compute Optimized + DataSync/NFS):</p><p> &nbsp; &nbsp; - Overkill for 6 TB (Snowcone is sufficient).</p><p> &nbsp; &nbsp; - Sensors don’t support NFS (only FTP).</p><p> &nbsp; - B (Snowcone + shell script to download data):</p><p> &nbsp; &nbsp; - Sensors can’t be pulled from (they only push via FTP).</p><p> &nbsp; &nbsp; - Data ends up on EBS volume, not S3 (directly).</p><p> &nbsp; - D (Snowcone + FSx + DataSync):</p><p> &nbsp; &nbsp; - FSx is unnecessary complexity for FTP.</p><p> &nbsp; &nbsp; - DataSync requires internet (not available in this scenario).</p><p> Key Benefits of Option C:</p><p>✔ Matches sensor capabilities (FTP-only support). &nbsp;</p><p>✔ Snowcone is cost-effective for 6 TB. &nbsp;</p><p>✔ Automatic S3 upload after device return. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4891d0fbf0044f4a932112b3a9b9689b",
            "questionNumber": 450,
            "type": "single",
            "content": "<p>Question #450</p><p>A company that has multiple business units is using AWS Organizations with all features enabled. The company has implemented an account structure in which each business unit has its own AWS account. Administrators in each AWS account need to view detailed cost and utilization data for their account by using Amazon Athena.</p><p>Each business unit can have access to only its own cost and utilization data. The IAM policies that govern the ability to set up AWS Cost and Usage Reports are in place. A central Cost and Usage Report that contains all data for the organization is already available in an Amazon S3 bucket.</p><p>Which solution will meet these requirements with the LEAST operational complexity?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "In the organization&#39;s management account, use AWS Resource Access Manager (AWS RAM) to share the Cost and Usage Report data with each member account."
                },
                {
                    "label": "B",
                    "content": "In the organization&#39;s management account, configure an S3 event to invoke an AWS Lambda function each time a new file arrives in the S3 bucket that contains the central Cost and Usage Report. Configure the Lambda function to extract each member account&rsquo;s data and to place the data in Amazon S3 under a separate prefix. Modify the S3 bucket policy to allow each member account to access its own prefix."
                },
                {
                    "label": "C",
                    "content": "In each member account, access AWS Cost Explorer. Create a new report that contains relevant cost information for the account. Save the report in Cost Explorer. Provide instructions that the account administrators can use to access the saved report."
                },
                {
                    "label": "D",
                    "content": "In each member account, create a new S3 bucket to store Cost and Usage Report data. Set up a Cost and Usage Report to deliver the data to the new S3 bucket."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct choice as it provides an automated solution that minimizes operational complexity. By using a Lambda function triggered by S3 events, the process of separating and storing each member account's data is automated, requiring no manual intervention. Each account is then given access to only its own data prefix in the S3 bucket, ensuring that business units cannot access each other's data.</p><p>The requirements are: &nbsp;</p><p>1. Each business unit (AWS account) should have access to only its own cost and usage data. &nbsp;</p><p>2. The solution should use the existing centralized Cost and Usage Report (CUR) in an S3 bucket. &nbsp;</p><p>3. The solution should have the least operational complexity. &nbsp;</p><p>Let's analyze the options: &nbsp;</p><p> Option A: Using AWS RAM to share the CUR data &nbsp;</p><p>- AWS RAM is typically used for sharing resources like VPC subnets or License Manager configurations, not S3 data. &nbsp;</p><p>- Even if shared, the entire CUR dataset would be accessible to all accounts, violating the requirement of restricting access to only each account's own data. &nbsp;</p><p>- Not a valid solution. &nbsp;</p><p> Option B: Using S3 events + Lambda to filter and partition data &nbsp;</p><p>1. A Lambda function is triggered whenever a new CUR file arrives in the S3 bucket. &nbsp;</p><p>2. The Lambda function extracts data per account and stores it under separate prefixes (e.g., `BusinessUnitA/`, `BusinessUnitB/`). &nbsp;</p><p>3. An S3 bucket policy is updated to restrict each account to its own prefix using IAM conditions (e.g., `s3:prefix` or `aws:PrincipalAccount`). &nbsp;</p><p>- This ensures least operational complexity because: &nbsp;</p><p> &nbsp; - No manual setup is needed for each new account. &nbsp;</p><p> &nbsp; - The CUR remains centralized, but access is restricted per account. &nbsp;</p><p>- Meets all requirements. &nbsp;</p><p> Option C: Using AWS Cost Explorer &nbsp;</p><p>- Cost Explorer provides high-level cost data, not the detailed CUR data required for Athena queries. &nbsp;</p><p>- Saved reports in Cost Explorer cannot be queried with Athena. &nbsp;</p><p>- Does not meet the requirement. &nbsp;</p><p> Option D: Setting up separate CURs per account &nbsp;</p><p>- This would require managing multiple CURs, increasing operational overhead. &nbsp;</p><p>- The question states that a central CUR is already available, so this approach is redundant. &nbsp;</p><p>- Higher operational complexity. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option B is the best solution because it: &nbsp;</p><p>✔ Uses the existing centralized CUR. &nbsp;</p><p>✔ Automatically filters and partitions data per account. &nbsp;</p><p>✔ Restricts access using S3 bucket policies. &nbsp;</p><p>✔ Minimizes operational complexity. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "933cab30cc0c47668655d5752b91ca8b",
            "questionNumber": 451,
            "type": "single",
            "content": "<p>Question #451</p><p>A company is designing an AWS environment for a manufacturing application. The application has been successful with customers, and the application's user base has increased. The company has connected the AWS environment to the company's on-premises data center through a 1 Gbps AWS Direct Connect connection. The company has configured BGP for the connection.</p><p>The company must update the existing network connectivity solution to ensure that the solution is highly available, fault tolerant, and secure.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Add a dynamic private IP AWS Site-to-Site VPN as a secondary path to secure data in transit and provide resilience for the Direct Connect connection. Configure MACsec to encrypt traffic inside the Direct Connect connection."
                },
                {
                    "label": "B",
                    "content": "Provision another Direct Connect connection between the company&#39;s on-premises data center and AWS to increase the transfer speed and provide resilience. Configure MACsec to encrypt traffic inside the Direct Connect connection."
                },
                {
                    "label": "C",
                    "content": "Configure multiple private VIFs. Load balance data across the VIFs between the on-premises data center and AWS to provide resilience."
                },
                {
                    "label": "D",
                    "content": "Add a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resilience for the Direct Connect connection."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Add a static AWS Site-to-Site VPN as a secondary path to secure data in transit and to provide resilience for the Direct Connect connection.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>High Availability and Fault Tolerance:</p><p>The combination of Direct Connect and Site-to-Site VPN provides two diverse network paths</p><p>If the Direct Connect connection fails, traffic can automatically failover to the VPN connection</p><p>This creates a highly available and fault-tolerant network architecture without requiring a second Direct Connect connection</p><p>Enhanced Security:</p><p>The Site-to-Site VPN adds encryption for data in transit, which Direct Connect alone doesn't provide</p><p>This addresses the security requirement by ensuring that all traffic is protected, even if it has to traverse the public internet via the VPN connection</p><p>The VPN connection uses IPsec encryption protocols to secure the data</p><p>Cost-Effectiveness:</p><p>Adding a Site-to-Site VPN is significantly less expensive than provisioning a second Direct Connect connection</p><p>VPN connections have a low monthly cost plus data transfer charges</p><p>This approach maximizes the existing investment in Direct Connect while adding resilience at minimal additional cost</p><p>Simplicity of Implementation:</p><p>Setting up a Site-to-Site VPN as a backup path is relatively straightforward</p><p>It can be configured to work with the existing BGP setup for automatic failover</p><p>No additional physical infrastructure is required at the Direct Connect location</p><p>Option C (configuring multiple private VIFs and load balancing) would not be as effective because:</p><p>It doesn't inherently provide encryption for data in transit</p><p>Multiple private VIFs on the same physical connection don't provide true path diversity or resilience against physical connection failures</p><p>If the single 1 Gbps Direct Connect connection fails, all VIFs would be affected</p><p>This approach doesn't address the security requirement as effectively as adding encryption</p><p>The combination of Direct Connect with a Site-to-Site VPN backup provides the optimal balance of high availability, fault tolerance, and security while being the most cost-effective solution. This hybrid connectivity model is a recommended AWS best practice for creating resilient network connections between on-premises environments and AWS.</p><p>Sources</p><p>AWS Direct Connect + AWS Site-to-Site VPN - Amazon Virtual Private Cloud Connectivity Options （https://docs.aws.amazon.com/whitepapers/latest/aws-vpc-connectivity-options/aws-direct-connect-site-to-site-vpn.html）</p><p>Client VPN with VPC peering vs Client VPN withTransit gateway | AWS re:Post (https://repost.aws/questions/QUUotXjy2mTV6noMA-J86AXQ/client-vpn-with-vpc-peering-vs-client-vpn-withtransit-gateway)</p><p>REL02-BP02 Provision redundant connectivity between private networks in the cloud and on-premises environments - Reliability Pillar (https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/rel_planning_network_topology_ha_conn_private_networks.html)</p><p>Security - Hybrid Connectivity (https://docs.aws.amazon.com/whitepapers/latest/hybrid-connectivity/security.html)</p><p>REL02-BP02 Provision redundant connectivity between private networks in the cloud and on-premises environments - AWS Well-Architected Framework (https://docs.aws.amazon.com/wellarchitected/2024-06-27/framework/rel_planning_network_topology_ha_conn_private_networks.html)</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "61c438d797bd47038336993d965039ee",
            "questionNumber": 452,
            "type": "single",
            "content": "<p>Question #452</p><p>A company needs to modernize an application and migrate the application to AWS. The application stores user profile data as text in a single table in an on-premises MySQL database.</p><p>After the modernization, users will use the application to upload video files that are up to 4 GB in size. Other users must be able to download the video files from the application. The company needs a video storage solution that provides rapid scaling. The solution must not affect application performance.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the database to Amazon Aurora PostgreSQL by using AWS Database Migration Service (AWS DMS). Store the videos as base64 encoded strings in a TEXT column in the database."
                },
                {
                    "label": "B",
                    "content": "Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 key in the corresponding DynamoDB item."
                },
                {
                    "label": "C",
                    "content": "Migrate the database to Amazon Keyspaces (for Apache Cassandra) by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 object identifier in the corresponding Amazon Keyspaces entry."
                },
                {
                    "label": "D",
                    "content": "Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as base64-encoded strings in the corresponding DynamoDB item."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Migrate the database to Amazon DynamoDB by using AWS Database Migration Service (AWS DMS) with the AWS Schema Conversion Tool (AWS SCT). Store the videos as objects in Amazon S3. Store the S3 key in the corresponding DynamoDB item.</p><p>This solution will meet the requirements for the following reasons:</p><p>Rapid Scaling for Video Storage: Amazon S3 is specifically designed for object storage and can scale virtually without limits to store large files like the 4GB videos mentioned in the requirements. S3 can handle any number of uploads and downloads concurrently without performance degradation.</p><p>Performance Optimization: By storing only the S3 object keys (references) in DynamoDB and keeping the actual video files in S3, the database remains lightweight and optimized for fast queries. This separation ensures that the application's performance is not affected by the large video files.</p><p>Cost Efficiency: S3's tiered storage options provide cost-effective storage for large binary files like videos, which would be much more expensive to store in a database.</p><p>Appropriate Use of Services: This solution follows AWS best practices by using each service for its intended purpose:</p><p>DynamoDB for fast, scalable access to structured user profile data</p><p>S3 for scalable, durable storage of large binary objects (videos)</p><p>Migration Path: AWS DMS with AWS SCT provides a straightforward path to migrate from the on-premises MySQL database to DynamoDB, handling the schema differences between relational and NoSQL databases.</p><p>Option A (storing videos as base64-encoded strings in Aurora PostgreSQL) would not meet the requirements because:</p><p>Poor Scalability: Storing large binary files as base64 strings in a database doesn't scale well as the number of videos increases.</p><p>Performance Impact: Base64 encoding increases the size of the data by approximately 33%, making a 4GB video closer to 5.3GB when stored. This would significantly impact database performance, query times, and backup/restore operations.</p><p>Database Bloat: As more videos are uploaded, the database would grow rapidly, leading to increased costs and management complexity.</p><p>Inefficient Use of Resources: Relational databases are not optimized for storing and retrieving large binary objects, especially at scale.</p><p>The solution that uses DynamoDB for user profile data and S3 for video storage represents a modern, cloud-native approach that aligns with AWS best practices for handling different types of data. It provides the rapid scaling required for video storage while ensuring that application performance remains optimal.</p><p>Sources</p><p>Migrating an Amazon RDS for MySQL Database to an Amazon DynamoDB target - Database Migration Guide （https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.mysql2dynamodb.html）</p><p>Best practices for storing large items and attributes in DynamoDB - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-use-s3-too.html）</p><p>Modernizing your application by migrating from an RDBMS to Amazon DynamoDB - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/modernization-rdbms-dynamodb/welcome.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "fd795dbef8b34d8b8722eafe9117a355",
            "questionNumber": 453,
            "type": "single",
            "content": "Question #453<p>A company stores and manages documents in an Amazon Elastic File System (Amazon EFS) file system. The file system is encrypted with an AWS Key Management Service (AWS KMS) key. The file system is mounted to an Amazon EC2 instance that runs proprietary software.</p><p>The company has enabled automatic backups for the file system. The automatic backups use the AWS Backup default backup plan.</p><p>A solutions architect must ensure that deleted documents can be recovered within an RPO of 100 minutes.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new IAM role. Create a new backup plan. Use the new IAM role to create backups. Update the KMS key policy to allow the new IAM role to use the key. Implement an hourly backup schedule for the file system."
                },
                {
                    "label": "B",
                    "content": "Create a new backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Implement a custom cron expression to run a backup of the file system every 30 minutes."
                },
                {
                    "label": "C",
                    "content": "Create a new IAM role. Use the existing backup plan. Update the KMS key policy to allow the new IAM role to use the key. Enable continuous backups for point-in-time recovery."
                },
                {
                    "label": "D",
                    "content": "Use the existing backup plan. Update the KMS key policy to allow the AWSServiceRoleForBackup IAM role to use the key. Enable Cross Region Replication for the file system."
                }
            ],
            "correctAnswer": "A",
            "explanation": "Option A is correct because it allows for a more frequent backup schedule that meets the RPO requirement of 100 minutes. By creating a new backup plan with an hourly schedule, the company can ensure that deleted documents can be recovered within the specified time frame. The other options do not provide a means to achieve the RPO as effectively: Option B would require additional setup and may not integrate as seamlessly with AWS Backup, Option C's continuous backups are not supported for EFS, and Option D's Cross Region Replication does not provide point-in-time recovery for deleted documents.",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6245b207de654a9a8f645e556e1ed8af",
            "questionNumber": 454,
            "type": "single",
            "content": "<p>Question #454</p><p>A solutions architect must provide a secure way for a team of cloud engineers to use the AWS CLI to upload objects into an Amazon S3 bucket.</p><p>Each cloud engineer has an IAM user, IAM access keys, and a virtual multi-factor authentication (MFA) device. The IAM users for the cloud engineers are in a group that is named S3-access. The cloud engineers must use MFA to perform any actions in Amazon S3.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Attach a policy to the S3 bucket to prompt the IAM user for an MFA code when the IAM user performs actions on the S3 bucket. Use IAM access keys with the AWS CLI to call Amazon S3."
                },
                {
                    "label": "B",
                    "content": "Update the trust policy for the S3-access group to require principals to use MFA when principals assume the group. Use IAM access keys with the AWS CLI to call Amazon S3."
                },
                {
                    "label": "C",
                    "content": "Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Use IAM access keys with the AWS CLI to call Amazon S3."
                },
                {
                    "label": "D",
                    "content": "Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Request temporary credentials from AWS Security Token Service (AWS STS). Attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Attach a policy to the S3-access group to deny all S3 actions unless MFA is present. Request temporary credentials from AWS Security Token Service (AWS STS). Attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3.</p><p>This solution will meet the requirements for providing a secure way for cloud engineers to use the AWS CLI to upload objects to an S3 bucket with MFA for the following reasons:</p><p>MFA Enforcement: The policy attached to the S3-access group will deny all S3 actions unless MFA is present, ensuring that MFA is required for any S3 operations.</p><p>Temporary Credentials: Using AWS STS to request temporary credentials is more secure than using long-term IAM access keys directly. These temporary credentials:</p><p>Have a limited lifespan (typically 15 minutes to 36 hours)</p><p>Automatically expire after their configured duration</p><p>Reduce the risk if credentials are accidentally exposed</p><p>MFA Verification Process: When requesting temporary credentials from AWS STS using operations like GetSessionToken or AssumeRole, users must provide their MFA token as part of the request. This ensures that only authenticated users with physical possession of their MFA device can obtain valid credentials.</p><p>AWS CLI Profile: By attaching the temporary credentials to a profile that Amazon S3 will reference, the cloud engineers can use the AWS CLI with the proper authentication while maintaining security.</p><p>Security Best Practices: This approach follows AWS security best practices by:</p><p>Minimizing the use of long-term credentials</p><p>Enforcing MFA for sensitive operations</p><p>Using temporary, limited-privilege credentials</p><p>Implementing the principle of least privilege</p><p>Option C is less secure because it would use long-term IAM access keys directly with the AWS CLI. While the policy would still enforce MFA, using long-term credentials presents several security concerns:</p><p>They don't expire automatically</p><p>They remain valid until manually revoked</p><p>If compromised, they could potentially be used for longer periods</p><p>They don't provide the same level of auditability as temporary credentials</p><p>The solution in option D provides a more comprehensive security approach that aligns with AWS best practices for securing API access with MFA, making it the correct choice for this scenario.</p><p>Sources</p><p>Making requests - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/API/MakingRequests.html）</p><p>Secure API access with MFA - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_configure-api-require.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3380b2af88c44e35962853c52b755f4f",
            "questionNumber": 455,
            "type": "single",
            "content": "<p>Question #455</p><p>A company needs to migrate 60 on-premises legacy applications to AWS. The applications are based on the .NET Framework and run on Windows. The company needs a solution that minimizes migration time and requires no application code changes. The company also does not want to manage the infrastructure.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Refactor the applications and containerize them by using AWS Toolkit for .NET Refactoring. Use Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to host the containerized applications."
                },
                {
                    "label": "B",
                    "content": "Use the Windows Web Application Migration Assistant to migrate the applications to AWS Elastic Beanstalk. Use Elastic Beanstalk to deploy and manage the applications."
                },
                {
                    "label": "C",
                    "content": "Use the Windows Web Application Migration Assistant to migrate the applications to Amazon EC2 instances. Use the EC2 instances to deploy and manage the applications."
                },
                {
                    "label": "D",
                    "content": "Refactor the applications and containerize them by using AWS Toolkit for .NET Refactoring. Use Amazon Elastic Kubernetes Service (Amazon EKS) with the Fargate launch type to host the containerized applications."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use the Windows Web Application Migration Assistant to migrate the applications to AWS Elastic Beanstalk. Use Elastic Beanstalk to deploy and manage the applications.</p><p>This solution will meet the company's requirements for the following reasons:</p><p>Minimal Migration Time: The Windows Web Application Migration Assistant is specifically designed to streamline the migration of Windows-based applications to AWS with minimal effort. It automates much of the migration process, allowing for faster deployment compared to refactoring and containerizing applications.</p><p>No Application Code Changes: This approach allows the legacy .NET Framework applications to be migrated \"as-is\" without requiring any code modifications. The Windows Web Application Migration Assistant preserves the existing application structure and configurations, focusing on moving the applications rather than changing them.</p><p>Managed Infrastructure: AWS Elastic Beanstalk is a fully managed service that handles the underlying infrastructure, including capacity provisioning, load balancing, auto-scaling, and application health monitoring. The company won't need to manage servers, which aligns with their requirement of not wanting to manage infrastructure.</p><p>Windows Support: Elastic Beanstalk has built-in support for Windows environments and .NET applications, making it well-suited for hosting legacy .NET Framework applications.</p><p>Option A (refactoring and containerizing the applications) would not meet the requirements because:</p><p>It would require significant code changes through the refactoring process, which contradicts the requirement of \"no application code changes.\"</p><p>Containerizing 60 legacy applications would be time-consuming and complex, increasing the overall migration time.</p><p>While Fargate reduces infrastructure management compared to traditional container deployments, the containerization process itself would require additional expertise and effort that doesn't align with the goal of minimizing migration time.</p><p>The Windows Web Application Migration Assistant with Elastic Beanstalk provides the most straightforward path for migrating the legacy .NET Framework applications to AWS while meeting all the stated requirements: minimizing migration time, requiring no application code changes, and eliminating the need to manage infrastructure.</p><p>Sources</p><p>Migrating and Rehosting .NET Applications - Developing and Deploying .NET Applications on AWS （https://docs.aws.amazon.com/whitepapers/latest/develop-deploy-dotnet-apps-on-aws/migrating-and-rehosting-.net-applications.html）</p><p>Easily Migrate .NET Web Apps to AWS with Elastic Beanstalk, Part 1 | .NET on AWS Blog（https://aws.amazon.com/cn/blogs/dotnet/easily-migrate-net-web-apps-to-aws-with-elastic-beanstalk-part-1/） </p><p>Modernize your .NET application using AWS App2Container and AWS Toolkit for .NET | Containers （https://aws.amazon.com/cn/blogs/containers/replatform-and-refactor-your-workload-to-aws-using-aws-app2container-and-aws-toolkit-for-net-refactoring-application/）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "cf51a9a0b340483b9cbb646ddb52e9ae",
            "questionNumber": 456,
            "type": "single",
            "content": "<p>Question #456</p><p>A company needs to run large batch-processing jobs on data that is stored in an Amazon S3 bucket. The jobs perform simulations. The results of the jobs are not time sensitive, and the process can withstand interruptions.</p><p>Each job must process 15-20 GB of data when the data is stored in the S3 bucket. The company will store the output from the jobs in a different Amazon S3 bucket for further analysis.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a serverless data pipeline. Use AWS Step Functions for orchestration. Use AWS Lambda functions with provisioned capacity to process the data."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Batch compute environment that includes Amazon EC2 Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Batch compute environment that includes Amazon EC2 On-Demand Instances and Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy for the Spot Instances."
                },
                {
                    "label": "D",
                    "content": "Use Amazon Elastic Kubernetes Service (Amazon EKS) to run the processing jobs. Use managed node groups that contain a combination of Amazon EC2 On-Demand Instances and Spot Instances."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS Batch compute environment that includes Amazon EC2 Spot Instances. Specify the SPOT_CAPACITY_OPTIMIZED allocation strategy.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>Large Data Processing Capability: AWS Batch is specifically designed to handle large-scale batch processing jobs. It can easily process the 15-20 GB of data per job without the limitations that would affect a serverless approach.</p><p>Cost Optimization: EC2 Spot Instances offer significant cost savings (up to 90% compared to On-Demand instances) for workloads that can tolerate interruptions. Since the requirements explicitly state that \"the process can withstand interruptions\" and \"results are not time sensitive,\" Spot Instances are an ideal fit.</p><p>Interruption Handling: The SPOT_CAPACITY_OPTIMIZED allocation strategy specifically selects Spot Instances from the pools that are least likely to be interrupted, further improving job reliability while maintaining cost benefits.</p><p>Flexibility for Resource-Intensive Jobs: AWS Batch allows you to select appropriate instance types and sizes to efficiently handle the computational requirements of the simulation jobs, which may be resource-intensive.</p><p>Built-in Job Management: AWS Batch includes features for job queuing, dependency tracking, and automatic retries, which are valuable for managing batch processing workflows.</p><p>Option A (using Step Functions with Lambda functions) would be less suitable because:</p><p>Lambda Limitations: Lambda functions have constraints that make them less ideal for processing large datasets:</p><p>15-minute maximum execution time</p><p>Memory limitations (up to 10 GB)</p><p>Temporary storage limitations</p><p>Data Size Challenges: Processing 15-20 GB of data would likely require complex chunking and orchestration with Lambda, increasing development complexity.</p><p>Cost Considerations: Provisioned capacity for Lambda functions can be expensive for long-running or resource-intensive workloads compared to Spot Instances.</p><p>Resource Efficiency: Lambda is optimized for short, event-driven processing rather than large-scale batch computations.</p><p>AWS Batch with EC2 Spot Instances provides a purpose-built solution for exactly this type of workload - large-scale, interruptible batch processing jobs - at the lowest possible cost, making it the most cost-effective solution that meets all the stated requirements.</p><p>Sources</p><p>Use Amazon EC2 Spot best practices for AWS Batch - AWS Batch （https://docs.aws.amazon.com/batch/latest/userguide/bestpractice6.html）</p><p>Running batch jobs at scale with EC2 Spot Instances （https://aws.amazon.com/cn/tutorials/run-batch-jobs-at-scale-with-ec2-spot/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1bb8f9cecedd4d7a8478866374d1818a",
            "questionNumber": 457,
            "type": "single",
            "content": "<p>Question #457</p><p>A company has an application that analyzes and stores image data on premises. The application receives millions of new image files every day. Files are an average of 1 MB in size. The files are analyzed in batches of 1 GB. When the application analyzes a batch, the application zips the images together. The application then archives the images as a single file in an on-premises NFS server for long-term storage.</p><p>The company has a Microsoft Hyper-V environment on premises and has compute capacityavailable. The company does not have storage capacity and wants to archive the images on AWS. The company needs the ability to retrieve archived data within 1 week of a request.</p><p>The company has a 10 Gbps AWS Direct Connect connection between its on-premises data center and AWS. The company needs to set bandwidth limits and schedule archived images to be copied to AWS during non-business hours.</p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy an AWS DataSync agent on a new GPU-based Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Instant Retrieval. After the successful copy, delete the data from the on-premises storage."
                },
                {
                    "label": "B",
                    "content": "Deploy an AWS DataSync agent as a Hyper-V VM on premises. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive. After the successful copy, delete the data from the on-premises storage."
                },
                {
                    "label": "C",
                    "content": "Deploy an AWS DataSync agent on a new general purpose Amazon EC2 instance. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Standard. After the successful copy, delete the data from the on-premises storage. Create an S3 Lifecycle rule to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 day."
                },
                {
                    "label": "D",
                    "content": "Deploy an AWS Storage Gateway Tape Gateway on premises in the Hyper-V environment. Connect the Tape Gateway to AWS. Use automatic tape creation. Specify an Amazon S3 Glacier Deep Archive pool. Eject the tape after the batch of images is copied."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the most cost-effective solution for archiving image data to AWS, considering the company's requirements. Using AWS DataSync with a Hyper-V VM on-premises allows for efficient data transfer to Amazon S3 Glacier Deep Archive, which is an affordable storage option for long-term archiving. This approach also aligns with the need to retrieve archived data within 1 week, as S3 Glacier Deep Archive provides retrieval options that can meet this requirement. Option A's Instant Retrieval is not necessary for data that is accessed within a week, and Option C's S3 Standard is more expensive for storing large amounts of data before transitioning to Deep Archive. Option D's Tape Gateway is less efficient and more complex for the described use case.</p><p>Deploy an AWS DataSync agent as a Hyper-V VM on premises. Configure the DataSync agent to copy the batch of files from the NFS on-premises server to Amazon S3 Glacier Deep Archive. After the successful copy, delete the data from the on-premises storage.</p><p> Why? &nbsp;</p><p>1. Cost-Effective Storage: &nbsp;</p><p> &nbsp; - S3 Glacier Deep Archive is the cheapest storage option for long-term archival (retrieval within 12 hours or more, but the requirement allows for retrieval within 1 week, which fits). &nbsp;</p><p> &nbsp; - S3 Standard (Option C) is more expensive for long-term storage, and transitioning to Glacier Deep Archive after 1 day adds complexity. &nbsp;</p><p>2. Efficient Transfer: &nbsp;</p><p> &nbsp; - AWS DataSync is optimized for high-speed data transfer over Direct Connect (10 Gbps). &nbsp;</p><p> &nbsp; - Running DataSync on-premises as a Hyper-V VM (instead of an EC2 instance) saves costs since the company already has available compute capacity. &nbsp;</p><p>3. Bandwidth Control & Scheduling: &nbsp;</p><p> &nbsp; - DataSync allows bandwidth throttling and scheduling transfers during non-business hours, meeting the requirement. &nbsp;</p><p>4. No Unnecessary Components: &nbsp;</p><p> &nbsp; - Tape Gateway (Option D) is overkill since the data is already in files (not tapes). &nbsp;</p><p> &nbsp; - GPU-based EC2 (Option A) is unnecessary because no GPU processing is required. &nbsp;</p><p> &nbsp; - S3 Glacier Instant Retrieval (Option A) is more expensive than needed (retrieval in milliseconds is not required). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4820a18a838740388f15688cd8785715",
            "questionNumber": 458,
            "type": "single",
            "content": "<p>Question #458</p><p>A company wants to record key performance indicators (KPIs) from its application as part of a strategy to convert to a user-based licensing schema. The application is a multi-tier application with a web-based UI. The company saves all log files to Amazon CloudWatch by using the CloudWatch agent. All logins to the application are saved in a log file.</p><p>As part of the new license schema, the company needs to find out how many unique users each client has on a daily basis, weekly basis, and monthly basis.</p><p>Which solution will provide this information with the LEAST change to the application?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure an Amazon CloudWatch Logs metric filter that saves each successful login as a metric. Configure the user name and client name as dimensions for the metric."
                },
                {
                    "label": "B",
                    "content": "Change the application logic to make each successful login generate a call to the AWS SDK to increment a custom metric that records user name and client name dimensions in CloudWatch."
                },
                {
                    "label": "C",
                    "content": "Configure the CloudWatch agent to extract successful login metrics from the logs. Additionally, configure the CloudWatch agent to save the successful login metrics as a custom metric that uses the user name and client name as dimensions for the metric."
                },
                {
                    "label": "D",
                    "content": "Configure an AWS Lambda function to consume an Amazon CloudWatch Logs stream of the application logs. Additionally, configure the Lambda function to increment a custom metric in CloudWatch that uses the user name and client name as dimensions for the metric."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Configure an Amazon CloudWatch Logs metric filter that saves each successful login as a metric. Configure the user name and client name as dimensions for the metric.</p><p>This solution will provide the required information with the least change to the application for the following reasons:</p><p>No Application Code Changes: The CloudWatch Logs metric filter approach works with the existing logs that are already being saved to CloudWatch using the CloudWatch agent. This means no modifications to the application code are required.</p><p>Leveraging Existing Infrastructure: Since the company is already saving all log files to CloudWatch and login information is already being recorded in a log file, this solution simply extracts and transforms that existing data.</p><p>Powerful Metric Creation: CloudWatch Logs metric filters can parse log entries to extract specific information (like user names and client names) and convert them into CloudWatch metrics with dimensions.</p><p>Dimensional Analysis: By configuring user name and client name as dimensions for the metric, the company can easily analyze unique user counts per client across different time periods (daily, weekly, monthly) using CloudWatch's built-in aggregation capabilities.</p><p>Flexibility: If requirements change in the future, the metric filters can be modified without touching the application code.</p><p>Option B would require changing the application logic to make API calls to CloudWatch for each successful login, which represents a more significant change to the application. This approach would:</p><p>Require code modifications to the login process</p><p>Introduce dependencies on the AWS SDK within the application</p><p>Need deployment and testing of the modified application</p><p>Potentially impact application performance with additional API calls</p><p>Create tighter coupling between the application and AWS services</p><p>The CloudWatch Logs metric filter approach (Option A) achieves the same outcome with no application changes, making it the solution that provides the required information with the least change to the application. It's a non-invasive way to extract the KPIs needed for the new licensing schema while leveraging the existing logging infrastructure.</p><p>Sources</p><p>Create a metric filter for a log group - Amazon CloudWatch Logs （https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CreateMetricFilterProcedure.html）</p><p>Metric filter for specific log-stream | AWS re:Post （https://repost.aws/questions/QUJhK3p6NQRXyTeJliG-a-bg/metric-filter-for-specific-log-stream）</p><p>Application-level CloudWatch configuration - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/implementing-logging-monitoring-cloudwatch/application-level-configuration.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "168672addc554d8ebdba1241df05be97",
            "questionNumber": 459,
            "type": "single",
            "content": "<p>Question #459</p><p>A company is using GitHub Actions to run a CI/CD pipeline that accesses resources on AWS. The company has an IAM user that uses a secret key in the pipeline to authenticate to AWS. An existing IAM role with an attached policy grants the required permissions to deploy resources.</p><p>The company’s security team implements a new requirement that pipelines can no longer use long-lived secret keys. A solutions architect must replace the secret key with a short-lived solution.</p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an IAM SAML 2.0 identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Update GitHub to use SAML authentication for the pipeline."
                },
                {
                    "label": "B",
                    "content": "Create an IAM OpenID Connect (OIDC) identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP. Update GitHub to assume the role for the pipeline."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon Cognito identity pool. Configure the authentication provider to use GitHub. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub authentication provider. Configure the pipeline to use Cognito as its authentication provider."
                },
                {
                    "label": "D",
                    "content": "Create a trust anchor to AWS Private Certificate Authority. Generate a client certificate to use with AWS IAM Roles Anywhere. Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRole API call. Attach the existing IAM policy to the new IAM role. Configure the pipeline to use the credential helper tool and to reference the client certificate public key to assume the new IAM role."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an IAM OpenID Connect (OIDC) identity provider (IdP) in AWS Identity and Access Management (IAM). Create a new IAM role with the appropriate trust policy that allows the sts:AssumeRoleWithWebIdentity API call from the GitHub OIDC IdP. Update GitHub to assume the role for the pipeline.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>Short-lived Credentials: Using OIDC federation allows GitHub Actions to obtain temporary security credentials through the AssumeRoleWithWebIdentity API call. These credentials are automatically rotated and have a limited lifespan, satisfying the security team's requirement to eliminate long-lived secret keys.</p><p>Minimal Operational Overhead: Once the initial setup is complete, the solution requires very little ongoing maintenance:</p><p>No manual credential rotation is needed</p><p>No secrets need to be stored in GitHub</p><p>No additional services or infrastructure to manage</p><p>Direct Integration: GitHub Actions has built-in support for OIDC, making it straightforward to configure workflows to use this authentication method.</p><p>Security Best Practices: This approach follows the principle of least privilege by:</p><p>Eliminating long-term credentials</p><p>Using temporary, scoped permissions</p><p>Creating a trust relationship specific to your GitHub repository/organization</p><p>Simplified Workflow: The CI/CD pipeline can authenticate to AWS without managing or rotating credentials, reducing complexity and potential points of failure.</p><p>The implementation process involves:</p><p>Creating an OIDC provider in IAM that trusts GitHub's OIDC service</p><p>Creating an IAM role with a trust policy that allows GitHub Actions to assume the role</p><p>Attaching the existing permissions policy to this new role</p><p>Updating the GitHub Actions workflow to use the OIDC provider for authentication</p><p>This federated approach provides a secure, automated way to handle authentication between GitHub Actions and AWS services without storing any long-lived credentials. It represents the most modern and secure approach to CI/CD authentication with the least operational overhead, making it the best solution for the given requirements.</p><p>Sources</p><p>Security best practices - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/terraform-aws-provider-best-practices/security.html）</p><p>SEC02-BP02 Use temporary credentials - AWS Well-Architected Framework (2022-03-31) （https://docs.aws.amazon.com/wellarchitected/2022-03-31/framework/sec_identities_unique.html）</p><p>Best practices working with self-hosted GitHub Action runners at scale on AWS | AWS DevOps & Developer Productivity Blog （https://aws.amazon.com/cn/blogs/devops/best-practices-working-with-self-hosted-github-action-runners-at-scale-on-aws/）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4e1a7f88ee5845ea8bcd12e4ba9e4b49",
            "questionNumber": 460,
            "type": "multiple",
            "content": "<p>Question #460</p><p>A company is running a web-crawling process on a list of target URLs to obtain training documents for machine learning training algorithms. A fleet of Amazon EC2 t2.micro instances pulls the target URLs from an Amazon Simple Queue Service (Amazon SQS) queue. The instances then write the result of the crawling algorithm as a .csv file to an Amazon Elastic File System (Amazon EFS) volume. The EFS volume is mounted on all instances of the fleet.</p><p><br></p><p>A separate system adds the URLs to the SQS queue at infrequent rates. The instances crawl each URL in 10 seconds or less. </p><p><br></p><p>Metrics indicate that some instances are idle when no URLs are in the SQS queue. A solutions architect needs to redesign the architecture to optimize costs.</p><p><br></p><p>Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use m5.8xlarge instances instead of t2.micro instances for the web-crawling process. Reduce the number of instances in the fleet by 50%."
                },
                {
                    "label": "B",
                    "content": "Convert the web-crawling process into an AWS Lambda function. Configure the Lambda function to pull URLs from the SQS queue."
                },
                {
                    "label": "C",
                    "content": "Modify the web-crawling process to store results in Amazon Neptune."
                },
                {
                    "label": "D",
                    "content": "Modify the web-crawling process to store results in an Amazon Aurora Serverless MySQL instance."
                },
                {
                    "label": "E",
                    "content": "Modify the web-crawling process to store results in Amazon S3."
                }
            ],
            "correctAnswer": "BE",
            "explanation": "<p>Option B is correct as converting the web-crawling process into an AWS Lambda function would allow for on-demand execution without the need to maintain a fleet of EC2 instances. This would significantly reduce costs when the workload is variable and sporadic. Option E is also correct as storing results in Amazon S3 is a cost-effective solution for storing large amounts of data generated by the web-crawling process. S3 is a scalable and durable storage solution that can handle the output of the crawling process efficiently. Option A is not cost-effective as larger instances are more expensive and do not address the issue of idle time. Options C and D involve using more expensive database services that are not necessary for the use case and do not optimize for cost.</p><p> Answer: B & E &nbsp;</p><p>B. Convert the web-crawling process into an AWS Lambda function. Configure the Lambda function to pull URLs from the SQS queue. &nbsp;</p><p>E. Modify the web-crawling process to store results in Amazon S3. &nbsp;</p><p> Why? &nbsp;</p><p>1. Cost Optimization with AWS Lambda (Option B) &nbsp;</p><p> &nbsp; - Currently, EC2 t2.micro instances run continuously, leading to idle time when no URLs are in the queue. &nbsp;</p><p> &nbsp; - Lambda is event-driven, so it only runs when there’s a message in the SQS queue, eliminating idle costs. &nbsp;</p><p> &nbsp; - Since each crawl takes ≤10 seconds, Lambda fits well within its execution limits. &nbsp;</p><p> &nbsp; - Lambda scales automatically, removing the need to manage an EC2 fleet. &nbsp;</p><p>2. Cheaper Storage with Amazon S3 (Option E) &nbsp;</p><p> &nbsp; - EFS is expensive for storing large volumes of `.csv` files (charged per GB, plus throughput costs). &nbsp;</p><p> &nbsp; - S3 is significantly cheaper for object storage (pay only for storage used, no mounting or throughput fees). &nbsp;</p><p> &nbsp; - S3 is highly durable and integrates well with Lambda. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (m5.8xlarge instances): Overkill for this workload (high cost, still runs idle). &nbsp;</p><p>- C (Neptune): A graph database, unsuitable for `.csv` storage (expensive for this use case). &nbsp;</p><p>- D (Aurora Serverless): More expensive than S3 for simple `.csv` storage (better for structured queries). &nbsp;</p><p> Conclusion &nbsp;</p><p>By using Lambda (B) for compute and S3 (E) for storage, the solution eliminates idle costs and reduces storage expenses, making it the most cost-effective choice. ✅</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "9ad51934c3154c6e9b2398b734ee9887",
            "questionNumber": 461,
            "type": "single",
            "content": "<p>Question #461</p><p>A company needs to migrate its website from an on-premises data center to AWS. The website consists of a load balancer, a content management system (CMS) that runs on a Linux operating system, and a MySQL database.</p><p>The CMS requires persistent NFS-compatible storage for a file system. The new solution on AWS must be able to scale from 2 Amazon EC2 instances to 30 EC2 instances in response to unpredictable traffic increases. The new solution also must require no changes to the website and must prevent data loss.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon Elastic File System (Amazon EFS) file system. Deploy the CMS to AWS Elastic Beanstalk with an Application Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EFS file system to the EC2 instances. Create an Amazon Aurora MySQL database that is separate from the Elastic Beanstalk environment."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Deploy the CMS to AWS Elastic Beanstalk with a Network Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EBS volume to the EC2 instances. Create an Amazon RDS for MySQL database in the Elastic Beanstalk environment."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon Elastic File System (Amazon EFS) file system. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create a Network Load Balancer to distribute traffic. Create an Amazon Aurora MySQL database. Use an EC2 Auto Scaling scale-in lifecycle hook to mount the EFS file system to the EC2 instances."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon Elastic Block Store (Amazon EBS) Multi-Attach volume. Create a launch template and an Auto Scaling group to launch EC2 instances to support the CMS. Create an Application Load Balancer to distribute traffic. Create an Amazon ElastiCache for Redis cluster to support the MySQL database. Use EC2 user data to attach the EBS volume to the EC2 instances."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create an Amazon ElasticFile System (Amazon EFS) file system. Deploy the CMS to AWS Elastic Beanstalk with an Application Load Balancer and an Auto Scaling group. Use .ebextensions to mount the EFS file system to the EC2 instances. Create an Amazon Aurora MySQL database that is separate from the Elastic Beanstalk environment.</p><p>This solution will meet all the requirements for the following reasons:</p><p>NFS-compatible Storage Requirement:</p><p>Amazon EFS provides a fully managed, scalable NFS file system that can be mounted on multiple EC2 instances simultaneously.</p><p>This satisfies the CMS requirement for persistent NFS-compatible storage for its file system.</p><p>Scalability Requirement:</p><p>AWS Elastic Beanstalk with an Auto Scaling group can easily handle scaling from 2 to 30 EC2 instances based on traffic demands.</p><p>The Application Load Balancer will distribute incoming traffic across all available instances, ensuring optimal performance during traffic spikes.</p><p>No Changes to Website Requirement:</p><p>By using .ebextensions to mount the EFS file system to the EC2 instances, the shared storage configuration is handled automatically for each instance.</p><p>This ensures that all instances access the same files, maintaining consistency across the environment without requiring changes to the website code.</p><p>The CMS will continue to operate as it did on-premises, with its file system accessible via the same paths.</p><p>Data Loss Prevention Requirement:</p><p>Amazon EFS provides high durability and availability for the file system data, storing data redundantly across multiple Availability Zones.</p><p>Amazon Aurora MySQL offers enhanced durability and reliability compared to standard MySQL, with automated backups and point-in-time recovery.</p><p>Separating the database from the Elastic Beanstalk environment ensures that database data persists independently of the application tier.</p><p>Additional Benefits:</p><p>Elastic Beanstalk simplifies deployment and management of the application environment.</p><p>Aurora MySQL is compatible with standard MySQL, allowing for a seamless migration from the on-premises MySQL database.</p><p>The architecture separates concerns (compute, storage, database) for better scalability and management.</p><p>This solution provides a comprehensive approach that addresses all the requirements while leveraging AWS managed services to reduce operational overhead. It creates a highly available, scalable, and resilient architecture that can handle unpredictable traffic patterns while ensuring data persistence and requiring no changes to the existing website.</p><p>Sources</p><p>Moving to an AWS architecture - Hosting Static Websites on AWS （https://docs.aws.amazon.com/whitepapers/latest/build-static-websites-aws/moving-to-an-aws-architecture.html）</p><p>Reference architecture - Best Practices for WordPress on AWS （https://docs.aws.amazon.com/whitepapers/latest/best-practices-wordpress/reference-architecture.html）</p><p>Deploying a high-availability Drupal website with an external Amazon RDS database to Elastic Beanstalk - AWS Elastic Beanstalk （https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/php-hadrupal-tutorial.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "610e4044bd7045c0bc3693656b51266c",
            "questionNumber": 462,
            "type": "single",
            "content": "<p>Question #462</p><p>A company needs to implement disaster recovery for a critical application that runs in a single AWS Region. The application's users interact with a web frontend that is hosted on Amazon EC2 instances behind an Application Load Balancer (ALB). The application writes to an Amazon RDS for MySQL DB instance. The application also outputs processed documents that are stored in an Amazon S3 bucket.</p><p>The company’s finance team directly queries the database to run reports. During busy periods, these queries consume resources and negatively affect application performance.</p><p>A solutions architect must design a solution that will provide resiliency during a disaster. The solution must minimize data loss and must resolve the performance problems that result from the finance team's queries.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the database to Amazon DynamoDB and use DynamoDB global tables. Instruct the finance team to query a global table in a separate Region. Create an AWS Lambda function to periodically synchronize the contents of the original S3 bucket to a new S3 bucket in the separate Region. Launch EC2 instances and create an ALB in the separate Region. Configure the application to point to the new S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Launch additional EC2 instances that host the application in a separate Region. Add the additional instances to the existing ALB in the separate Region, create a read replica of the RDS DB instance. Instruct the finance team to run queries against the read replica. Use S3 Cross Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Configure the application to point to the new S3 bucket and to the newly promoted read replica."
                },
                {
                    "label": "C",
                    "content": "Create a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket."
                },
                {
                    "label": "D",
                    "content": "Create hourly snapshots of the RDS DB instance. Copy the snapshots to a separate Region. Add an Amazon ElastiCache cluster in front of the existing RDS database. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, restore the database from the latest RDS snapshot. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct solution as it addresses both disaster recovery and performance optimization. By creating a read replica of the RDS DB instance in a separate Region, the solution ensures that the finance team can run their queries without impacting the performance of the primary application. Additionally, the use of AMIs for EC2 instances allows for rapid recovery in the event of a disaster, and S3 Cross-Region Replication ensures that the documents stored in S3 are also protected. Option A is less suitable because migrating to DynamoDB would require changes to the application, which is not desired. Option B is not as efficient because it does not address the performance issue during normal operations. Option D is less preferable because it relies on snapshots, which do not provide the same level of real-time replication and availability as a read replica.</p><p> Answer: C &nbsp;</p><p>Create a read replica of the RDS DB instance in a separate Region. Instruct the finance team to run queries against the read replica. Create AMIs of the EC2 instances that host the application frontend. Copy the AMIs to the separate Region. Use S3 Cross-Region Replication (CRR) from the original S3 bucket to a new S3 bucket in the separate Region. During a disaster, promote the read replica to a standalone DB instance. Launch EC2 instances from the AMIs and create an ALB to present the application to end users. Configure the application to point to the new S3 bucket. &nbsp;</p><p> Why? &nbsp;</p><p>1. Disaster Recovery (Minimal Data Loss & Fast Recovery) &nbsp;</p><p> &nbsp; - RDS Read Replica in a separate Region provides near real-time replication (low RPO). &nbsp;</p><p> &nbsp; - Promoting the read replica during a disaster ensures fast failover (low RTO). &nbsp;</p><p> &nbsp; - S3 Cross-Region Replication (CRR) ensures documents are available in the secondary Region. &nbsp;</p><p> &nbsp; - AMI copy + ALB setup allows quick application deployment in the secondary Region. &nbsp;</p><p>2. Performance Issue Resolution &nbsp;</p><p> &nbsp; - Finance team queries run on the read replica, preventing impact on the primary database. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (DynamoDB + Lambda sync): &nbsp;</p><p> &nbsp;- Migrating to DynamoDB is unnecessary (RDS is suitable for MySQL workloads). &nbsp;</p><p> &nbsp;- Lambda sync for S3 is less efficient than built-in S3 CRR. &nbsp;</p><p> &nbsp;- DynamoDB global tables are for multi-active workloads, not disaster recovery. &nbsp;</p><p>- B (Adding instances to ALB in a separate Region): &nbsp;</p><p> &nbsp;- ALB does not support cross-Region routing, so this setup is invalid. &nbsp;</p><p> &nbsp;- The rest is similar to Option C, but the ALB approach is incorrect. &nbsp;</p><p>- D (RDS Snapshots + ElastiCache): &nbsp;</p><p> &nbsp;- Hourly snapshots mean potential data loss (RPO of 1 hour) vs. near real-time with read replicas. &nbsp;</p><p> &nbsp;- ElastiCache helps with performance but doesn’t solve disaster recovery as effectively as a read replica. &nbsp;</p><p> Conclusion &nbsp;</p><p>Option C provides: &nbsp;</p><p>✅ Minimal data loss (read replica sync). &nbsp;</p><p>✅ Fast recovery (promote replica, deploy AMIs). &nbsp;</p><p>✅ Performance isolation (finance team uses read replica). &nbsp;</p><p>✅ S3 CRR for document resilience. &nbsp;</p><p>Thus, C is the best solution. ✅</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ac4710fbb9224d88925fdc1affd21995",
            "questionNumber": 463,
            "type": "single",
            "content": "<p>Question #463</p><p>A company has many services running in its on-premises data center. The data center is connected to AWS using AWS Direct Connect (DX) and an IPSec VPN. The service data is sensitive and connectivity cannot traverse the internet. The company wants to expand into a new market segment and begin offering its services to other companies that are using AWS.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a VPC Endpoint Service that accepts TCP traffic, host it behind a Network Load Balancer, and make the service available over DX."
                },
                {
                    "label": "B",
                    "content": "Create a VPC Endpoint Service that accepts HTTP or HTTPS traffic, host it behind an Application Load Balancer, and make the service available over DX."
                },
                {
                    "label": "C",
                    "content": "Attach an internet gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic."
                },
                {
                    "label": "D",
                    "content": "Attach a NAT gateway to the VPC, and ensure that network access control and security group rules allow the relevant inbound and outbound traffic."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a VPC Endpoint Service that accepts TCP traffic, host it behind a Network Load Balancer, and make the service available over DX.</p><p>This solution will meet the requirements for the following reasons:</p><p>Private Connectivity Requirement: The company needs to ensure that sensitive service data does not traverse the internet. Using a VPC Endpoint Service with AWS PrivateLink in combination with Direct Connect creates a private connection that keeps all traffic off the public internet.</p><p>Network Load Balancer Advantage: A Network Load Balancer (NLB) operates at Layer 4 (transport layer) and supports TCP traffic, which provides:</p><p>Greater protocol flexibility for various types of services</p><p>Lower latency and higher throughput for data-intensive applications</p><p>Support for both web and non-web based protocols</p><p>AWS PrivateLink Integration: VPC Endpoint Services are designed to work with AWS PrivateLink, allowing the company to expose their services to other AWS customers without requiring internet connectivity. The service can be made available to specific AWS accounts, organizations, or publicly to all AWS customers.</p><p>Direct Connect Compatibility: The solution leverages the existing Direct Connect connection between the on-premises data center and AWS, ensuring that traffic between the company's on-premises services and AWS remains on a private network.</p><p>Security for Sensitive Data: By keeping all traffic on private networks (either Direct Connect or AWS's private network), the solution maintains the security of sensitive data by eliminating exposure to the public internet.</p><p>Option B would not be as suitable because:</p><p>An Application Load Balancer (ALB) only supports HTTP/HTTPS traffic, which may be too restrictive if the company's services require other protocols</p><p>ALBs operate at Layer 7 (application layer), which adds overhead and may not be necessary for all types of services</p><p>Some services may not be compatible with HTTP/HTTPS protocols</p><p>The solution in Option A provides the most flexible and secure approach for the company to expand its market segment by offering its on-premises services to other AWS customers while ensuring that sensitive data remains protected and never traverses the public internet.</p><p>Sources</p><p>AWS PrivateLink: Enhancing Security and Connectivity with AWS PrivateLink | AWS re:Post（https://repost.aws/articles/ARw-phoCRnSlqXN512Ibr9SA/aws-privatelink-enhancing-security-and-connectivity-with-aws-privatelink） </p><p>AWS Direct Connect &nbsp;- Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/direct-connect.html）</p><p>AWS PrivateLink: Enhancing Security and Connectivity with AWS PrivateLink | AWS re:Post （https://repost.aws/articles/AR3l2gl0W5TpePneYc_E1mDA/aws-privatelink-enhancing-security-and-connectivity-with-aws-privatelink）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "df34762b52374fc0b3d6b2982f45271d",
            "questionNumber": 464,
            "type": "single",
            "content": "<p>Question #464</p><p>A company uses AWS Organizations to manage its AWS accounts. A solutions architect must design a solution in which only administrator roles are allowed to use IAM actions. However, the solutions architect does not have access to all the AWS accounts throughout the company.</p><p>Which solution meets these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an SCP that applies to all the AWS accounts to allow IAM actions only for administrator roles. Apply the SCP to the root OU."
                },
                {
                    "label": "B",
                    "content": "Configure AWS CloudTrail to invoke an AWS Lambda function for each event that is related to IAM actions. Configure the function to deny the action if the user who invoked the action is not an administrator."
                },
                {
                    "label": "C",
                    "content": "Create an SCP that applies to all the AWS accounts to deny IAM actions for all users except for those with administrator roles. Apply the SCP to the root OU."
                },
                {
                    "label": "D",
                    "content": "Set an IAM permissions boundary that allows IAM actions. Attach the permissions boundary to every administrator role across all the AWS accounts."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirement to allow only administrator roles to use IAM actions across AWS accounts in an organization with the least operational overhead, the correct answer is:</p><p>C. Create an SCP that applies to all the AWS accounts to deny IAM actions for all users except for those with administrator roles. Apply the SCP to the root OU.</p><p>This solution is the most effective with the least operational overhead for several key reasons:</p><p>1.Centralized implementation: By applying the SCP to the root OU, it automatically affects all accounts in the organization without requiring access to each individual account.</p><p>2.Deny-based approach: Using a deny statement with an exception is more secure than an allow-based approach. SCPs use a deny-by-default model, so explicitly denying IAM actions except for administrator roles creates a stronger security boundary.</p><p>3.Low maintenance: Once implemented, this solution requires minimal ongoing management. The SCP will automatically apply to new accounts added to the organization.</p><p>4.Preventive control: This approach prevents unauthorized IAM actions before they occur, rather than detecting and responding to them after the fact.</p><p><br></p><p>The other options have significant drawbacks:</p><p>A. Creating an SCP that only allows IAM actions for administrator roles is less effective because SCPs work as permission boundaries (they don't grant permissions, they limit them). An allow-only SCP would need to be carefully crafted to avoid unintended consequences and would be more complex to maintain.</p><p><br></p><p>B. Using CloudTrail with Lambda would be a detective and reactive approach rather than preventive. This would have much higher operational overhead due to:</p><p><br></p><p>Setting up and maintaining the Lambda function</p><p>Processing CloudTrail logs (which could incur additional costs)</p><p>Potential latency between when an action occurs and when it's detected</p><p>The solution would only detect violations after they occur rather than preventing them</p><p>The SCP approach in option C provides a preventive, organization-wide control that requires minimal maintenance and doesn't require access to individual accounts, making it the solution with the least operational overhead.</p><p>Sources</p><p>AWS Identity and Access Management and AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-iam.html）</p><p>How to implement a read-only service control policy (SCP) for accounts in AWS Organizations | AWS Cloud Operations & Migrations Blog （https://aws.amazon.com/blogs/mt/implement-read-only-service-control-policy-in-aws-organizations/）</p><p>Service control policies (SCPs) - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a48308fee18d410387d5fbf66c9ea458",
            "questionNumber": 465,
            "type": "single",
            "content": "<p>Question #465</p><p>A company uses an organization in AWS Organizations to manage multiple AWS accounts. The company hosts some applications in a VPC in the company's shared services account.</p><p>The company has attached a transit gateway to the VPC in the shared services account.</p><p>The company is developing a new capability and has created a development environment that requires access to the applications that are in the shared services account. The company intends to delete and recreate resources frequently in the development account. The company also wants to give a development team the ability to recreate the team's connection to the shared services account as required.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a transit gateway in the development account. Create a transit gateway peering request to the shared services account. Configure the shared services transit gateway to automatically accept peering connections."
                },
                {
                    "label": "B",
                    "content": "Turn on automatic acceptance for the transit gateway in the shared services account. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway resource in the shared services account with the development account. Accept the resource in the development account. Create a transit gateway attachment in the development account."
                },
                {
                    "label": "C",
                    "content": "Turn on automatic acceptance for the transit gateway in the shared services account. Create a VPC endpoint. Use the endpoint policy to grant permissions on the VPC endpoint for the development account. Configure the endpoint service to automatically accept connection requests."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon EventBridge rule to invoke an AWS Lambda function that accepts the transit gateway attachment when the development account makes an attachment request. Use AWS Network Manager to share the transit gateway in the shared services account with the development account. Accept the transit gateway in the development account."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Option B is the correct solution as it allows for the sharing of the transit gateway with the development account using AWS Resource Access Manager, which supports the ability to automatically accept connections. This approach provides the flexibility for the development team to recreate their connection to the shared services account as needed, meeting the requirement for frequent deletion and recreation of resources. Option A is incorrect because peering is used between VPCs in different accounts, not for connecting to a transit gateway. Option C is incorrect because VPC endpoints do not support automatic acceptance of connections in this context. Option D is overly complex and does not directly address the requirement for transit gateway connectivity.</p><p>Turn on automatic acceptance for the transit gateway in the shared services account. Use AWS Resource Access Manager (AWS RAM) to share the transit gateway resource in the shared services account with the development account. Accept the resource in the development account. Create a transit gateway attachment in the development account. &nbsp;</p><p> Why? &nbsp;</p><p>1. Meets Key Requirements: &nbsp;</p><p> &nbsp; - Frequent Recreations in Dev Account: The development team can create and delete transit gateway attachments as needed (self-service capability). &nbsp;</p><p> &nbsp; - Access to Shared Services: AWS RAM allows secure sharing of the transit gateway from the shared services account to the development account. &nbsp;</p><p> &nbsp; - Automatic Acceptance: Enabling auto-acceptance ensures seamless connectivity without manual approvals. &nbsp;</p><p>2. Simplified & Scalable: &nbsp;</p><p> &nbsp; - AWS RAM is the recommended way to share transit gateways across accounts in an AWS Organization. &nbsp;</p><p> &nbsp; - No complex scripting (Lambda/EventBridge) is needed (unlike Option D). &nbsp;</p><p> &nbsp; - No peering (Option A) is required, which is more complex and unnecessary for this use case. &nbsp;</p><p> &nbsp; - VPC endpoints (Option C) are not suitable for transit gateway connectivity. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (Transit Gateway Peering): &nbsp;</p><p> &nbsp;- Peering is not needed—RAM sharing is simpler and more efficient. &nbsp;</p><p> &nbsp;- Peering requires manual acceptance unless auto-accept is configured, but RAM is still the better choice. &nbsp;</p><p>- C (VPC Endpoint): &nbsp;</p><p> &nbsp;- VPC endpoints are for private AWS services (S3, DynamoDB, etc.), not transit gateway connections. &nbsp;</p><p> &nbsp;- This does not provide the required networking setup. &nbsp;</p><p>- D (EventBridge + Lambda + Network Manager): &nbsp;</p><p> &nbsp;- Overly complex—RAM already provides a native, automated way to share transit gateways. &nbsp;</p><p> &nbsp;- Network Manager is for monitoring, not resource sharing. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "50ee4dd5b5e0428dbced91a98916403a",
            "questionNumber": 466,
            "type": "single",
            "content": "<p>Question #466</p><p>A company wants to migrate virtual Microsoft workloads from an on-premises data center to AWS. The company has successfully tested a few sample workloads on AWS. The company also has created an AWS Site-to-Site VPN connection to a VPC. A solutions architect needs to generate a total cost of ownership (TCO) report for the migration of all the workloads from the data center.</p><p>Simple Network Management Protocol (SNMP) has been enabled on each VM in the data center. The company cannot add more VMs in the data center and cannot install additional software on the VMs. The discovery data must be automatically imported into AWS Migration Hub.</p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use the AWS Application Migration Service agentless service and the AWS Migration Hub Strategy Recommendations to generate the TCO report."
                },
                {
                    "label": "B",
                    "content": "Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Evaluator to generate the TCO report."
                },
                {
                    "label": "C",
                    "content": "Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Hub to generate the TCO report."
                },
                {
                    "label": "D",
                    "content": "Use the AWS Migration Readiness Assessment tool inside the VPC. Configure Migration Evaluator to generate the TCO report."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Launch a Windows Amazon EC2 instance. Install the Migration Evaluator agentless collector on the EC2 instance. Configure Migration Evaluator to generate the TCO report.</p><p>This solution will meet the requirements for the following reasons:</p><p>Non-intrusive Data Collection: The Migration Evaluator agentless collector can gather information from the on-premises VMs that have SNMP enabled without requiring any additional software installation on those VMs. This satisfies the requirement that the company cannot install additional software on the VMs.</p><p>TCO Report Generation: Migration Evaluator (formerly TSO Logic) is specifically designed to generate total cost of ownership (TCO) reports for migrations to AWS. It can analyze the current on-premises environment, including Microsoft workloads, and provide detailed cost comparisons between on-premises and AWS environments.</p><p>Utilization of Existing Infrastructure: The solution leverages the existing AWS Site-to-Site VPN connection to the VPC, allowing the EC2 instance running the Migration Evaluator agentless collector to communicate with the on-premises environment.</p><p>No Additional On-premises VMs: Since the collector is installed on an EC2 instance in AWS, there's no need to add more VMs in the on-premises data center, which the company cannot do according to the requirements.</p><p>Microsoft Workload Support: Migration Evaluator has specific capabilities for assessing Microsoft workloads, including analyzing server profiles, utilization data, and Microsoft licensing considerations.</p><p>While the Migration Evaluator itself doesn't directly integrate with AWS Migration Hub for automatic import of discovery data, it can collect the necessary information to generate the TCO report, which is the primary requirement stated in the question. The EC2 instance with the agentless collector can communicate with the on-premises VMs via SNMP over the existing Site-to-Site VPN connection.</p><p>This solution provides the most efficient way to generate a TCO report for the migration without installing additional software on the VMs or adding new VMs to the on-premises environment, making it the best match for the stated requirements.</p><p>Sources</p><p>Creating a directional business case - AWS Prescriptive Guidance（https://docs.aws.amazon.com/prescriptive-guidance/latest/application-portfolio-assessment-guide/directional-business-case.html） </p><p>Seeking Advice on Utilizing Migration Evaluator for AWS Migration Business Cases | AWS re:Post （https://repost.aws/questions/QU11RsGVWsQpOYHehhrbHpPA/seeking-advice-on-utilizing-migration-evaluator-for-aws-migration-business-cases）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "dbdaf690faa34c2c9dd50d4a30010428",
            "questionNumber": 467,
            "type": "single",
            "content": "Question #467<p>A company that is developing a mobile game is making game assets available in two AWS Regions. Game assets are served from a set of Amazon EC2 instances behind an Application Load Balancer (ALB) in each Region. The company requires game assets to be fetched from the closest Region. If game assets become unavailable in the closest Region, they should be fetched from the other Region.</p><p>What should a solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon CloudFront distribution. Create an origin group with one origin for each ALB. Set one of the origins as primary."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon Route 53 health check for each ALB. Create a Route 53 failover routing record pointing to the two ALBs. Set the Evaluate Target Health value to Yes."
                },
                {
                    "label": "C",
                    "content": "Create two Amazon CloudFront distributions, each with one ALB as the origin. Create an Amazon Route 53 failover routing record pointing to the two CloudFront distributions. Set the Evaluate Target Health value to Yes."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon Route 53 health check for each ALB. Create a Route 53 latency alias record pointing to the two ALBs. Set the Evaluate Target Health value to Yes."
                }
            ],
            "correctAnswer": "D",
            "explanation": "To meet the company's requirements for serving game assets from the closest AWS Region and providing failover capability, the best solution is:<p><br></p><p>D. Create an Amazon Route 53 health check for each ALB. Create a Route 53 latency alias record pointing to the two ALBs. Set the Evaluate Target Health value to Yes.</p><p><br></p><p>Here's why this solution is the most appropriate:</p><p>1. Latency-based routing: Route 53 latency routing allows you to direct traffic to the Region with the lowest latency for each user, ensuring game assets are fetched from the closest Region.</p><p>2. Health checks: By creating health checks for each ALB, Route 53 can detect if a Region's assets become unavailable.</p><p>3. Failover capability: Setting \"Evaluate Target Health\" to Yes enables automatic failover. If the closest Region's ALB fails the health check, Route 53 will route traffic to the other Region.</p><p>4. Alias records: Using alias records allows you to route traffic directly to ALBs, which is more efficient and cost-effective than using non-alias records.</p><p>This solution provides several benefits:</p><p>- Improved performance by serving assets from the closest Region</p><p>- Automatic failover for high availability</p><p>- Cost-effective implementation using native AWS services</p><p>To implement this solution:</p><p><br></p><p>1. Create a Route 53 health check for each ALB to monitor their availability.</p><p>2. Set up a latency-based routing policy in Route 53.</p><p>3. Create alias records for each ALB, associating them with the latency-based routing policy.</p><p>4. Enable \"Evaluate Target Health\" for each record.</p><p><br></p><p>Remember to configure appropriate security groups and network ACLs for your ALBs and EC2 instances to ensure secure communication. Also, regularly review and update your routing policies and health check configurations to maintain optimal performance and reliability.</p><p>Sources</p><p>[1] [Use an Application Load Balancer for Amazon ECS - Amazon Elastic Container Service] (https://docs.aws.amazon.com/AmazonECS/latest/developerguide/alb.html)</p><p>[3] [Application Load Balancers - Elastic Load Balancing] (https://docs.aws.amazon.com/elasticloadbalancing/latest/application/application-load-balancers.html)</p><p>[6] [Load Balancing - Amazon EKS] (https://docs.aws.amazon.com/eks/latest/best-practices/load-balancing.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6c04e6cad13148978bf8c1914c0c240d",
            "questionNumber": 468,
            "type": "single",
            "content": "<p>Question #468</p><p>A company deploys workloads in multiple AWS accounts. Each account has a VPC with VPC flow logs published in text log format to a centralized Amazon S3 bucket. Each log file is compressed with gzip compression. The company must retain the log files indefinitely.</p><p>A security engineer occasionally analyzes the logs by using Amazon Athena to query the VPC flow logs. The query performance is degrading over time as the number of ingested logs is growing. A solutions architect must improve the performance of the log analysis and reduce the storage space that the VPC flow logs use.</p><p>Which solution will meet these requirements with the LARGEST performance improvement?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Lambda function to decompress the gzip files and to compress the files with bzip2 compression. Subscribe the Lambda function to an s3:ObjectCreated:Put S3 event notification for the S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Enable S3 Transfer Acceleration for the S3 bucket. Create an S3 Lifecycle configuration to move files to the S3 Intelligent-Tiering storage class as soon as the files are uploaded."
                },
                {
                    "label": "C",
                    "content": "Update the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files."
                },
                {
                    "label": "D",
                    "content": "Create a new Athena workgroup without data usage control limits. Use Athena engine version 2."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Update the VPC flow log configuration to store the files in Apache Parquet format. Specify hourly partitions for the log files.</p><p>This solution will provide the largest performance improvement for the following reasons:</p><p>Columnar Storage Format: Apache Parquet is a columnar storage format that significantly improves query performance in Athena compared to text log formats. When querying VPC flow logs, you typically only need specific columns (like source IP, destination IP, or time ranges), and Parquet allows Athena to read only the required columns rather than scanning the entire dataset.</p><p>Compression Efficiency: Parquet provides better compression than gzip-compressed text files, typically reducing storage space by about 20%. This addresses the requirement to reduce storage space while also improving query performance by reducing the amount of data that needs to be scanned.</p><p>Hourly Partitioning: Partitioning the data by hour creates a time-based directory structure that allows Athena to skip irrelevant data blocks during queries. When the security engineer runs time-based queries (which are common for log analysis), Athena will only scan the partitions that match the time range in the query, dramatically reducing the amount of data processed.</p><p>Predicate Pushdown: Parquet format enables predicate pushdown, which pushes filtering operations closer to the data source. This means filtering happens before data is loaded into memory, significantly reducing the amount of data that needs to be processed.</p><p>Long-term Scalability: As the company needs to retain logs indefinitely, the performance benefits of Parquet format and partitioning become increasingly important over time as the dataset grows.</p><p>Option D (creating a new Athena workgroup without data usage control limits and using Athena engine version 2) would provide some performance improvements but doesn't address the fundamental issue of inefficient data format and organization. While Athena engine version 2 is faster than version 1, the performance gains would be much smaller compared to the optimization of the data format itself.</p><p>The Parquet format with hourly partitioning directly addresses both requirements: improving query performance and reducing storage space. This approach provides a more sustainable solution as the volume of log data continues to grow indefinitely.</p><p>Sources</p><p>Create tables for flow logs in Apache Parquet format - Amazon Athena （https://docs.aws.amazon.com/athena/latest/ug/vpc-flow-logs-parquet.html）</p><p>Create tables for flow logs in Apache Parquet format using partition projection - Amazon Athena （https://docs.aws.amazon.com/athena/latest/ug/vpc-flow-logs-partition-projection-parquet-example.html）</p><p>Monitoring and optimizing the data lake environment - Storage Best Practices for Data and Analytics Applications （https://docs.aws.amazon.com/whitepapers/latest/building-data-lakes/monitoring-optimizing-data-lake-environment.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "54cf7bbd57654d08a249427d123dd4ce",
            "questionNumber": 469,
            "type": "multiple",
            "content": "<p>Question #469</p><p>A company wants to establish a dedicated connection between its on-premises infrastructure and AWS. The company is setting up a 1 Gbps AWS Direct Connect connection to its account VPC. The architecture includes a transit gateway and a Direct Connect gateway to connect multiple VPCs and the on-premises infrastructure.</p><p><br></p><p>The company must connect to VPC resources over a transit VIF by using the Direct Connect connection.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Update the 1 Gbps Direct Connect connection to 10 Gbps."
                },
                {
                    "label": "B",
                    "content": "Advertise the on-premises network prefixes over the transit VIF."
                },
                {
                    "label": "C",
                    "content": "Advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF."
                },
                {
                    "label": "D",
                    "content": "Update the Direct Connect connection&#39;s MACsec encryption mode attribute to must_encrypt."
                },
                {
                    "label": "E",
                    "content": "Associate a MACsec Connection Key Name/Connectivity Association Key (CKN/CAK) pair with the Direct Connect connection."
                }
            ],
            "correctAnswer": "BC",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>B. Advertise the on-premises network prefixes over the transit VIF. </p><p>C. Advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF.</p><p>These two steps are essential for establishing connectivity between the on-premises infrastructure and VPC resources over a transit VIF using the Direct Connect connection. Here's why these are the correct choices:</p><p>Advertise the on-premises network prefixes over the transit VIF (Option B):</p><p>This step is crucial because it informs the AWS network (specifically the Direct Connect gateway and transit gateway) about the reachable destinations in the on-premises network.</p><p>Without this advertisement, the transit gateway wouldn't know which traffic should be routed to the on-premises network via the Direct Connect gateway.</p><p>This enables traffic from VPC resources to reach the on-premises infrastructure by providing the necessary routing information.</p><p>Advertise the VPC prefixes from the Direct Connect gateway to the on-premises network over the transit VIF (Option C):</p><p>This step is equally important as it informs the on-premises network about the reachable AWS VPC destinations.</p><p>Without this advertisement, the on-premises routers wouldn't know which traffic should be sent over the Direct Connect connection to reach VPC resources.</p><p>This enables traffic from the on-premises infrastructure to reach the VPC resources by providing the necessary routing information.</p><p>The other options are not required for the specified setup:</p><p>Option A (Update the 1 Gbps Direct Connect connection to 10 Gbps): The existing 1 Gbps connection is sufficient for the setup. While increasing bandwidth might improve performance, it's not a requirement for establishing connectivity.</p><p>Option D (Update the Direct Connect connection's MACsec encryption mode attribute to must_encrypt) and Option E (Associate a MACsec Connection Key Name/Connectivity Association Key (CKN/CAK) pair): These options relate to MACsec encryption, which is only applicable for 10 Gbps and 100 Gbps Direct Connect connections at select locations. Since the company is using a 1 Gbps connection, MACsec encryption is not applicable.</p><p>The combination of options B and C establishes the bidirectional routing path necessary for connectivity between the on-premises infrastructure and VPC resources over the transit VIF using the Direct Connect connection, transit gateway, and Direct Connect gateway.</p><p>Sources</p><p>How to advertise remote VPC subnets into a local VPC Direct connect connection | AWS re:Post （https://repost.aws/questions/QU5X-SBJS1TOGg4o5VE-5WiA/how-to-advertise-remote-vpc-subnets-into-a-local-vpc-direct-connect-connection）</p><p>Allowed prefixes interactions for AWS Direct Connect gateways - AWS Direct Connect （https://docs.aws.amazon.com/directconnect/latest/UserGuide/allowed-to-prefixes.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "19855c38ff5c480abc4e2479a078f2a0",
            "questionNumber": 470,
            "type": "single",
            "content": "<p>Question #470</p><p>A company wants to use Amazon WorkSpaces in combination with thin client devices to replace aging desktops. Employees use the desktops to access applications that work with Clinical trial data. Corporate security policy states that access to the applications must be restricted to only company branch office locations. The company is considering adding an additional branch office in the next 6 months.</p><p>Which solution meets these requirements with the MOST operational efficiency?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an IP access control group rule with the list of public addresses from the branch offices. Associate the IP access control group with the WorkSpaces directory."
                },
                {
                    "label": "B",
                    "content": "Use AWS Firewall Manager to create a web ACL rule with an IPSet with the list of public addresses from the branch office locations. Associate the web ACL with the WorkSpaces directory."
                },
                {
                    "label": "C",
                    "content": "Use AWS Certificate Manager (ACM) to issue trusted device certificates to the machines deployed in the branch office locations. Enable restricted access on the WorkSpaces directory."
                },
                {
                    "label": "D",
                    "content": "Create a custom WorkSpace image with Windows Firewall configured to restrict access to the public addresses of the branch offices. Use the image to deploy the WorkSpaces."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create an IP access control group rule with the list of public addresses from the branch offices. Associate the IP access control group with the WorkSpaces directory.</p><p>This solution meets the requirements with the most operational efficiency for the following reasons:</p><p>Purpose-Built Feature: IP access control groups are a native feature of Amazon WorkSpaces specifically designed to restrict access based on IP addresses. This makes it the most straightforward and efficient solution for the stated requirement.</p><p>Direct Integration: The IP access control groups integrate directly with the WorkSpaces directory, providing a clear and simple way to manage access restrictions without additional services or configurations.</p><p>Easy Management: When the company adds the new branch office in the next 6 months, updating the IP access control group will be a simple process of adding the new office's public IP address range to the existing group. This can be done quickly through the WorkSpaces console.</p><p>No Additional Services: This solution doesn't require setting up or managing any additional AWS services, reducing operational complexity and potential points of failure.</p><p>Cost-Effective: IP access control groups are included with WorkSpaces at no additional cost, making this solution more cost-effective than alternatives that might require additional services.</p><p>Compliance Support: This approach provides a clear and auditable method for enforcing the corporate security policy that restricts access to clinical trial data applications to only company branch office locations.</p><p>Option B (using AWS Firewall Manager with a web ACL) would be more complex and less operationally efficient because:</p><p>It involves setting up and managing additional AWS services (Firewall Manager and AWS WAF).</p><p>Web ACLs are primarily designed for web applications, not desktop access scenarios.</p><p>It introduces unnecessary complexity for a requirement that can be met with a simpler, built-in feature.</p><p>It would likely require more administrative overhead when adding the new branch office.</p><p>The IP access control group solution provides the most direct and operationally efficient way to meet the security requirements while also accommodating the planned expansion to an additional branch office in the near future.</p><p>Sources</p><p>IP access control groups for WorkSpaces Personal - Amazon WorkSpaces （https://docs.aws.amazon.com/workspaces/latest/adminguide/amazon-workspaces-ip-access-control-groups.html）</p><p>Workspaces - IP Restriction and Access via Corporate VPN | AWS re:Post （https://repost.aws/questions/QUJrRjN05oRaydKYEXhfcF-Q/workspaces-ip-restriction-and-access-via-corporate-vpn）</p><p>Managing IP access controls in Amazon WorkSpaces Secure Browser - Amazon WorkSpaces Secure Browser （https://docs.aws.amazon.com/workspaces-web/latest/adminguide/ip-access-controls.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b3db23fcf5864b18abba2d58bae47f12",
            "questionNumber": 471,
            "type": "multiple",
            "content": "<p>Question #471</p><p>A company uses AWS Organizations. The company runs two firewall appliances in a centralized networking account. Each firewall appliance runs on a manually configured highly available Amazon EC2 instance. A transit gateway connects the VPC from the centralized networking account to VPCs of member accounts. Each firewall appliance uses a static private IP address that is then used to route traffic from the member accounts to the internet.</p><p>During a recent incident, a badly configured script initiated the termination of both firewall appliances. During the rebuild of the firewall appliances, the company wrote a new script to configure the firewall appliances at startup.</p><p>The company wants to modernize the deployment of the firewall appliances. The firewall appliances need the ability to scale horizontally to handle increased traffic when the network expands. The company must continue to use the firewall appliances to comply with company policy. The provider of the firewall appliances has confirmed that the latest version of the firewall code will work with all AWS services.</p><p>Which combination of steps should the solutions architect recommend to meet these requirements MOST cost-effectively? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy a Gateway Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink."
                },
                {
                    "label": "B",
                    "content": "Deploy a Network Load Balancer in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink."
                },
                {
                    "label": "C",
                    "content": "Create an Auto Scaling group and a launch template that uses the new script as user data to configure the firewall appliances. Create a target group that uses the instance target type."
                },
                {
                    "label": "D",
                    "content": "Create an Auto Scaling group. Configure an AWS Launch Wizard deployment that uses the new script as user data to configure the firewall appliances. Create a target group that uses the IP target type."
                },
                {
                    "label": "E",
                    "content": "Create VPC endpoints in each member account. Update the route tables to point to the VPC endpoints."
                },
                {
                    "label": "F",
                    "content": "Create VPC endpoints in the centralized networking account. Update the route tables in each member account to point to the VPC endpoints."
                }
            ],
            "correctAnswer": "ACF",
            "explanation": "<p>The company needs a scalable, highly available, and cost-effective solution for deploying firewall appliances while maintaining compliance. Here's why these choices are the best: &nbsp;</p><p> A. Deploy a Gateway Load Balancer (GWLB) in the centralized networking account. Set up an endpoint service that uses AWS PrivateLink. &nbsp;</p><p>- Gateway Load Balancer (GWLB) is specifically designed for deploying third-party virtual appliances (like firewalls) in a scalable way. &nbsp;</p><p>- It integrates with AWS PrivateLink, allowing member accounts to securely route traffic through the centralized firewalls. &nbsp;</p><p>- This is more efficient than a Network Load Balancer (NLB) for this use case because GWLB handles traffic inspection and forwarding at scale. &nbsp;</p><p> C. Create an Auto Scaling group and a launch template that uses the new script as user data to configure the firewall appliances. Create a target group that uses the instance target type. &nbsp;</p><p>- Auto Scaling ensures high availability and horizontal scaling of firewall instances. &nbsp;</p><p>- The launch template with user data automates the configuration of new instances, eliminating manual setup. &nbsp;</p><p>- The target group (instance type) registers the firewall instances with the GWLB for traffic distribution. &nbsp;</p><p> F. Create VPC endpoints in the centralized networking account. Update the route tables in each member account to point to the VPC endpoints. &nbsp;</p><p>- VPC endpoints (Gateway Load Balancer endpoints, GWLBE) allow member accounts to route traffic to the centralized firewalls securely. &nbsp;</p><p>- Updating route tables in member accounts ensures all traffic passes through the firewall for inspection. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B (Network Load Balancer + PrivateLink): NLB is not optimized for firewall traffic inspection like GWLB. &nbsp;</p><p>- D (AWS Launch Wizard + IP target type): Launch Wizard is unnecessary here, and IP target type is less efficient than instance target type for this scenario. &nbsp;</p><p>- E (VPC endpoints in member accounts): The endpoints should be in the centralized account, not member accounts, for proper traffic flow. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A, C, F provide the most scalable, automated, and cost-effective solution while maintaining compliance. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "882518934fed47aa85aaf830e42a1c04",
            "questionNumber": 472,
            "type": "multiple",
            "content": "<p>Question #472</p><p>A solutions architect must implement a multi-Region architecture for an Amazon RDS for PostgreSQL database that supports a web application.</p><p>The database launches from an AWS CloudFormation template that includes AWS services and features that are present in both the primary and secondary Regions.</p><p>The database is configured for automated backups, and it has an RTO of 15 minutes and an RPO of 2 hours. The web application is configured to use an Amazon Route 53 record to route traffic to the database.</p><p>Which combination of steps will result in a highly available architecture that meets all the requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a cross-Region read replica of the database in the secondary Region. Configure an AWS Lambda function in the secondary Region to promote the read replica during a failover event."
                },
                {
                    "label": "B",
                    "content": "In the primary Region, create a health check on the database that will invoke an AWS Lambda function when a failure is detected. Program the Lambda function to recreate the database from the latest database snapshot in the secondary Region and update the Route 53 host records for the database."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Lambda function to copy the latest automated backup to the secondary Region every 2 hours."
                },
                {
                    "label": "D",
                    "content": "Create a failover routing policy in Route 53 for the database DNS record. Set the primary and secondary endpoints to the endpoints in each Region."
                },
                {
                    "label": "E",
                    "content": "Create a hot standby database in the secondary Region. Use an AWS Lambda function to restore the secondary database to the latest RDS automatic backup in the event that the primary database fails."
                }
            ],
            "correctAnswer": "AD",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Create a cross-Region read replica of the database in the secondary Region. Configure an AWS Lambda function in the secondary Region to promote the read replica during a failover event.</p><p>D. Create a failover routing policy in Route 53 for the database DNS record. Set the primary and secondary endpoints to the endpoints in each Region.</p><p>This combination of steps will result in a highly available multi-Region architecture that meets all the requirements for the following reasons:</p><p>Cross-Region Read Replica with Lambda-Automated Promotion (Option A):</p><p>This addresses the RTO requirement of 15 minutes by providing a standby database in the secondary Region that can be quickly promoted to primary status.</p><p>The Lambda function automates the promotion process, minimizing human intervention and reducing the time to failover.</p><p>Cross-Region replication for RDS PostgreSQL typically has very low replication lag (often seconds), which easily meets the RPO requirement of 2 hours.</p><p>This approach leverages the CloudFormation template mentioned in the scenario, as the same services and features are present in both Regions.</p><p>Route 53 Failover Routing Policy (Option D):</p><p>This ensures that the web application can seamlessly connect to the database after a failover event.</p><p>The failover routing policy automatically routes traffic to the healthy endpoint based on health checks.</p><p>When the primary database fails, Route 53 will detect this through health checks and automatically route traffic to the secondary Region's database endpoint.</p><p>This complements the database failover mechanism by handling the DNS resolution aspect of the architecture.</p><p>Together, these steps create a comprehensive disaster recovery solution that:</p><p>Maintains a synchronized copy of the database in a secondary Region (meeting the RPO requirement)</p><p>Provides automated promotion of the read replica during failover (meeting the RTO requirement)</p><p>Automatically redirects application traffic to the new primary database (ensuring application continuity)</p><p>Leverages the existing CloudFormation template and services available in both Regions</p><p>This combination provides the necessary components for a highly available multi-Region architecture that can withstand Region-level failures while meeting the specified RTO of 15 minutes and RPO of 2 hours.</p><p>Sources</p><p>How a large financial AWS customer implemented high availability and fast disaster recovery for Amazon Aurora PostgreSQL using Global Database and Amazon RDS Proxy | AWS Database Blog （https://aws.amazon.com/blogs/database/how-a-large-financial-aws-customer-implemented-ha-and-dr-for-amazon-aurora-postgresql-using-global-database-and-amazon-rds-proxy/）</p><p>AWS Backup vs global table/database for cost-effective RTO,RPO requirements | AWS re:Post（https://repost.aws/questions/QUWf0GuonKSvW8U2N4o8huHw/aws-backup-vs-global-table-database-for-cost-effective-rto-rpo-requirements） </p><p>Choosing the right database for your RTO and RPO requirements - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/strategy-database-disaster-recovery/choosing-database.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "71c4d9ea68f141a3b6e5c86e49f43393",
            "questionNumber": 473,
            "type": "single",
            "content": "<p>Question #473</p><p>An ecommerce company runs an application on AWS. The application has an Amazon API Gateway API that invokes an AWS Lambda function. The data is stored in an Amazon RDS for PostgreSQL DB instance.</p><p>During the company’s most recent flash sale, a sudden increase in API calls negatively affected the application's performance. A solutions architect reviewed the Amazon CloudWatch metrics during that time and noticed a significant increase in Lambda invocations and database connections. The CPU utilization also was high on the DB instance.</p><p>What should the solutions architect recommend to optimize the application's performance?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Increase the memory of the Lambda function. Modify the Lambda function to close the database connections when the data is retrieved."
                },
                {
                    "label": "B",
                    "content": "Add an Amazon ElastiCache for Redis cluster to store the frequently accessed data from the RDS database."
                },
                {
                    "label": "C",
                    "content": "Create an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint."
                },
                {
                    "label": "D",
                    "content": "Modify the Lambda function to connect to the database outside of the function&#39;s handler. Check for an existing database connection before creating a new connection."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an RDS proxy by using the Lambda console. Modify the Lambda function to use the proxy endpoint.</p><p>This solution will best optimize the application's performance for the following reasons:</p><p>Connection Pooling: During the flash sale, the sudden increase in API calls led to a significant increase in Lambda invocations, which in turn created many database connections. RDS Proxy provides efficient connection pooling, allowing multiple Lambda invocations to share a smaller pool of database connections rather than each creating its own connection.</p><p>Reduced Database Load: The high CPU utilization on the DB instance was likely caused by the overhead of establishing and managing numerous database connections. RDS Proxy reduces this overhead by maintaining persistent connections to the database and reusing them across multiple Lambda invocations.</p><p>Improved Scalability: RDS Proxy can handle connection surges during flash sales by queuing connection requests when the database reaches its connection limit, preventing connection errors and allowing the application to scale more effectively.</p><p>Minimal Code Changes: Implementing RDS Proxy requires only changing the database endpoint in the Lambda function, making it a relatively simple solution to implement compared to more complex connection management logic.</p><p>Automatic Failover Support: RDS Proxy provides seamless handling of database failovers, which improves the overall reliability of the application during high-traffic events.</p><p>Option D (modifying the Lambda function to connect outside the handler and check for existing connections) would provide some benefits by reusing connections within the same Lambda container instance. However, this approach has limitations:</p><p>It only works within a single Lambda container instance, and during high-traffic events, many new container instances would still be created, each establishing its own connection.</p><p>It requires more complex code changes and careful management of connection timeouts and error handling.</p><p>It doesn't address the fundamental issue of too many simultaneous connections to the database during traffic spikes.</p><p>RDS Proxy is specifically designed to address the exact issues observed during the flash sale: high numbers of Lambda invocations creating too many database connections and causing high CPU utilization on the database. By implementing RDS Proxy, the solutions architect can optimize the application's performance for future flash sales with minimal code changes and maximum effectiveness.</p><p>Sources</p><p>Which is preferred, Lambda-&gt;RDS Proxy-&gt;RDS or Lambda-&gt;RDS? | AWS re:Post （https://repost.aws/questions/QU1U72jyYWTcisPaIF3CSSqg/which-is-preferred-lambda-rds-proxy-rds-or-lambda-rds）</p><p>Amazon RDS Proxy - Amazon Relational Database Service （https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-proxy.html）</p><p>Reuse of database connections by a lambda. | AWS re:Post （https://repost.aws/questions/QUABFESymvR1aJ7MGijjp5SA/reuse-of-database-connections-by-a-lambda）</p><p>RDS Proxy causing a consistent increase in DB Connections from Lambda | AWS re:Post （https://repost.aws/questions/QU2hiJKCLUQcq5ApSro0ixMQ/rds-proxy-causing-a-consistent-increase-in-db-connections-from-lambda）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5a7054921abb423c90d84dd03d127c0d",
            "questionNumber": 474,
            "type": "single",
            "content": "<p>Question #474</p><p>A retail company wants to improve its application architecture. The company's applications register new orders, handle returns of merchandise, and provide analytics. The applications store retail data in a MySQL database and an Oracle OLAP analytics database. All the applications and databases are hosted on Amazon EC2 instances. Each application consists of several components that handle different parts of the order process. These components use incoming data from different sources. A separate ETL job runs every week and copies data from each application to the analytics database.<br><br></p><p>A solutions architect must redesign the architecture into an event-driven solution that uses serverless services. The solution must provide updated analytics in near real time.<br><br>Which solution will meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the individual applications as microservices to Amazon Elastic Container Service (Amazon ECS) containers that use AWS Fargate. Keep the retail MySQL database on Amazon EC2. Move the analytics database to Amazon Neptune. Use Amazon Simple Queue Service (Amazon SQS) to send all the incoming data to the microservices and the analytics database."
                },
                {
                    "label": "B",
                    "content": "Create an Auto Scaling group for each application. Specify the necessary number of EC2 instances in each Auto Scaling group. Migrate the retail MySQL database and the analytics database to Amazon Aurora MySQL. Use Amazon Simple Notification Service (Amazon SNS) to send all the incoming data to the correct EC2 instances and the analytics database."
                },
                {
                    "label": "C",
                    "content": "Migrate the individual applications as microservices to Amazon Elastic Kubernetes Service (Amazon EKS) containers that use AWS Fargate. Migrate the retail MySQL database to Amazon Aurora Serverless MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use Amazon EventBridge to send all the incoming data to the microservices and the analytics database."
                },
                {
                    "label": "D",
                    "content": "Migrate the individual applications as microservices to Amazon AppStream 2.0. Migrate the retail MySQL database to Amazon Aurora MySQL. Migrate the analytics database to Amazon Redshift Serverless. Use AWS IoT Core to send all the incoming data to the microservices and the analytics database."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The company needs an event-driven, serverless architecture that provides near-real-time analytics. Here’s why Option C is the best choice: &nbsp;</p><p> 1. Microservices on Amazon EKS with AWS Fargate &nbsp;</p><p>- Amazon EKS (Elastic Kubernetes Service) is ideal for running microservices in a scalable way. &nbsp;</p><p>- AWS Fargate provides serverless compute, eliminating the need to manage EC2 instances. &nbsp;</p><p>- This setup ensures automatic scaling and cost efficiency for the order-processing applications. &nbsp;</p><p> 2. Serverless Databases (Aurora MySQL & Redshift Serverless) &nbsp;</p><p>- Amazon Aurora Serverless (MySQL-compatible) scales automatically, reducing management overhead for the retail database. &nbsp;</p><p>- Amazon Redshift Serverless is optimized for near-real-time analytics and scales based on demand. &nbsp;</p><p>- Both databases support event-driven updates, eliminating the need for weekly ETL jobs. &nbsp;</p><p> 3. Event-Driven Data Flow with Amazon EventBridge &nbsp;</p><p>- Amazon EventBridge is a serverless event bus that can route events from different sources (e.g., order processing, returns) to microservices and the analytics database. &nbsp;</p><p>- This enables real-time data processing instead of relying on batch ETL jobs. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (ECS + Neptune + SQS): &nbsp;</p><p> &nbsp;- Amazon Neptune is a graph database, not suitable for OLAP analytics. &nbsp;</p><p> &nbsp;- SQS is for queuing, not real-time event processing. &nbsp;</p><p>- B (Auto Scaling EC2 + Aurora MySQL + SNS): &nbsp;</p><p> &nbsp;- Still relies on EC2 instances, not fully serverless. &nbsp;</p><p> &nbsp;- SNS is for pub/sub messaging, not as flexible as EventBridge for event-driven workflows. &nbsp;</p><p>- D (AppStream 2.0 + IoT Core): &nbsp;</p><p> &nbsp;- AppStream 2.0 is for virtual desktop streaming, not microservices. &nbsp;</p><p> &nbsp;- AWS IoT Core is for IoT devices, not retail order processing. &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option C provides a fully serverless, event-driven, and scalable solution with real-time analytics, meeting all requirements. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "231ddc74ca28442a9d2cfc25e383e6bd",
            "questionNumber": 475,
            "type": "single",
            "content": "<p>Question #475</p><p>A company is planning a migration from an on-premises data center to the AWS Cloud. The company plans to use multiple AWS accounts that are managed in an organization in AWS Organizations. The company will create a small number of accounts initially and will add accounts as needed. A solutions architect must design a solution that turns on AWS CloudTrail in all AWS accounts.<br><br>What is the MOST operationally efficient solution that meets these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Lambda function that creates a new CloudTrail trail in all AWS accounts in the organization. Invoke the Lambda function daily by using a scheduled action in Amazon EventBridge."
                },
                {
                    "label": "B",
                    "content": "Create a new CloudTrail trail in the organization&#39;s management account. Configure the trail to log all events for all AWS accounts in the organization."
                },
                {
                    "label": "C",
                    "content": "Create a new CloudTrail trail in all AWS accounts in the organization. Create new trails whenever a new account is created. Define an SCP that prevents deletion or modification of trails. Apply the SCP to the root OU."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Systems Manager Automation runbook that creates a CloudTrail trail in all AWS accounts in the organization. Invoke the automation by using Systems Manager State Manager."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create a new CloudTrail trail in the organization's management account. Configure the trail to log all events for all AWS accounts in the organization.</p><p>This solution is the most operationally efficient for the following reasons:</p><p>Centralized Management: Creating an organization trail in the management account provides a single point of management for CloudTrail across all accounts in the organization. This significantly reduces the operational overhead compared to managing individual trails in each account.</p><p>Automatic Coverage for New Accounts: When new AWS accounts are added to the organization, they are automatically included in the organization trail without any additional configuration. This is particularly important since the company plans to \"add accounts as needed.\"</p><p>Consistent Logging: An organization trail ensures uniform event logging across all accounts, providing a comprehensive and consistent audit trail for the entire organization.</p><p>No Custom Code Required: Unlike the Lambda function approach, this solution uses native AWS functionality and doesn't require writing, testing, and maintaining custom code.</p><p>No Scheduling or Monitoring Overhead: There's no need to set up and monitor scheduled EventBridge rules or Lambda functions, eliminating potential points of failure.</p><p>Immediate Activation: The organization trail begins logging events for new accounts as soon as they're added to the organization, without any delay or manual intervention.</p><p>Cost Efficiency: While both solutions incur CloudTrail and S3 storage costs, the Lambda function approach would add additional costs for Lambda executions and potential API call charges.</p><p>Option A (using a Lambda function with EventBridge) is less efficient because:</p><p>It introduces unnecessary complexity with custom code</p><p>It requires ongoing maintenance of the Lambda function</p><p>There could be a delay between account creation and CloudTrail enablement</p><p>It might miss newly created accounts if they're added between scheduled Lambda executions</p><p>It creates additional points of failure in the logging system</p><p>The organization trail approach (Option B) is specifically designed for multi-account environments in AWS Organizations and represents the AWS recommended best practice for enabling CloudTrail across an organization.</p><p>Sources</p><p>CloudTrail concepts - AWS CloudTrail （https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-concepts.html）</p><p>AWS CloudTrail and AWS Organizations - AWS Organizations （https://docs.aws.amazon.com/organizations/latest/userguide/services-that-can-integrate-cloudtrail.html）</p><p>Working with CloudTrail trails - AWS CloudTrail（https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-trails.html） </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1229443d006b437e8eedbd3d346ed1a7",
            "questionNumber": 476,
            "type": "single",
            "content": "<p>Question #476</p><p>A software development company has multiple engineers who are working remotely. The company is running Active Directory Domain Services (AD DS) on an Amazon EC2 instance. The company's security policy states that all internal, nonpublic services that are deployed in a VPC must be accessible through a VPN. Multi-factor authentication (MFA) must be used for access to a VPN.<br><br>What should a solutions architect do to meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Site-to-Site VPN connection. Configure integration between a VPN and AD DS. Use an Amazon WorkSpaces client with MFA support enabled to establish a VPN connection."
                },
                {
                    "label": "B",
                    "content": "Create an AWS Client VPN endpoint. Create an AD Connector directory for integration with AD DS. Enable MFA for AD Connector. Use AWS Client VPN to establish a VPN connection."
                },
                {
                    "label": "C",
                    "content": "Create multiple AWS Site-to-Site VPN connections by using AWS VPN CloudHub. Configure integration between AWS VPN CloudHub and AD DS. Use AWS Copilot to establish a VPN connection."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon WorkLink endpoint. Configure integration between Amazon WorkLink and AD DS. Enable MFA in Amazon WorkLink. Use AWS Client VPN to establish a VPN connection."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an AWS Client VPN endpoint. Create an AD Connector directory for integration with AD DS. Enable MFA for AD Connector. Use AWS Client VPN to establish a VPN connection.</p><p>This solution meets the requirements for the following reasons:</p><p>Remote Access Requirement: The company has multiple engineers working remotely who need access to internal, non-public services deployed in a VPC. AWS Client VPN is specifically designed for this scenario, allowing individual users to establish secure connections to AWS resources from any location.</p><p>VPN Access Requirement: The security policy states that all internal services must be accessible through a VPN. AWS Client VPN provides this secure tunnel for remote users to access resources within the VPC.</p><p>Active Directory Integration: The company is running AD DS on an EC2 instance. AD Connector provides a way to integrate AWS Client VPN with the existing Active Directory infrastructure, allowing users to authenticate using their existing AD credentials.</p><p>Multi-Factor Authentication (MFA) Requirement: The security policy mandates MFA for VPN access. AD Connector supports enabling MFA, which adds an additional layer of security beyond just username and password authentication.</p><p>Scalability: As the company has multiple engineers working remotely, this solution scales well to accommodate individual user connections without requiring additional hardware or complex configurations.</p><p>Option A (AWS Site-to-Site VPN with WorkSpaces) is not appropriate for this scenario because:</p><p>Site-to-Site VPN is designed to connect entire networks (like an on-premises data center to AWS), not individual remote users. It would require remote engineers to first connect to a central location before accessing AWS resources.</p><p>Amazon WorkSpaces is a virtual desktop solution, not a VPN solution. While it can be used with MFA, it doesn't fulfill the requirement that \"all internal services must be accessible through a VPN.\"</p><p>This approach would be more complex and costly than necessary for the given requirements.</p><p>AWS Client VPN with AD Connector and MFA provides the most direct and appropriate solution for enabling remote engineers to securely access internal services in the VPC while meeting all the security requirements specified in the scenario.</p><p>Sources</p><p>Active Directory authentication in Client VPN - AWS Client VPN （https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/ad.html）</p><p>Enabling multi-factor authentication for AWS Managed Microsoft AD - AWS Directory Service （https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_mfa.html）</p><p>Connect on-prem AD to VPC, is it a AD connector required? | AWS re:Post （https://repost.aws/questions/QUNPG4paiiRJWzFobuUtbyTQ/connect-on-prem-ad-to-vpc-is-it-a-ad-connector-required）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "55e2a7c93e954686a8aa618a2f80d94f",
            "questionNumber": 477,
            "type": "multiple",
            "content": "<p>Question #477</p><p><br></p><p>A company is running a three-tier web application in an on-premises data center. The frontend is served by an Apache web server, the middle tier is a monolithic Java application, and the storage tier is a PostgreSQL database. During a recent marketing promotion, customers could not place orders through the application because the application crashed. An analysis showed that all three tiers were overloaded. The application became unresponsive, and the database reached its capacity limit because of read operations. The company already has several similar promotions scheduled in the near future.</p><p><br></p><p>A solutions architect must develop a plan for migration to AWS to resolve these issues. The solution must maximize scalability and must minimize operational effort.</p><p><br></p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Refactor the frontend so that static assets can be hosted on Amazon S3. Use Amazon CloudFront to serve the frontend to customers. Connect the frontend to the Java application."
                },
                {
                    "label": "B",
                    "content": "Rehost the Apache web server of the frontend on Amazon EC2 instances that are in an Auto Scaling group. Use a load balancer in front of the Auto Scaling group. Use Amazon Elastic File System (Amazon EFS) to host the static assets that the Apache web server needs."
                },
                {
                    "label": "C",
                    "content": "Rehost the Java application in an AWS Elastic Beanstalk environment that includes auto scaling."
                },
                {
                    "label": "D",
                    "content": "Refactor the Java application, Develop a Docker container to run the Java application. Use AWS Fargate to host the container."
                },
                {
                    "label": "E",
                    "content": "Use AWS Database Migration Service (AWS DMS) to replatform the PostgreSQL database to an Amazon Aurora PostgreSQL database. Use Aurora Auto Scaling for read replicas."
                },
                {
                    "label": "F",
                    "content": "Rehost the PostgreSQL database on an Amazon EC2 instance that has twice as much memory as the on-premises server. "
                }
            ],
            "correctAnswer": "ACE",
            "explanation": "<p>The company needs a scalable, highly available, and low-maintenance solution on AWS. Here’s why Option A, C, E is the best choice: &nbsp;</p><p> A. Refactor the frontend to use Amazon S3 + CloudFront &nbsp;</p><p>- Amazon S3 is ideal for hosting static assets (HTML, CSS, JS, images) with high durability and scalability. &nbsp;</p><p>- Amazon CloudFront (CDN) improves performance by caching content globally, reducing load on backend servers. &nbsp;</p><p>- This reduces the need for Apache web servers to serve static files, improving scalability. &nbsp;</p><p> C. Rehost the Java application in AWS Elastic Beanstalk with Auto Scaling &nbsp;</p><p>- Elastic Beanstalk automates deployment, scaling, and monitoring of Java applications. &nbsp;</p><p>- Auto Scaling ensures the application can handle traffic spikes during promotions. &nbsp;</p><p>- Since the Java app is monolithic, rehosting (lift-and-shift) is faster than refactoring into microservices. &nbsp;</p><p> E. Replatform PostgreSQL to Amazon Aurora with Auto Scaling for read replicas &nbsp;</p><p>- Amazon Aurora PostgreSQL is fully compatible but offers better performance, scalability, and reliability. &nbsp;</p><p>- Aurora Auto Scaling automatically adds read replicas during high traffic, preventing database overload. &nbsp;</p><p>- This solves the issue of read capacity limits during promotions. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B (Rehost Apache on EC2 + EFS): &nbsp;</p><p> &nbsp;- Still requires managing EC2 instances (not fully serverless). &nbsp;</p><p> &nbsp;- EFS is overkill for static assets (S3 + CloudFront is better). &nbsp;</p><p>- D (Refactor Java into Docker + Fargate): &nbsp;</p><p> &nbsp;- While Fargate is serverless, refactoring a monolithic Java app into containers is time-consuming and not necessary for immediate scalability. &nbsp;</p><p> &nbsp;- Elastic Beanstalk (Option C) provides a simpler, faster solution. &nbsp;</p><p>- F (Rehost PostgreSQL on a larger EC2 instance): &nbsp;</p><p> &nbsp;- Vertical scaling (bigger EC2 instance) is not as scalable as Aurora with read replicas (Option E). &nbsp;</p><p> &nbsp;- Requires manual intervention to scale, unlike Aurora Auto Scaling. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A, C, E provides the best balance of scalability, minimal operational effort, and AWS-managed services to handle future promotions. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "332fe63629804bf9af124232c798e7ab",
            "questionNumber": 478,
            "type": "single",
            "content": "<p>Question #478</p><p><br></p><p>A company is deploying a new application on AWS. The application consists of an Amazon Elastic Kubernetes Service (Amazon EKS) cluster and an Amazon Elastic Container Registry (Amazon ECR) repository. The EKS cluster has an AWS managed node group.</p><p><br></p><p>The company's security guidelines state that all resources on AWS must be continuously scanned for security vulnerabilities.</p><p><br></p><p>Which solution will meet this requirement with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Activate AWS Security Hub. Configure Security Hub to scan the EKS nodes and the ECR repository."
                },
                {
                    "label": "B",
                    "content": "Activate Amazon Inspector to scan the EKS nodes and the ECR repository."
                },
                {
                    "label": "C",
                    "content": "Launch a new Amazon EC2 instance and install a vulnerability scanning tool from AWS Marketplace. Configure the EC2 instance to scan the EKS nodes. Configure Amazon ECR to perform a basic scan on push."
                },
                {
                    "label": "D",
                    "content": "Install the Amazon CloudWatch agent on the EKS nodes. Configure the CloudWatch agent to scan continuously. Configure Amazon ECR to perform a basic scan on push."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Activate Amazon Inspector to scan the EKS nodes and the ECR repository.</p><p>This solution will meet the requirement for continuous security vulnerability scanning with the least operational overhead for the following reasons:</p><p>Purpose-Built for Vulnerability Scanning: Amazon Inspector is specifically designed for automated vulnerability management and continuous scanning of AWS resources, including EKS nodes and ECR repositories.</p><p>Native Container Support: Amazon Inspector has built-in capabilities to scan container images in ECR repositories for software vulnerabilities, identifying security issues in the container images that will be deployed to the EKS cluster.</p><p>EKS Node Scanning: Amazon Inspector can automatically scan the EC2 instances in the AWS managed node group for the EKS cluster, detecting operating system vulnerabilities and misconfigurations.</p><p>Minimal Configuration: Once activated, Amazon Inspector automatically discovers supported resources and begins scanning them without requiring extensive setup or configuration.</p><p>Continuous Monitoring: Amazon Inspector provides continuous, automated scanning that detects new vulnerabilities as they emerge, meeting the company's requirement for continuous security scanning.</p><p>Low Operational Overhead: As a fully managed service, Amazon Inspector requires minimal ongoing maintenance and automatically updates its vulnerability database, reducing the operational burden on the company's team.</p><p>Option A (AWS Security Hub) would not be the optimal choice for this specific requirement because:</p><p>Security Hub is primarily an aggregation and management service that collects security findings from various AWS services, including Amazon Inspector.</p><p>Security Hub itself doesn't perform the actual vulnerability scanning of EKS nodes and ECR repositories - it would still rely on other services like Amazon Inspector to perform the actual scans.</p><p>Setting up Security Hub to scan these resources would require additional configuration and integration with other services, increasing the operational overhead.</p><p>By activating Amazon Inspector, the company can implement continuous vulnerability scanning for both their EKS cluster nodes and ECR repository with minimal setup and ongoing maintenance, making it the solution with the least operational overhead that meets the security requirement.</p><p>Sources</p><p>Scan images for software vulnerabilities in Amazon ECR - Amazon ECR （https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html）</p><p>Automated Vulnerability Management - Amazon Inspector Features - AWS （https://aws.amazon.com/cn/inspector/features/）</p><p>Amazon Inspector enhances container security by mapping Amazon ECR images to running containers | AWS News Blog （https://aws.amazon.com/cn/blogs/aws/amazon-inspector-enhances-container-security-by-mapping-amazon-ecr-images-to-running-containers/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "63f7e65c02354cf2a25851ce0a41a5c4",
            "questionNumber": 479,
            "type": "single",
            "content": "<p>Question #479</p><p><br></p><p>A company needs to improve the reliability of its ticketing application. The application runs on an Amazon Elastic Container Service (Amazon ECS) cluster. The company uses Amazon CloudFront to serve the application. A single ECS service of the ECS cluster is the CloudFront distribution’s origin.</p><p><br></p><p>The application allows only a specific number of active users to enter a ticket purchasing flow. These users are identified by an encrypted attribute in their JSON Web Token (JWT). All other users are redirected to a waiting room module until there is available capacity for purchasing.</p><p><br></p><p>The application is experiencing high loads. The waiting room module is working as designed, but load on the waiting room is disrupting the application's availability.</p><p><br></p><p>This disruption is negatively affecting the application's ticket sale transactions.</p><p><br></p><p>Which solution will provide the MOST reliability for ticket sale transactions during periods of high load?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Ensure that the ticketing service uses the JWT information and appropriately forwards requests to the waiting room service."
                },
                {
                    "label": "B",
                    "content": "Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Make the ticketing pod part of a StatefulSet. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod."
                },
                {
                    "label": "C",
                    "content": "Create a separate service in the ECS cluster for the waiting room. Use a separate scaling configuration. Create a CloudFront function that inspects the JWT information and appropriately forwards requests to the ticketing service or the waiting room service."
                },
                {
                    "label": "D",
                    "content": "Move the application to an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. Split the waiting room module into a pod that is separate from the ticketing pod. Use AWS App Mesh by provisioning the App Mesh controller for Kubernetes. Enable mTLS authentication and service-to-service authentication for communication between the ticketing pod and the waiting room pod. Ensure that the ticketing pod uses the JWT information and appropriately forwards requests to the waiting room pod."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The company needs to improve reliability by isolating the waiting room traffic from the ticketing service while efficiently routing users based on their JWT. Here’s why Option C is the best choice: &nbsp;</p><p> 1. Separate ECS Service for the Waiting Room + Independent Scaling &nbsp;</p><p>- Isolating the waiting room into its own ECS service prevents it from consuming resources needed for ticket sales. &nbsp;</p><p>- Independent scaling allows the waiting room to handle high loads without affecting the ticketing service. &nbsp;</p><p> 2. CloudFront Function for JWT-Based Routing &nbsp;</p><p>- CloudFront Functions (edge compute) can inspect the JWT and route requests before they hit the origin. &nbsp;</p><p> &nbsp;- If the JWT allows ticket purchasing → forward to the ticketing service. &nbsp;</p><p> &nbsp;- If the user must wait → forward to the waiting room service. &nbsp;</p><p>- This reduces load on the ECS cluster by offloading routing logic to CloudFront. &nbsp;</p><p> 3. Why This is the Most Reliable Solution &nbsp;</p><p>- No unnecessary migration (unlike Options B & D, which suggest moving to EKS). &nbsp;</p><p>- Serverless routing (CloudFront Functions) is low-latency and scalable. &nbsp;</p><p>- Decouples the waiting room from the ticketing service, ensuring ticket sales remain stable. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- A (Separate ECS service but no CloudFront routing): &nbsp;</p><p> &nbsp;- Still relies on the ticketing service to route traffic, which adds unnecessary load. &nbsp;</p><p>- B (Move to EKS + StatefulSet): &nbsp;</p><p> &nbsp;- Overkill—ECS is sufficient, and StatefulSet is unnecessary for this stateless app. &nbsp;</p><p> &nbsp;- No CloudFront optimization, so routing still happens at the cluster level. &nbsp;</p><p>- D (Move to EKS + App Mesh + mTLS): &nbsp;</p><p> &nbsp;- Complexity is unnecessary—CloudFront Functions provide a simpler, more scalable solution. &nbsp;</p><p> &nbsp;- mTLS and App Mesh add overhead without solving the core issue (waiting room load). &nbsp;</p><p> Conclusion: &nbsp;</p><p>Option C provides the most reliable, scalable, and cost-effective solution by: &nbsp;</p><p>✅ Isolating the waiting room (separate ECS service + scaling). &nbsp;</p><p>✅ Offloading routing logic to CloudFront (JWT inspection at the edge). &nbsp;</p><p>✅ Minimizing changes (no migration to EKS). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "184134b953d044688a49b9fe4cc9a02a",
            "questionNumber": 480,
            "type": "single",
            "content": "<p>Question #480</p><p><br></p><p>A solutions architect is creating an AWS CloudFormation template from an existing manually created non-production AWS environment. The CloudFormation template can be destroyed and recreated as needed. The environment contains an Amazon EC2 instance. The EC2 instance has an instance profile that the EC2 instance uses to assume a role in a parent account.</p><p><br></p><p>The solutions architect recreates the role in a CloudFormation template and uses the same role name. When the CloudFormation template is launched in the child account, the EC2 instance can no longer assume the role in the parent account because of insufficient permissions.</p><p><br></p><p>What should the solutions architect do to resolve this issue?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "In the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Ensure that the target role ARN in the existing statement that allows the sts:AssumeRole action is correct. Save the trust policy."
                },
                {
                    "label": "B",
                    "content": "In the parent account, edit the trust policy for the role that the EC2 instance needs to assume. Add a statement that allows the sts:AssumeRole action for the root principal of the child account. Save the trust policy."
                },
                {
                    "label": "C",
                    "content": "Update the CloudFormation stack again. Specify only the CAPABILITY_NAMED_IAM capability."
                },
                {
                    "label": "D",
                    "content": "Update the CloudFormation stack again. Specify the CAPABILITY_IAM capability and the CAPABILITY_NAMED_IAM capability."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The issue occurs because the trust policy in the parent account’s role must explicitly allow the EC2 instance’s instance profile (or its role) in the child account to assume it. When the role was recreated in the CloudFormation template, the trust relationship was not updated to include the new role’s ARN (even if the name is the same, the ARN changes when recreated). &nbsp;</p><p> Why Option A is Correct: &nbsp;</p><p>1. The root cause is the trust policy in the parent account’s role. &nbsp;</p><p>2. The existing `sts:AssumeRole` statement must be updated to include the new role ARN from the child account. &nbsp;</p><p>3. This ensures the EC2 instance’s instance profile (or its role) can still assume the parent account’s role. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B (Add a statement allowing the child account’s root principal): &nbsp;</p><p> &nbsp;- Security risk: Granting `sts:AssumeRole` to the entire child account’s root principal is overly permissive. &nbsp;</p><p> &nbsp;- Best practice: Only allow specific roles (like the EC2 instance profile) to assume the role. &nbsp;</p><p>- C (Specify `CAPABILITY_NAMED_IAM`): &nbsp;</p><p> &nbsp;- This is irrelevant—the issue is not about CloudFormation permissions, but about IAM trust relationships. &nbsp;</p><p>- D (Specify both `CAPABILITY_IAM` and `CAPABILITY_NAMED_IAM`): &nbsp;</p><p> &nbsp;- Again, this is not the issue—CloudFormation can create the role, but the trust policy must be manually updated in the parent account. &nbsp;</p><p> Conclusion: &nbsp;</p><p>The trust policy in the parent account must be updated to allow the newly recreated role in the child account to assume it. This is a manual step because CloudFormation cannot modify IAM policies in another account. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "892ae1d204244d168d24e0b4c364459d",
            "questionNumber": 481,
            "type": "single",
            "content": "<p>Question #481</p><p><br></p><p>A company's web application has reliability issues. The application serves customers globally. The application runs on a single Amazon EC2 instance and performs read-intensive operations on an Amazon RDS for MySQL database.</p><p><br></p><p>During high load, the application becomes unresponsive and requires a manual restart of the EC2 instance. A solutions architect must improve the application's reliability.</p><p><br></p><p>Which solution will meet this requirement with the LEAST development effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon CloudFront distribution. Specify the EC2 instance as the distribution&rsquo;s origin. Configure a Multi-AZ deployment for the RDS for MySQL database. Use the standby DB instance for the read-intensive operations."
                },
                {
                    "label": "B",
                    "content": "Run the application on EC2 instances that are in an Auto Scaling group. Place the EC2 instances behind an Elastic Load Balancing (ELB) load balancer. Replace the database service with Amazon Aurora. Use Aurora Replicas for the read-intensive operations."
                },
                {
                    "label": "C",
                    "content": "Deploy AWS Global Accelerator. Configure a Multi-AZ deployment for the RDS for MySQL database. Use the standby DB instance for the read-intensive operations."
                },
                {
                    "label": "D",
                    "content": "Migrate the application to AWS Lambda functions. Create read replicas for the RDS for MySQL database. Use the read replicas for the read-intensive operations."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Run the application on EC2 instances that are in an Auto Scaling group. Place the EC2 instances behind an Elastic Load Balancing (ELB) load balancer. Replace the database service with Amazon Aurora. Use Aurora Replicas for the read-intensive operations.</p><p>This solution will improve the application's reliability with the least development effort for the following reasons:</p><p>Addressing the Single Point of Failure: The current setup has a single EC2 instance that becomes unresponsive during high load. An Auto Scaling group with multiple EC2 instances behind an ELB eliminates this single point of failure by:</p><p>Automatically distributing traffic across multiple instances</p><p>Automatically replacing unhealthy instances</p><p>Scaling the number of instances based on demand</p><p>Handling Read-Intensive Database Operations: The application performs read-intensive operations on the database, which likely contributes to the performance issues. Aurora with Aurora Replicas provides:</p><p>Better read scalability by offloading read operations to multiple Aurora Replicas</p><p>Higher performance compared to standard RDS MySQL</p><p>Automatic failover capabilities</p><p>Global Customer Base: Since the application serves customers globally, the ELB and Auto Scaling group will ensure better availability and performance across different regions.</p><p>Minimal Development Effort: While both solutions require some changes, option B requires less development effort because:</p><p>Aurora is MySQL-compatible, so the application's database queries would require minimal changes</p><p>The application code itself wouldn't need significant modifications to work with an Auto Scaling group and ELB</p><p>The migration from RDS MySQL to Aurora can be done with minimal downtime using AWS migration tools</p><p>Option A (CloudFront with Multi-AZ RDS MySQL) would not be as effective because:</p><p>It doesn't address the EC2 instance becoming unresponsive under high load, as it still relies on a single instance</p><p>Multi-AZ RDS MySQL provides high availability but doesn't improve read performance as effectively as Aurora Replicas</p><p>Using the standby instance for read operations in Multi-AZ RDS MySQL is more complex and less efficient than using Aurora Replicas</p><p>The Auto Scaling group with ELB and Aurora solution provides comprehensive improvements to both the application tier and database tier, addressing the root causes of the reliability issues while requiring minimal development changes to the application code.</p><p>Sources</p><p>Key AWS services - Financial Services Industry Lens （https://docs.aws.amazon.com/wellarchitected/latest/financial-services-industry-lens/key-aws-services-rel.html）</p><p>High availability for Amazon Aurora - Amazon Aurora （https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.AuroraHighAvailability.html）</p><p>Enhance performance of generative language models with self-consistency prompting on Amazon Bedrock | AWS Machine Learning Blog （https://aws.amazon.com/cn/blogs/machine-learning/enhance-performance-of-generative-language-models-with-self-consistency-prompting-on-amazon-bedrock/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "fd20d74d49af4384a702cecaaaa651c3",
            "questionNumber": 482,
            "type": "single",
            "content": "<p>Question #482</p><p><br></p><p>A company needs to use an AWS Transfer Family SFTP-enabled server with an Amazon S3 bucket to receive updates from a third-party data supplier. The data is encrypted with Pretty Good Privacy (PGP) encryption. The company needs a solution that will automatically decrypt the data after the company receives the data.</p><p><br></p><p>A solutions architect will use a Transfer Family managed workflow. The company has created an IAM service role by using an IAM policy that allows access to AWS Secrets Manager and the S3 bucket. The role’s trust relationship allows the transfer `amazonaws.com` service to assume the role.</p><p><br></p><p>What should the solutions architect do next to complete the solution for automatic decryption?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Store the PGP public key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP encryption parameters in the nominal step. Associate the workflow with the Transfer Family server."
                },
                {
                    "label": "B",
                    "content": "Store the PGP private key in Secrets Manager. Add an exception-handling step in the Transfer Family managed workflow to decrypt files. Configure PGP encryption parameters in the exception handler. Associate the workflow with the SFTP user."
                },
                {
                    "label": "C",
                    "content": "Store the PGP private key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the nominal step. Associate the workflow with the Transfer Family server."
                },
                {
                    "label": "D",
                    "content": "Store the PGP public key in Secrets Manager. Add an exception-handling step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the exception handler. Associate the workflow with the SFTP user."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Store the PGP private key in Secrets Manager. Add a nominal step in the Transfer Family managed workflow to decrypt files. Configure PGP decryption parameters in the nominal step. Associate the workflow with the Transfer Family server.</p><p>This is the correct next step to complete the solution for automatic decryption because:</p><p>PGP Private Key Storage: The PGP private key needs to be securely stored in AWS Secrets Manager. This is a critical step because the key will be used to decrypt the files received from the third-party data supplier. Secrets Manager provides secure storage with encryption at rest and access control through IAM policies.</p><p>Managed Workflow with Decrypt Step: AWS Transfer Family managed workflows include predefined \"nominal steps\" that can be configured for specific actions. Adding a decrypt step is necessary to automatically process the encrypted files as they arrive via SFTP.</p><p>PGP Decryption Parameters: The decrypt step needs specific configuration parameters, including:</p><p>Reference to the PGP private key stored in Secrets Manager</p><p>Source file location (the encrypted file uploaded to the S3 bucket)</p><p>Destination for the decrypted file</p><p>Error handling options</p><p>Workflow Association: The configured workflow needs to be associated with the Transfer Family SFTP server to ensure it runs automatically when files are uploaded.</p><p>This solution leverages the existing IAM service role that has already been created with the necessary permissions to access Secrets Manager and the S3 bucket, and with the trust relationship allowing the transfer.amazonaws.com service to assume the role.</p><p>The solution provides a fully automated process where:</p><p>The third-party uploads PGP-encrypted files to the SFTP server</p><p>Files are stored in the S3 bucket</p><p>The Transfer Family managed workflow automatically triggers</p><p>The decrypt step retrieves the private key from Secrets Manager</p><p>The files are decrypted and stored back in the S3 bucket</p><p>This approach requires minimal ongoing maintenance and provides a secure, automated solution for handling the encrypted data from the third-party supplier.</p><p>Sources</p><p>Architecting secure and compliant managed file transfers with AWS Transfer Family SFTP connectors and PGP encryption | AWS Storage Blog （https://aws.amazon.com/cn/blogs/storage/architecting-secure-and-compliant-managed-file-transfers-with-aws-transfer-family-sftp-connectors-and-pgp-encryption/）</p><p>AWS Transfer Family does not use S3 default encryption - SFTP - access denied | AWS re:Post （https://repost.aws/questions/QUYHwOd1zqQfqfwVNogoAvVQ/aws-transfer-family-does-not-use-s3-default-encryption-sftp-access-denied）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8feea4ede319437dbf0589196cd40c21",
            "questionNumber": 483,
            "type": "single",
            "content": "Question #483<p><br></p><p>A company is migrating infrastructure for its massive multiplayer game to AWS. The game’s application features a leaderboard where players can see rankings in real time. The leaderboard requires microsecond reads and single-digit-millisecond write latencies. The datasets are single-digit terabytes in size and must be available to accept writes in less than a minute if a primary node failure occurs.</p><p><br></p><p>The company needs a solution in which data can persist for further analytical processing through a data pipeline.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon RDS database with a read replica. Configure the application to point writes to the writer endpoint. Configure the application to point reads to the reader endpoint."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon MemoryDB for Redis cluster in Multi-AZ mode. Configure the application to interact with the primary node."
                },
                {
                    "label": "C",
                    "content": "Create multiple Redis nodes on Amazon EC2 instances that are spread across multiple Availability Zones. Configure backups to Amazon S3."
                }
            ],
            "correctAnswer": "B",
            "explanation": "Based on the company's requirements for their massive multiplayer game's leaderboard, the solution that will meet these requirements with the least operational overhead is:<p>B. Create an Amazon MemoryDB for Redis cluster in Multi-AZ mode. Configure the application to interact with the primary node.</p><p>Here's why this solution is the most suitable:</p><p>1. Performance: Amazon MemoryDB for Redis offers microsecond read latencies and single-digit millisecond write latencies, which meets the game's leaderboard requirements.</p><p>2. Scalability: MemoryDB can handle up to 160 million transactions per second (TPS) per cluster, which is suitable for a massive multiplayer game.</p><p>3. Data persistence: MemoryDB ensures data durability through a Multi-AZ transactional log, allowing data to persist for further analytical processing through a data pipeline.</p><p>4. High availability: Multi-AZ mode provides automatic failover and node recovery, ensuring that the system can accept writes in less than a minute if a primary node failure occurs.</p><p>5. Data size: MemoryDB supports up to 100 TB of storage per cluster, which is more than sufficient for the single-digit terabyte datasets mentioned.</p><p>6. Operational overhead: As a fully managed service, MemoryDB requires minimal operational overhead compared to self-managed solutions.</p><p>7. Compatibility: MemoryDB supports Redis data structures and APIs, making it easy to integrate with existing game applications.</p><p>To implement this solution:</p><p>1. Create a MemoryDB cluster in Multi-AZ mode within your preferred AWS region.</p><p>2. Configure the cluster with appropriate node types and settings to handle your expected workload.</p><p>3. Set up encryption at rest and in transit for enhanced security.</p><p>4. Implement proper access controls using AWS Identity and Access Management (IAM) and Access Control Lists (ACLs).</p><p>5. Configure your game application to interact with the primary node of the MemoryDB cluster for both reads and writes.</p><p>6. Set up monitoring and alerting using Amazon CloudWatch to track the cluster's performance and health.</p><p>For data pipeline integration:</p><p>1. Use Amazon Kinesis Data Firehose to capture and stream data from MemoryDB to Amazon S3 for long-term storage.</p><p>2. Utilize Amazon Glue for ETL processes to prepare the data for analysis.</p><p>3. Consider using Amazon Redshift or Amazon Athena for analytical processing of the persisted data.</p><p>Remember to review the AWS documentation for the most up-to-date information on MemoryDB features, best practices, and any limitations. Additionally, always test your configuration in a non-production environment before deploying to production.</p><p>Sources</p><p>[1] [Managing clusters - Amazon MemoryDB for Redis] (https://docs.aws.amazon.com/memorydb/latest/devguide/clusters.html)</p><p>[3] [Launch a scalable serverless data pipeline to ingest, store, and analyze telemetry data generated from games and services - Game Analytics Pipeline] (https://docs.aws.amazon.com/solutions/latest/game-analytics-pipeline/welcome.html)</p><p>[6] [Features of MemoryDB - Amazon MemoryDB] (https://docs.aws.amazon.com/memorydb/latest/devguide/servicename-feature-overview.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5b53e917e50448fe8570ad0c23b55cd9",
            "questionNumber": 484,
            "type": "single",
            "content": "<p>Question #484</p><p><br></p><p>A company is running several applications in the AWS Cloud. The applications are specific to separate business units in the company. The company is running the components of the applications in several AWS accounts that are in an organization in AWS Organizations.</p><p><br></p><p>Every cloud resource in the company’s organization has a tag named BusinessUnit. Every tag already has the appropriate value of the business unit name.</p><p><br></p><p>The company needs to allocate its cloud costs to different business units. The company also needs to visualize the cloud costs for each business unit.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "In the organization&#39;s management account, create a cost allocation tag named BusinessUnit. Also in the management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization."
                },
                {
                    "label": "B",
                    "content": "In each member account, create a cost allocation tag named BusinessUnit. In the organization&rsquo;s management account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure the S3 bucket as the destination for the AWS CUR. Create an Amazon CloudWatch dashboard for visualization."
                },
                {
                    "label": "C",
                    "content": "In the organization&#39;s management account, create a cost allocation tag named BusinessUnit. In each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR."
                },
                {
                    "label": "D",
                    "content": "&nbsp;In each member account, create a cost allocation tag named BusinessUnit. Also in each member account, create an Amazon S3 bucket and an AWS Cost and Usage Report (AWS CUR). Configure each S3 bucket as the destination for its respective AWS CUR. From the management account, query the AWS CUR data by using Amazon Athena. Use Amazon QuickSight for visualization."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The question describes a scenario where a company needs to allocate and visualize AWS costs by business units using the existing `BusinessUnit` tag. Here's why option A is the best solution: &nbsp;</p><p>1. Cost Allocation Tag in Management Account: &nbsp;</p><p> &nbsp; - The `BusinessUnit` tag already exists on resources. Activating it as a cost allocation tag in the management account ensures that AWS Cost and Usage Reports (CUR) will include this tag for cost allocation. &nbsp;</p><p> &nbsp; - Cost allocation tags only need to be activated once in the management account (for organization-wide tagging). &nbsp;</p><p>2. AWS Cost and Usage Report (CUR): &nbsp;</p><p> &nbsp; - The CUR provides detailed cost and usage data, including tags. &nbsp;</p><p> &nbsp; - Configuring the CUR in the management account with an S3 bucket as the destination ensures consolidated billing data is available for analysis. &nbsp;</p><p>3. Amazon Athena & QuickSight for Analysis & Visualization: &nbsp;</p><p> &nbsp; - Amazon Athena can query the CUR data stored in S3. &nbsp;</p><p> &nbsp; - Amazon QuickSight is a powerful visualization tool that integrates with Athena to create dashboards for business unit cost tracking. &nbsp;</p><p> Why Not the Other Options? &nbsp;</p><p>- B: Incorrect because CloudWatch is not designed for cost visualization (it’s for monitoring). Also, cost allocation tags don’t need to be activated in each member account. &nbsp;</p><p>- C: Incorrect because creating separate CURs in each member account is unnecessary and complicates cost aggregation. CloudWatch is not the right tool for cost visualization. &nbsp;</p><p>- D: Incorrect because activating cost allocation tags in each member account is redundant, and managing multiple CURs is inefficient. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the correct choice because it efficiently centralizes cost allocation tag activation, uses CUR for detailed reporting, and leverages Athena + QuickSight for visualization. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a9091f1d74ab4e858b78c8fbcf2a2a74",
            "questionNumber": 485,
            "type": "multiple",
            "content": "<p>Question #485</p><p><br></p><p>A utility company wants to collect usage data every 5 minutes from its smart meters to facilitate time-of-use metering. When a meter sends data to AWS, the data is sent to Amazon API Gateway, processed by an AWS Lambda function, and stored in an Amazon DynamoDB table. During the pilot phase, the Lambda functions took from 3 to 5 seconds to complete.</p><p><br></p><p>As more smart meters are deployed, the engineers notice the Lambda functions are taking from 1 to 2 minutes to complete. The functions are also increasing in duration as new types of metrics are collected from the devices. There are many `ProvisionedThroughputExceededException` errors while performing PUT operations on DynamoDB, and there are also many `TooManyRequestsException` errors from Lambda.</p><p><br></p><p>Which combination of changes will resolve these issues? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Increase the write capacity units to the DynamoDB table."
                },
                {
                    "label": "B",
                    "content": "Increase the memory available to the Lambda functions."
                },
                {
                    "label": "C",
                    "content": "Increase the payload size from the smart meters to send more data."
                },
                {
                    "label": "D",
                    "content": "Stream the data into an Amazon Kinesis data stream from API Gateway and process the data in batches."
                },
                {
                    "label": "E",
                    "content": "Collect data in an Amazon SQS FIFO queue, which triggers a Lambda function to process each message."
                }
            ],
            "correctAnswer": "AD",
            "explanation": "<p>The problem describes two key issues: &nbsp;</p><p>1. DynamoDB Throttling (`ProvisionedThroughputExceededException`) → Solution: Increase write capacity (A) &nbsp;</p><p> &nbsp; - The DynamoDB table is unable to handle the increased write load, leading to throttling errors. &nbsp;</p><p> &nbsp; - Increasing write capacity units (WCUs) will allow DynamoDB to handle more write requests per second. &nbsp;</p><p>2. Lambda Throttling (`TooManyRequestsException`) & Slow Processing → Solution: Stream data via Kinesis (D) &nbsp;</p><p> &nbsp; - Lambda is struggling with high concurrency and long execution times (1-2 minutes). &nbsp;</p><p> &nbsp; - Instead of processing each request individually via API Gateway → Lambda, streaming data into Kinesis allows: &nbsp;</p><p> &nbsp; &nbsp; - Batching of records for more efficient processing. &nbsp;</p><p> &nbsp; &nbsp; - Better scaling (Kinesis can handle high throughput). &nbsp;</p><p> &nbsp; &nbsp; - Reduced Lambda invocations (processing multiple records per invocation). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Increase Lambda memory): While this might speed up individual executions, it doesn’t solve throttling or DynamoDB issues. &nbsp;</p><p>- C (Increase payload size): This would worsen Lambda execution time and DynamoDB throttling. &nbsp;</p><p>- E (SQS FIFO queue): FIFO queues have lower throughput (300 msg/sec) and aren’t ideal for high-volume IoT data. Kinesis (D) is better for streaming. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A (Increase DynamoDB WCUs) and D (Use Kinesis for batching) are the best solutions to resolve both DynamoDB throttling and Lambda scaling issues. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8bb6a2cc70234f17879df5f822637baf",
            "questionNumber": 486,
            "type": "single",
            "content": "<p>Question #486</p><p><br></p><p>A company recently completed a successful proof of concept of Amazon WorkSpaces. A solutions architect needs to make the solution highly available across two AWS Regions. Amazon WorkSpaces is deployed in a failover Region, and a hosted zone is deployed in Amazon Route 53.</p><p><br></p><p>What should the solutions architect do to configure high availability for the solution?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in each Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes."
                },
                {
                    "label": "B",
                    "content": "Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in the primary Region. Create a Route 53 multivalue answer routing policy."
                },
                {
                    "label": "C",
                    "content": "Create a connection alias in the primary Region. Associate the connection alias with a directory in the primary Region. Create a Route 53 weighted routing policy."
                },
                {
                    "label": "D",
                    "content": "Create a connection alias in the primary Region. Associate the connection alias with a directory in the failover Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a connection alias in the primary Region and in the failover Region. Associate the connection aliases with a directory in each Region. Create a Route 53 failover routing policy. Set Evaluate Target Health to Yes.</p><p>This solution provides the most effective high availability configuration for Amazon WorkSpaces across two AWS Regions for the following reasons:</p><p>Complete Regional Independence: By creating connection aliases in both the primary and failover Regions and associating each with its own directory, you establish fully independent WorkSpaces environments in each Region. This ensures that if one Region experiences an outage, the other can continue to operate without dependency on the affected Region.</p><p>Automated Health-Based Routing: The Route 53 failover routing policy with \"Evaluate Target Health\" set to Yes actively monitors the health of your WorkSpaces environment. If the primary Region becomes unavailable or unhealthy, Route 53 will automatically redirect users to the failover Region without requiring manual intervention.</p><p>True Disaster Recovery: This configuration provides genuine disaster recovery capabilities, as users can be redirected to a completely functional WorkSpaces environment in the failover Region if the primary Region experiences issues.</p><p>Minimal Downtime: The automatic failover mechanism ensures that users experience minimal disruption during a regional outage, as they are quickly redirected to the healthy Region.</p><p>Option B (creating connection aliases in both Regions but associating them with a directory only in the primary Region and using a multivalue answer routing policy) is less effective because:</p><p>Single Point of Failure: Having the directory only in the primary Region creates a dependency that could prevent the failover Region from functioning if the primary Region becomes unavailable.</p><p>No Health-Based Routing: The multivalue answer routing policy doesn't consider the health of the endpoints. It simply returns multiple values in a random order, which doesn't guarantee that users will be directed to the available Region during an outage.</p><p>Inconsistent User Experience: Without health checks, some users might be directed to the unavailable Region, resulting in connection failures and a poor user experience.</p><p>The solution in option A follows AWS best practices for creating a highly available WorkSpaces deployment across multiple Regions, providing both redundancy and automated failover capabilities to ensure business continuity.</p><p>Sources</p><p>Amazon WorkSpaces FAQs | Persistent Desktop Virtualization （https://aws.amazon.com/cn/workspaces-family/workspaces/faqs/）</p><p>Business continuity for WorkSpaces Personal - Amazon WorkSpaces （https://docs.aws.amazon.com/workspaces/latest/adminguide/business-continuity.html）</p><p>https://docs.aws.amazon.com/workspaces/latest/adminguide/cross-region-redirection.html#cross-region-redirection-associate-connection-aliases</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "87534d3a30d143289d62341cbba2a61f",
            "questionNumber": 487,
            "type": "single",
            "content": "Question #487<p><br></p><p>A company plans to migrate many VMs from an on-premises environment to AWS. The company requires an initial assessment of the on-premises environment before the migration, a visualization of the dependencies between applications that run on the VMs, and a report that provides an assessment of the on-premises environment.</p><p><br></p><p>To get this information, the company has initiated a Migration Evaluator assessment request. The company has the ability to install collector software in its on-premises environment without any constraints.</p><p><br></p><p>Which solution will provide the company with the required information with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Install the AWS Application Discovery Agent on each on-premises VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick insights assessment report from Migration Hub."
                },
                {
                    "label": "B",
                    "content": "Install the Migration Evaluator Collector on each on-premises VM. After the data collection period ends, use Migration Evaluator to view the application dependencies. Download and export the discovered server list from Migration Evaluator. Upload the list to Amazon QuickSight. When the QuickSight report is generated, download the Quick Insights assessment report."
                },
                {
                    "label": "C",
                    "content": "Setup the AWS Application Discovery Service Agentless Collector in the on-premises environment. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Export the discovered server list from Application Discovery Service. Upload the list to Migration Evaluator. When the Migration Evaluator report is generated, download the Quick Insights assessment."
                },
                {
                    "label": "D",
                    "content": "Set up the Migration Evaluator Collector in the on-premises environment. Install the AWS Application Discovery Agent on each VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick Insights assessment report from Migration Evaluator."
                }
            ],
            "correctAnswer": "A",
            "explanation": "The company is looking for a solution that provides an initial assessment of their on-premises environment, visualizes application dependencies, and offers a report with an assessment, all with minimal operational overhead. Let's evaluate the options:<p><br></p><p>A. Install the AWS Application Discovery Agent on each on-premises VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick insights assessment report from Migration Hub.</p><p>   - This option is straightforward and uses a single agent for data collection. AWS Migration Hub integrates with the AWS Application Discovery Service to provide a visualization of application dependencies and an assessment report with minimal effort.</p><p><br></p><p>B. Install the Migration Evaluator Collector on each on-premises VM. After the data collection period ends, use Migration Evaluator to view the application dependencies. Download and export the discovered server list from Migration Evaluator. Upload the list to Amazon QuickSight. When the QuickSight report is generated, download the Quick Insights assessment report.</p><p>   - This option involves additional steps, such as exporting and uploading server lists to Amazon QuickSight, which adds operational overhead.</p><p><br></p><p>C. Set up the AWS Application Discovery Service Agentless Collector in the on-premises environment. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Export the discovered server list from Application Discovery Service. Upload the list to Migration Evaluator. When the Migration Evaluator report is generated, download the Quick Insights assessment.</p><p>   - This option is more complex because it involves setting up an agentless collector and additional steps to upload the server list to Migration Evaluator.</p><p><br></p><p>D. Set up the Migration Evaluator Collector in the on-premises environment. Install the AWS Application Discovery Agent on each VM. After the data collection period ends, use AWS Migration Hub to view the application dependencies. Download the Quick Insights assessment report from Migration Evaluator.</p><p>   - This option is not practical as it suggests using two different collectors, which is unnecessary and would create additional work.</p><p><br></p><p>Option A is the most efficient because it requires the least operational overhead by using a single agent and leveraging AWS Migration Hub for dependency visualization and report generation.</p><p><br></p><p>For the most up-to-date information on AWS services and best practices, refer to the official AWS documentation.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "fd8533389cf947afb11a4402271f4769",
            "questionNumber": 488,
            "type": "single",
            "content": "<p>Question #488</p><p><br></p><p>A company hosts its primary API on AWS by using an Amazon API Gateway API and AWS Lambda functions that contain the logic for the API methods. The company’s internal applications use the API for core functionality and business logic. The company’s customers use the API to access data from their accounts. Several customers also have access to a legacy API that is running on a single standalone Amazon EC2 instance.</p><p><br></p><p>The company wants to increase the security for these APIs to better prevent denial of service (DoS) attacks, check for vulnerabilities, and guard against common exploits.</p><p><br></p><p>What should a solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS WAF to protect both APIs. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs."
                },
                {
                    "label": "B",
                    "content": "Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze both APIs. Configure Amazon GuardDuty to block malicious attempts to access the APIs."
                },
                {
                    "label": "C",
                    "content": "Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to analyze the legacy API. Configure Amazon GuardDuty to monitor for malicious attempts to access the APIs."
                },
                {
                    "label": "D",
                    "content": "Use AWS WAF to protect the API Gateway API. Configure Amazon Inspector to protect the legacy API. Configure Amazon GuardDuty to block malicious attempts to access the APIs."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The question requires a solution that: &nbsp;</p><p>1. Increases security for both the API Gateway-based API and the legacy EC2-hosted API. &nbsp;</p><p>2. Prevents DoS attacks, checks for vulnerabilities, and guards against common exploits. &nbsp;</p><p>Option C provides the most complete and accurate approach: &nbsp;</p><p>- AWS WAF (Web Application Firewall): &nbsp;</p><p> &nbsp;- Protects the API Gateway API from common web exploits (e.g., SQL injection, XSS) and helps mitigate DoS attacks. &nbsp;</p><p> &nbsp;- *Note: AWS WAF cannot be directly applied to the legacy EC2 API (since it’s not behind an ALB, CloudFront, or API Gateway).* &nbsp;</p><p>- Amazon Inspector: &nbsp;</p><p> &nbsp;- Scans the legacy EC2 instance for vulnerabilities (OS/app-level misconfigurations, CVEs). &nbsp;</p><p>- Amazon GuardDuty: &nbsp;</p><p> &nbsp;- Monitors for malicious activity (e.g., unusual API access patterns, brute force attempts) but does not block traffic directly (it provides threat detection, not enforcement). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A: Incorrect because AWS WAF cannot protect the legacy EC2 API (unless it’s behind a supported service like ALB/CloudFront). &nbsp;</p><p>- B: Incorrect because: &nbsp;</p><p> &nbsp;- Inspector cannot analyze API Gateway (it’s for EC2, ECR, Lambda, etc., not managed services). &nbsp;</p><p> &nbsp;- GuardDuty cannot block traffic (only detects threats; enforcement requires integration with other services). &nbsp;</p><p>- D: Incorrect because: &nbsp;</p><p> &nbsp;- Inspector does not \"protect\" (it only assesses vulnerabilities). &nbsp;</p><p> &nbsp;- GuardDuty cannot block traffic directly (same issue as B). &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the correct choice because it: &nbsp;</p><p>✔ Uses AWS WAF for API Gateway (protection against exploits/DoS). &nbsp;</p><p>✔ Uses Inspector for legacy EC2 (vulnerability assessment). &nbsp;</p><p>✔ Uses GuardDuty for monitoring (threat detection). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b328e0d6d395489692e1ceaec1ad2122",
            "questionNumber": 489,
            "type": "single",
            "content": "<p>Question #489</p><p><br></p><p>A company is running a serverless ecommerce application on AWS. The application uses Amazon API Gateway to invoke AWS Lambda Java functions. The Lambda functions connect to an Amazon RDS for MySQL database to store data.</p><p><br></p><p>During a recent sale event, a sudden increase in web traffic resulted in poor API performance and database connection failures. The company needs to implement a solution to minimize the latency for the Lambda functions and to support bursts in traffic.</p><p><br></p><p>Which solution will meet these requirements with the LEAST amount of change to the application?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Update the code of the Lambda functions so that the Lambda functions open the database connection outside of the function handler. Increase the provisioned concurrency for the Lambda functions."
                },
                {
                    "label": "B",
                    "content": "Create an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the provisioned concurrency for the Lambda functions."
                },
                {
                    "label": "C",
                    "content": "Create a custom parameter group. Increase the value of the max_connections parameter. Associate the custom parameter group with the RDS DB instance and schedule a reboot. Increase the reserved concurrency for the Lambda functions."
                },
                {
                    "label": "D",
                    "content": "Create an RDS Proxy endpoint for the database. Store database secrets in AWS Secrets Manager. Set up the required IAM permissions. Update the Lambda functions to connect to the RDS Proxy endpoint. Increase the reserved concurrency for the Lambda functions."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The main issues during the sale event were: &nbsp;</p><p>1. Database connection failures due to high Lambda concurrency overwhelming the RDS MySQL database. &nbsp;</p><p>2. Increased API latency due to Lambda cold starts and connection management overhead. &nbsp;</p><p> Why Option B is the Best Solution? &nbsp;</p><p>- RDS Proxy helps manage database connections efficiently by pooling and reusing connections, preventing exhaustion when Lambda scales up. &nbsp;</p><p>- Secrets Manager securely stores database credentials, which RDS Proxy integrates with. &nbsp;</p><p>- Provisioned Concurrency reduces Lambda cold starts, improving latency during traffic bursts. &nbsp;</p><p>- This solution requires minimal code changes—only updating the Lambda functions to point to the RDS Proxy endpoint. &nbsp;</p><p> Why Other Options Are Not Ideal? &nbsp;</p><p>- A: While moving the connection outside the handler and using provisioned concurrency helps, it doesn’t solve the RDS connection exhaustion issue. &nbsp;</p><p>- C: Increasing `max_connections` in RDS may help, but it doesn’t efficiently manage connection pooling like RDS Proxy. Reserved concurrency (unlike provisioned concurrency) doesn’t prevent cold starts. &nbsp;</p><p>- D: Reserved concurrency limits Lambda scaling, which is counterproductive during traffic bursts. Provisioned concurrency (as in B) is the correct choice for reducing latency. &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the best solution because it combines RDS Proxy (for connection management) and provisioned concurrency (to reduce latency) with minimal changes. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a0b3f17263664833b52edc2a209dca24",
            "questionNumber": 490,
            "type": "single",
            "content": "<p>Question #490</p><p><br></p><p>A company requires that all internal application connectivity use private IP addresses. To facilitate this policy, a solutions architect has created interface endpoints to connect to AWS Public services. Upon testing, the solutions architect notices that the service names are resolving to public IP addresses, and that internal services cannot connect to the interface endpoints.</p><p><br></p><p>Which step should the solutions architect take to resolve this issue?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Update the subnet route table with a route to the interface endpoint."
                },
                {
                    "label": "B",
                    "content": "Enable the private DNS option on the VPC attributes."
                },
                {
                    "label": "C",
                    "content": "Configure the security group on the interface endpoint to allow connectivity to the AWS services."
                },
                {
                    "label": "D",
                    "content": "Configure an Amazon Route 53 private hosted zone with a conditional forwarder for the internal application."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Enable the private DNS option on the VPC attributes.</p><p>This is the correct solution because:</p><p>The problem described is that service names are resolving to public IP addresses instead of private IP addresses, even though interface endpoints have been created. This indicates a DNS resolution issue.</p><p>When you create interface endpoints to AWS services, by default they come with their own endpoint-specific DNS names that resolve to the private IP addresses of the endpoint network interfaces in your VPC. However, applications typically use the standard public DNS names of AWS services (like s3.amazonaws.com).</p><p>To make the standard public DNS names of AWS services resolve to the private IP addresses of your interface endpoints, you need to enable the private DNS option. This creates a private hosted zone in Route 53 that overrides the default public DNS resolution within your VPC.</p><p>When the private DNS option is enabled, requests from within your VPC to the standard AWS service DNS names will resolve to the private IP addresses of your interface endpoints, allowing your internal applications to connect to AWS services using private IP addresses as required by the company policy.</p><p>The other options are incorrect for the following reasons:</p><p>A. Updating the subnet route table with a route to the interface endpoint - Interface endpoints don't require specific routes in the route table. They work at the DNS level by resolving service names to private IP addresses.</p><p>C. Configuring the security group on the interface endpoint - While security groups are important for controlling access to interface endpoints, they don't affect DNS resolution. The issue described is about DNS resolution, not access control.</p><p>D. Configuring an Amazon Route 53 private hosted zone with a conditional forwarder - This is a more complex solution than necessary. Enabling the private DNS option automatically creates the required private hosted zone with the correct DNS records.</p><p>By enabling the private DNS option, the solutions architect will ensure that AWS service names resolve to the private IP addresses of the interface endpoints, allowing internal applications to connect to AWS services using private IP addresses as required by the company policy.</p><p>Sources</p><p>Centralized access to VPC private endpoints - Building a Scalable and Secure Multi-VPC AWS Network Infrastructure （https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/centralized-access-to-vpc-private-endpoints.html）</p><p>Troubleshoot resolving domain name over a VPC peering connection | AWS re:Post（https://repost.aws/knowledge-center/vpc-peering-troubleshoot-dns-resolution） </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "da5c5c8e10544597a1cbd549c9fdb0ab",
            "questionNumber": 491,
            "type": "single",
            "content": "<p>Question #491</p><p><br></p><p>A company is developing a latency-sensitive application. Part of the application includes several AWS Lambda functions that need to initialize as quickly as possible. The Lambda functions are written in Java and contain initialization code outside the handlers to load libraries, initialize classes, and generate unique IDs.</p><p><br></p><p>Which solution will meet the startup performance requirement MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Move all the initialization code to the handlers for each Lambda function. Activate Lambda SnapStart for each Lambda function. Configure SnapStart to reference the $LATEST version of each Lambda function."
                },
                {
                    "label": "B",
                    "content": "Publish a version of each Lambda function. Create an alias for each Lambda function. Configure each alias to point to its corresponding version. Set up a provisioned concurrency configuration for each Lambda function to point to the corresponding alias."
                },
                {
                    "label": "C",
                    "content": "&nbsp;Publish a version of each Lambda function. Set up a provisioned concurrency configuration for each Lambda function to point to the corresponding version. Activate Lambda SnapStart for the published versions of the Lambda functions."
                },
                {
                    "label": "D",
                    "content": "Update the Lambda functions to add a pre-snapshot hook. Move the code that generates unique IDs into the handlers. Publish a version of each Lambda function. Activate Lambda SnapStart for the published versions of the Lambda functions."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Update the Lambda functions to add a pre-snapshot hook. Move the code that generates unique IDs into the handlers. Publish a version of each Lambda function. Activate Lambda SnapStart for the published versions of the Lambda functions.</p><p>This solution will meet the startup performance requirement most cost-effectively for the following reasons:</p><p>Lambda SnapStart for Java: Lambda SnapStart significantly improves the cold start performance of Java functions by taking a snapshot of the initialized execution environment and reusing it for subsequent invocations. This directly addresses the requirement for Lambda functions to \"initialize as quickly as possible.\"</p><p>Moving Unique ID Generation to Handlers: The current Lambda functions generate unique IDs during initialization (outside the handlers). This is problematic with SnapStart because the same snapshot is used for multiple function invocations, which would result in duplicate IDs. Moving the unique ID generation code into the handlers ensures that each function invocation generates a new, unique ID.</p><p>Pre-snapshot Hooks: Adding pre-snapshot hooks allows for proper preparation of the execution environment before the snapshot is taken. This ensures that libraries and classes are properly initialized and optimized before being captured in the snapshot, leading to better performance when the function is invoked.</p><p>Cost-Effectiveness: This solution is more cost-effective than option C because it doesn't use provisioned concurrency, which incurs charges even when the functions aren't being invoked. With option D, you only pay for the actual function invocations and the standard SnapStart costs.</p><p>Option C (using provisioned concurrency with SnapStart) would also improve startup performance but at a higher cost. Provisioned concurrency keeps execution environments warm and ready to respond, but you pay for this capacity regardless of whether it's being used. This makes it less cost-effective, especially for applications with variable or unpredictable traffic patterns.</p><p>By optimizing the code structure for SnapStart (moving unique ID generation to handlers and using pre-snapshot hooks) and avoiding the ongoing costs of provisioned concurrency, option D provides the best balance of meeting the startup performance requirements while minimizing costs.</p><p>Sources</p><p>What Are The Best Practices for Managing Cold Starts in AWS Lambda ? | AWS re:Post （https://repost.aws/questions/QUCMh4YyniS4WieB786GGM9A/what-are-the-best-practices-for-managing-cold-starts-in-aws-lambda）</p><p>Reduce SDK startup time for AWS Lambda - AWS SDK for Java 2.x （https://docs.aws.amazon.com/sdk-for-java/latest/developer-guide/lambda-optimize-starttime.html）</p><p>Improving API performance at Sonar with Lambda SnapStart and Micronaut | AWS Open Source Blog （https://aws.amazon.com/cn/blogs/opensource/improving-api-performance-at-sonar-with-lambda-snapstart-and-micronaut/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "04c89f669cd6487dba265ad16d021173",
            "questionNumber": 492,
            "type": "multiple",
            "content": "<p>Question #492</p><p><br></p><p>A solutions architect is importing a VM from an on-premises environment by using the Amazon EC2 VM Import feature of AWS Import/Export. The solutions architect has created an AMI and has provisioned an Amazon EC2 instance that is based on that AMI. The EC2 instance runs inside a public subnet in a VPC and has a public IP address assigned.</p><p><br></p><p>The EC2 instance does not appear as a managed instance in the AWS Systems Manager console.</p><p><br></p><p>Which combination of steps should the solutions architect take to troubleshoot this issue? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Verify that Systems Manager Agent is installed on the instance and is running."
                },
                {
                    "label": "B",
                    "content": "Verify that the instance is assigned an appropriate IAM role for Systems Manager."
                },
                {
                    "label": "C",
                    "content": "Verify the existence of a VPC endpoint on the VPC."
                },
                {
                    "label": "D",
                    "content": "Verify that the AWS Application Discovery Agent is configured."
                },
                {
                    "label": "E",
                    "content": "Verify the correct configuration of service-linked roles for Systems Manager."
                }
            ],
            "correctAnswer": "AB",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Verify that Systems Manager Agent is installed on the instance and is running. B. Verify that the instance is assigned an appropriate IAM role for Systems Manager.</p><p>These are the two critical steps needed to troubleshoot why the EC2 instance doesn't appear as a managed instance in AWS Systems Manager:</p><p>Systems Manager Agent (SSM Agent) Verification (Option A):</p><p>When importing a VM from an on-premises environment using EC2 VM Import, the SSM Agent is not automatically installed on the resulting AMI.</p><p>The SSM Agent is essential for Systems Manager to communicate with and manage the EC2 instance.</p><p>The solutions architect needs to verify that the SSM Agent is properly installed and running on the imported instance.</p><p>Without the SSM Agent, the instance cannot communicate with the Systems Manager service and won't appear in the console.</p><p>IAM Role Assignment (Option B):</p><p>EC2 instances require an IAM role with appropriate permissions to communicate with Systems Manager.</p><p>The imported VM likely doesn't have the necessary IAM role attached to it.</p><p>The solutions architect needs to create and attach an IAM role with the AmazonSSMManagedInstanceCore managed policy (or equivalent permissions) to the instance.</p><p>Without this role, the instance won't have permission to communicate with the Systems Manager service.</p><p>The other options are not necessary or relevant for this specific issue:</p><p>C. Verify the existence of a VPC endpoint on the VPC - While VPC endpoints can enhance security by keeping traffic within the AWS network, they are optional. Since the instance is in a public subnet with a public IP address, it can communicate with Systems Manager over the internet without requiring VPC endpoints.</p><p>D. Verify that the AWS Application Discovery Agent is configured - The Application Discovery Agent is part of AWS Application Discovery Service, which is used for migration planning. It's not related to or required for Systems Manager functionality.</p><p>E. Verify the correct configuration of service-linked roles for Systems Manager - Service-linked roles are primarily used for AWS services to interact with other services on your behalf. They are not typically the cause of an instance not appearing in Systems Manager.</p><p>The most common reasons for an EC2 instance not appearing in Systems Manager are the absence of the SSM Agent or missing IAM permissions, which is why options A and B are the correct troubleshooting steps.</p><p>Sources</p><p>Troubleshooting managed node issues in Systems Manager with SAW | AWS re:Post （https://repost.aws/articles/ARRZ_rHRGiRQ2_MaNPDhISSw/troubleshooting-managed-node-issues-in-systems-manager-with-saw）</p><p>Configure instance permissions required for Systems Manager - AWS Systems Manager （https://docs.aws.amazon.com/systems-manager/latest/userguide/setup-instance-permissions.html）</p><p>Unable to Connect to EC2 Instance via SSM Session Manager | AWS re:Post（https://repost.aws/questions/QUmw-Dgnm0RuaCyCLsOVuo5Q/unable-to-connect-to-ec2-instance-via-ssm-session-manager） </p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a2e86d096f194d628aaeca34dba6a0a5",
            "questionNumber": 493,
            "type": "single",
            "content": "<p>Question #493</p><p><br></p><p>A company is using AWS CloudFormation as its deployment tool for all applications. It stages all application binaries and templates within Amazon S3 buckets with versioning enabled. Developers have access to an Amazon EC2 instance that hosts the integrated development environment (IDE). The developers download the application binaries from Amazon S3 to the EC2 instance, make changes, and upload the binaries to an S3 bucket after running the unit tests locally. The developers want to improve the existing deployment mechanism and implement CI/CD using AWS CodePipeline.</p><p><br></p><p>The developers have the following requirements:</p><p><br></p><p>- Use AWS CodeCommit for source control.</p><p>- Automate unit testing and security scanning.</p><p>- Alert the developers when unit tests fail.</p><p>- Turn application features on and off, and customize deployment dynamically as part of CI/CD.</p><p>- Have the lead developer provide approval before deploying an application.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS CodeBuild to run unit tests and security scans. Use an Amazon EventBridge rule to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Cloud Development Kit (AWS CDK) constructs for different solution features, and use a manifest file to turn features on and off in the AWS CDK application. Use a manual approval stage in the pipeline to allow the lead developer to approve applications."
                },
                {
                    "label": "B",
                    "content": "Use AWS Lambda to run unit tests and security scans. Use Lambda in a subsequent stage in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Write AWS Amplify plugins for different solution features and utilize user prompts to turn features on and off. Use Amazon SES in the pipeline to allow the lead developer to approve applications."
                },
                {
                    "label": "C",
                    "content": "Use Jenkins to run unit tests and security scans. Use an Amazon EventBridge rule in the pipeline to send Amazon SES alerts to the developers when unit tests fail. Use AWS CloudFormation nested stacks for different solution features and parameters to turn features on and off. Use AWS Lambda in the pipeline to allow the lead developer to approve applications."
                },
                {
                    "label": "D",
                    "content": "Use AWS CodeDeploy to run unit tests and security scans. Use an Amazon CloudWatch alarm in the pipeline to send Amazon SNS alerts to the developers when unit tests fail. Use Docker images for different solution features and the AWS CLI to turn features on and off. Use a manual approval stage in the pipeline to allow the lead developer to approve applications."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The developers want to implement a CI/CD pipeline using AWS CodePipeline with the following key requirements: &nbsp;</p><p>1. Use AWS CodeCommit for source control. &nbsp;</p><p>2. Automate unit testing and security scanning (AWS CodeBuild is ideal for this). &nbsp;</p><p>3. Alert developers when unit tests fail (Amazon EventBridge + SNS can send notifications). &nbsp;</p><p>4. Turn features on/off dynamically (AWS CDK constructs + manifest file allow feature toggling). &nbsp;</p><p>5. Lead developer approval before deployment (manual approval stage in CodePipeline). &nbsp;</p><p> Why Option A is the Best Solution? &nbsp;</p><p>✅ AWS CodeBuild – Runs unit tests and security scans efficiently. &nbsp;</p><p>✅ Amazon EventBridge + SNS – Sends alerts when tests fail. &nbsp;</p><p>✅ AWS CDK constructs + manifest file – Enables dynamic feature toggling. &nbsp;</p><p>✅ Manual approval stage – Allows lead developer to review before deployment. &nbsp;</p><p> Why Other Options Are Not Ideal? &nbsp;</p><p>❌ B: &nbsp;</p><p>- Using Lambda for tests/scans is inefficient (CodeBuild is purpose-built). &nbsp;</p><p>- AWS Amplify plugins are not suitable for CloudFormation-based deployments. &nbsp;</p><p>- Amazon SES is for email, not pipeline approvals. &nbsp;</p><p>❌ C: &nbsp;</p><p>- Jenkins is not a fully managed AWS-native solution (CodeBuild is preferred). &nbsp;</p><p>- Amazon SES is not ideal for alerts (SNS is better). &nbsp;</p><p>- Lambda for approvals is unnecessary (CodePipeline has built-in manual approvals). &nbsp;</p><p>❌ D: &nbsp;</p><p>- AWS CodeDeploy is for deployment, not running tests/scans (CodeBuild is correct). &nbsp;</p><p>- Docker images + AWS CLI are not the best way to toggle features (AWS CDK is better). &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the best solution because it uses AWS-native tools (CodeBuild, CDK, EventBridge, SNS) and meets all requirements with minimal complexity. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8401936c2ef541de944e6dd61c37be9a",
            "questionNumber": 494,
            "type": "single",
            "content": "<p>Question #494</p><p><br></p><p>A global e-commerce company has many data centers around the world. With the growth of its stored data, the company needs to set up a solution to provide scalable storage for legacy on-premises file applications. The company must be able to take point-in-time copies of volumes using AWS Backup and must retain low-latency access to frequently accessed data. The company also needs to have storage volumes that can be mounted as Internet Small Computer System Interface (iSCSI) devices from the company’s on-premises application servers.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Provision an AWS Storage Gateway tape gateway. Configure the tape gateway to store data in an Amazon S3 bucket. Deploy AWS Backup to take point-in-time copies of the volumes."
                },
                {
                    "label": "B",
                    "content": "Provision an Amazon FSx File Gateway and an Amazon S3 File Gateway. Deploy AWS Backup to take point-in-time copies of the data."
                },
                {
                    "label": "C",
                    "content": "Provision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup."
                },
                {
                    "label": "D",
                    "content": "Provision an AWS Storage Gateway file gateway in cache mode. Deploy AWS Backup to take point-in-time copies of the volumes."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Provision an AWS Storage Gateway volume gateway in cache mode. Back up the on-premises Storage Gateway volumes with AWS Backup.</p><p>This solution meets all the requirements stated in the question for the following reasons:</p><p>iSCSI Connectivity Requirement: The company needs storage volumes that can be mounted as iSCSI devices from on-premises application servers. AWS Storage Gateway volume gateway specifically provides iSCSI connectivity, allowing on-premises applications to access cloud storage using the iSCSI protocol. File gateway, on the other hand, uses NFS or SMB protocols, not iSCSI.</p><p>Point-in-Time Copies with AWS Backup: The company needs to take point-in-time copies of volumes using AWS Backup. AWS Storage Gateway volume gateway integrates with AWS Backup, allowing the creation and management of point-in-time snapshots of the volumes. These snapshots are stored as Amazon EBS snapshots in AWS.</p><p>Low-Latency Access to Frequently Accessed Data: The cache mode of the volume gateway keeps frequently accessed data locally on-premises while storing the primary data in Amazon S3. This provides low-latency access to commonly used data, meeting the company's requirement for fast access to frequently accessed information.</p><p>Scalable Storage for Legacy Applications: Volume gateway in cache mode provides virtually unlimited storage capacity by leveraging Amazon S3 for primary storage, while only requiring local storage for the cache. This addresses the company's need for scalable storage as their data continues to grow.</p><p>Option D (AWS Storage Gateway file gateway in cache mode) is incorrect because:</p><p>File gateway does not provide iSCSI connectivity, which is a specific requirement mentioned in the question.</p><p>File gateway uses file-level protocols (NFS/SMB) rather than block-level protocols like iSCSI.</p><p>While file gateway can work with data stored in S3, it doesn't integrate with AWS Backup in the same way that volume gateway does for taking point-in-time copies of volumes.</p><p>The volume gateway in cache mode is the appropriate solution for this scenario as it provides the required iSCSI connectivity, works with AWS Backup for point-in-time copies, offers low-latency access to frequently accessed data, and provides scalable storage for the company's growing data needs.</p><p>Sources</p><p>Backup and recovery from on-premises infrastructure to AWS - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/on-premises-to-aws.html）</p><p>Storage Gateway - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/configuring-storage-offload-vmware-cloud-aws/storage-gateway-option.html）</p><p>Volume Gateway | Amazon Web Services （https://aws.amazon.com/cn/storagegateway/volume/）</p><p>Backing up your volumes - AWS Storage Gateway （https://docs.aws.amazon.com/storagegateway/latest/vgw/backing-up-volumes.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f36a7c666b394425a64bc53e93fff45d",
            "questionNumber": 495,
            "type": "single",
            "content": "<p>Question #495</p><p><br></p><p>A company has an application that uses AWS Key Management Service (AWS KMS) to encrypt and decrypt data. The application stores data in an Amazon S3 bucket in an AWS Region. Company security policies require the data to be encrypted before the data is placed into the S3 bucket. The application must decrypt the data when the application reads files from the S3 bucket.</p><p><br></p><p>The company replicates the S3 bucket to other Regions. A solutions architect must design a solution so that the application can encrypt and decrypt data across Regions. The application must use the same key to decrypt the data in each Region.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a KMS multi-Region primary key. Use the KMS multi-Region primary key to create a KMS multi-Region replica key in each additional Region where the application is running. Update the application code to use the specific replica key in each Region."
                },
                {
                    "label": "B",
                    "content": "Create a new customer managed KMS key in each additional Region where the application is running. Update the application code to use the specific KMS key in each Region."
                },
                {
                    "label": "C",
                    "content": "Use AWS Private Certificate Authority to create a new certificate authority (CA) in the primary Region. Issue a new private certificate from the CA for the application&rsquo;s website URL. Share the CA with the additional Regions by using AWS Resource Access Manager (AWS RAM). Update the application code to use the shared CA certificates in each Region."
                },
                {
                    "label": "D",
                    "content": "Use AWS Systems Manager Parameter Store to create a parameter in each additional Region where the application is running. Export the key material from the KMS key in the primary Region. Store the key material in the parameter in each Region. Update the application code to use the key data from the parameter in each Region."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The company needs a solution that: &nbsp;</p><p>1. Encrypts/decrypts data in multiple Regions using the same KMS key. &nbsp;</p><p>2. Maintains security compliance (data must be encrypted before being stored in S3). &nbsp;</p><p>3. Avoids managing multiple keys (to simplify decryption across Regions). &nbsp;</p><p> Why Option A is the Best Solution? &nbsp;</p><p>✅ KMS Multi-Region Keys (MRK) allow: &nbsp;</p><p> &nbsp; - A primary key in one Region. &nbsp;</p><p> &nbsp; - Replica keys in other Regions that share the same key material. &nbsp;</p><p> &nbsp; - The application can use the local replica key in each Region, ensuring low latency while maintaining consistent encryption/decryption (since the keys are cryptographically identical). &nbsp;</p><p>✅ Minimal code changes – The application only needs to reference the replica key in its current Region. &nbsp;</p><p> Why Other Options Are Not Ideal? &nbsp;</p><p>❌ B: &nbsp;</p><p>- Different KMS keys per Region mean the application cannot decrypt data across Regions without re-encrypting it with the new key. &nbsp;</p><p>- Violates the requirement of using the same key. &nbsp;</p><p>❌ C: &nbsp;</p><p>- AWS Private CA is for TLS certificates, not KMS encryption. &nbsp;</p><p>- Does not solve the KMS key synchronization problem. &nbsp;</p><p>❌ D: &nbsp;</p><p>- Exporting KMS key material is highly discouraged (security risk). &nbsp;</p><p>- Parameter Store is not designed for secure key material storage (KMS is the correct service). &nbsp;</p><p> Conclusion: &nbsp;</p><p>A is the best solution because KMS Multi-Region Keys allow the same encryption key to be used across Regions securely, with minimal application changes. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d7707098ce384a1b9deff6d55a06d8bc",
            "questionNumber": 496,
            "type": "single",
            "content": "<p>Question #496</p><p><br></p><p>A company hosts an application that uses several Amazon EC2 instances in an Auto Scaling group behind an Application Load Balancer (ALB). During the initial startup of the EC2 instances, the EC2 instances run user data scripts to download critical content for the application from an Amazon S3 bucket.</p><p><br></p><p>The EC2 instances are launching correctly. However, after a period of time, the EC2 instances are terminated with the following error message: “An instance was taken out of service in response to an ELB system health check failure.” EC2 instances continue to launch and be terminated because of Auto Scaling events in an endless loop.</p><p><br></p><p>The only recent change to the deployment is that the company added a large amount of critical content to the S3 bucket. The company does not want to alter the user data scripts in production.</p><p><br></p><p>What should a solutions architect do so that the production environment can deploy successfully?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "&nbsp;Increase the size of the EC2 instances."
                },
                {
                    "label": "B",
                    "content": "Increase the health check timeout for the ALB."
                },
                {
                    "label": "C",
                    "content": "Change the health check path for the ALB."
                },
                {
                    "label": "D",
                    "content": "&nbsp;Increase the health check grace period for the Auto Scaling group."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The issue occurs because: &nbsp;</p><p>1. EC2 instances take longer to start due to downloading a large amount of content from S3 during user data execution. &nbsp;</p><p>2. ALB health checks start too early before the instances are fully ready, causing them to be marked as unhealthy and terminated. &nbsp;</p><p>3. This creates a loop where new instances launch, fail health checks, and get terminated repeatedly. &nbsp;</p><p> Why Option D is the Best Solution? &nbsp;</p><p>✅ Increase the health check grace period for the Auto Scaling group: &nbsp;</p><p> &nbsp; - This gives instances more time to complete their startup tasks (downloading from S3) before health checks begin. &nbsp;</p><p> &nbsp; - Prevents premature termination while keeping the existing user data scripts unchanged. &nbsp;</p><p> &nbsp; - No changes to ALB settings or instance size are needed. &nbsp;</p><p> Why Other Options Are Not Ideal? &nbsp;</p><p>❌ A: Increasing EC2 instance size &nbsp;</p><p> &nbsp; - Might help with download speed, but does not guarantee instances will be ready before health checks start. &nbsp;</p><p> &nbsp; - Unnecessary cost increase if the issue is timing, not compute power. &nbsp;</p><p>❌ B: Increasing ALB health check timeout &nbsp;</p><p> &nbsp; - This only extends how long the ALB waits for a health check response, not when health checks start. &nbsp;</p><p> &nbsp; - Does not solve the root cause (instances need more time to initialize). &nbsp;</p><p>❌ C: Changing the ALB health check path &nbsp;</p><p> &nbsp; - If the application is not yet running, changing the path won’t help—the instance still fails checks. &nbsp;</p><p> &nbsp; - Does not address the delay in startup tasks. &nbsp;</p><p> Conclusion: &nbsp;</p><p>D is the best solution because it delays health checks until instances are fully initialized, breaking the termination loop without modifying user data scripts. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c38e009c80f24dcda485eb1b6cca3831",
            "questionNumber": 497,
            "type": "single",
            "content": "<p>Question #497</p><p><br></p><p>A company needs to move some on-premises Oracle databases to AWS. The company has chosen to keep some of the databases on premises for business compliance reasons.</p><p><br></p><p>The on-premises databases contain spatial data and run cron jobs for maintenance. The company needs to connect to the on-premises systems directly from AWS to query data as a foreign table.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create Amazon DynamoDB global tables with auto scaling enabled. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to DynamoDB. Create an AWS Lambda function to move the spatial data to Amazon S3. Query the data by using Amazon Athena. Use Amazon EventBridge to schedule jobs in DynamoDB for maintenance. Use Amazon API Gateway for foreign table support."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon RDS for Microsoft SQL Server DB instance. Use native replication to move the data from on premises to the DB instance. Use the AWS Schema Conversion Tool (AWS SCT) to modify the SQL Server schema as needed after replication. Move the spatial data to Amazon Redshift. Use stored procedures for system maintenance. Create AWS Glue crawlers to connect to the on-premises Oracle databases for foreign table support."
                },
                {
                    "label": "C",
                    "content": "Launch Amazon EC2 instances to host the Oracle databases. Place the EC2 instances in an Auto Scaling group. Use AWS Application Migration Service to move the data from on premises to the EC2 instances and for real-time bidirectional change data capture (CDC) synchronization. Use Oracle native spatial data support. Create an AWS Lambda function to run maintenance jobs as part of an AWS Step Functions workflow. Create an internet gateway for foreign table support."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon RDS for PostgreSQL DB instance. Use the AWS Schema Conversion Tool (AWS SCT) and AWS Database Migration Service (AWS DMS) to move the data from on premises to the DB instance. Use PostgreSQL native spatial data support. Run cron jobs on the DB instance for maintenance. Use AWS Direct Connect to connect the DB instance to the on-premises environment for foreign table support."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. Migrate Oracle databases to AWS while keeping some on-premises for compliance. &nbsp;</p><p>2. Spatial data support – PostgreSQL has native support for spatial data (PostGIS extension). &nbsp;</p><p>3. Cron jobs for maintenance – PostgreSQL allows scheduled jobs (via `pg_cron` or external schedulers). &nbsp;</p><p>4. Query on-premises data as foreign tables – AWS Direct Connect provides a secure, high-speed connection to on-premises systems, and PostgreSQL supports Foreign Data Wrappers (FDW) to query remote databases. &nbsp;</p><p> Why not the other options? &nbsp;</p><p>- A: DynamoDB is not suitable for spatial data or complex SQL queries. Athena is for S3-based analytics, not Oracle foreign tables. &nbsp;</p><p>- B: SQL Server is not the best choice for migrating from Oracle, and Redshift is for analytics, not OLTP workloads. &nbsp;</p><p>- C: Running Oracle on EC2 is costly and doesn’t leverage AWS managed services. An internet gateway is not secure for direct database connections. &nbsp;</p><p> Why D is correct: &nbsp;</p><p>- Amazon RDS for PostgreSQL is a managed database service with PostGIS for spatial data. &nbsp;</p><p>- AWS SCT + DMS handles schema conversion and migration. &nbsp;</p><p>- Cron jobs can be implemented via `pg_cron` or Amazon EventBridge. &nbsp;</p><p>- AWS Direct Connect ensures secure, low-latency connectivity for querying on-premises Oracle databases as foreign tables. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ef9cef22625043e1a303f9db2109f1cb",
            "questionNumber": 498,
            "type": "single",
            "content": "<p>Question #498</p><p><br></p><p>Accompany runs an application on Amazon EC2 and AWS Lambda. The application stores temporary data in Amazon S3. The S3 objects are deleted after 24 hours.</p><p><br></p><p>The company deploys new versions of the application by launching AWS CloudFormation stacks. The stacks create the required resources. After validating a new version, the company deletes the old stack. The deletion of an old development stack recently failed.</p><p><br></p><p>A solutions architect needs to resolve this issue without major architecture changes.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a Lambda function to delete objects from an S3 bucket. Add the Lambda function as a custom resource in the CloudFormation stack with a DependsOn attribute that points to the S3 bucket resource."
                },
                {
                    "label": "B",
                    "content": "Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket."
                },
                {
                    "label": "C",
                    "content": "Update the CloudFormation stack to add a DeletionPolicy attribute with a value of Snapshot for the S3 bucket resource."
                },
                {
                    "label": "D",
                    "content": "Update the CloudFormation template to create an Amazon Elastic File System (Amazon EFS) file system to store temporary files instead of Amazon S3. Configure the Lambda functions to run in the same VPC as the EFS file system."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Create a Lambda function to delete objects from an S3 bucket. Add the Lambda function as a custom resource in the CloudFormation stack with a DependsOn attribute that points to the S3 bucket resource.</p><p>This solution will resolve the CloudFormation stack deletion failure issue for the following reasons:</p><p>Root Cause of the Problem: The stack deletion is failing because CloudFormation cannot delete an S3 bucket that contains objects. Even though the application is designed to delete objects after 24 hours, there might still be objects in the bucket at the time of stack deletion.</p><p>Why Option A Works: Creating a Lambda function as a custom resource with a DependsOn attribute ensures that:</p><p>The Lambda function will run during the stack deletion process</p><p>It will delete all objects from the S3 bucket before CloudFormation attempts to delete the bucket itself</p><p>The DependsOn attribute ensures proper sequencing, making sure the Lambda function executes before CloudFormation tries to delete the bucket</p><p>Advantages of This Approach:</p><p>It handles the cleanup of objects automatically during stack deletion</p><p>It works even if there are versioned objects or objects that were missed by the application's 24-hour deletion process</p><p>It's a more robust solution that will work consistently across different environments</p><p>Option B (Modify the CloudFormation stack to attach a DeletionPolicy attribute with a value of Delete to the S3 bucket) is incorrect because:</p><p>The DeletionPolicy attribute with a value of Delete only tells CloudFormation to delete the bucket during stack deletion, but it doesn't automatically delete the objects within the bucket.</p><p>If the bucket contains any objects, the stack deletion will still fail even with this policy in place.</p><p>CloudFormation requires buckets to be empty before they can be deleted, regardless of the DeletionPolicy setting.</p><p>The Lambda function approach (Option A) is more comprehensive as it actively empties the bucket before CloudFormation attempts to delete it, addressing the root cause of the stack deletion failure. This solution meets the requirement of resolving the issue without major architecture changes, as it only adds a custom resource to the existing CloudFormation stack without modifying the core application architecture.</p><p>Sources</p><p>Issue deleting S3 Bucket via CloudFormation | AWS re:Post （https://repost.aws/questions/QUBbQap76TQ8-lp-j8Gr7XQw/issue-deleting-s3-bucket-via-cloudformation）</p><p>S3 Bucket Not Deleted When Deleting CloudFormation Stack | AWS re:Post（https://repost.aws/questions/QUWQxEijWhSS6TtNh_Qu8gwg/s3-bucket-not-deleted-when-deleting-cloudformation-stack） </p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4f24ce36c17e4beebbc2a9fd7f47bf7d",
            "questionNumber": 499,
            "type": "multiple",
            "content": "<p>Question #499</p><p><br></p><p>A company has an application that stores user-uploaded videos in an Amazon S3 bucket that uses S3 Standard storage. Users access the videos frequently in the first 180 days after the videos are uploaded. Access after 180 days is rare. Named users and anonymous users access the videos.</p><p><br></p><p>Most of the videos are more than 100 MB in size. Users often have poor internet connectivity when they upload videos, resulting in failed uploads.</p><p>The company uses multipart uploads for the videos.</p><p>A solutions architect needs to optimize the S3 costs of the application.</p><p>Which combination of actions will meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the S3 bucket to be a Requester Pays bucket."
                },
                {
                    "label": "B",
                    "content": "Use S3 Transfer Acceleration to upload the videos to the S3 bucket."
                },
                {
                    "label": "C",
                    "content": "Create an S3 Lifecycle configuration to expire incomplete multipart uploads 7 days after initiation."
                },
                {
                    "label": "D",
                    "content": "Create an S3 Lifecycle configuration to transition objects to S3 Glacier Instant Retrieval after 1 day."
                },
                {
                    "label": "E",
                    "content": "Create an S3 Lifecycle configuration to transition objects to S3 Standard-infrequent Access (S3 Standard-IA) after 180 days."
                }
            ],
            "correctAnswer": "CE",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>C. Create an S3 Lifecycle configuration to expire incomplete multipart uploads 7 days after initiation. E. Create an S3 Lifecycle configuration to transition objects to S3 Standard-Infrequent Access (S3 Standard-IA) after 180 days.</p><p>These two actions together will optimize the S3 costs for the company's application for the following reasons:</p><p>Expiring incomplete multipart uploads (Option C):</p><p>The scenario mentions that users often have poor internet connectivity when uploading videos, resulting in failed uploads.</p><p>When multipart uploads fail or are abandoned, the partially uploaded parts remain in the S3 bucket and continue to incur storage costs.</p><p>Creating a lifecycle configuration to expire incomplete multipart uploads after 7 days will automatically clean up these abandoned upload parts, preventing unnecessary storage costs.</p><p>This is especially important for large files like videos (over 100MB), as multipart uploads consist of multiple parts that can add up to significant storage costs if left unmanaged.</p><p>Transitioning to S3 Standard-IA after 180 days (Option E):</p><p>The scenario states that videos are frequently accessed in the first 180 days after upload, but access after that is rare.</p><p>S3 Standard-IA is designed for data that is accessed less frequently but requires rapid access when needed.</p><p>S3 Standard-IA offers a lower storage cost compared to S3 Standard, making it ideal for the rarely accessed older videos.</p><p>Since the videos are larger than 100MB, they easily meet the 128KB minimum object size for cost-effective use of S3 Standard-IA.</p><p>The transition after 180 days aligns perfectly with the described access pattern, keeping the videos in the most cost-effective storage class based on their access frequency.</p><p>Together, these two lifecycle configurations address both the cost optimization for long-term storage and the cleanup of failed uploads, which are the two main cost concerns mentioned in the scenario. The first ensures you're not paying for abandoned upload parts, while the second ensures you're using the most cost-effective storage class based on the access patterns of the videos.</p><p>Sources</p><p>How to Optimize AWS S3 Storage Costs? | AWS re:Post （https://repost.aws/questions/QU97-ATukcRz-XashRXJSkHA/how-to-optimize-aws-s3-storage-costs）</p><p>Cost-optimized log aggregation and archival in Amazon S3 using s3tar | AWS Storage Blog （https://aws.amazon.com/cn/blogs/storage/cost-optimized-log-aggregation-and-archival-in-amazon-s3-using-s3tar/）</p><p>Secure archive storage – Amazon S3 Glacier storage classes – AWS （https://aws.amazon.com/cn/s3/storage-classes/glacier/?ams%23interactive-card-vertical%23pattern-data.filter=%257B%2522filters%2522%253A%255B%255D%257D）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "9acf48a07cc64a67be8383b8c71cd54e",
            "questionNumber": 500,
            "type": "single",
            "content": "<p>Question #500</p><p><br></p><p>A company runs an e-commerce web application on AWS. The web application is hosted as a static website on Amazon S3 with Amazon CloudFront for content delivery. An Amazon API Gateway API invokes AWS Lambda functions to handle user requests and order processing for the web application. The Lambda functions store data in an Amazon RDS for MySQL DB cluster that uses On-Demand instances. The DB cluster usage has been consistent in the past 12 months.</p><p><br></p><p>Recently, the website has experienced SQL injection and web exploit attempts. Customers also report that order processing time has increased during periods of peak usage. During these periods, the Lambda functions often have cold starts. As the company grows, the company needs to ensure scalability and low-latency access during traffic peaks. The company also must optimize the database costs and add protection against the SQL injection and web exploit attempts.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the Lambda functions to have an increased timeout value during peak periods. Use RDS Reserved Instances for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts."
                },
                {
                    "label": "B",
                    "content": "Increase the memory of the Lambda functions, Transition to Amazon Redshift for the database. Integrate Amazon Inspector with CloudFront to protect against the SQL injection and web exploit attempts."
                },
                {
                    "label": "C",
                    "content": "&nbsp;Use Lambda functions with provisioned concurrency for compute during peak periods, Transition to Amazon Aurora Serverless for the database. Use CloudFront and subscribe to AWS Shield Advanced to protect against the SQL injection and web exploit attempts."
                },
                {
                    "label": "D",
                    "content": "Use Lambda functions with provisioned concurrency for compute during peak periods. Use RDS Reserved Instances for the database. Integrate AWS WAF with CloudFront to protect against the SQL injection and web exploit attempts."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The requirements are: &nbsp;</p><p>1. Improve scalability & reduce latency during peak traffic (cold starts in Lambda, slow order processing). &nbsp;</p><p>2. Optimize database costs (consistent usage over 12 months suggests Reserved Instances can save costs). &nbsp;</p><p>3. Protect against SQL injection & web exploits (security vulnerabilities). &nbsp;</p><p> Why Option D is Correct: &nbsp;</p><p>- Lambda Provisioned Concurrency eliminates cold starts during peak traffic, improving performance. &nbsp;</p><p>- RDS Reserved Instances reduce costs for the MySQL DB cluster (since usage is consistent). &nbsp;</p><p>- AWS WAF + CloudFront blocks SQL injection and web exploits (WAF inspects HTTP/HTTPS requests). &nbsp;</p><p> Why Other Options Are Wrong: &nbsp;</p><p>- A: Increasing Lambda timeout doesn’t solve cold starts. AWS Shield Advanced protects against DDoS, not SQL injection. &nbsp;</p><p>- B: Amazon Redshift is for analytics, not transactional workloads. Amazon Inspector scans for vulnerabilities but doesn’t block attacks in real time. &nbsp;</p><p>- C: Aurora Serverless is good for unpredictable workloads, but Reserved Instances are better for consistent usage. Shield Advanced doesn’t block SQL injection (WAF does). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        }
    ],
    "totalQuestions": 529,
    "hasNextPage": true,
    "page": 5,
    "pageSize": 100
}

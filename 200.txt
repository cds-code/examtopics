{
    "questions": [
        {
            "id": "1917d5e4d19d40d39b15c944b2375690",
            "questionNumber": 101,
            "type": "single",
            "content": "Question #101<p>A company is running applications on AWS in a multi-account environment. The company's sales team and marketing team use separate AWS accounts in AWS Organizations.<br><br></p><p>The sales team stores petabytes of data in an Amazon S3 bucket. The marketing team uses Amazon QuickSight for data visualizations. The marketing team needs access to data that the sates team stores in the S3 bucket. The company has encrypted the S3 bucket with an AWS Key Management Service (AWS KMS) key. The marketing team has already created the IAM service role for QuickSight to provide QuickSight access in the marketing AWS account. The company needs a solution that will provide secure access to the data in the S3 bucket across AWS accounts. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new S3 bucket in the marketing account. Create an S3 replication rule in the sales account to copy the objects to the new S3 bucket in the marketing account. Update the QuickSight permissions in the marketing account to grant access to the new S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Create an SCP to grant access to the S3 bucket to the marketing account. Use AWS Resource Access Manager (AWS RAM) to share the KMS key from the sales account with the marketing account. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket."
                },
                {
                    "label": "C",
                    "content": "Update the S3 bucket policy in the marketing account to grant access to the QuickSight role. Create a KMS grant for the encryption key that is used in the S3 bucket. Grant decrypt access to the QuickSight role. Update the QuickSight permissions in the marketing account to grant access to the S3 bucket.&nbsp;"
                },
                {
                    "label": "D",
                    "content": "Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight role to create a trust relationship with the new IAM role in the sales account."
                }
            ],
            "correctAnswer": "D",
            "explanation": "To provide secure access to the S3 bucket across AWS accounts with the least operational overhead, I recommend the following solution:<p>1. Create a cross-account IAM role in the sales team's AWS account:</p><p>   - This role should grant access to the S3 bucket and the KMS key used for encryption.</p><p>   - The trust policy should allow the marketing team's AWS account to assume this role.</p><p>2. Update the existing IAM service role for QuickSight in the marketing team's account:</p><p>   - Add permissions to assume the cross-account role created in step 1.</p><p>3. Configure QuickSight in the marketing team's account:</p><p>   - Set up a new data source using the cross-account role to access the S3 bucket.</p><p>4. (Optional) Use AWS Lake Formation for fine-grained access control:</p><p>   - If more granular control over data access is needed, consider implementing Lake Formation.</p><p>This solution provides several benefits:</p><p>- Minimal operational overhead: Leverages existing QuickSight setup and requires minimal new resources.</p><p>- Secure: Maintains the principle of least privilege by using cross-account roles.</p><p>- Scalable: Can easily accommodate future needs for data sharing between accounts.</p><p>Best practices to consider:</p><p>- Regularly review and audit the permissions granted to ensure they remain appropriate.</p><p>- Use condition keys in the IAM policies to further restrict access (e.g., by IP range or time of day).</p><p>- Enable AWS CloudTrail to monitor and log access to the S3 bucket and KMS key.</p><p>- Implement a process for regular rotation of the KMS key used for S3 bucket encryption.</p><p>Remember to test this setup in a non-production environment before implementing it in production. Also, consult the AWS documentation for the most up-to-date information on cross-account access and QuickSight configuration.</p><p>Sources</p><p>[1] [Example 4 - Bucket owner granting cross-account permission to objects it does not own - Amazon Simple Storage Service] (https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-walkthroughs-managing-access-example4.html)</p><p>[3] [Community | AWS Cross Account S3 Access Through Lambda Functions] (https://community.aws/content/2cCcsg05ZX2PoMHwMcWhv2BMlPL/aws-cross-account-s3-access-through-lambda-functions)</p><p>[6] [Configure IAM roles - AWS Transfer Family] (https://docs.aws.amazon.com/transfer/latest/userguide/webapp-roles.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5d495919b12040368f019c2924f702c1",
            "questionNumber": 102,
            "type": "single",
            "content": "<p>Question #102</p><p>A company is planning to migrate its business-critical applications from an on-premises data center to AWS. The company has an on-premises installation of a Microsoft SQL Server Always On cluster. The company wants to migrate to an AWS managed database service.A solutions architect must design a heterogeneous database migration on AWS. </p><p><br></p><p>Which solution will meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the SQL Server databases to Amazon RDS for MySQL by using backup and restore utilities."
                },
                {
                    "label": "B",
                    "content": "Use an AWS Snowball Edge Storage Optimized device to transfer data to Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT."
                },
                {
                    "label": "C",
                    "content": "Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS."
                },
                {
                    "label": "D",
                    "content": "<p>Use AWS DataSync to migrate data over the network between on-premises storage and Amazon S3. Set up Amazon RDS for MySQL. Use S3 integration with SQL Server features, such as BULK INSERT.</p>"
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use the AWS Schema Conversion Tool to translate the database schema to Amazon RDS for MySQL. Then use AWS Database Migration Service (AWS DMS) to migrate the data from on-premises databases to Amazon RDS.</p><p>This is the appropriate solution because:</p><p>The company needs to perform a heterogeneous database migration (moving from Microsoft SQL Server to a different database engine).</p><p>AWS Schema Conversion Tool (SCT) is specifically designed to convert database schemas from one database engine to another, handling the differences in data types and SQL syntax.</p><p>AWS Database Migration Service (DMS) is the recommended service for migrating data between different database platforms with minimal downtime.</p><p>This approach provides a comprehensive migration path that addresses both schema conversion and data migration requirements.</p><p>Options A and D don't provide proper schema conversion capabilities needed for heterogeneous migrations, and they would likely result in compatibility issues when moving from SQL Server to MySQL.</p><p>Sources</p><p>Migrating a SQL Server Always On Database to Amazon Web Services - Database Migration Guide （https://docs.aws.amazon.com/dms/latest/sbs/chap-manageddatabases.sqlserveralwayson.html）</p><p>SQL Server database migration strategies - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/strategies.html）</p><p>Migrate legacy database | AWS re:Post （https://repost.aws/questions/QUTC4STLNPTNeCYjbQMSErcA/migrate-legacy-database）</p><p>Tools for heterogeneous database migrations - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-sql-server/heterogeneous-migration-tools.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b103d915bc0d4dbbb1099acab27a8987",
            "questionNumber": 103,
            "type": "multiple",
            "content": "<p>Question #103</p><p>A publishing company's design team updates the icons and other static assets that an ecommerce web application uses. The company serves the icons and assets from an Amazon S3 bucket that is hosted in the company's production account. The company also uses a development account that members of the design team can access.<br><br><br></p><p>After the design team tests the static assets in the development account, the design team needs to load the assets into the S3 bucket in the production account. A solutions architect must provide the design team with access to the production account without exposing other parts of the web application to the risk of unwanted changes. </p><p><br></p><p>Which combination of steps will meet these requirements? (Choose three.) </p>",
            "options": [
                {
                    "label": "A",
                    "content": "In the production account, create a new IAM policy that allows read and write access to the S3 bucket."
                },
                {
                    "label": "B",
                    "content": "In the development account, create a new IAM policy that allows read and write access to the S3 bucket."
                },
                {
                    "label": "C",
                    "content": "In the production account, create a role. Attach the new policy to the role. Define the development account as a trusted entity."
                },
                {
                    "label": "D",
                    "content": "In the development account, create a role. Attach the new policy to the role. Define the production account as a trusted entity."
                },
                {
                    "label": "E",
                    "content": "In the development account, create a group that contains all the IAM users of the design team. Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role In the production account."
                }
            ],
            "correctAnswer": "ACE",
            "explanation": "<p>Answer: &nbsp;</p><p>A, C, E &nbsp;</p><p> Explanation: &nbsp;Key Requirements: &nbsp;</p><p>1. Design Team Access: &nbsp;</p><p> &nbsp; - The design team (in the development account) needs read/write access to a specific S3 bucket in the production account. &nbsp;</p><p>2. Least Privilege: &nbsp;</p><p> &nbsp; - Restrict access only to the S3 bucket (no other production resources). &nbsp;</p><p>3. Security: &nbsp;</p><p> &nbsp; - Avoid sharing production account credentials. &nbsp;</p><p> Correct Steps (A, C, E): &nbsp;</p><p>1. A (Production Account - IAM Policy for S3): &nbsp;</p><p> &nbsp; - Create a policy in the production account granting `s3:GetObject`, `s3:PutObject`, etc., only for the target bucket. &nbsp;</p><p> &nbsp; ```json</p><p> &nbsp; {</p><p> &nbsp; &nbsp; \"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; \"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; \"Action\": [\"s3:GetObject\", \"s3:PutObject\"],</p><p> &nbsp; &nbsp; &nbsp; \"Resource\": \"arn:aws:s3:::production-bucket/*\"</p><p> &nbsp; &nbsp; }]</p><p> &nbsp; }</p><p> &nbsp; ``` &nbsp;</p><p>2. C (Production Account - Role with Trust): &nbsp;</p><p> &nbsp; - Create an IAM role in the production account, attach the policy from A, and trust the development account. &nbsp;</p><p> &nbsp; ```json</p><p> &nbsp; {</p><p> &nbsp; &nbsp; \"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; \"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; \"Principal\": { \"AWS\": \"arn:aws:iam::DevelopmentAccountID:root\" },</p><p> &nbsp; &nbsp; &nbsp; \"Action\": \"sts:AssumeRole\"</p><p> &nbsp; &nbsp; }]</p><p> &nbsp; }</p><p> &nbsp; ``` &nbsp;</p><p>3. E (Development Account - Group with AssumeRole): &nbsp;</p><p> &nbsp; - Create a group for the design team in the development account and attach a policy allowing `sts:AssumeRole` on the production account’s role. &nbsp;</p><p> &nbsp; ```json</p><p> &nbsp; {</p><p> &nbsp; &nbsp; \"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; \"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; \"Action\": \"sts:AssumeRole\",</p><p> &nbsp; &nbsp; &nbsp; \"Resource\": \"arn:aws:iam::ProductionAccountID:role/DesignTeamRole\"</p><p> &nbsp; &nbsp; }]</p><p> &nbsp; }</p><p> &nbsp; ``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Policy in Dev Account): &nbsp;</p><p> &nbsp;- Policies in the dev account can’t grant access to production resources. &nbsp;</p><p>- D (Role in Dev Account): &nbsp;</p><p> &nbsp;- Roles in the dev account can’t access production S3 directly. &nbsp;</p><p>- F (AssumeRole on Dev Role): &nbsp;</p><p> &nbsp;- Doesn’t solve cross-account access. &nbsp;</p><p> Workflow: &nbsp;</p><p>1. Design team members (in dev account) assume the production role. &nbsp;</p><p>2. AWS grants temporary credentials to upload/download S3 assets. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A, C, E is the secure, least-privilege solution for cross-account S3 access. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "18f660ab74704fee970bf2e4d6689ac0",
            "questionNumber": 104,
            "type": "single",
            "content": "<p>Question #104</p><p>A company developed a pilot application by using AWS Elastic Beanstalk and Java. To save costs during development, the company's development team deployed the application into a single-instance environment. Recent tests indicate that the application consumes more CPU than expected. CPU utilization is regularly greater than 85%, which causes some performance bottlenecks.<br><br></p><p>A solutions architect must mitigate the performance issues before the company launches the application to production. <br><br></p><p>Which solution will meet these requirements with the LEAST operational overhead? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new Elastic Beanstalk application. Select a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the maximum CPU utilization is over 85% for 5 minutes."
                },
                {
                    "label": "B",
                    "content": "Create a second Elastic Beanstalk environment. Apply the traffic-splitting deployment policy. Specify a percentage of incoming traffic to direct to the new environment in the average CPU utilization is over 85% for 5 minutes."
                },
                {
                    "label": "C",
                    "content": "Modify the existing environment&rsquo;s capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes."
                },
                {
                    "label": "D",
                    "content": "Select the Rebuild environment action with the load balancing option. Select an Availability Zones. Add a scale-out rule that will run if the sum CPU utilization is over 85% for 5 minutes."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Answer: &nbsp;</p><p>C. Modify the existing environment’s capacity configuration to use a load-balanced environment type. Select all Availability Zones. Add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Performance Bottleneck: &nbsp;</p><p> &nbsp; - High CPU utilization (&gt;85%) in the single-instance Elastic Beanstalk environment. &nbsp;</p><p>2. Production Readiness: &nbsp;</p><p> &nbsp; - Need scalability (auto-scaling) and high availability (multi-AZ). &nbsp;</p><p>3. Least Operational Overhead: &nbsp;</p><p> &nbsp; - Avoid recreating the application or managing multiple environments. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- Convert to Load-Balanced Environment: &nbsp;</p><p> &nbsp;- Upgrades the existing single-instance environment to: &nbsp;</p><p> &nbsp; &nbsp;- Auto-scaling group (handles CPU spikes). &nbsp;</p><p> &nbsp; &nbsp;- Multi-AZ deployment (improves availability). &nbsp;</p><p>- Auto-Scaling Rule: &nbsp;</p><p> &nbsp;- Scales out when CPU &gt;85% for 5 minutes (matches observed bottleneck). &nbsp;</p><p>- Minimal Changes: &nbsp;</p><p> &nbsp;- No need to: &nbsp;</p><p> &nbsp; &nbsp;- Rebuild the app (unlike Option D). &nbsp;</p><p> &nbsp; &nbsp;- Split traffic manually (unlike Option B). &nbsp;</p><p> &nbsp; &nbsp;- Create a new app (unlike Option A). &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- A (New Application): &nbsp;</p><p> &nbsp;- Redundant: Requires redeploying the app to a new environment. &nbsp;</p><p>- B (Traffic Splitting): &nbsp;</p><p> &nbsp;- Complex: Manages two environments and splits traffic (not true auto-scaling). &nbsp;</p><p>- D (Rebuild Environment): &nbsp;</p><p> &nbsp;- Risky: \"Rebuild\" may cause downtime; also only selects one AZ (not highly available). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. In Elastic Beanstalk console: &nbsp;</p><p> &nbsp; - Go to Configuration → Capacity. &nbsp;</p><p> &nbsp; - Change Environment type to \"Load balanced\". &nbsp;</p><p> &nbsp; - Enable all Availability Zones. &nbsp;</p><p>2. Add auto-scaling rule: &nbsp;</p><p> &nbsp; - Metric: CPUUtilization. &nbsp;</p><p> &nbsp; - Threshold: 85% for 5 minutes. &nbsp;</p><p> &nbsp; - Action: Add instances. &nbsp;</p><p> Cost Consideration: &nbsp;</p><p>- Load balancer + multi-AZ adds cost but is necessary for production. &nbsp;</p><p> Conclusion: &nbsp;</p><p>C provides the simplest, most reliable path to production readiness with auto-scaling. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "14fb1eb99ccf4a719e8fb11302e3d074",
            "questionNumber": 105,
            "type": "single",
            "content": "<p>Question #105</p><p>A finance company is running its business-critical application on current-generation Linux EC2 instances. The application includes a self-managed MySQL database performing heavy I/O operations. The application is working fine to handle a moderate amount of traffic during the month. However, it slows down during the final three days of each month due to month-end reporting, even though the company is using Elastic Load Balancers and Auto Scaling within its infrastructure to meet the increased demand.<br><br>Which of the following actions would allow the database to handle the month-end load with the LEAST impact on performance?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Pre-warming Elastic Load Balancers, using a bigger instance type, changing all Amazon EBS volumes to GP2 volumes."
                },
                {
                    "label": "B",
                    "content": "Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month."
                },
                {
                    "label": "C",
                    "content": "Using Amazon CloudWatch with AWS Lambda to change the type, size, or IOPS of Amazon EBS volumes in the cluster based on a specific CloudWatch metric."
                },
                {
                    "label": "D",
                    "content": "Replacing all existing Amazon EBS volumes with new PIOPS volumes that have the maximum available storage size and I/O per second by taking snapshots before the end of the month and reverting back afterwards."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Performing a one-time migration of the database cluster to Amazon RDS, and creating several additional read replicas to handle the load during end of month.</p><p>This solution addresses the core issue in the most effective way because:</p><p>Migrating to Amazon RDS removes the operational overhead of managing the MySQL database, allowing the company to focus on their application rather than database administration.</p><p>Creating additional read replicas specifically for the month-end reporting would distribute the read-heavy workload across multiple database instances, significantly improving performance during these peak periods.</p><p>Read replicas can be automatically scaled up for the month-end reporting period and scaled down afterward, providing cost-effective performance optimization.</p><p>Amazon RDS is designed to handle heavy I/O operations and provides optimized performance for database workloads.</p><p>Option A (pre-warming ELBs, using bigger instances, and changing to GP2 volumes) would provide some improvement but doesn't address the fundamental issue as effectively as distributing the read workload across multiple database instances.</p><p>Sources</p><p>Conclusion - Optimizing MySQL Running on Amazon EC2 Using Amazon EBS （https://docs.aws.amazon.com/whitepapers/latest/optimizing-mysql-on-ec2-using-amazon-ebs/conclusion.html）</p><p>Ola Money achieved operational excellence, disaster recovery site in Asia Pacific (Hyderabad) Region, and up to 60% cost savings using Amazon Aurora | AWS Database Blog （https://aws.amazon.com/blogs/database/ola-money-achieved-operational-excellence-disaster-recovery-site-in-asia-pacific-hyderabad-region-and-up-to-60-cost-savings-using-amazon-aurora/）</p><p>Strategies for Optimizing Amazon RDS Performance for High-Read Workloads | AWS re:Post （https://repost.aws/questions/QUVtqJgk3PSfe2SRFypOKOGw/strategies-for-optimizing-amazon-rds-performance-for-high-read-workloads）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4080cab9692244e684c3158e5265138d",
            "questionNumber": 106,
            "type": "single",
            "content": "<p>Question #106</p><p>A company runs a Java application that has complex dependencies on VMs that are in the company's data center. The application is stable but the company wants to modernize the technology stack. The company wants to migrate the application to AWS and minimize the administrative overhead to maintain the servers.<br><br>Which solution will meet these requirements with the LEAST code changes?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission to access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application."
                },
                {
                    "label": "B",
                    "content": "Migrate the application code to a container that runs in AWS Lambda. Build an Amazon API Gateway REST API with Lambda integration. Use API Gateway to interact with the application."
                },
                {
                    "label": "C",
                    "content": "Migrate the application to Amazon Elastic Kubernetes Service (Amazon EKS) on EKS managed node groups by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Give the EKS nodes permission to access the ECR image repository. Use Amazon API Gateway to interact with the application."
                },
                {
                    "label": "D",
                    "content": "Migrate the application code to a container that runs in AWS Lambda. Configure Lambda to use an Application Load Balancer (ALB). Use the ALB to interact with the application."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Migrate the application to Amazon Elastic Container Service (Amazon ECS) on AWS Fargate by using AWS App2Container. Store container images in Amazon Elastic Container Registry (Amazon ECR). Grant the ECS task execution role permission 10 access the ECR image repository. Configure Amazon ECS to use an Application Load Balancer (ALB). Use the ALB to interact with the application.</p><p>This solution is the best choice because:</p><p>AWS App2Container is specifically designed to containerize existing applications with minimal code changes, which directly addresses the requirement to minimize code changes.</p><p>Using Amazon ECS on AWS Fargate provides a fully managed container orchestration service that eliminates the need to provision and maintain servers, reducing administrative overhead.</p><p>This approach preserves the application's existing architecture and dependencies while modernizing the infrastructure.</p><p>The containerization approach allows the complex dependencies to be packaged together with the application, ensuring compatibility.</p><p>Option B (migrating to AWS Lambda with API Gateway) would require significant refactoring of the Java application to fit the serverless model, especially considering the complex dependencies. Lambda has limitations on execution time and package size that might not be suitable for a complex Java application, and would likely require substantial code changes.</p><p>Sources</p><p>Migrate on-premises Java applications to AWS using AWS App2Container - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-on-premises-java-applications-to-aws-using-aws-app2container.html）</p><p>Containerizing traditional Java EE applications for the AWS Cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/modernization-containerize-javaee/welcome.html）</p><p>Containerize and migrate - AWS Prescriptive Guidance (https://docs.aws.amazon.com/prescriptive-guidance/latest/containerize-java-a2c/containerize.html)</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b78a5cc4c42a443abe3feeddd5a20e16",
            "questionNumber": 107,
            "type": "single",
            "content": "<p>Question #107</p><p>A company has an asynchronous HTTP application that is hosted as an AWS Lambda function. A public Amazon API Gateway endpoint invokes the Lambda function. The Lambda function and the API Gateway endpoint reside in the us-east-1 Region. A solutions architect needs to redesign the application to support failover to another AWS Region.<br><br>Which solution will meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an API Gateway endpoint in the us-west-2 Region to direct traffic to the Lambda function in us-east-1. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure API Gateway to direct traffic to the SQS queue instead of to the Lambda function. Configure the Lambda function to pull messages from the queue for processing."
                },
                {
                    "label": "C",
                    "content": "Deploy the Lambda function to the us-west-2 Region. Create an API Gateway endpoint in us-west-2 to direct traffic to the Lambda function in us-west-2. Configure AWS Global Accelerator and an Application Load Balancer to manage traffic across the two API Gateway endpoints."
                },
                {
                    "label": "D",
                    "content": "Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Deploy the Lambda function and an API Gateway endpoint to the us-west-2 Region. Configure Amazon Route 53 to use a failover routing policy to route traffic for the two API Gateway endpoints.</p><p>This solution is the best choice because:</p><p>It provides true regional redundancy by having both the Lambda function and API Gateway deployed in two separate regions (us-east-1 and us-west-2).</p><p>Using Route 53 with a failover routing policy allows automatic traffic redirection to the healthy region if one region experiences an outage.</p><p>This approach creates a complete, independent stack in each region, eliminating cross-region dependencies that could become points of failure.</p><p>Having both components (Lambda and API Gateway) in the same region reduces latency and improves performance.</p><p>Option A (creating only an API Gateway in us-west-2 that points to the Lambda function in us-east-1) would not provide true regional failover capability because the Lambda function would still be a single point of failure in us-east-1. If that region experienced an outage, the application would still fail despite having an API Gateway endpoint in another region.</p><p>Sources</p><p>Implementing multi-Region failover for Amazon API Gateway | AWS Compute Blog （https://aws.amazon.com/cn/blogs/compute/implementing-multi-region-failover-for-amazon-api-gateway/）</p><p>Does Route 53 latency-based routing perform failover when one region performance degrades? | AWS re:Post （https://repost.aws/questions/QUGaNY82V0QlCZyjpgRvFwIQ/does-route-53-latency-based-routing-perform-failover-when-one-region-performance-degrades）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "dd48837d95944956a3ae0eeccd78ba35",
            "questionNumber": 108,
            "type": "single",
            "content": "<p>Question #108</p><p>A retail company has structured its AWS accounts to be part of an organization in AWS Organizations. The company has set up consolidated billing and has mapped its departments to the following OUs: Finance, Sales, Human Resources (HR), Marketing, and Operations. Each OU has multiple AWS accounts, one for each environment within a department. These environments are development, test, pre-production, and production.<br><br>The HR department is releasing a new system that will launch in 3 months. In preparation, the HR department has purchased several Reserved Instances (RIs) in its production AWS account. The HR department wants to make sure that other departments cannot share the RI discounts.<br><br>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "In the AWS Billing and Cost Management console for the HR department&#39;s production account turn off RI sharing."
                },
                {
                    "label": "B",
                    "content": "Remove the HR department&#39;s production AWS account from the organization. Add the account to the consolidating billing configuration only."
                },
                {
                    "label": "C",
                    "content": "In the AWS Billing and Cost Management console. use the organization&rsquo;s management account to turn off RI Sharing for the HR departments production AWS account."
                },
                {
                    "label": "D",
                    "content": "Create an SCP in the organization to restrict access to the RIs. Apply the SCP to the OUs of the other departments."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Answer: &nbsp;</p><p>C. In the AWS Billing and Cost Management console, use the organization’s management account to turn off RI Sharing for the HR department’s production AWS account. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Restrict RI Discount Sharing: &nbsp;</p><p> &nbsp; - HR’s Reserved Instances (RIs) should only apply to their production account. &nbsp;</p><p> &nbsp; - Prevent other departments (Finance, Sales, etc.) from benefiting from the discounts. &nbsp;</p><p>2. AWS Organizations Context: &nbsp;</p><p> &nbsp; - The company uses consolidated billing with multiple OUs/departments. &nbsp;</p><p> &nbsp; - By default, RIs are shared across all accounts in an organization. &nbsp;</p><p> Why Option C is Correct? &nbsp;</p><p>- RI Sharing Settings: &nbsp;</p><p> &nbsp;- Only the management account of an AWS Organization can disable RI sharing. &nbsp;</p><p> &nbsp;- Navigate to: &nbsp;</p><p> &nbsp; &nbsp;AWS Billing Console → Preferences → Disable \"RI Discount Sharing\" for the HR production account. &nbsp;</p><p>- Effect: &nbsp;</p><p> &nbsp;- HR’s RIs will only discount their production account’s usage. &nbsp;</p><p> &nbsp;- Other departments’ accounts cannot use the discounts. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- A (HR Account Console): &nbsp;</p><p> &nbsp;- Individual accounts cannot modify RI sharing—only the management account can. &nbsp;</p><p>- B (Remove Account from Org): &nbsp;</p><p> &nbsp;- Overkill: Loses OU benefits (e.g., SCPs, centralized logging). &nbsp;</p><p> &nbsp;- Doesn’t block sharing: RIs are still account-specific unless sharing is disabled. &nbsp;</p><p>- D (SCP to Block RI Access): &nbsp;</p><p> &nbsp;- SCPs can’t control RI sharing—they only deny permissions, not billing behaviors. &nbsp;</p><p> How RI Sharing Works: &nbsp;</p><p>| Setting &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Default (Enabled) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Disabled (Option C) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp;</p><p>|----------------------------------|---------------------------------|-----------------------------------| &nbsp;</p><p>| RI Discount Scope &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| All linked accounts in the Org &nbsp;| Only the purchasing account &nbsp; &nbsp; &nbsp; | &nbsp;</p><p>| Management Required? &nbsp; &nbsp; &nbsp; &nbsp; | No &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Yes (management account only) &nbsp; &nbsp; | &nbsp;</p><p> Conclusion: &nbsp;</p><p>To ensure HR’s RIs aren’t shared, C is the only valid solution via the management account’s billing console. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2a7d86fe4c674954a057e676c82e9d3f",
            "questionNumber": 109,
            "type": "single",
            "content": "<p>Question #109</p><p>A large company is running a popular web application. The application runs on several Amazon EC2 Linux instances in an Auto Scaling group in a private subnet. An Application Load Balancer is targeting the instances in the Auto Scaling group in the private subnet. AWS Systems Manager Session Manager is confi gured, and AWS Systems Manager Agent is running on all the EC2 instances. </p><p><br></p><p>The company recently released a new version of the application. Some EC2 instances are now being marked as unhealthy and are being terminated. As a result, the application is running at reduced capacity. A solutions architect tries to determine the root cause by analyzing Amazon CloudWatch logs that are collected from the application, but the logs are inconclusive. </p><p><br></p><p>How should the solutions architect gain access to an EC2 instance to troubleshoot the issue?</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Suspend the Auto Scaling group&rsquo;s HealthCheck scaling process. Use Session Manager to log in to an instance that is marked as unhealthy. "
                },
                {
                    "label": "B",
                    "content": "Enable EC2 instance termination protection. Use Session Manager to log in to an instance that is marked as unhealthy"
                },
                {
                    "label": "C",
                    "content": "&nbsp;Set the termination policy to OldestInstance on the Auto Scaling group. Use Session Manager to log in to an instance that is marked an unhealthy."
                },
                {
                    "label": "D",
                    "content": "Suspend the Auto Scaling group&rsquo;s Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy. "
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Answer: &nbsp;</p><p>D. Suspend the Auto Scaling group’s Terminate process. Use Session Manager to log in to an instance that is marked as unhealthy. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Troubleshoot Unhealthy Instances: &nbsp;</p><p> &nbsp; - EC2 instances are being marked unhealthy and terminated before investigation. &nbsp;</p><p> &nbsp; - Need to preserve an unhealthy instance for debugging. &nbsp;</p><p>2. Access Method: &nbsp;</p><p> &nbsp; - Session Manager is already configured (SSH/RDP not required). &nbsp;</p><p>3. Minimal Disruption: &nbsp;</p><p> &nbsp; - Avoid affecting healthy instances or application availability. &nbsp;</p><p> Why Option D is Correct? &nbsp;</p><p>- Suspend Terminate Process: &nbsp;</p><p> &nbsp;- Temporarily stops Auto Scaling from terminating unhealthy instances. &nbsp;</p><p> &nbsp;- Allows time to: &nbsp;</p><p> &nbsp; &nbsp;1. Use Session Manager to access the instance. &nbsp;</p><p> &nbsp; &nbsp;2. Check logs/processes (`/var/log/`, `systemctl status`). &nbsp;</p><p> &nbsp; &nbsp;3. Identify root cause (e.g., failed health checks, app crashes). &nbsp;</p><p>- Safe and Reversible: &nbsp;</p><p> &nbsp;- Resume termination after debugging. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- A (Suspend HealthCheck): &nbsp;</p><p> &nbsp;- Doesn’t prevent termination—unhealthy instances will still be replaced. &nbsp;</p><p>- B (Termination Protection): &nbsp;</p><p> &nbsp;- EC2 termination protection doesn’t block Auto Scaling terminations. &nbsp;</p><p>- C (OldestInstance Policy): &nbsp;</p><p> &nbsp;- Doesn’t preserve unhealthy instances—only affects which instance is terminated first. &nbsp;</p><p> Steps to Implement: &nbsp;</p><p>1. Suspend Terminate Process: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws autoscaling suspend-processes --auto-scaling-group-name my-asg --scaling-processes Terminate</p><p> &nbsp; ``` &nbsp;</p><p>2. Access Instance via Session Manager: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ssm start-session --target i-1234567890abcdef0</p><p> &nbsp; ``` &nbsp;</p><p>3. Resume After Debugging: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws autoscaling resume-processes --auto-scaling-group-name my-asg --scaling-processes Terminate</p><p> &nbsp; ``` &nbsp;</p><p> Common Root Causes for Unhealthy Instances: &nbsp;</p><p>- Application crashes (check `journalctl -u &lt;service&gt;`). &nbsp;</p><p>- Failed ALB health checks (check `/var/log/nginx/error.log`). &nbsp;</p><p>- Resource exhaustion (`top`, `df -h`). &nbsp;</p><p> Conclusion: &nbsp;</p><p>D is the only option that lets you inspect unhealthy instances before termination, minimizing downtime. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "24ce1b4c908c4d1eb25474b123f0a3a1",
            "questionNumber": 110,
            "type": "single",
            "content": "<p>Question #110</p><p>A company wants to deploy an AWS WAF solution to manage AWS WAF rules across multiple AWS accounts. The accounts are managed under different OUs in AWS Organizations. </p><p><br></p><p>Administrators must be able to add or remove accounts or OUs from managed AWS WAF rule sets as needed. Administrators also must have the ability to automatically update and remediate noncompliant AWS WAF rules in all accounts.<br><br>Which solution meets these requirements with the LEAST amount of operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account."
                },
                {
                    "label": "B",
                    "content": "Deploy an organization-wide AWS Config rule that requires all resources in the selected OUs to associate the AWS WAF rules. Deploy automated remediation actions by using AWS Lambda to fix noncompliant resources. Deploy AWS WAF rules by using an AWS CloudFormation stack set to target the same OUs where the AWS Config rule is applied."
                },
                {
                    "label": "C",
                    "content": "Create AWS WAF rules in the management account of the organization. Use AWS Lambda environment variables to store account numbers and OUs to manage. Update environment variables as needed to add or remove accounts or OUs. Create cross-account IAM roles in member accounts. Assume the roles by using AWS Security Token Service (AWS STS) in the Lambda function to create and update AWS WAF rules in the member accounts."
                },
                {
                    "label": "D",
                    "content": "Use AWS Control Tower to manage AWS WAF rules across accounts in the organization. Use AWS Key Management Service (AWS KMS) to store account numbers and OUs to manage. Update AWS KMS as needed to add or remove accounts or OUs. Create IAM users in member accounts. Allow AWS Control Tower in the management account to use the access key and secret access key to create and update AWS WAF rules in the member accounts."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Answer: &nbsp;</p><p>A. Use AWS Firewall Manager to manage AWS WAF rules across accounts in the organization. Use an AWS Systems Manager Parameter Store parameter to store account numbers and OUs to manage. Update the parameter as needed to add or remove accounts or OUs. Use an Amazon EventBridge rule to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Centralized AWS WAF Management: &nbsp;</p><p> &nbsp; - Deploy and manage WAF rules across multiple accounts/OUs in AWS Organizations. &nbsp;</p><p>2. Dynamic Account/OU Management: &nbsp;</p><p> &nbsp; - Add or remove accounts/OUs from WAF rule sets without manual updates. &nbsp;</p><p>3. Automated Remediation: &nbsp;</p><p> &nbsp; - Detect and fix noncompliant WAF rules automatically. &nbsp;</p><p> Why Option A is Best? &nbsp;</p><p>- AWS Firewall Manager: &nbsp;</p><p> &nbsp;- Native AWS service for managing WAF rules across accounts in an organization. &nbsp;</p><p> &nbsp;- Supports automatic remediation of noncompliant resources. &nbsp;</p><p>- Systems Manager Parameter Store: &nbsp;</p><p> &nbsp;- Stores account/OU lists dynamically (easy to update). &nbsp;</p><p>- EventBridge + Lambda: &nbsp;</p><p> &nbsp;- Triggers policy updates when accounts/OUs are added/removed (low overhead). &nbsp;</p><p> Workflow: &nbsp;</p><p>1. Firewall Manager Admin Account: &nbsp;</p><p> &nbsp; - Defines WAF rule sets and target OUs/accounts. &nbsp;</p><p>2. Parameter Store: &nbsp;</p><p> &nbsp; - Maintains a list of managed accounts/OUs (e.g., `/firewall-manager/targets`). &nbsp;</p><p>3. EventBridge Rule: &nbsp;</p><p> &nbsp; - Monitors Parameter Store for changes → invokes Lambda to update Firewall Manager policies. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- B (AWS Config + CloudFormation Stack Sets): &nbsp;</p><p> &nbsp;- No centralized WAF management (manual stack updates required). &nbsp;</p><p> &nbsp;- AWS Config rules cannot enforce WAF associations. &nbsp;</p><p>- C (Lambda + Cross-Account Roles): &nbsp;</p><p> &nbsp;- High overhead: Custom code to manage WAF rules (no native automation). &nbsp;</p><p>- D (AWS Control Tower + KMS): &nbsp;</p><p> &nbsp;- Control Tower doesn’t manage WAF rules directly. &nbsp;</p><p> &nbsp;- KMS is not designed for account/OU tracking. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Enable Firewall Manager in the management account. &nbsp;</p><p>2. Create a Parameter Store parameter: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ssm put-parameter --name \"/firewall-manager/targets\" --type \"StringList\" --value \"ou-1234,ou-5678\"</p><p> &nbsp; ``` &nbsp;</p><p>3. Deploy EventBridge + Lambda: &nbsp;</p><p> &nbsp; - Lambda updates Firewall Manager policies when the parameter changes. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A provides the simplest, most scalable solution with native AWS services and minimal custom code. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d8ffa4fff19f46c58c5c3ed7cca1dadf",
            "questionNumber": 111,
            "type": "single",
            "content": "Question #111<p>A solutions architect is auditing the security setup of an AWS Lambda function for a company. The Lambda function retrieves the latest changes from an Amazon Aurora database. The Lambda function and the database run in the same VPC. Lambda environment variables are providing the database credentials to the Lambda function.<br><br></p><p>The Lambda function aggregates data and makes the data available in an Amazon S3 bucket that is confi gured for server-side encryption with AWS KMS managed encryption keys (SSE-KMS). The data must not travel across the Internet. If any database credentials become compromised, the company needs a solution that minimizes the impact of the compromise. </p><p><br>What should the solutions architect recommend to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Deploy a gateway VPC endpoint for Amazon S3 in the VPC."
                },
                {
                    "label": "B",
                    "content": "Enable IAM database authentication on the Aurora DB cluster. Change the IAM role for the Lambda function to allow the function to access the database by using IAM database authentication. Enforce HTTPS on the connection to Amazon S3 during data transfers."
                },
                {
                    "label": "C",
                    "content": "Save the database credentials in AWS Systems Manager Parameter Store. Set up password rotation on the credentials in Parameter Store. Change the IAM role for the Lambda function to allow the function to access Parameter Store. Modify the Lambda function to retrieve the credentials from Parameter Store.Modify the Lambda function to retrieve the credentials from Parameter Store. Deploy a gateway VPC endpoint for Amazon S3 in the VPC."
                },
                {
                    "label": "D",
                    "content": "Save the database credentials in AWS Secrets Manager. Set up password rotation on the credentials in Secrets Manager. Change the IAM role for the Lambda function to allow the function to access Secrets Manager. Modify the Lambda function to retrieve the credentials from Secrets Manager. Enforce HTTPS on the connection to Amazon S3 during data transfers."
                }
            ],
            "correctAnswer": "A",
            "explanation": "Option A is the correct answer as it leverages IAM database authentication for the Aurora DB cluster, which eliminates the need to store and manage database credentials in the Lambda environment variables. By deploying a gateway VPC endpoint for Amazon S3, data transfers to S3 are kept within the AWS network, reducing the risk of data exposure over the internet. This solution provides both security and compliance benefits by using AWS managed services and features.",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "df86fdab54d94753964ac010d974fd6e",
            "questionNumber": 112,
            "type": "single",
            "content": "<p>Question #112</p><p>A large mobile gaming company has successfully migrated all of its on-premises infrastructure to the AWS Cloud. A solutions architect is reviewing the environment to ensure that it was built according to the design and that it is running in alignment with the Well-Architected Framework.<br><br></p><p>While reviewing previous monthly costs in Cost Explorer, the solutions architect notices that the creation and subsequent termination of several &nbsp;large instance types account for a high proportion of the costs. The solutions architect finds out that the company’s developers are launching new Amazon EC2 instances as part of their testing and that the developers are not using the appropriate instance types. </p><p><br></p><p>The solutions architect must implement a control mechanism to limit the instance types that only the developers can launch. </p><p><br></p><p>Which solution will meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a desired-instance-type managed rule in AWS Config. Configure the rule with the instance types that are allowed. Attach the rule to an event to run each time a new EC2 instance is launched."
                },
                {
                    "label": "B",
                    "content": "In the EC2 console, create a launch template that specifies the instance types that are allowed. Assign the launch template to the developers&rsquo; IAM accounts."
                },
                {
                    "label": "C",
                    "content": "Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers."
                },
                {
                    "label": "D",
                    "content": "Use EC2 Image Builder to create an image pipeline for the developers and assist them in the creation of a golden image."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is the correct answer as it involves creating a custom IAM policy that restricts the instance types that developers can launch. By specifying allowed instance types in the policy and attaching it to an IAM group associated with the developers, the company can enforce cost optimization and prevent the use of non-compliant instance types. This approach provides direct control over resource usage and aligns with the principle of least privilege.</p><p>C. Create a new IAM policy. Specify the instance types that are allowed. Attach the policy to an IAM group that contains the IAM accounts for the developers. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Limit EC2 Instance Types: &nbsp;</p><p> &nbsp; - Restrict developers to specific instance types (e.g., t3.medium, t3.large). &nbsp;</p><p>2. Cost Control: &nbsp;</p><p> &nbsp; - Prevent use of large/expensive instances (e.g., m5.24xlarge). &nbsp;</p><p>3. Alignment with Well-Architected Framework: &nbsp;</p><p> &nbsp; - Cost Optimization: Enforce least-privilege resource provisioning. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- IAM Policy Granular Control: &nbsp;</p><p> &nbsp;- Create a policy that denies `ec2:RunInstances` for all instance types except approved ones: &nbsp;</p><p> &nbsp; &nbsp;```json</p><p> &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; &nbsp;\"Statement\": [</p><p> &nbsp; &nbsp; &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Effect\": \"Deny\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Action\": \"ec2:RunInstances\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Resource\": \"arn:aws:ec2:*:*:instance/*\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Condition\": {</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"StringNotEquals\": {</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"ec2:InstanceType\": [\"t3.medium\", \"t3.large\"]</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp;]</p><p> &nbsp; &nbsp;}</p><p> &nbsp; &nbsp;``` &nbsp;</p><p> &nbsp;- Attach to the developers’ IAM group (centralized management). &nbsp;</p><p>- Advantages: &nbsp;</p><p> &nbsp;- No manual oversight (automatically enforced). &nbsp;</p><p> &nbsp;- No infrastructure changes (unlike launch templates/Image Builder). &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>- A (AWS Config Rule): &nbsp;</p><p> &nbsp;- Reactive: Only detects noncompliant instances after creation (doesn’t prevent launch). &nbsp;</p><p>- B (Launch Template): &nbsp;</p><p> &nbsp;- Easily bypassed: Developers can ignore templates and launch instances directly. &nbsp;</p><p>- D (EC2 Image Builder): &nbsp;</p><p> &nbsp;- Doesn’t restrict instance types (only standardizes AMIs). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create IAM Policy: &nbsp;</p><p> &nbsp; - Use the example above, adjusting allowed `InstanceType` values. &nbsp;</p><p>2. Attach to Developers’ Group: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws iam attach-group-policy --group-name Developers --policy-arn arn:aws:iam::123456789012:policy/RestrictInstanceTypes</p><p> &nbsp; ``` &nbsp;</p><p>3. Test: &nbsp;</p><p> &nbsp; - Verify developers cannot launch unapproved instances. &nbsp;</p><p> Alternative (SCP for Organizations): &nbsp;</p><p>If using AWS Organizations, apply a Service Control Policy (SCP) at the OU level: &nbsp;</p><p>```json</p><p>{</p><p> &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp;\"Statement\": [</p><p> &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp;\"Effect\": \"Deny\",</p><p> &nbsp; &nbsp; &nbsp;\"Action\": \"ec2:RunInstances\",</p><p> &nbsp; &nbsp; &nbsp;\"Resource\": \"*\",</p><p> &nbsp; &nbsp; &nbsp;\"Condition\": {</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"StringNotEquals\": {</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"ec2:InstanceType\": [\"t3.medium\", \"t3.large\"]</p><p> &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp;}</p><p> &nbsp;]</p><p>}</p><p>``` &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1a46f7e517c54601aaab6443e39fb780",
            "questionNumber": 113,
            "type": "multiple",
            "content": "<p>Question #113</p><p>A company is developing and hosting several projects in the AWS Cloud. The projects are developed across multiple AWS accounts under the same organization in AWS Organizations. The company requires the cost for cloud infrastructure to be allocated to the owning project. The team responsible for all of the AWS accounts has discovered that several Amazon EC2 instances are lacking the Project tag used for cost allocation.</p><p><br></p><p>Which actions should a solutions architect lake to resolve the problem and prevent it from happening in the future? (Choose three.) </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Config rule in each account to find resources with missing tags."
                },
                {
                    "label": "B",
                    "content": "Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing."
                },
                {
                    "label": "C",
                    "content": "Use Amazon Inspector in the organization to find resources with missing tags."
                },
                {
                    "label": "D",
                    "content": "Create an IAM policy in each account with a deny action for ec2:RunInstances if the Project tag is missing."
                },
                {
                    "label": "E",
                    "content": "Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag."
                },
                {
                    "label": "F",
                    "content": "Use AWS Security Hub to aggregate a list of EC2 instances with the missing Project tag."
                }
            ],
            "correctAnswer": "ABE",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Create an AWS Config rule in each account to find resources with missing tags. </p><p>B. Create an SCP in the organization with a deny action for ec2:RunInstances if the Project tag is missing. </p><p>E. Create an AWS Config aggregator for the organization to collect a list of EC2 instances with the missing Project tag.</p><p>These three actions together provide a comprehensive solution to both resolve the current issue and prevent it from happening in the future:</p><p>AWS Config rules in each account (option A) will help identify existing EC2 instances that are missing the required Project tag. This addresses the immediate problem of finding resources that need to be tagged.</p><p>Creating an SCP with a deny action for ec2:RunInstances if the Project tag is missing (option B) is a preventive measure that ensures all new EC2 instances will have the required tag. This prevents the problem from recurring in the future by enforcing the tagging policy at the organization level.</p><p>Setting up an AWS Config aggregator (option E) provides centralized visibility across all accounts in the organization. This allows the team responsible for all AWS accounts to monitor compliance with tagging policies across the entire organization from a single location.</p><p>Together, these actions create a robust solution that addresses both the current missing tags and prevents future instances from being created without proper cost allocation tags.</p><p>Sources</p><p>Building a cost allocation strategy - Best Practices for Tagging AWS Resources （https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/building-a-cost-allocation-strategy.html）</p><p>Implementing and enforcing tagging - Best Practices for Tagging AWS Resources （https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/implementing-and-enforcing-tagging.html）</p><p>AWS Organizations – Tag policies - Best Practices for Tagging AWS Resources （https://docs.aws.amazon.com/whitepapers/latest/tagging-best-practices/aws-organizations-tag-policies.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "adc6fa9edeb74d49a378d0c80258f8dd",
            "questionNumber": 114,
            "type": "multiple",
            "content": "Question #114<p>A company has an on-premises monitoring solution using a PostgreSQL database for persistence of events. The database is unable to scale due to heavy ingestion and it frequently runs out of storage. </p><p>The company wants to create a hybrid solution and has already set up a VPN connection between its network and AWS. The solution should include the following attributes: </p><p>• Managed AWS services to minimize operational complexity. </p><p>• A buffer that automatically scales to match the throughput of data and requires no ongoing administration. </p><p>• A visualization tool to create dashboards to observe events in near-real time. </p><p>• Support for semi-structured JSON data and dynamic schemas. </p><p><br></p><p>Which combination of components will enable the company to create a monitoring solution that will satisfy these requirements? (Choose two.) </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Amazon Kinesis Data Firehose to buffer events. Create an AWS Lambda function to process and transform events."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon Kinesis data stream to buffer events. Create an AWS Lambda function to process and transform events."
                },
                {
                    "label": "C",
                    "content": "Configure an Amazon Aurora PostgreSQL DB cluster to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards."
                },
                {
                    "label": "D",
                    "content": "Configure Amazon Elasticsearch Service (Amazon ES) to receive events. Use the Kibana endpoint deployed with Amazon ES to create near-real-time visualizations and dashboards."
                },
                {
                    "label": "E",
                    "content": "Configure an Amazon Neptune DB instance to receive events. Use Amazon QuickSight to read from the database and create near-real-time visualizations and dashboards."
                }
            ],
            "correctAnswer": "AD",
            "explanation": "Options A and D are correct as they provide scalable and managed solutions for handling heavy data ingestion. Amazon Kinesis Data Firehose (Option A) can buffer and transport data to other services like Amazon S3, while Amazon Elasticsearch Service (Option D) provides a distributed search and analytics engine that can handle large volumes of data and offer near-real-time insights through Kibana visualizations.<p><br></p><p>还是建议选择AD</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "9ede583da6d0413cbffdcd85d433d7a3",
            "questionNumber": 115,
            "type": "single",
            "content": "Question #115<p>A team collects and routes behavioral data for an entire company. The company runs a Multi-AZ VPC environment with public subnets, private subnets, and in internet gateway. Each public subnet also contains a NAT gateway. Most of the company’s applications read from and write to Amazon Kinesis Data Streams. Most of the workloads run in private subnets. </p><p><br></p><p>A solutions architect must review the infrastructure. The solution architect needs to reduce costs and maintain the function of the applications. </p><p><br></p><p>The solutions architect uses Cost Explorer and notices that the cost in the EC2-Other category is consistently high. A further review shows that NatGateway-Bytes charges are increasing the cost in the EC2-Other category. </p><p><br></p><p>What should the solutions architect do to meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Enable VPC Flow Logs. Use Amazon Athena to analyze the logs for traffic that can be removed. Ensure that security groups are blocking traffic that is responsible for high costs."
                },
                {
                    "label": "B",
                    "content": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that applications have the correct IAM permissions to use the interface VPC endpoint."
                },
                {
                    "label": "C",
                    "content": "Enable VPC Flow Logs and Amazon Detective. Review Detective findings for traffic that is not related to Kinesis Data Streams. Configure security groups to block that traffic."
                },
                {
                    "label": "D",
                    "content": "Add an interface VPC endpoint for Kinesis Data Streams to the VPC. Ensure that the VPC endpoint policy allows traffic from the applications."
                }
            ],
            "correctAnswer": "D",
            "explanation": "Option D is the correct choice as it involves adding a VPC endpoint for Kinesis Data Streams, which allows applications to access the service without going through the internet gateway or NAT gateway, thus reducing costs associated with NAT gateway usage. The VPC endpoint policy ensures that only the necessary traffic from the applications is allowed, which helps maintain the function of the applications while optimizing for cost.",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a1f3a8decfe44c5eb9b931d0b6b6cba8",
            "questionNumber": 116,
            "type": "single",
            "content": "<p>Question #116</p><p>A retail company has an on-premises data center in Europe. The company also has a multi-Region AWS presence that includes the eu-west-1 and us-east-1 Regions. The company wants to be able to route network traffi c from its on-premises infrastructure into VPCs in either of those Regions. The company also needs to support traffi c that is routed directly between VPCs in those Regions. No single points of failure can exist on the network. </p><p><br></p><p>The company already has created two 1 Gbps AWS Direct Connect connections from its on-premises data center. Each connection goes into a separate Direct Connect location in Europe for high availability. These two locations are named DX-A and DX-B, respectively. Each Region has a single AWS Transit Gateway that is confi gured to route all inter-VPC traffi c within that Region. </p><p><br></p><p>Which solution will meet these requirements? </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a private VIF from the DX-A connection into a Direct Connect gateway. Create a private VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with the Direct Connect gateway.Peer the transit gateways with each other to support cross-Region routing.&nbsp;"
                },
                {
                    "label": "B",
                    "content": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Associate the eu-west-1 transit gateway with this Direct Connect gateway. Create a transit VIF from the DX-B connection into a separate Direct Connect gateway. Associate the us-east-1 transit gateway with this separate Direct Connect gateway. Peer the Direct Connect gateways with each other to support high availability and cross Region routing."
                },
                {
                    "label": "C",
                    "content": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway.Configure the Direct Connect gateway to route traffi c between the transit gateways.&nbsp;"
                },
                {
                    "label": "D",
                    "content": "Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Answer: &nbsp;</p><p>D. Create a transit VIF from the DX-A connection into a Direct Connect gateway. Create a transit VIF from the DX-B connection into the same Direct Connect gateway for high availability. Associate both the eu-west-1 and us-east-1 transit gateways with this Direct Connect gateway. Peer the transit gateways with each other to support cross-Region routing. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Hybrid Connectivity: &nbsp;</p><p> &nbsp; - On-premises (Europe) → AWS (eu-west-1 + us-east-1). &nbsp;</p><p>2. Inter-Region VPC Routing: &nbsp;</p><p> &nbsp; - Direct traffic between eu-west-1 and us-east-1 VPCs. &nbsp;</p><p>3. High Availability (No SPOF): &nbsp;</p><p> &nbsp; - Leverage two Direct Connect (DX) connections (DX-A + DX-B). &nbsp;</p><p> Why Option D is Correct? &nbsp;</p><p>- Transit VIFs + Direct Connect Gateway: &nbsp;</p><p> &nbsp;- Transit VIFs (from DX-A/DX-B) connect to a single Direct Connect gateway. &nbsp;</p><p> &nbsp;- Direct Connect gateway associates with both Transit Gateways (eu-west-1 + us-east-1). &nbsp;</p><p>- Transit Gateway Peering: &nbsp;</p><p> &nbsp;- Enables cross-region VPC routing (eu-west-1 ↔ us-east-1). &nbsp;</p><p>- High Availability: &nbsp;</p><p> &nbsp;- Two DX connections → no single point of failure. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. On-Premises → AWS: &nbsp;</p><p> &nbsp; - Traffic flows via DX-A/DX-B → Direct Connect gateway → Transit Gateways. &nbsp;</p><p>2. Inter-Region (eu-west-1 ↔ us-east-1): &nbsp;</p><p> &nbsp; - Transit Gateway peering routes traffic directly. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Private VIFs): &nbsp;</p><p> &nbsp;- Private VIFs only work with VPCs, not Transit Gateways. &nbsp;</p><p>- B (Separate Direct Connect Gateways): &nbsp;</p><p> &nbsp;- No cross-region routing: Direct Connect gateways cannot peer with each other. &nbsp;</p><p>- C (Direct Connect Gateway Routing): &nbsp;</p><p> &nbsp;- Direct Connect gateways don’t route between Transit Gateways (peering is required). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create Transit VIFs: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws directconnect create-transit-virtual-interface --connection-id dxcon-xxxx --vlan 100 --direct-connect-gateway-id dgw-xxxx</p><p> &nbsp; ``` &nbsp;</p><p>2. Associate Transit Gateways: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws directconnect associate-transit-gateway-with-direct-connect-gateway --direct-connect-gateway-id dgw-xxxx --transit-gateway-id tgw-xxxx</p><p> &nbsp; ``` &nbsp;</p><p>3. Peer Transit Gateways: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 create-transit-gateway-peering-attachment --transit-gateway-id tgw-eu-west-1 --peer-transit-gateway-id tgw-us-east-1</p><p> &nbsp; ``` &nbsp;</p><p> Conclusion: &nbsp;</p><p>D is the only solution that: &nbsp;</p><p>- Provides hybrid + inter-region routing. &nbsp;</p><p>- Ensures high availability (dual DX connections). &nbsp;</p><p>- Uses native AWS networking services. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2208c6edd2a44a6da2f75bb5355bd28c",
            "questionNumber": 117,
            "type": "multiple",
            "content": "<p>Question #117</p><p>A company is running an application in the AWS Cloud. The company's security team must approve the creation of all new IAM users. When a new IAM user is created, all access for the user must be removed automatically. The security team must then receive a notification to approve the user.<br><br>Which combination of steps will meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser."
                },
                {
                    "label": "B",
                    "content": "Configure CloudTrail to send a notification for the CreateUser event to an Amazon Simple Notification Service (Amazon SNS) topic."
                },
                {
                    "label": "C",
                    "content": "Invoke a container that runs in Amazon Elastic Container Service (Amazon ECS) with AWS Fargate technology to remove access."
                },
                {
                    "label": "D",
                    "content": "Invoke an AWS Step Functions state machine to remove access."
                },
                {
                    "label": "E",
                    "content": "Use Amazon Simple Notification Service (Amazon SNS) to notify the security team."
                },
                {
                    "label": "F",
                    "content": "Use Amazon Pinpoint to notify the security team."
                }
            ],
            "correctAnswer": "ADE",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Create an Amazon EventBridge (Amazon CloudWatch Events) rule. Define a pattern with the detail-type value set to AWS API Call via CloudTrail and an eventName of CreateUser.</p><p>D. Invoke an AWS Step Functions state machine to remove access.</p><p>E. Use Amazon Simple Notification Service (Amazon SNS) to notify the security team.</p><p>These three options together create an automated workflow that meets all the requirements:</p><p>Option A establishes the detection mechanism. By creating an EventBridge rule that monitors CloudTrail for CreateUser events, the system can automatically detect whenever a new IAM user is created in the AWS account.</p><p>Option D provides the automation to remove access. When the EventBridge rule detects a new user creation, it invokes a Step Functions state machine that can execute the necessary steps to remove all access for the newly created user.</p><p>Option E handles the notification requirement. Amazon SNS is the appropriate service for sending notifications to the security team about the new user creation, allowing them to review and approve the user as needed.</p><p>Option F (using Amazon Pinpoint) is not the best choice for this scenario. While Pinpoint is a good service for customer engagement and marketing communications, Amazon SNS is more appropriate for operational notifications to internal teams like the security team in this case.</p><p>This combination creates an effective automated workflow that detects new IAM user creations, removes access, and notifies the security team for approval, meeting all the stated requirements.</p><p>Sources</p><p>Send a notification when an IAM user is created - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/send-a-notification-when-an-iam-user-is-created.html）</p><p>Need some help with my setting(EventBridge) | AWS re:Post （https://repost.aws/questions/QUfeVDFmhfTpSWLNQHGUylmg/need-some-help-with-my-setting-eventbridge）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3e6d9b6b49c34c93b2e4f641f012fcaf",
            "questionNumber": 118,
            "type": "multiple",
            "content": "<p>Question #118</p><p>A company wants to migrate to AWS. The company wants to use a multi-account structure with centrally managed access to all accounts and applications. The company also wants to keep the traffic on a private network. Multi-factor authentication (MFA) is required at login, and specific roles are assigned to user groups.<br><br></p><p>The company must create separate accounts for development. staging, production, and shared network. The production account and the shared network account must have connectivity to all accounts. The development account and the staging account must have access only to each other. </p><p><br></p><p>Which combination of steps should a solutions architect take 10 meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations."
                },
                {
                    "label": "B",
                    "content": "Enable AWS Security Hub in all accounts to manage cross-account access. Collect findings through AWS CloudTrail to force MFA login."
                },
                {
                    "label": "C",
                    "content": "Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables."
                },
                {
                    "label": "D",
                    "content": "Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts."
                },
                {
                    "label": "E",
                    "content": "Enable AWS Control Tower in all accounts to manage routing between accounts. Collect findings through AWS CloudTrail to force MFA login."
                },
                {
                    "label": "F",
                    "content": "Create IAM users and groups. Configure MFA for all users. Set up Amazon Cognito user pools and Identity pools to manage access to accounts and between accounts."
                }
            ],
            "correctAnswer": "ACD",
            "explanation": "<p>Answer: &nbsp;</p><p>A. Deploy a landing zone environment by using AWS Control Tower. Enroll accounts and invite existing accounts into the resulting organization in AWS Organizations. &nbsp;</p><p>C. Create transit gateways and transit gateway VPC attachments in each account. Configure appropriate route tables. &nbsp;</p><p>D. Set up and enable AWS IAM Identity Center (AWS Single Sign-On). Create appropriate permission sets with required MFA for existing accounts. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Multi-Account Structure: &nbsp;</p><p> &nbsp; - Separate accounts for development, staging, production, shared network. &nbsp;</p><p>2. Centralized Access & MFA: &nbsp;</p><p> &nbsp; - Single sign-on (SSO) with MFA enforcement. &nbsp;</p><p>3. Private Network Connectivity: &nbsp;</p><p> &nbsp; - Production/shared network accounts connect to all accounts. &nbsp;</p><p> &nbsp; - Development/staging accounts connect only to each other. &nbsp;</p><p> Why A + C + D? &nbsp;</p><p>- A (AWS Control Tower Landing Zone): &nbsp;</p><p> &nbsp;- Automates multi-account setup with best practices (e.g., guardrails, logging). &nbsp;</p><p> &nbsp;- Enrolls accounts into AWS Organizations for central management. &nbsp;</p><p>- C (Transit Gateway Routing): &nbsp;</p><p> &nbsp;- Private connectivity between VPCs across accounts: &nbsp;</p><p> &nbsp; &nbsp;- Transit Gateway in each account + VPC attachments. &nbsp;</p><p> &nbsp; &nbsp;- Route tables control traffic flow (e.g., block dev ↔ prod). &nbsp;</p><p>- D (IAM Identity Center): &nbsp;</p><p> &nbsp;- Centralized SSO with MFA enforcement. &nbsp;</p><p> &nbsp;- Permission sets define role-based access (e.g., \"Dev-ReadOnly\"). &nbsp;</p><p> Workflow: &nbsp;</p><p>1. Deploy Control Tower (A): &nbsp;</p><p> &nbsp; - Creates OU structure (e.g., `Dev`, `Staging`, `Prod`, `Shared`). &nbsp;</p><p> &nbsp; - Applies guardrails (e.g., \"MFA required\"). &nbsp;</p><p>2. Configure Transit Gateway (C): &nbsp;</p><p> &nbsp; - Shared Network TGW peers with all accounts. &nbsp;</p><p> &nbsp; - Dev/Staging TGWs peer only with each other. &nbsp;</p><p>3. Set Up IAM Identity Center (D): &nbsp;</p><p> &nbsp; - Define permission sets (e.g., `Prod-Admin`, `Dev-ReadOnly`). &nbsp;</p><p> &nbsp; - Assign users/groups to accounts via SSO. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Security Hub + CloudTrail): &nbsp;</p><p> &nbsp;- Security Hub doesn’t manage access (only detects misconfigurations). &nbsp;</p><p> &nbsp;- CloudTrail doesn’t enforce MFA (IAM Identity Center does). &nbsp;</p><p>- E (Control Tower for Routing): &nbsp;</p><p> &nbsp;- Control Tower doesn’t manage network routing (Transit Gateway does). &nbsp;</p><p>- F (IAM Users + Cognito): &nbsp;</p><p> &nbsp;- Manual overhead: IAM users don’t scale for multi-account. &nbsp;</p><p> &nbsp;- Cognito is for app auth, not AWS account access. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Deploy Control Tower: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws controltower create-landing-zone --region us-east-1 --manifest s3://my-bucket/control-tower-config.yaml</p><p> &nbsp; ``` &nbsp;</p><p>2. Create Transit Gateways: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 create-transit-gateway --description \"Shared-Network-TGW\"</p><p> &nbsp; ``` &nbsp;</p><p>3. Configure IAM Identity Center: &nbsp;</p><p> &nbsp; - Enable MFA and assign permission sets in the AWS SSO console. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A + C + D is the only combination that: &nbsp;</p><p>- Automates account setup (Control Tower). &nbsp;</p><p>- Enforces private routing (Transit Gateway). &nbsp;</p><p>- Centralizes access (IAM Identity Center + MFA). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b4c5fd31ce6a48e3af59209948de89e9",
            "questionNumber": 119,
            "type": "single",
            "content": "Question #119<p>A company runs its application in the eu-west-1 Region and has one account for each of its environments: development, testing, and production. All the environments are running 24 hours a day, 7 days a week by using stateful Amazon EC2 instances and Amazon RDS for MySQL databases. The databases are between 500 GB and 800 GB in size. </p><p><br></p><p>The development team and testing team work on business days during business hours, but the production environment operates 24 hours a day, 7 days a week. The company wants to reduce costs. All resources are tagged with an environment tag with either development, testing, or production as the key. </p><p><br></p><p>What should a solutions architect do to reduce costs with the LEAST operational effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon EventBridge rule that runs once every day. Configure the rule to invoke one AWS Lambda function that starts or stops instances based on the tag, day, and time."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon EventBridge rule that runs every business day in the evening. Configure the rule to invoke an AWS Lambda function that stops instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule to invoke another Lambda function that starts instances based on the tag."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon EventBridge rule that runs every business day in the evening, Configure the rule to invoke an AWS Lambda function that terminates instances based on the tag. Create a second EventBridge rule that runs every business day in the morning. Configure the second rule to invoke another Lambda function that restores the instances from their last backup based on the tag."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon EventBridge rule that runs every hour. Configure the rule to invoke one AWS Lambda function that terminates or restores instances from their last backup based on the tag, day, and time."
                }
            ],
            "correctAnswer": "B",
            "explanation": "The correct answer is B. Creating an Amazon EventBridge rule that runs every business day in the evening to stop instances and another rule that runs every business day in the morning to start instances based on the tag will reduce costs with the least operational effort. This approach allows for instances to be stopped during non-business hours when they are not in use, reducing the costs associated with running them. It also allows for instances to be started again in the morning when the development and testing teams need to use them.",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d4ceccbb458c4975a8f3a656093f4bb0",
            "questionNumber": 120,
            "type": "single",
            "content": "<p>Question #120</p><p>A company is building a software-as-a-service (SaaS) solution on AWS. The company has deployed an Amazon API Gateway REST API with AWS Lambda integration in multiple AWS Regions and in the same production account. </p><p><br></p><p>The company offers tiered pricing that gives customers the ability to pay for the capacity to make a certain number of API calls per second. The premium tier offers up to 3,000 calls per second, and customers are identified by a unique API key. Several premium tier customers in various Regions report that they receive error responses of 429 Too Many Requests from multiple API methods during peak usage hours. Logs indicate that the Lambda function is never invoked. </p><p><br></p><p>What could be the cause of the error messages for these customers?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "The Lambda function reached its concurrency limit."
                },
                {
                    "label": "B",
                    "content": "The Lambda function its Region limit for concurrency."
                },
                {
                    "label": "C",
                    "content": "The company reached its API Gateway account limit for calls per second."
                },
                {
                    "label": "D",
                    "content": "The company reached its API Gateway default per-method limit for calls per second."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. The company reached its API Gateway account limit for calls per second.</p><p>This is the most likely cause of the error messages because:</p><p>The error response \"429 Too Many Requests\" is occurring across multiple API methods during peak usage hours, suggesting a broader throttling issue rather than a method-specific one.</p><p>The Lambda function is never invoked, which indicates that the requests are being throttled at the API Gateway level before they even reach the Lambda integration.</p><p>API Gateway has default account-level throttling limits that apply across all APIs in an account within a specific Region. These account limits are separate from and take precedence over any method-level limits.</p><p>Since the company has premium tier customers allowed to make up to 3,000 calls per second, and these customers are distributed across various Regions, the combined traffic during peak hours is likely exceeding the default account-level throttling limits in those Regions.</p><p>Option D (reaching the default per-method limit) is less likely to be the cause because:</p><p>If it were a per-method limit issue, we would expect to see the problem only on specific methods, not across multiple methods.</p><p>The company has likely configured higher method-level limits for premium tier customers, but these custom limits would still be constrained by the overall account-level limits.</p><p>To resolve this issue, the company should request an increase to their API Gateway account-level throttling limits in the affected Regions through AWS Support.</p><p>Sources</p><p>REL05-BP02 Throttle requests - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/latest/framework/rel_mitigate_interaction_failure_throttle_requests.html）</p><p>Throttle requests to your REST APIs for better throughput in API Gateway - Amazon API Gateway （https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-throttling.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "db6d9833214540ba8b2931c1778b6541",
            "questionNumber": 121,
            "type": "multiple",
            "content": "<p>Question #121</p><p>A financial company is planning to migrate its web application from on premises to AWS. The company uses a third-party security tool to monitor the inbound traffic to the application. The company has used the security tool for the last 15 years, and the tool has no cloud solutions available from its vendor. </p><p><br></p><p>The company's security team is concerned about how to integrate the security tool with AWS technology. </p><p>The company plans to deploy the application migration to AWS on Amazon EC2 instances. </p><p>The EC2 instances will run in an Auto Scaling group in a dedicated VPC. </p><p>The company needs to use the security tool to inspect all packets that come in and out of the VPC. </p><p><br></p><p>This inspection must occur in real time and must not affect the application's performance. A solutions architect must design a target architecture on AWS that is highly available within an AWS Region. </p><p><br></p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC"
                },
                {
                    "label": "B",
                    "content": "Deploy the web application behind a Network Load Balancer"
                },
                {
                    "label": "C",
                    "content": "Deploy an Application Load Balancer in front of the security tool instances"
                },
                {
                    "label": "D",
                    "content": "Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool"
                },
                {
                    "label": "E",
                    "content": "Provision a transit gateway to facilitate communication between VPCs."
                }
            ],
            "correctAnswer": "AD",
            "explanation": "<p><br></p><p> Answer: &nbsp;</p><p>A. Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC. &nbsp;</p><p>D. Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Real-Time Traffic Inspection: &nbsp;</p><p> &nbsp; - All inbound/outbound VPC traffic must be inspected by the legacy security tool. &nbsp;</p><p>2. High Availability: &nbsp;</p><p> &nbsp; - Solution must be resilient within a single AWS Region. &nbsp;</p><p>3. No Performance Impact: &nbsp;</p><p> &nbsp; - Inspection must not degrade application performance. &nbsp;</p><p>4. Legacy Tool Integration: &nbsp;</p><p> &nbsp; - The security tool has no cloud-native version (must run on EC2). &nbsp;</p><p> Why A + D? &nbsp;</p><p>- Gateway Load Balancer (D): &nbsp;</p><p> &nbsp;- Traffic Mirroring: Redirects all VPC traffic to security tool instances. &nbsp;</p><p> &nbsp;- High Availability: Deploy one per AZ (ensures redundancy). &nbsp;</p><p> &nbsp;- No Performance Hit: Operates at line speed (unlike inline proxies). &nbsp;</p><p>- Security Tool on EC2 (A): &nbsp;</p><p> &nbsp;- Auto Scaling group ensures tool availability. &nbsp;</p><p> &nbsp;- Same VPC avoids cross-VPC latency. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. Inbound Traffic: &nbsp;</p><p> &nbsp; - Internet → Gateway Load Balancer → Security Tool EC2 Instances → Web Application EC2. &nbsp;</p><p>2. Outbound Traffic: &nbsp;</p><p> &nbsp; - Web Application EC2 → Gateway Load Balancer → Security Tool EC2 Instances → Internet. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Network Load Balancer): &nbsp;</p><p> &nbsp;- Doesn’t enable traffic inspection (passes traffic directly to apps). &nbsp;</p><p>- C (Application Load Balancer): &nbsp;</p><p> &nbsp;- L7 only: Can’t inspect raw packets (security tool needs L3/L4). &nbsp;</p><p>- E (Transit Gateway): &nbsp;</p><p> &nbsp;- Irrelevant: No multi-VPC requirement. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Deploy Gateway Load Balancer: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 create-load-balancer --type gateway --name my-gwlb --subnets subnet-123456 subnet-789012</p><p> &nbsp; ``` &nbsp;</p><p>2. Register Security Tool Targets: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 register-targets --target-group-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:targetgroup/my-gwlb-tg/1234567890123456 --targets Id=i-1234567890abcdef0</p><p> &nbsp; ``` &nbsp;</p><p>3. Update VPC Route Tables: &nbsp;</p><p> &nbsp; - Route 0.0.0.0/0 to the Gateway Load Balancer. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A + D is the only combination that: &nbsp;</p><p>- Integrates the legacy tool (EC2-based). &nbsp;</p><p>- Inspects all traffic (Gateway LB). &nbsp;</p><p>- Ensures high availability. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3db5edd6beeb4d8681b2f1a9ce812744",
            "questionNumber": 122,
            "type": "single",
            "content": "<p>Question #122</p><p>A company has purchased appliances from different vendors. The appliances all have IoT sensors. The sensors send status information in the vendors' proprietary formats to a legacy application that parses the information into JSON. The parsing is simple, but each vendor has a unique format. Once daily, the application parses all the JSON records and stores the records in a relational database for analysis. </p><p><br></p><p>The company needs to design a new data analysis solution that can deliver faster and optimize costs. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon S3. Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis."
                },
                {
                    "label": "B",
                    "content": "Migrate the application server to AWS Fargate, which will receive the information from IoT sensors and parse the information into a relational format. Save the parsed information to Amazon Redshift for analysis."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Transfer for SFTP server. Update the IoT sensor code to send the information as a .csv file through SFTP to the server. Use AWS Glue to catalog the files. Use Amazon Athena for analysis."
                },
                {
                    "label": "D",
                    "content": "Use AWS Snowball Edge to collect data from the IoT sensors directly to perform local analysis. Periodically collect the data into Amazon Redshift to perform global analysis."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Answer: &nbsp;</p><p>A. Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon S3. Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Real-Time Data Ingestion: &nbsp;</p><p> &nbsp; - IoT sensors send data in proprietary formats (need parsing). &nbsp;</p><p>2. Cost Optimization & Speed: &nbsp;</p><p> &nbsp; - Replace the legacy relational database with a serverless, scalable solution. &nbsp;</p><p>3. Data Analysis: &nbsp;</p><p> &nbsp; - Support ad-hoc queries and visualizations. &nbsp;</p><p> Why Option A is Best? &nbsp;</p><p>- AWS IoT Core: &nbsp;</p><p> &nbsp;- Centralized ingestion for all IoT devices (handles vendor diversity). &nbsp;</p><p> &nbsp;- Rules Engine routes data to Lambda for parsing. &nbsp;</p><p>- Lambda Parsing: &nbsp;</p><p> &nbsp;- Converts proprietary formats → standardized JSON/CSV. &nbsp;</p><p> &nbsp;- Serverless: Scales with sensor volume (no EC2 costs). &nbsp;</p><p>- S3 + Glue + Athena: &nbsp;</p><p> &nbsp;- S3: Low-cost storage for parsed data. &nbsp;</p><p> &nbsp;- Glue Crawler: Auto-discovers schema (no manual ETL). &nbsp;</p><p> &nbsp;- Athena: Serverless SQL queries (pay per query). &nbsp;</p><p> &nbsp;- QuickSight: Visualizations with minimal setup. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. IoT Sensors → AWS IoT Core (ingest raw data). &nbsp;</p><p>2. IoT Rule → Lambda (parse to CSV). &nbsp;</p><p>3. Lambda → S3 (store parsed data). &nbsp;</p><p>4. Glue Catalog → Athena/QuickSight (analyze). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Fargate + Redshift): &nbsp;</p><p> &nbsp;- Overkill: Redshift is expensive for sporadic analysis. &nbsp;</p><p> &nbsp;- No real-time parsing (daily batch remains). &nbsp;</p><p>- C (SFTP Server): &nbsp;</p><p> &nbsp;- Manual effort: Requires sensor code changes (to CSV). &nbsp;</p><p> &nbsp;- No real-time processing. &nbsp;</p><p>- D (Snowball Edge): &nbsp;</p><p> &nbsp;- Irrelevant: Sensors are connected; no need for physical collection. &nbsp;</p><p> Cost & Performance Benefits: &nbsp;</p><p>| Component &nbsp; &nbsp; &nbsp; | Advantage &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;</p><p>|----------------|-------------------------------------------| &nbsp;</p><p>| AWS IoT Core | No servers to manage; scales with devices. | &nbsp;</p><p>| Lambda &nbsp; &nbsp; &nbsp;| Pay per parse (~$0.20 per million requests). | &nbsp;</p><p>| S3/Athena &nbsp; | ~$5/TB scanned (vs. Redshift’s ~$1,000/TB/year). | &nbsp;</p><p> Conclusion: &nbsp;</p><p>A delivers real-time parsing, serverless scalability, and cost-efficient analysis—perfect for IoT sensor data. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "9841d7ec09e24d3a9c5fa191fa9d9d4a",
            "questionNumber": 123,
            "type": "multiple",
            "content": "<p>Question #123</p><p>A company is migrating some of its applications to AWS. The company wants to migrate and modernize the applications quickly after it finalizes networking and security strategies. The company has set up an AWS Direct Connect connection in a central network account. <br><br>The company expects to have hundreds of AWS accounts and VPCs in the near future. The corporate network must be able to access the resources on AWS seamlessly and also must be able to communicate with all the VPCs. The company also wants to route its cloud resources to the internet through its on-premises data center. <br><br>Which combination of steps will meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a Direct Connect gateway in the central account. In each of the accounts, create an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway."
                },
                {
                    "label": "B",
                    "content": "Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF."
                },
                {
                    "label": "C",
                    "content": "Provision an internet gateway. Attach the internet gateway to subnets. Allow internet traffic through the gateway."
                },
                {
                    "label": "D",
                    "content": "Share the transit gateway with other accounts. Attach VPCs to the transit gateway."
                },
                {
                    "label": "E",
                    "content": "Provision VPC peering as necessary."
                },
                {
                    "label": "F",
                    "content": "Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center."
                }
            ],
            "correctAnswer": "BDF",
            "explanation": "<p>Answer: &nbsp;</p><p>B. Create a Direct Connect gateway and a transit gateway in the central network account. Attach the transit gateway to the Direct Connect gateway by using a transit VIF. &nbsp;</p><p>D. Share the transit gateway with other accounts. Attach VPCs to the transit gateway. &nbsp;</p><p>F. Provision only private subnets. Open the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Hybrid Connectivity: &nbsp;</p><p> &nbsp; - Direct Connect (DX) for on-premises ↔ AWS (central network account). &nbsp;</p><p> &nbsp; - Seamless access across hundreds of accounts/VPCs. &nbsp;</p><p>2. Internet Routing via On-Premises: &nbsp;</p><p> &nbsp; - Cloud resources must egress to the internet through the data center. &nbsp;</p><p>3. Scalability: &nbsp;</p><p> &nbsp; - Avoid VPC peering (doesn’t scale to hundreds of VPCs). &nbsp;</p><p> Why B + D + F? &nbsp;</p><p>- B (Direct Connect Gateway + Transit Gateway): &nbsp;</p><p> &nbsp;- Transit VIF connects DX to the Transit Gateway (central routing hub). &nbsp;</p><p> &nbsp;- Direct Connect Gateway enables cross-account DX access. &nbsp;</p><p>- D (Transit Gateway Sharing): &nbsp;</p><p> &nbsp;- Share Transit Gateway with other accounts via AWS Resource Access Manager (RAM). &nbsp;</p><p> &nbsp;- Attach VPCs to the Transit Gateway (replaces VPC peering). &nbsp;</p><p>- F (Private Subnets + NAT via On-Premises): &nbsp;</p><p> &nbsp;- No public subnets: Forces all internet traffic through on-premises NAT. &nbsp;</p><p> &nbsp;- Route tables direct `0.0.0.0/0` to the Transit Gateway → DX → on-premises. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. On-Premises → Direct Connect → Transit Gateway → VPCs (all accounts). &nbsp;</p><p>2. VPCs → Transit Gateway → Direct Connect → On-Premises NAT → Internet. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Direct Connect Gateway Associations): &nbsp;</p><p> &nbsp;- Deprecated: Virtual private gateways are legacy (use Transit Gateway). &nbsp;</p><p>- C (Internet Gateway): &nbsp;</p><p> &nbsp;- Violates requirement (must use on-premises egress). &nbsp;</p><p>- E (VPC Peering): &nbsp;</p><p> &nbsp;- Doesn’t scale to hundreds of VPCs (n² complexity). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create Transit Gateway (Central Account): &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ec2 create-transit-gateway --description \"Central-TGW\"</p><p> &nbsp; ``` &nbsp;</p><p>2. Share Transit Gateway via RAM: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws ram create-resource-share --name TGW-Share --resource-arns arn:aws:ec2:us-east-1:123456789012:transit-gateway/tgw-1234567890abcdef0</p><p> &nbsp; ``` &nbsp;</p><p>3. Configure On-Premises NAT: &nbsp;</p><p> &nbsp; - Add a route for `0.0.0.0/0` to the customer gateway. &nbsp;</p><p> Conclusion: &nbsp;</p><p>B + D + F is the only combination that: &nbsp;</p><p>- Scales to hundreds of accounts/VPCs (Transit Gateway). &nbsp;</p><p>- Enforces on-premises internet egress (private subnets + NAT). &nbsp;</p><p>- Minimizes operational overhead (centralized networking). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1c3703b5d93b44b1af4109514d0bc7ea",
            "questionNumber": 124,
            "type": "multiple",
            "content": "<p>Question #124</p><p>A company has hundreds of AWS accounts. The company recently implemented a centralized internal process for purchasing new Reserved Instances and modifying existing Reserved Instances. This process requires all business units that want to purchase or modify Reserved Instances to submit requests to a dedicated team for procurement. Previously, business units directly purchased or modified Reserved Instances in their own respective AWS accounts autonomously. </p><p><br></p><p>A solutions architect needs to enforce the new process in the most secure way possible. Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled."
                },
                {
                    "label": "B",
                    "content": "Use AWS Config to report on the attachment of an IAM policy that denies access to the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action."
                },
                {
                    "label": "C",
                    "content": "In each AWS account, create an IAM policy that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action."
                },
                {
                    "label": "D",
                    "content": "Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization."
                },
                {
                    "label": "E",
                    "content": "Ensure that all AWS accounts are part of an organization in AWS Organizations that uses the consolidated billing feature."
                }
            ],
            "correctAnswer": "AD",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Ensure that all AWS accounts are part of an organization in AWS Organizations with all features enabled.</p><p>D. Create an SCP that denies the ec2:PurchaseReservedInstancesOffering action and the ec2:ModifyReservedInstances action. Attach the SCP to each OU of the organization.</p><p>These two steps together provide the most secure way to enforce the new centralized process for Reserved Instance management:</p><p>AWS Organizations with all features enabled (option A) is essential because:</p><p>It provides the foundation for centralized management across hundreds of AWS accounts</p><p>It enables the use of Service Control Policies (SCPs), which are necessary for enforcing permissions at scale</p><p>It supports consolidated billing, which is important for Reserved Instance sharing and management</p><p>Creating an SCP that denies the specific Reserved Instance actions (option D) is the most effective enforcement mechanism because:</p><p>It prevents users in individual accounts from purchasing or modifying Reserved Instances directly</p><p>SCPs are preventative controls that cannot be overridden by account administrators</p><p>By attaching the SCP to each OU, the policy applies consistently across all accounts in the organization</p><p>This approach follows the principle of least privilege by specifically denying only the actions that need to be restricted</p><p>Together, these steps ensure that only the dedicated procurement team has the ability to purchase and modify Reserved Instances, effectively enforcing the new centralized process across the entire organization in the most secure way possible.</p><p>Sources</p><p>Using AWS Organizations for security - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/security-reference-architecture/organizations-security.html）</p><p>Managing access to AWS accounts from Microsoft Teams and Slack at scale using AWS Organizations and AWS Chatbot | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/managing-access-to-aws-accounts-from-microsoft-teams-and-slack-at-scale-using-aws-organizations-and-aws-chatbot/）</p><p>AWS account management and separation - Security Pillar （https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/aws-account-management-and-separation.html）</p><p>Protect your AWS resources from unauthorized access using AWS Organizations integrations | AWS Cloud Operations Blog （https://aws.amazon.com/cn/blogs/mt/protect-your-aws-resources-from-unauthorized-access-using-aws-organizations-integrations/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c255b7ba75454027adbdb1f3cd844618",
            "questionNumber": 125,
            "type": "multiple",
            "content": "<p>Question #125</p><p>A company is running a critical application that uses an Amazon RDS for MySQL database to store data. The RDS DB instance is deployed in Multi-AZ mode. <br><br>A recent RDS database failover test caused a 40-second outage to the application. A solutions architect needs to design a solution to reduce the outage time to less than 20 seconds. <br><br>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Amazon ElastiCache for Memcached in front of the database"
                },
                {
                    "label": "B",
                    "content": "Use Amazon ElastiCache for Redis in front of the database"
                },
                {
                    "label": "C",
                    "content": "Use RDS Proxy in front of the database."
                },
                {
                    "label": "D",
                    "content": "Migrate the database to Amazon Aurora MySQL."
                },
                {
                    "label": "E",
                    "content": "Create an Amazon Aurora Replica."
                },
                {
                    "label": "F",
                    "content": "Create an RDS for MySQL read replica"
                }
            ],
            "correctAnswer": "CDE",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>C. Use RDS Proxy in front of the database. </p><p>D. Migrate the database to Amazon Aurora MySQL. </p><p>E. Create an Amazon Aurora Replica.</p><p>These three steps together provide the most effective solution to reduce the database failover outage time to less than 20 seconds:</p><p>Using RDS Proxy (option C) is crucial because:</p><p>It significantly reduces failover times by maintaining a connection pool and preserving application connections during database failovers</p><p>It eliminates the DNS propagation delay that typically occurs during failovers</p><p>RDS Proxy can reduce failover times to as low as 3-5 seconds in many cases</p><p>Migrating to Amazon Aurora MySQL (option D) is important because:</p><p>Aurora has a different architecture than standard RDS for MySQL that's specifically designed for faster failovers</p><p>Aurora's storage layer is independent of the database instances, allowing for quicker recovery</p><p>Aurora typically achieves failover times under 30 seconds, often as low as 10-20 seconds</p><p>Creating an Amazon Aurora Replica (option E) further improves the solution because:</p><p>Having Aurora Replicas in different Availability Zones provides additional failover targets</p><p>Aurora can quickly promote a replica to the primary role, reducing downtime</p><p>Multiple replicas improve the overall availability and resilience of the database system</p><p>Options A and B (using ElastiCache for Memcached or Redis) would improve application performance by caching frequently accessed data but wouldn't directly address the database failover time issue.</p><p>Option F (creating an RDS for MySQL read replica) wouldn't significantly impact the failover time for the primary instance in a Multi-AZ setup.</p><p>The combination of RDS Proxy, Aurora MySQL, and Aurora Replicas provides a comprehensive solution that addresses the core requirement of reducing failover outage time to less than 20 seconds.</p><p>Sources</p><p>Using RDS Proxy with Amazon RDS Multi-AZ DB instance deployment to improve planned failover time | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/using-rds-proxy-with-amazon-rds-multi-az-db-instance-deployment-to-improve-planned-failover-time/）</p><p>Amazon Aurora for Core Banking Systems | AWS for Industries （https://aws.amazon.com/cn/blogs/industries/amazon-aurora-for-core-banking-systems/）</p><p>Targeted business outcomes - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/amazon-rds-proxy/targeted-business-outcomes.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "864a1f6a463b469ebbed46ea66fd582b",
            "questionNumber": 126,
            "type": "single",
            "content": "<p>Question #126</p><p>An AWS partner company is building a service in AWS Organizations using its organization named org1. This service requires the partner company to have access to AWS resources in a customer account, which is in a separate organization named org2. The company must establish least privilege security access using an API or command line tool to the customer account. </p><p><br></p><p>What is the MOST secure way to allow org1 to access resources in org2?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "The customer should provide the partner company with their AWS account access keys to log in and perform the required tasks."
                },
                {
                    "label": "B",
                    "content": "The customer should create an IAM user and assign the required permissions to the IAM user. The customer should then provide the credentials to the partner company to log in and perform the required tasks."
                },
                {
                    "label": "C",
                    "content": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role&rsquo;s Amazon Resource Name (ARN) when requesting access to perform the required tasks."
                },
                {
                    "label": "D",
                    "content": "The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role&rsquo;s Amazon Resource Name (ARN), including the external ID in the IAM role&rsquo;s trust policy, when requesting access to perform the required tasks."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role's Amazon Resource Name (ARN), including the external ID in the IAM role's trust policy, when requesting access to perform the required tasks.</p><p>This is the MOST secure approach because:</p><p>It follows the principle of least privilege by allowing the customer to precisely define what actions the partner company can perform through the IAM role's permissions.</p><p>It uses temporary security credentials rather than long-term access keys, significantly reducing security risks.</p><p>The external ID in the trust policy provides an additional layer of security against the \"confused deputy\" problem, where a third party might trick the partner into accessing resources they shouldn't.</p><p>The partner company can use AWS Security Token Service (STS) to assume the role and obtain temporary credentials, which automatically expire after a defined period.</p><p>All actions performed using the assumed role are logged in AWS CloudTrail, providing full auditability.</p><p>Option A (providing AWS account access keys) is highly insecure because:</p><p>It involves sharing long-term credentials</p><p>It doesn't follow the principle of least privilege</p><p>It makes it difficult to revoke access if needed</p><p>It creates significant security risks if the keys are compromised</p><p>Using IAM roles with external IDs is the AWS recommended best practice for cross-account access, especially in partner scenarios where one organization needs to access resources in another organization's account.</p><p>Sources</p><p>Securely Accessing Customer AWS Accounts with Cross-Account IAM Roles | AWS Partner Network (APN) Blog （https://aws.amazon.com/cn/blogs/apn/securely-accessing-customer-aws-accounts-with-cross-account-iam-roles/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "b3f86fbca336423db421bde4ce3b9c53",
            "questionNumber": 127,
            "type": "single",
            "content": "<p>Question #127</p><p>A delivery company needs to migrate its third-party route planning application to AWS. The third party supplies a supported Docker image from a public registry. The image can run in as many containers as required to generate the route map. </p><p><br></p><p>The company has divided the delivery area into sections with supply hubs so that delivery drivers travel the shortest distance possible from the hubs to the customers. To reduce the time necessary to generate route maps, each section uses its own set of Docker containers with a custom configuration that processes orders only in the section's area.</p><p><br></p><p> The company needs the ability to allocate resources cost-effectively based on the number of running containers. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on Amazon EC2. Use the Amazon EKS CLI to launch the planning application in pods by using the --tags option to assign a custom tag to the pod."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon Elastic Kubernetes Service (Amazon EKS) cluster on AWS Fargate. Use the Amazon EKS CLI to launch the planning application. Use the AWS CLI tag-resource API call to assign a custom tag to the pod."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on Amazon EC2. Use the AWS CLI with run-tasks set to true to launch the planning application by using the --tags option to assign a custom tag to the task."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The correct answer is D. By creating an Amazon ECS cluster on AWS Fargate and using the AWS CLI run-task command with enableECSManagedTags set to true, the company can launch the planning application with the least operational overhead. This approach allows for serverless container management, as Fargate handles the underlying infrastructure. Additionally, using tags with ECS tasks helps in managing and tracking costs effectively.</p><p>Answer: &nbsp;</p><p>D. Create an Amazon Elastic Container Service (Amazon ECS) cluster on AWS Fargate. Use the AWS CLI run-task command and set enableECSManagedTags to true to launch the planning application. Use the --tags option to assign a custom tag to the task. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Cost-Effective Resource Allocation: &nbsp;</p><p> &nbsp; - Pay only for running containers (no idle EC2 costs). &nbsp;</p><p>2. Custom Configuration per Section: &nbsp;</p><p> &nbsp; - Each delivery area section needs dedicated containers with unique settings. &nbsp;</p><p>3. Least Operational Overhead: &nbsp;</p><p> &nbsp; - Avoid managing servers (e.g., EC2 nodes, Kubernetes control plane). &nbsp;</p><p> Why Option D is Best? &nbsp;</p><p>- ECS on Fargate: &nbsp;</p><p> &nbsp;- Serverless: No EC2 management; scales containers automatically. &nbsp;</p><p> &nbsp;- Cost-Efficient: Billed per vCPU/memory used (no over-provisioning). &nbsp;</p><p>- Task Tags: &nbsp;</p><p> &nbsp;- Use `--tags` to label tasks by delivery section (e.g., `section=North`). &nbsp;</p><p> &nbsp;- Enables cost tracking per section via AWS Cost Explorer. &nbsp;</p><p>- Simplified CLI Deployment: &nbsp;</p><p> &nbsp;```bash</p><p> &nbsp;aws ecs run-task \\</p><p> &nbsp; &nbsp;--cluster route-planning \\</p><p> &nbsp; &nbsp;--task-definition planner-task \\</p><p> &nbsp; &nbsp;--enable-ecs-managed-tags \\</p><p> &nbsp; &nbsp;--tags \"key=section,value=North\"</p><p> &nbsp;``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A/B (EKS on EC2/Fargate): &nbsp;</p><p> &nbsp;- Overkill: Kubernetes adds complexity (no need for multi-cloud/advanced orchestration). &nbsp;</p><p> &nbsp;- EKS CLI not required (ECS CLI is simpler). &nbsp;</p><p>- C (ECS on EC2): &nbsp;</p><p> &nbsp;- EC2 overhead: Must manage nodes/scaling (violates \"least operational overhead\"). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "89d5a830b3a5428388c795f4c3eb7ce6",
            "questionNumber": 128,
            "type": "multiple",
            "content": "<p>Question #128</p><p>A software company hosts an application on AWS with resources in multiple AWS accounts and Regions. The application runs on a group of Amazon EC2 instances in an application VPC located in the us-east-1 Region with an IPv4 CIDR block of 10.10.0.0/16. In a different AWS account, a shared services VPC is located in the us-east-2 Region with an IPv4 CIDR block of 10.10.10.0/24. When a cloud engineer uses AWS CloudFormation to attempt to peer the application VPC with the shared services VPC, an error message indicates a peering failure. <br><br>Which factors could cause this error? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "The IPv4 CIDR ranges of the two VPCs overlap"
                },
                {
                    "label": "B",
                    "content": "The VPCs are not in the same Region"
                },
                {
                    "label": "C",
                    "content": "One or both accounts do not have access to an Internet gateway"
                },
                {
                    "label": "D",
                    "content": "One of the VPCs was not shared through AWS Resource Access Manager"
                },
                {
                    "label": "E",
                    "content": "The IAM role in the peer accepter account does not have the correct permissions"
                }
            ],
            "correctAnswer": "AE",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. The IPv4 CIDR ranges of the two VPCs overlap </p><p>E. The IAM role in the peer accepter account does not have the correct permissions</p><p>These two factors could cause the VPC peering failure for the following reasons:</p><p>CIDR overlap (option A): The application VPC has a CIDR block of 10.10.0.0/16, while the shared services VPC has a CIDR block of 10.10.10.0/24. The shared services VPC CIDR (10.10.10.0/24) is completely contained within the application VPC CIDR (10.10.0.0/16), creating an overlap. AWS VPC peering explicitly prohibits connecting VPCs with overlapping CIDR blocks, as this would create routing ambiguity.</p><p>IAM permissions (option E): When creating a VPC peering connection between accounts, the user or role in the accepter account must have the appropriate permissions to accept the peering request. If the IAM role being used in the CloudFormation template doesn't have these permissions, the peering creation will fail.</p><p>Option B (different regions) is not correct because AWS supports inter-region VPC peering, allowing VPCs in different regions to be peered together.</p><p>Option C (Internet gateway access) is not correct because VPC peering doesn't require either VPC to have access to an Internet gateway. VPC peering creates a direct network connection between the VPCs and operates independently of Internet access.</p><p>Option D (AWS Resource Access Manager) is not correct because RAM is used for sharing resources like subnets within a VPC, not for VPC peering. VPC peering is established using the VPC peering feature, not through RAM.</p><p>Sources</p><p>how to Connect VPCs with Overlapping CIDRs Across Accounts | AWS re:Post （https://repost.aws/questions/QU0z4ZmjEESRuDr1K1_ZYjYw/how-to-connect-vpcs-with-overlapping-cidrs-across-accounts）</p><p>prerequisites to ensure vpc peering | AWS re:Post （https://repost.aws/questions/QUkDFWtBUlQfyETl1h5Z2skw/prerequisites-to-ensure-vpc-peering）</p><p>Troubleshoot a VPC peering connection - Amazon Virtual Private Cloud （https://docs.aws.amazon.com/vpc/latest/peering/troubleshoot-vpc-peering-connections.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "965006d93ae345c693b490d491534109",
            "questionNumber": 129,
            "type": "single",
            "content": "<p>Question #129</p><p>An external audit of a company’s serverless application reveals IAM policies that grant too many permissions. These policies are attached to the company's AWS Lambda execution roles. Hundreds of the company's Lambda functions have broad access permissions such as full access to Amazon S3 buckets and Amazon DynamoDB tables. The company wants each function to have only the minimum permissions that the function needs to complete its task. </p><p><br></p><p>A solutions architect must determine which permissions each Lambda function needs.</p><p><br></p><p> What should the solutions architect do to meet this requirement with the LEAST amount of effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up Amazon CodeGuru to profile the Lambda functions and search for AWS API calls. Create an inventory of the required API calls and resources for each Lambda function. Create new IAM access policies for each Lambda function. Review the new policies to ensure that they meet the company&#39;s business requirements."
                },
                {
                    "label": "B",
                    "content": "Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company&#39;s business requirements."
                },
                {
                    "label": "C",
                    "content": "Turn on AWS CloudTrail logging for the AWS account. Create a script to parse the CloudTrail log, search for AWS API calls by Lambda execution role, and create a summary report. Review the report. Create IAM access policies that provide more restrictive permissions for each Lambda function."
                },
                {
                    "label": "D",
                    "content": "Turn on AWS CloudTrail logging for the AWS account. Export the CloudTrail logs to Amazon S3. Use Amazon EMR to process the CloudTrail logs in Amazon S3 and produce a report of API calls and resources used by each execution role. Create a new IAM access policy for each role. Export the generated roles to an S3 bucket. Review the generated policies to ensure that they meet the company&rsquo;s business requirements."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Answer: &nbsp;</p><p>B. Turn on AWS CloudTrail logging for the AWS account. Use AWS Identity and Access Management Access Analyzer to generate IAM access policies based on the activity recorded in the CloudTrail log. Review the generated policies to ensure that they meet the company's business requirements. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Least-Privilege Permissions: &nbsp;</p><p> &nbsp; - Each Lambda function should have only the permissions it actually uses. &nbsp;</p><p>2. Least Effort: &nbsp;</p><p> &nbsp; - Avoid manual analysis or custom scripting. &nbsp;</p><p>3. Audit Trail: &nbsp;</p><p> &nbsp; - Use existing logs to identify used permissions. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- IAM Access Analyzer: &nbsp;</p><p> &nbsp;- Automatically analyzes CloudTrail logs to generate least-privilege policies. &nbsp;</p><p> &nbsp;- Policy Generation: Creates IAM policies based on actual API calls made by Lambda functions. &nbsp;</p><p> &nbsp;- No Custom Code: Fully managed service (no EMR/scripts needed). &nbsp;</p><p>- Steps: &nbsp;</p><p> &nbsp;1. Enable CloudTrail (if not already on). &nbsp;</p><p> &nbsp;2. Run Access Analyzer to generate policies. &nbsp;</p><p> &nbsp;3. Review/Attach new policies to Lambda roles. &nbsp;</p><p> Example Access Analyzer Output: &nbsp;</p><p>```json</p><p>{</p><p> &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp;\"Statement\": [</p><p> &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp;\"Action\": [</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"s3:GetObject\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"dynamodb:Query\"</p><p> &nbsp; &nbsp; &nbsp;],</p><p> &nbsp; &nbsp; &nbsp;\"Resource\": [</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"arn:aws:s3:::my-bucket/*\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp;\"arn:aws:dynamodb:us-east-1:123456789012:table/MyTable\"</p><p> &nbsp; &nbsp; &nbsp;]</p><p> &nbsp; &nbsp;}</p><p> &nbsp;]</p><p>}</p><p>``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (CodeGuru): &nbsp;</p><p> &nbsp;- CodeGuru doesn’t track runtime API calls (only code quality/performance). &nbsp;</p><p>- C (Custom Script): &nbsp;</p><p> &nbsp;- High effort: Requires writing/maintaining a log parser. &nbsp;</p><p>- D (EMR): &nbsp;</p><p> &nbsp;- Overkill: EMR is for big data, not IAM policy optimization. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Enable CloudTrail: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws cloudtrail create-trail --name LambdaAudit --s3-bucket-name my-log-bucket</p><p> &nbsp; ``` &nbsp;</p><p>2. Generate Policies with Access Analyzer: &nbsp;</p><p> &nbsp; - Navigate to IAM → Access Analyzer → Policy Generation. &nbsp;</p><p>3. Apply Policies: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws iam put-role-policy --role-name LambdaExecutionRole --policy-name LeastPrivilege --policy-document file://policy.json</p><p> &nbsp; ``` &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the fastest, most accurate way to enforce least privilege: &nbsp;</p><p>- Automated policy generation (Access Analyzer). &nbsp;</p><p>- No manual analysis (unlike scripting/EMR). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "bda87abc8d7d4e5d9c3d0d701634d79d",
            "questionNumber": 130,
            "type": "single",
            "content": "<p>Question #130</p><p>A solutions architect must analyze a company’s Amazon EC2 instances and Amazon Elastic Block Store (Amazon EBS) volumes to determine whether the company is using resources efficiently. The company is running several large, high-memory EC2 instances to host database clusters that are deployed in active/passive configurations. The utilization of these EC2 instances varies by the applications that use the databases, and the company has not identified a pattern. </p><p><br></p><p>The solutions architect must analyze the environment and take action based on the findings. </p><p><br></p><p>Which solution meets these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a dashboard by using AWS Systems Manager OpsCenter. Configure visualizations for Amazon CloudWatch metrics that are associated with the EC2 instances and their EBS volumes. Review the dashboard periodically, and identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics."
                },
                {
                    "label": "B",
                    "content": "Turn on Amazon CloudWatch detailed monitoring for the EC2 instances and their EBS volumes. Create and review a dashboard that is based on the metrics. Identify usage patterns. Rightsize the EC2 instances based on the peaks in the metrics."
                },
                {
                    "label": "C",
                    "content": "Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed."
                },
                {
                    "label": "D",
                    "content": "Sign up for the AWS Enterprise Support plan. Turn on AWS Trusted Advisor. Wait 12 hours. Review the recommendations from Trusted Advisor, and rightsize the EC2 instances as directed."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Answer: &nbsp;</p><p>C. Install the Amazon CloudWatch agent on each of the EC2 instances. Turn on AWS Compute Optimizer, and let it run for at least 12 hours. Review the recommendations from Compute Optimizer, and rightsize the EC2 instances as directed. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Efficiency Analysis: &nbsp;</p><p> &nbsp; - Assess EC2 instance and EBS volume utilization (CPU, memory, disk I/O). &nbsp;</p><p>2. Cost-Effective Optimization: &nbsp;</p><p> &nbsp; - Rightsize instances based on actual usage (avoid over-provisioning). &nbsp;</p><p>3. No Usage Patterns Identified: &nbsp;</p><p> &nbsp; - Need automated recommendations (manual dashboards won’t scale). &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- AWS Compute Optimizer: &nbsp;</p><p> &nbsp;- Automatically analyzes CloudWatch metrics (after installing the agent). &nbsp;</p><p> &nbsp;- Generates rightsizing recommendations (e.g., downgrade `r5.8xlarge` to `r5.4xlarge`). &nbsp;</p><p> &nbsp;- Considers memory/CPU/EBS (unlike Trusted Advisor’s basic checks). &nbsp;</p><p>- CloudWatch Agent: &nbsp;</p><p> &nbsp;- Collects system-level metrics (e.g., memory utilization) not available in basic CloudWatch. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Install CloudWatch Agent: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; sudo amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c ssm:AmazonCloudWatch-linux -s</p><p> &nbsp; ``` &nbsp;</p><p>2. Enable Compute Optimizer: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws compute-optimizer start-recommendation-export --s3-destination-bucket my-bucket</p><p> &nbsp; ``` &nbsp;</p><p>3. Review Recommendations: &nbsp;</p><p> &nbsp; - In Compute Optimizer console or via API. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A/B (Manual Dashboards): &nbsp;</p><p> &nbsp;- Time-consuming: Requires manual pattern identification. &nbsp;</p><p> &nbsp;- No actionable recommendations (unlike Compute Optimizer). &nbsp;</p><p>- D (Trusted Advisor): &nbsp;</p><p> &nbsp;- Limited to basic checks (e.g., idle instances, not memory optimization). &nbsp;</p><p> &nbsp;- Enterprise Support required (extra cost). &nbsp;</p><p> Cost Savings Example: &nbsp;</p><p>| Instance Type (Before) | Recommended Type (After) | Monthly Savings | &nbsp;</p><p>|------------------------|--------------------------|----------------| &nbsp;</p><p>| `r5.8xlarge` (256 GB) &nbsp;| `r5.4xlarge` (128 GB) &nbsp; &nbsp;| ~$1,000 &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the most cost-effective solution because: &nbsp;</p><p>- Automated rightsizing (Compute Optimizer). &nbsp;</p><p>- No manual analysis (unlike dashboards). &nbsp;</p><p>- No extra support costs (unlike Trusted Advisor). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8f356eeb160a4a7daff8d82d2794e3ec",
            "questionNumber": 131,
            "type": "single",
            "content": "<p>Question #131</p><p>A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company uses AWS Control Tower for governance and uses AWS Transit Gateway for VPC connectivity across accounts. </p><p><br></p><p>In an AWS application account, the company’s application team has deployed a web application that uses AWS Lambda and Amazon RDS. The company's database administrators have a separate DBA account and use the account to centrally manage all the databases across the organization. The database administrators use an Amazon EC2 instance that is deployed in the DBA account to access an RDS database that is deployed in the application account. </p><p><br></p><p>The application team has stored the database credentials as secrets in AWS Secrets Manager in the application account. The application team is manually sharing the secrets with the database administrators. The secrets are encrypted by the default AWS managed key for Secrets Manager in the application account. A solutions architect needs to implement a solution that gives the database administrators access to the database and eliminates the need to manually share the secrets. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Resource Access Manager (AWS RAM) to share the secrets from the application account with the DBA account. In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the shared secrets. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
                },
                {
                    "label": "B",
                    "content": "In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
                },
                {
                    "label": "C",
                    "content": "In the DBA account create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets and the default AWS managed key in the application account. In the application account, attach resource-based policies to the key to allow access from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
                },
                {
                    "label": "D",
                    "content": "In the DBA account, create an IAM role that is named DBA-Admin. Grant the role the required permissions to access the secrets in the application account. Attach an SCP to the application account to allow access to the secrets from the DBA account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Answer: &nbsp;</p><p>B. In the application account, create an IAM role that is named DBA-Secret. Grant the role the required permissions to access the secrets. In the DBA account, create an IAM role that is named DBA-Admin. Grant the DBA-Admin role the required permissions to assume the DBA-Secret role in the application account. Attach the DBA-Admin role to the EC2 instance for access to the cross-account secrets. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Cross-Account Secret Access: &nbsp;</p><p> &nbsp; - DBAs (in DBA account) need access to Secrets Manager secrets (in application account). &nbsp;</p><p>2. No Manual Sharing: &nbsp;</p><p> &nbsp; - Eliminate manual credential sharing. &nbsp;</p><p>3. Security: &nbsp;</p><p> &nbsp; - Secrets encrypted with AWS managed key (no custom KMS policy changes needed). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>- IAM Role Delegation: &nbsp;</p><p> &nbsp;1. Application Account: &nbsp;</p><p> &nbsp; &nbsp; - Create `DBA-Secret` role with `secretsmanager:GetSecretValue` permission. &nbsp;</p><p> &nbsp; &nbsp; - Trust policy allows `DBA-Admin` role (from DBA account) to assume it. &nbsp;</p><p> &nbsp; &nbsp; ```json</p><p> &nbsp; &nbsp; {</p><p> &nbsp; &nbsp; &nbsp; \"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; &nbsp; \"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Principal\": { \"AWS\": \"arn:aws:iam::DBA_ACCOUNT_ID:role/DBA-Admin\" },</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Action\": \"sts:AssumeRole\"</p><p> &nbsp; &nbsp; &nbsp; }]</p><p> &nbsp; &nbsp; }</p><p> &nbsp; &nbsp; ``` &nbsp;</p><p> &nbsp;2. DBA Account: &nbsp;</p><p> &nbsp; &nbsp; - Attach `DBA-Admin` role to EC2 instance. &nbsp;</p><p> &nbsp; &nbsp; - Role has `sts:AssumeRole` permission for `DBA-Secret`. &nbsp;</p><p>- Workflow: &nbsp;</p><p> &nbsp;- EC2 instance (DBA account) → Assumes `DBA-Secret` role → Accesses secrets. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (AWS RAM Sharing): &nbsp;</p><p> &nbsp;- Secrets Manager doesn’t support cross-account sharing via RAM. &nbsp;</p><p>- C (KMS Key Policies): &nbsp;</p><p> &nbsp;- Overkill: AWS managed keys already allow cross-account access via IAM roles. &nbsp;</p><p>- D (SCP Allow): &nbsp;</p><p> &nbsp;- SCPs cannot grant permissions (only deny). &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. In Application Account: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws iam create-role --role-name DBA-Secret --assume-role-policy-document file://trust-policy.json</p><p> &nbsp; aws iam attach-role-policy --role-name DBA-Secret --policy-arn arn:aws:iam::aws:policy/SecretsManagerReadWrite</p><p> &nbsp; ``` &nbsp;</p><p>2. In DBA Account: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws iam create-role --role-name DBA-Admin --assume-role-policy-document file://ec2-trust-policy.json</p><p> &nbsp; aws iam attach-role-policy --role-name DBA-Admin --policy-arn arn:aws:iam::aws:policy/AmazonEC2RoleforSSM</p><p> &nbsp; ``` &nbsp;</p><p>3. On EC2 Instance: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws sts assume-role --role-arn arn:aws:iam::APPLICATION_ACCOUNT_ID:role/DBA-Secret --role-session-name DBASession</p><p> &nbsp; ``` &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the only secure, scalable solution that: &nbsp;</p><p>- Automates secret access (no manual sharing). &nbsp;</p><p>- Uses IAM roles (no KMS/S3/SCP workarounds). &nbsp;</p><p>- Aligns with AWS best practices. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5dd7efd1a6d2469d950b371038424d1d",
            "questionNumber": 132,
            "type": "multiple",
            "content": "<p>Question #132</p><p>A company manages multiple AWS accounts by using AWS Organizations. Under the root OU, the company has two OUs</p><p><br></p><p>.Research and DataOps. Because of regulatory requirements, all resources that the company deploys in the organization must reside in the ap-northeast-1 Region. Additionally, EC2 instances that the company deploys in the DataOps OU must use a predefined list of instance types. </p><p><br></p><p>A solutions architect must implement a solution that applies these restrictions. The solution must maximize operational efficiency and must minimize ongoing maintenance. <br><br>Which combination of steps will meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an IAM role in one account under the DataOps OU. Use the ec2:InstanceType condition key in an inline policy on the role to restrict access to specific instance types."
                },
                {
                    "label": "B",
                    "content": "Create an IAM user in all accounts under the root OU. Use the aws:RequestedRegion condition key in an inline policy on each user to restrict access to all AWS Regions except ap-northeast-1."
                },
                {
                    "label": "C",
                    "content": "Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU."
                },
                {
                    "label": "D",
                    "content": "Create an SCP. Use the ec2:Region condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU, the DataOps OU, and the Research OU."
                },
                {
                    "label": "E",
                    "content": "Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU."
                }
            ],
            "correctAnswer": "CE",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>C. Create an SCP. Use the aws:RequestedRegion condition key to restrict access to all AWS Regions except ap-northeast-1. Apply the SCP to the root OU.</p><p>E. Create an SCP. Use the ec2:InstanceType condition key to restrict access to specific instance types. Apply the SCP to the DataOps OU.</p><p>These two steps together provide the most efficient solution that meets all the requirements:</p><p>Creating an SCP with the aws:RequestedRegion condition key (option C) addresses the regulatory requirement that all resources must reside in the ap-northeast-1 Region. By applying this SCP to the root OU, the restriction cascades down to all accounts in the organization, ensuring that resources can only be deployed in the specified region. This is more efficient than applying the policy to each OU separately.</p><p>Creating a separate SCP with the ec2:InstanceType condition key (option E) addresses the requirement to restrict EC2 instance types in the DataOps OU. By applying this SCP specifically to the DataOps OU, it ensures that only the predefined list of instance types can be used within that organizational unit, while not affecting the Research OU.</p><p>This combination maximizes operational efficiency by:</p><p>Using SCPs, which are centrally managed policies that apply to multiple accounts</p><p>Applying policies at the appropriate level in the organizational hierarchy</p><p>Using AWS-managed condition keys that are designed for these specific restrictions</p><p>It also minimizes ongoing maintenance because:</p><p>Changes to the policies can be made in one place and automatically apply to all affected accounts</p><p>No need to manage individual IAM policies across multiple accounts</p><p>No need for custom scripts or monitoring solutions to enforce the restrictions</p><p>Service Control Policies (SCPs) are the most appropriate tool for implementing organization-wide governance in AWS Organizations, making this combination the optimal solution for the requirements.</p><p>Sources</p><p>Defining Boundaries for Regional Services Access - Navigating GDPR Compliance on AWS （https://docs.aws.amazon.com/whitepapers/latest/navigating-gdpr-compliance/defining-boundaries-for-regional-services-access.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c4fba426484848cc9307d5d6ed1a8444",
            "questionNumber": 133,
            "type": "multiple",
            "content": "<p>Question #133</p><p>A company runs a serverless application in a single AWS Region. The application accesses external URLs and extracts metadata from those sites. The company uses an Amazon Simple Notification Service (Amazon SNS) topic to publish URLs to an Amazon Simple Queue Service (Amazon SQS) queue. An AWS Lambda function uses the queue as an event source and processes the URLs from the queue. Results are saved to an Amazon S3 bucket. The company wants to process each URL in other Regions to compare possible differences in site localization. URLs must be published from the existing Region. Results must be written to the existing S3 bucket in the current Region. </p><p><br></p><p>Which combination of changes will produce a multi-Region deployment that meets these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy the SQS queue with the Lambda function to other Regions."
                },
                {
                    "label": "B",
                    "content": "Subscribe the SNS topic in each Region to the SQS queue."
                },
                {
                    "label": "C",
                    "content": "Subscribe the SQS queue in each Region to the SNS topic."
                },
                {
                    "label": "D",
                    "content": "Configure the SQS queue to publish URLs to SNS topics in each Region."
                },
                {
                    "label": "E",
                    "content": "Deploy the SNS topic and the Lambda function to other Regions."
                }
            ],
            "correctAnswer": "AC",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Deploy the SQS queue with the Lambda function to other Regions. </p><p>C. Subscribe the SQS queue in each Region to the SNS topic.</p><p>These two steps together provide the solution that meets all the requirements:</p><p>Deploying the SQS queue with the Lambda function to other Regions (option A) is necessary because:</p><p>Each region needs its own processing capability to handle the URLs</p><p>This allows the application to process the same URL in different regions to compare site localization differences</p><p>The Lambda functions in each region can process the URLs based on their regional context</p><p>Subscribing the SQS queue in each Region to the SNS topic (option C) is required because:</p><p>The requirement states that URLs must be published from the existing Region, meaning the original SNS topic remains in the primary region</p><p>By subscribing regional SQS queues to the central SNS topic, messages published to the topic will be distributed to all regions</p><p>This creates a fan-out pattern where a single message published to the SNS topic is delivered to multiple SQS queues across regions</p><p>This combination allows the application to:</p><p>Continue publishing URLs from the existing region (through the original SNS topic)</p><p>Process each URL in multiple regions (through regional SQS queues and Lambda functions)</p><p>Write results back to the existing S3 bucket in the current region (each Lambda function can be configured to write to the central S3 bucket)</p><p>Option B (subscribing the SNS topic in each Region to the SQS queue) doesn't make sense architecturally, as it would require multiple SNS topics to publish to a single SQS queue, which doesn't align with the requirement to process URLs in multiple regions.</p><p>Sources</p><p>Using Multi-Region Access Points with supported API operations - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/MrapOperations.html）</p><p>Chain AWS services together using a serverless approach - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/chain-aws-services-together-using-a-serverless-approach.html）</p><p>Subscribe a Lambda function to event notifications from S3 buckets in different AWS Regions - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/subscribe-a-lambda-function-to-event-notifications-from-s3-buckets-in-different-aws-regions.html）</p><p>REL10-BP01 Deploy the workload to multiple locations - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/rel_fault_isolation_multiaz_region_system.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "dc124eb28989476da62d93eed286e5eb",
            "questionNumber": 134,
            "type": "single",
            "content": "<p>Question #134</p><p>A company runs a proprietary stateless ETL application on an Amazon EC2 Linux instances. The application is a Linux binary, and the source code cannot be modified. The application is single-threaded, uses 2 GB of RAM, and is highly CPU intensive. The application is scheduled to run every 4 hours and runs for up to 20 minutes. A solutions architect wants to revise the architecture for the solution. </p><p><br></p><p>Which strategy should the solutions architect use?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Lambda to run the application. Use Amazon CloudWatch Logs to invoke the Lambda function every 4 hours."
                },
                {
                    "label": "B",
                    "content": "Use AWS Batch to run the application. Use an AWS Step Functions state machine to invoke the AWS Batch job every 4 hours."
                },
                {
                    "label": "C",
                    "content": "Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours."
                },
                {
                    "label": "D",
                    "content": "Use Amazon EC2 Spot Instances to run the application. Use AWS CodeDeploy to deploy and run the application every 4 hours."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Use AWS Fargate to run the application. Use Amazon EventBridge (Amazon CloudWatch Events) to invoke the Fargate task every 4 hours.</p><p>This is the best strategy for the solutions architect to revise the architecture for the following reasons:</p><p>AWS Fargate is well-suited for this workload because:</p><p>It can easily handle the 2GB RAM requirement and CPU-intensive workload</p><p>It supports containerized applications, which is perfect for packaging the Linux binary</p><p>It can run for more than 20 minutes without any time limitations</p><p>It's serverless, which means you don't need to manage the underlying infrastructure</p><p>Amazon EventBridge (formerly CloudWatch Events) provides a reliable way to schedule the task to run every 4 hours.</p><p>The other options have significant limitations:</p><p>A. AWS Lambda has a maximum execution time of 15 minutes, which is insufficient for an application that runs for up to 20 minutes. Also, CloudWatch Logs is not designed to invoke Lambda functions on a schedule.</p><p>B. AWS Batch would work for this workload, but it's more complex to set up and manage compared to Fargate for this relatively simple scheduling requirement. It's designed for more complex batch processing scenarios with dependencies and job queues.</p><p>D. Amazon EC2 Spot Instances could be interrupted during processing, which would disrupt the ETL job. Additionally, AWS CodeDeploy is a deployment service, not a scheduling tool, so it's not appropriate for invoking the application on a regular schedule.</p><p>AWS Fargate with EventBridge scheduling provides the right balance of simplicity, reliability, and cost-effectiveness for this stateless ETL application.</p><p>Sources</p><p>AWS Fargate or AWS Lambda? - AWS Fargate or AWS Lambda? （https://docs.aws.amazon.com/decision-guides/latest/fargate-or-lambda/fargate-or-lambda.html）</p><p>REL05-BP06 Make services stateless where possible - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/rel_mitigate_interaction_failure_stateless.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "65b908dc36f149d3a202083c2b6fc473",
            "questionNumber": 135,
            "type": "single",
            "content": "Question #135<p>A company is creating a sequel for a popular online game. A large number of users from all over the world will play the game within the first week after launch. Currently, the game consists of the following components deployed in a single AWS Region: <br>an Amazon S3 bucket that stores game assets;<br>an Amazon DynamoDB table that stores player scores;</p><p><br></p><p>A solutions architect needs to design a multi-Region solution that will reduce latency, improve reliability, and require the least effort to implement. </p><p><br></p><p>What should the solutions architect do to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Cross-Region Replication. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon CloudFront distribution to serve assets from the S3 bucket. Configure S3 Same-Region Replication. Create a new DynamoDB table in a new Region. Configure asynchronous replication between the DynamoDB tables by using AWS Database Migration Service (AWS DMS) with change data capture (CDC)."
                },
                {
                    "label": "C",
                    "content": "Create another S3 bucket in a new Region, and configure S3 Cross-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets in each Region. Configure DynamoDB global tables by enabling Amazon DynamoDB Streams, and add a replica table in a new Region."
                },
                {
                    "label": "D",
                    "content": "Create another S3 bucket in the same Region, and configure S3 Same-Region Replication between the buckets. Create an Amazon CloudFront distribution and configure origin failover with two origins accessing the S3 buckets. Create a new DynamoDB table in a new Region. Use the new table as a replica target for DynamoDB global tables."
                }
            ],
            "correctAnswer": "C",
            "explanation": "To meet the requirements of reduced latency, improved reliability, and minimal implementation effort, the solutions architect should choose Option C. This option involves creating an additional S3 bucket in a new Region and setting up S3 Cross-Region Replication to serve game assets globally while reducing latency. Additionally, using Amazon CloudFront with origin failover provides high availability for the game assets. Setting up DynamoDB global tables with a replica in a new Region ensures reliability and fast access to player scores worldwide.",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c1b0bdda171e4692b7fd9e37e9e7c42a",
            "questionNumber": 136,
            "type": "single",
            "content": "<p>Question #136</p><p>A company has an on-premises website application that provides real estate information for potential renters and buyers. The website uses a Java backend and a NoSQL MongoDB database to store subscriber data.</p><p><br></p><p>The company needs to migrate the entire application to AWS with a similar structure. The application must be deployed for high availability, and the company cannot make changes to the application. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use an Amazon Aurora DB cluster as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application."
                },
                {
                    "label": "B",
                    "content": "Use MongoDB on Amazon EC2 instances as the database for the subscriber data. Deploy EC2 instances in an Auto Scaling group in a single Availability Zone for the Java backend application."
                },
                {
                    "label": "C",
                    "content": "Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application."
                },
                {
                    "label": "D",
                    "content": "Configure Amazon DocumentDB (with MongoDB compatibility) in on-demand capacity mode in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Answer: &nbsp;</p><p>C. Configure Amazon DocumentDB (with MongoDB compatibility) with appropriately sized instances in multiple Availability Zones as the database for the subscriber data. Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. No Application Changes: &nbsp;</p><p> &nbsp; - Must retain MongoDB compatibility (cannot switch to Aurora). &nbsp;</p><p>2. High Availability: &nbsp;</p><p> &nbsp; - Database and backend must be multi-AZ. &nbsp;</p><p>3. Managed Services Preferred: &nbsp;</p><p> &nbsp; - Minimize operational overhead where possible. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- Amazon DocumentDB: &nbsp;</p><p> &nbsp;- Fully compatible with MongoDB (no app changes needed). &nbsp;</p><p> &nbsp;- Built-in multi-AZ replication (automated failover). &nbsp;</p><p> &nbsp;- Managed service (no EC2 patching/scaling). &nbsp;</p><p>- EC2 Auto Scaling: &nbsp;</p><p> &nbsp;- Multi-AZ deployment ensures backend availability. &nbsp;</p><p> &nbsp;- No code changes (Java app runs as-is). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Aurora): &nbsp;</p><p> &nbsp;- Requires app changes (MongoDB → Aurora SQL). &nbsp;</p><p>- B (MongoDB on EC2): &nbsp;</p><p> &nbsp;- Single-AZ risk (violates high availability). &nbsp;</p><p> &nbsp;- Self-managed (operational overhead). &nbsp;</p><p>- D (DocumentDB On-Demand): &nbsp;</p><p> &nbsp;- On-demand is cost-prohibitive for steady workloads. &nbsp;</p><p> &nbsp;- Provisioned capacity (Option C) is more cost-effective. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Deploy DocumentDB: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws docdb create-db-cluster \\</p><p> &nbsp; &nbsp; --db-cluster-identifier real-estate-db \\</p><p> &nbsp; &nbsp; --engine docdb \\</p><p> &nbsp; &nbsp; --master-username admin \\</p><p> &nbsp; &nbsp; --master-user-password password \\</p><p> &nbsp; &nbsp; --availability-zones us-east-1a us-east-1b</p><p> &nbsp; ``` &nbsp;</p><p>2. Configure EC2 Auto Scaling: &nbsp;</p><p> &nbsp; - Use Launch Template with Java app AMI. &nbsp;</p><p> &nbsp; - Set min/max capacity across 2+ AZs. &nbsp;</p><p> High Availability Design: &nbsp;</p><p>| Component &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| Solution &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| HA Mechanism &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp;</p><p>|--------------------|-----------------------------------|----------------------------------| &nbsp;</p><p>| Database &nbsp; &nbsp; &nbsp; | Amazon DocumentDB &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 3 copies across AZs + auto-failover | &nbsp;</p><p>| Backend &nbsp; &nbsp; &nbsp; &nbsp;| EC2 Auto Scaling (multi-AZ) &nbsp; &nbsp; &nbsp; | Load balancer + health checks &nbsp; &nbsp;| &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the only option that: &nbsp;</p><p>- Preserves MongoDB compatibility (no app changes). &nbsp;</p><p>- Ensures high availability (multi-AZ for DB + backend). &nbsp;</p><p>- Minimizes management (DocumentDB is managed). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1fedd730e565427fa2360479cbdfb968",
            "questionNumber": 137,
            "type": "multiple",
            "content": "<p>Question #137</p><p>A digital marketing company has multiple AWS accounts that belong to various teams. The creative team uses an Amazon S3 bucket in its AWS account to securely store images and media files that are used as content for the company’s marketing campaigns. The creative team wants to share the S3 bucket with the strategy team so that the strategy team can view the objects. A solutions architect has created an IAM role that is named strategy_reviewer in the Strategy account. The solutions architect also has set up a custom AWS Key Management Service (AWS KMS) key in the Creative account and has associated the key with the S3 bucket. </p><p><br></p><p>However, when users from the Strategy account assume the IAM role and try to access objects in the S3 bucket, they receive an Access Denied error. The solutions architect must ensure that users in the Strategy account can access the S3 bucket. The solution must provide these users with only the minimum permissions that they need. </p><p><br></p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account."
                },
                {
                    "label": "B",
                    "content": "Update the strategy_reviewer IAM role to grant full permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key."
                },
                {
                    "label": "C",
                    "content": "Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role."
                },
                {
                    "label": "D",
                    "content": "Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to an anonymous user."
                },
                {
                    "label": "E",
                    "content": "Update the custom KMS key policy in the Creative account to grant encrypt permissions to the strategy_reviewer IAM role."
                },
                {
                    "label": "F",
                    "content": "Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key."
                }
            ],
            "correctAnswer": "ACF",
            "explanation": "<p>Answer: &nbsp;</p><p>A. Create a bucket policy that includes read permissions for the S3 bucket. Set the principal of the bucket policy to the account ID of the Strategy account. &nbsp;</p><p>C. Update the custom KMS key policy in the Creative account to grant decrypt permissions to the strategy_reviewer IAM role. &nbsp;</p><p>F. Update the strategy_reviewer IAM role to grant read permissions for the S3 bucket and to grant decrypt permissions for the custom KMS key. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Cross-Account S3 Access: &nbsp;</p><p> &nbsp; - Strategy team (Strategy account) needs read-only access to S3 bucket (Creative account). &nbsp;</p><p>2. KMS Encryption: &nbsp;</p><p> &nbsp; - Bucket uses a custom KMS key (requires decrypt permissions). &nbsp;</p><p>3. Least Privilege: &nbsp;</p><p> &nbsp; - Grant only `s3:GetObject` and `kms:Decrypt`. &nbsp;</p><p> Why A + C + F? &nbsp;</p><p>- A (Bucket Policy): &nbsp;</p><p> &nbsp;- Grants read access to the Strategy account (principal = Strategy account ID). &nbsp;</p><p> &nbsp;```json</p><p> &nbsp;{</p><p> &nbsp; &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp;\"Statement\": [{</p><p> &nbsp; &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp;\"Principal\": { \"AWS\": \"arn:aws:iam::STRATEGY_ACCOUNT_ID:root\" },</p><p> &nbsp; &nbsp; &nbsp;\"Action\": [\"s3:GetObject\"],</p><p> &nbsp; &nbsp; &nbsp;\"Resource\": \"arn:aws:s3:::creative-bucket/*\"</p><p> &nbsp; &nbsp;}]</p><p> &nbsp;}</p><p> &nbsp;``` &nbsp;</p><p>- C (KMS Key Policy): &nbsp;</p><p> &nbsp;- Allows `kms:Decrypt` for the `strategy_reviewer` role (required for encrypted objects). &nbsp;</p><p> &nbsp;```json</p><p> &nbsp;{</p><p> &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp;\"Principal\": { \"AWS\": \"arn:aws:iam::STRATEGY_ACCOUNT_ID:role/strategy_reviewer\" },</p><p> &nbsp; &nbsp;\"Action\": \"kms:Decrypt\",</p><p> &nbsp; &nbsp;\"Resource\": \"*\"</p><p> &nbsp;}</p><p> &nbsp;``` &nbsp;</p><p>- F (IAM Role Permissions): &nbsp;</p><p> &nbsp;- Attach inline policy to `strategy_reviewer` role: &nbsp;</p><p> &nbsp; &nbsp;```json</p><p> &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp;\"Version\": \"2012-10-17\",</p><p> &nbsp; &nbsp; &nbsp;\"Statement\": [</p><p> &nbsp; &nbsp; &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Action\": [\"s3:GetObject\"],</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Resource\": \"arn:aws:s3:::creative-bucket/*\"</p><p> &nbsp; &nbsp; &nbsp; &nbsp;},</p><p> &nbsp; &nbsp; &nbsp; &nbsp;{</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Effect\": \"Allow\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Action\": \"kms:Decrypt\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\"Resource\": \"arn:aws:kms:us-east-1:CREATIVE_ACCOUNT_ID:key/1234abcd-12ab-34cd-56ef-1234567890ab\"</p><p> &nbsp; &nbsp; &nbsp; &nbsp;}</p><p> &nbsp; &nbsp; &nbsp;]</p><p> &nbsp; &nbsp;}</p><p> &nbsp; &nbsp;``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- B (Full S3/KMS Permissions): &nbsp;</p><p> &nbsp;- Violates least privilege (`s3:*` is excessive). &nbsp;</p><p>- D (Anonymous Access): &nbsp;</p><p> &nbsp;- Security risk (public access to sensitive data). &nbsp;</p><p>- E (Encrypt Permission): &nbsp;</p><p> &nbsp;- Strategy team doesn’t need `kms:Encrypt` (only `Decrypt`). &nbsp;</p><p> Access Flow: &nbsp;</p><p>1. Strategy user assumes `strategy_reviewer` role. &nbsp;</p><p>2. Role permissions allow `s3:GetObject` + `kms:Decrypt`. &nbsp;</p><p>3. Bucket policy permits access from Strategy account. &nbsp;</p><p>4. KMS key policy permits decryption by the role. &nbsp;</p><p> Conclusion: &nbsp;</p><p>A + C + F is the only combination that: &nbsp;</p><p>- Grants minimal permissions (read-only + decrypt). &nbsp;</p><p>- Respects KMS encryption. &nbsp;</p><p>- Avoids security risks (no anonymous/full access). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2d4d2b52105c4913af0321e147bf851a",
            "questionNumber": 138,
            "type": "single",
            "content": "<p>Question #138</p><p>A life sciences company is using a combination of open source tools to manage data analysis workflows and Docker containers running on servers in its on-premises data center to process genomics data. Sequencing data is generated and stored on a local storage area network (SAN), and then the data is processed. The research and development teams are running into capacity issues and have decided to re-architect their genomics analysis platform on AWS to scale based on workload demands and reduce the turnaround time from weeks to days. </p><p><br></p><p>The company has a high-speed AWS Direct Connect connection. Sequencers will generate around 200 GB of data for each genome, and individual jobs can take several hours to process the data with ideal compute capacity. The end result will be stored in Amazon S3. The company is expecting 10-15 job requests each day. </p><p><br></p><p>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use regularly scheduled AWS Snowball Edge devices to transfer the sequencing data into AWS. When AWS receives the Snowball Edge device and the data is loaded into Amazon S3, use S3 events to trigger an AWS Lambda function to process the data."
                },
                {
                    "label": "B",
                    "content": "Use AWS Data Pipeline to transfer the sequencing data to Amazon S3. Use S3 events to trigger an Amazon EC2 Auto Scaling group to launch custom-AMI EC2 instances running the Docker containers to process the data."
                },
                {
                    "label": "C",
                    "content": "Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data."
                },
                {
                    "label": "D",
                    "content": "Use an AWS Storage Gateway file gateway to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Batch job that executes on Amazon EC2 instances running the Docker containers to process the data."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Answer: &nbsp;</p><p>C. Use AWS DataSync to transfer the sequencing data to Amazon S3. Use S3 events to trigger an AWS Lambda function that starts an AWS Step Functions workflow. Store the Docker images in Amazon Elastic Container Registry (Amazon ECR) and trigger AWS Batch to run the container and process the sequencing data. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. High-Volume Data Transfer: &nbsp;</p><p> &nbsp; - 200 GB per genome (10-15 jobs/day = 2–3 TB/day). &nbsp;</p><p> &nbsp; - Direct Connect available (prioritize fast, reliable transfers). &nbsp;</p><p>2. Scalable Compute: &nbsp;</p><p> &nbsp; - Jobs take hours to process (need burstable, high-performance compute). &nbsp;</p><p>3. Workflow Automation: &nbsp;</p><p> &nbsp; - Reduce turnaround time from weeks to days. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- AWS DataSync: &nbsp;</p><p> &nbsp;- Optimized for large data transfers over Direct Connect (faster than Snowball/Storage Gateway). &nbsp;</p><p> &nbsp;- Automated retries/checksums ensure data integrity. &nbsp;</p><p>- AWS Batch + ECR: &nbsp;</p><p> &nbsp;- Runs Docker containers at scale (no EC2 management). &nbsp;</p><p> &nbsp;- Auto-scales EC2 Spot/Fleet instances for cost efficiency. &nbsp;</p><p>- Step Functions: &nbsp;</p><p> &nbsp;- Orchestrates workflows (e.g., pre-processing → analysis → S3 export). &nbsp;</p><p>- Lambda Trigger: &nbsp;</p><p> &nbsp;- S3 events initiate pipelines instantly (no polling). &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. Sequencers → SAN → DataSync → S3 (raw data). &nbsp;</p><p>2. S3 Event → Lambda → Step Functions → AWS Batch (processing). &nbsp;</p><p>3. Batch pulls Docker images from ECR, processes data, writes results to S3. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Snowball Edge): &nbsp;</p><p> &nbsp;- Not real-time: Snowball adds days of latency (violates \"weeks to days\" goal). &nbsp;</p><p>- B (Data Pipeline + EC2 ASG): &nbsp;</p><p> &nbsp;- EC2 ASG is overkill (AWS Batch handles scaling better for batch jobs). &nbsp;</p><p> &nbsp;- Data Pipeline is legacy (Step Functions is modern alternative). &nbsp;</p><p>- D (Storage Gateway + Batch): &nbsp;</p><p> &nbsp;- Storage Gateway is for hybrid caching, not bulk transfers. &nbsp;</p><p> Cost Optimization: &nbsp;</p><p>- Use EC2 Spot Instances in AWS Batch (60-90% cost savings). &nbsp;</p><p>- DataSync pricing: $0.0125/GB (cheaper than manual transfers). &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the only solution that: &nbsp;</p><p>- Transfers data efficiently (DataSync). &nbsp;</p><p>- Scales compute dynamically (AWS Batch). &nbsp;</p><p>- Automates workflows (Step Functions). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "eeef6fb235ae48adab6a0fff6b0d05a9",
            "questionNumber": 139,
            "type": "single",
            "content": "<p>Question #139</p><p>A company runs a content management application on a single Windows Amazon EC2 instance in a development environment. The application reads and writes static content to a 2 TB Amazon Elastic Block Store (Amazon EBS) volume that is attached to the instance as the root device. The company plans to deploy this application in production as a highly available and fault-tolerant solution that runs on at least three EC2 instances across multiple Availability Zones. </p><p><br></p><p>A solutions architect must design a solution that joins all the instances that run the application to an Active Directory domain. The solution also must implement Windows ACLs to control access to file contents. The application always must maintain exactly the same content on all running instances at any given point in time. </p><p><br></p><p>Which solution will meet these requirements with the LEAST management overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon Elastic File System (Amazon EFS) file share. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application, join the instance to the AD domain, and mount the EFS file share."
                },
                {
                    "label": "B",
                    "content": "Create a new AMI from the current EC2 Instance that is running. Create an Amazon FSx for Lustre file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to join the instance to the AD domain and mount the FSx for Lustre file system."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon FSx for Windows File Server file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three instances. Implement a user data script to install the application and mount the FSx for Windows File Server file system. Perform a seamless domain join to join the instance to the AD domain."
                },
                {
                    "label": "D",
                    "content": "Create a new AMI from the current EC2 instance that is running. Create an Amazon Elastic File System (Amazon EFS) file system. Create an Auto Scaling group that extends across three Availability Zones and maintains a minimum size of three Instances. Perform a seamless domain join to join the instance to the AD domain."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. &nbsp;</p><p> Explanation:</p><p>The requirements are:</p><p>1. Highly available & fault-tolerant solution with at least 3 EC2 instances across multiple AZs.</p><p>2. Active Directory (AD) integration for domain joining and Windows ACLs for file access control.</p><p>3. Consistent content across all instances at any given time.</p><p>4. Least management overhead.</p><p>Let’s analyze the options:</p><p> Option A: Amazon EFS</p><p>- ❌ EFS is for Linux, not Windows. The question specifies a Windows EC2 instance, so EFS is not compatible.</p><p>- ❌ EFS does not natively support Windows ACLs or Active Directory integration.</p><p> Option B: Amazon FSx for Lustre</p><p>- ❌ FSx for Lustre is optimized for high-performance computing (HPC) and Linux, not Windows.</p><p>- ❌ Does not natively support Windows ACLs or seamless AD integration.</p><p> Option C: Amazon FSx for Windows File Server</p><p>- ✅ FSx for Windows File Server is fully compatible with Windows and supports Active Directory integration.</p><p>- ✅ Provides Windows ACLs for file access control.</p><p>- ✅ Auto Scaling ensures high availability across multiple AZs.</p><p>- ✅ Seamless domain join simplifies AD integration.</p><p>- ✅ Shared storage ensures all instances have the same content at all times.</p><p>- ✅ Least management overhead because FSx handles replication, backups, and high availability.</p><p> Option D: Amazon EFS (Again)</p><p>- ❌ Same issues as Option A: EFS is for Linux, not Windows, and does not support Windows ACLs or AD.</p><p> Conclusion:</p><p>Option C (Amazon FSx for Windows File Server) is the correct choice because it meets all requirements:</p><p>- Windows-compatible shared storage.</p><p>- Active Directory integration with seamless domain join.</p><p>- Windows ACL support.</p><p>- Automatic replication and high availability.</p><p>- Minimal management overhead.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "db5fb7d9900b4b22ab51f74b7b59eeb4",
            "questionNumber": 140,
            "type": "single",
            "content": "<p>Question #140</p><p>A software as a service (SaaS) based company provides a case management solution to customers. The company uses a standalone Simple Mail Transfer Protocol (SMTP) server to send email messages from an application. The application also stores an email template for acknowledgement email messages that populate customer data before the application sends the email message to the customer. </p><p><br></p><p>The company plans to migrate this messaging functionality to the AWS Cloud and needs to minimize operational overhead. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message."
                },
                {
                    "label": "B",
                    "content": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template in an Amazon S3 bucket. Create an AWS Lambda function to retrieve the template from the S3 bucket and to merge the customer data from the application with the template. Use an SDK in the Lambda function to send the email message."
                },
                {
                    "label": "C",
                    "content": "Set up an SMTP server on Amazon EC2 instances by using an AMI from the AWS Marketplace. Store the email template in Amazon Simple Email Service (Amazon SES) with parameters for the customer data. Create an AWS Lambda function to call the SES template and to pass customer data to replace the parameters. Use the AWS Marketplace SMTP server to send the email message."
                },
                {
                    "label": "D",
                    "content": "Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Set up Amazon Simple Email Service (Amazon SES) to send email messages. Store the email template on Amazon SES with parameters for the customer data. Create an AWS Lambda function to call the SendTemplatedEmail API operation and to pass customer data to replace the parameters and the email destination.</p><p>This solution is the most cost-effective approach that meets all the requirements for the following reasons:</p><p>Amazon SES is specifically designed as a cloud-based email sending service that replaces traditional SMTP servers with minimal operational overhead. It's highly scalable and cost-effective for sending transactional emails.</p><p>Amazon SES supports template storage directly within the service, allowing the company to store their acknowledgement email templates with parameters that can be dynamically populated with customer data.</p><p>AWS Lambda provides a serverless compute service that eliminates the need to provision or manage servers. Using Lambda to call the SendTemplatedEmail API operation means the company only pays for the actual compute time used when sending emails.</p><p>The combination of SES templates and Lambda functions creates a fully managed solution that minimizes operational overhead - there are no servers to maintain, patch, or scale.</p><p>The SendTemplatedEmail API operation is specifically designed for this use case - it allows the application to pass customer data that replaces parameters in the template before sending the email to the customer.</p><p>This solution provides the most direct replacement for the company's current SMTP server and email template functionality while moving to a fully managed AWS service model that reduces operational overhead and provides cost-effective pricing based on actual usage.</p><p>Sources</p><p>Using Amazon Simple Email Service (SES) to send emails and archive them | AWS re:Post （https://repost.aws/articles/ARz6UmX4epSJ-_SmCMMtFssg/using-amazon-simple-email-service-ses-to-send-emails-and-archive-them）</p><p>Use SendTemplatedEmail with an AWS SDK - Amazon Simple Email Service （https://docs.aws.amazon.com/ses/latest/dg/ses_example_ses_SendTemplatedEmail_section.html）</p><p>Using templates to send personalized email with the Amazon SES API - Amazon Simple Email Service （https://docs.aws.amazon.com/ses/latest/dg/send-personalized-email-api.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "c6773afdb4224eb289f0658278df8089",
            "questionNumber": 141,
            "type": "single",
            "content": "<p>Question #141</p><p>A company is processing videos in the AWS Cloud by Using Amazon EC2 instances in an Auto Scaling group. It takes 30 minutes to process a video. Several EC2 instances scale in and out depending on the number of videos in an Amazon Simple Queue Service (Amazon SQS) queue.</p><p><br></p><p>The company has configured the SQS queue with a redrive policy that specifies a target dead-letter queue and a maxReceiveCount of 1. The company has set the visibility timeout for the SQS queue to 1 hour. The company has set up an Amazon CloudWatch alarm to notify the development team when there are messages in the dead-letter queue.</p><p><br></p><p>Several times during the day, the development team receives notification that messages are in the dead-letter queue and that videos have not been processed properly. An investigation finds no errors in the application logs.</p><p><br></p><p>How can the company solve this problem?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Turn on termination protection for the EC2 Instances"
                },
                {
                    "label": "B",
                    "content": "Update the visibility timeout for the SQS queue to 3 hours"
                },
                {
                    "label": "C",
                    "content": "Configure scale-in protection for the instances during processing"
                },
                {
                    "label": "D",
                    "content": "Update the redrive policy and set maxReceiveCount to 0."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Configure scale-in protection for the instances during processing</p><p>This solution addresses the root cause of the problem. Here's why:</p><p>The issue occurs because the Auto Scaling group is terminating EC2 instances while they're still processing videos. When an instance is terminated during processing:</p><p>The video processing job is interrupted</p><p>The SQS message remains invisible for the remainder of the 1-hour visibility timeout</p><p>When the visibility timeout expires, the message becomes visible again</p><p>Since maxReceiveCount is set to 1, the message is immediately sent to the dead-letter queue</p><p>This triggers the CloudWatch alarm, notifying the development team</p><p>There are no application errors in the logs because the problem isn't with the application code - it's with the infrastructure setup. The videos aren't being processed properly because the instances processing them are being terminated before completion.</p><p>By configuring scale-in protection for instances during processing, the Auto Scaling group will be prevented from terminating instances that are actively working on video processing jobs. This allows the instances to complete their current work before being terminated during scale-in events.</p><p>Scale-in protection is the most direct solution because:</p><p>It addresses the exact problem (premature instance termination)</p><p>It doesn't require changing the application code</p><p>It doesn't require modifying the queue configuration</p><p>It ensures that once an instance starts processing a video, it will be allowed to complete the job</p><p>This solution maintains the efficiency of the Auto Scaling group while preventing the specific issue that's causing videos to end up in the dead-letter queue.</p><p>Sources</p><p>Infinite retries due to exceeded SQS visibility timeout | AWS re:Post （https://repost.aws/questions/QUOGzGulGSS1-wtgDIj22USw/infinite-retries-due-to-exceeded-sqs-visibility-timeout）</p><p>Processing messages in a timely manner in Amazon SQS - Amazon Simple Queue Service （https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/best-practices-processing-messages-timely-manner.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a08c2524928d457882718637123874fb",
            "questionNumber": 142,
            "type": "single",
            "content": "<p>Question #142</p><p>A company has developed APIs that use Amazon API Gateway with Regional endpoints. The APIs call AWS Lambda functions that use API Gateway authentication mechanisms. After a design review, a solutions architect identifies a set of APIs that do not require public access.</p><p><br></p><p>The solutions architect must design a solution to make the set of APIs accessible only from a VPC. All APIs need to be called with an authenticated user.</p><p><br></p><p>Which solution will meet these requirements with the LEAST amount of effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an internal Application Load Balancer (ALB). Create a target group. Select the Lambda function to call. Use the ALB DNS name to call the API from the VPC."
                },
                {
                    "label": "B",
                    "content": "Remove the DNS entry that is associated with the API in API Gateway. Create a hosted zone in Amazon Route 53. Create a CNAME record in the hosted zone. Update the API in API Gateway with the CNAME record. Use the CNAME record to call the API from the VPC."
                },
                {
                    "label": "C",
                    "content": "Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPC. Create a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC."
                },
                {
                    "label": "D",
                    "content": "Deploy the Lambda functions inside the VPC. Provision an EC2 instance, and install an Apache server. From the Apache server, call the Lambda functions. Use the internal CNAME record of the EC2 instance to call the API from the VPC."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Update the API endpoint from Regional to private in API Gateway. Create an interface VPC endpoint in the VPC. Create a resource policy, and attach it to the API. Use the VPC endpoint to call the API from the VPC.</p><p>This solution meets the requirements with the least amount of effort because:</p><p>Changing the API endpoint type from Regional to private is a straightforward configuration change in API Gateway that restricts access to the API to only be accessible through VPC endpoints.</p><p>Creating an interface VPC endpoint for API Gateway is a simple process that establishes a private connection between your VPC and API Gateway service without requiring public internet access.</p><p>Adding a resource policy to the API allows you to define specific access controls, ensuring that the API can only be called from the specified VPC endpoint.</p><p>This approach maintains the existing authentication mechanisms that are already in place for the APIs, as specified in the requirements.</p><p>The solution requires minimal changes to the existing architecture and doesn't require modifying the Lambda functions or deploying additional infrastructure.</p><p>Option D (deploying Lambda functions inside the VPC with an EC2 instance running Apache) would require significantly more effort because:</p><p>It would require moving Lambda functions into the VPC</p><p>It would need provisioning and maintaining an EC2 instance</p><p>It would require installing and configuring Apache server</p><p>It would introduce an unnecessary proxy layer between the API calls and Lambda functions</p><p>It would be more complex to manage and maintain over time</p><p>The private API Gateway endpoint with VPC endpoint approach is a purpose-built solution for this exact requirement, making it the option that requires the least amount of effort while still meeting all the security and access requirements.</p><p>Sources</p><p>Prioritize API security - AWS Serverless Multi-Tier Architectures with Amazon API Gateway and AWS Lambda （https://docs.aws.amazon.com/whitepapers/latest/serverless-multi-tier-architectures-api-gateway-lambda/prioritize-api-security.html）</p><p>Tutorial: Create a private REST API - Amazon API Gateway （https://docs.aws.amazon.com/apigateway/latest/developerguide/private-api-tutorial.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "051bf6ca0f5e412cbeec45d981d73449",
            "questionNumber": 143,
            "type": "multiple",
            "content": "<p>Question #143</p><p>A weather service provides high-resolution weather maps from a web application hosted on AWS in the eu-west-1 Region. The weather maps are updated frequently and stored in Amazon S3 along with static HTML content. The web application is fronted by Amazon CloudFront.</p><p>The company recently expanded to serve users in the us-east-1 Region, and these new users report that viewing their respective weather maps is slow from time to time.</p><p>Which combination of steps will resolve the us-east-1 performance issues? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the AWS Global Accelerator endpoint for the S3 bucket in eu-west-1. Configure endpoint groups for TCP ports 80 and 443 in us-east-1."
                },
                {
                    "label": "B",
                    "content": "Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1."
                },
                {
                    "label": "C",
                    "content": "Use Lambda@Edge to modify requests from North America to use the S3 Transfer Acceleration endpoint in us-east-1."
                },
                {
                    "label": "D",
                    "content": "Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1."
                },
                {
                    "label": "E",
                    "content": "<p> Configure the AWS Global Accelerator endpoint for us-east-1 as an origin on the CloudFront distribution. Use Lambda@Edge to modify requests from North America to use the new origin.</p>"
                }
            ],
            "correctAnswer": "BD",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>B. Create a new S3 bucket in us-east-1. Configure S3 cross-Region replication to synchronize from the S3 bucket in eu-west-1.</p><p>D. Use Lambda@Edge to modify requests from North America to use the S3 bucket in us-east-1.</p><p>These two steps together provide the most effective solution to resolve the performance issues for users in the us-east-1 Region:</p><p>Creating a new S3 bucket in us-east-1 with cross-Region replication (option B) addresses the fundamental issue by:</p><p>Storing the weather maps physically closer to the us-east-1 users</p><p>Using S3 cross-Region replication to automatically keep the content synchronized between regions</p><p>Ensuring that the frequently updated weather maps are available in both regions with minimal delay</p><p>Using Lambda@Edge to modify requests based on user location (option D) complements the first step by:</p><p>Intelligently routing users from North America to the us-east-1 S3 bucket</p><p>Ensuring that European users continue to access the eu-west-1 bucket</p><p>Providing dynamic request routing without changing the CloudFront distribution configuration</p><p>Maintaining a single CloudFront distribution while serving content from multiple origins</p><p>This combination provides a complete solution because:</p><p>The data is physically located closer to the users who need it (through replication)</p><p>The routing logic ensures users are directed to the closest data source (through Lambda@Edge)</p><p>The solution maintains the existing CloudFront distribution while improving performance</p><p>Option A (configuring AWS Global Accelerator) would not be as effective because:</p><p>Global Accelerator is more suitable for applications that need static IP addresses or for non-HTTP/S traffic</p><p>CloudFront is already optimized for content delivery of static assets</p><p>Global Accelerator wouldn't address the core issue of the content being physically distant from the users</p><p>The combination of regional S3 buckets with cross-Region replication and Lambda@Edge for intelligent routing provides the most comprehensive solution to the performance issues.</p><p>Sources</p><p>How can I optimize data transfer costs between Amazon S3 and EC2 instances across different AWS Regions while ensuring compliance with data residency requirements? | AWS re:Post （https://repost.aws/questions/QURehgqhADRRiJVHNoTykw6w/how-can-i-optimize-data-transfer-costs-between-amazon-s3-and-ec2-instances-across-different-aws-regions-while-ensuring-compliance-with-data-residency-requirements）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "e13302ce04984fddb8253a285b555e95",
            "questionNumber": 144,
            "type": "single",
            "content": "<p>Question #144</p><p>A solutions architect is investigating an issue in which a company cannot establish new sessions in Amazon WorkSpaces. An initial analysis indicates that the issue involves user profiles. The Amazon WorkSpaces environment is configured to use Amazon FSx for Windows File Server as the profile share storage. The FSx for Windows File Server file system is configured with 10 TB of storage.</p><p><br></p><p>The solutions architect discovers that the file system has reached its maximum capacity. The solutions architect must ensure that users can regain access. The solution also must prevent the problem from occurring again.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Remove old user profiles to create space. Migrate the user profiles to an Amazon FSx for Lustre file system."
                },
                {
                    "label": "B",
                    "content": "Increase capacity by using the update-file-system command. Implement an Amazon CloudWatch metric that monitors free space. Use Amazon EventBridge to invoke an AWS Lambda function to increase capacity as required."
                },
                {
                    "label": "C",
                    "content": "Monitor the file system by using the FreeStorageCapacity metric in Amazon CloudWatch. Use AWS Step Functions to increase the capacity as required."
                },
                {
                    "label": "D",
                    "content": "Remove old user profiles to create space. Create an additional FSx for Windows File Server file system. Update the user profile redirection for 50% of the users to use the new file system."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation:</p><p>The problem is that the FSx for Windows File Server has reached its 10 TB maximum capacity, preventing new Workspaces sessions. The solution must: &nbsp;</p><p>1. Immediately free up space to restore access. &nbsp;</p><p>2. Prevent future capacity issues by automating storage scaling. &nbsp;</p><p>Let’s analyze the options: &nbsp;</p><p> Option A: Migrate to FSx for Lustre &nbsp;</p><p>- ❌ FSx for Lustre is not compatible with Windows user profiles (it’s designed for high-performance Linux workloads). &nbsp;</p><p>- ❌ Simply deleting old profiles does not prevent future capacity issues. &nbsp;</p><p> Option B: Increase Capacity + Automate Scaling &nbsp;</p><p>- ✅ `update-file-system` allows increasing FSx storage capacity without downtime. &nbsp;</p><p>- ✅ CloudWatch metric (`FreeStorageCapacity`) monitors free space. &nbsp;</p><p>- ✅ EventBridge + Lambda can automatically scale storage when thresholds are met. &nbsp;</p><p>- ✅ Immediate fix: Free up space manually (if needed) while automation prevents future issues. &nbsp;</p><p> Option C: CloudWatch + Step Functions &nbsp;</p><p>- ❌ While monitoring is correct, Step Functions is overkill for this use case (Lambda is simpler and more cost-effective). &nbsp;</p><p>- ❌ Does not mention immediate remediation (like deleting old profiles first). &nbsp;</p><p> Option D: Add a Second FSx File System &nbsp;</p><p>- ❌ Splitting profiles across two file systems adds complexity and does not solve the root issue (storage scaling). &nbsp;</p><p>- ❌ Manual intervention is required to balance users, which is not scalable. &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- Short-term: Delete old profiles if needed to regain access. &nbsp;</p><p>- Long-term: Automate scaling using CloudWatch + Lambda to prevent future outages. &nbsp;</p><p>- FSx for Windows supports scaling up to 64 TB, so this is a sustainable solution. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "67d493b8f3a04c5a9b1acaa61c911d00",
            "questionNumber": 145,
            "type": "single",
            "content": "<p>Question #145</p><p>An international delivery company hosts a delivery management system on AWS. Drivers use the system to upload confirmation of delivery. Confirmation includes the recipient’s signature or a photo of the package with the recipient. The driver’s handheld device uploads signatures and photos through FTP to a single Amazon EC2 instance. Each handheld device saves a file in a directory based on the signed-in user, and the filename matches the delivery number. The EC2 instance then adds metadata to the file after querying a central database to pull delivery information. The file is then placed in Amazon S3 for archiving. </p><p><br></p><p>As the company expands, drivers report that the system is rejecting connections. The FTP server is having problems because of dropped connections and memory issues in response to these problems, a system engineer schedules a cron task to reboot the EC2 instance every 30 minutes. The billing team reports that files are not always in the archive and that the central system is not always updated. A solutions architect needs to design a solution that maximizes scalability to ensure that the archive always receives the files and that systems are always updated. </p><p><br></p><p>The handheld devices cannot be modified, so the company cannot deploy a new application.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AMI of the existing EC2 instance. Create an Auto Scaling group of EC2 instances behind an Application Load Balancer. Configure the Auto Scaling group to have a minimum of three instances."
                },
                {
                    "label": "B",
                    "content": "Use AWS Transfer Family to create an FTP server that places the files in Amazon Elastic File System (Amazon EFS). Mount the EFS volume to the existing EC2 instance. Point the EC2 instance to the new path for file processing."
                },
                {
                    "label": "C",
                    "content": "Use AWS Transfer Family to create an FTP server that places the files in Amazon S3. Use an S3 event notification through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system."
                },
                {
                    "label": "D",
                    "content": "Update the handheld devices to place the files directly in Amazon S3. Use an S3 event notification through Amazon Simple Queue Service (Amazon SQS) to invoke an AWS Lambda function. Configure the Lambda function to add the metadata and update the delivery system."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is correct. Using AWS Transfer Family to create an FTP server that places the files directly in Amazon S3 and using S3 event notifications through Amazon SNS to invoke an AWS Lambda function will ensure that the archive always receives the files and that the central system is always updated. This solution maximizes scalability and eliminates the need for manual intervention, such as rebooting the EC2 instance.</p><p>The correct answer is C. &nbsp;</p><p> Explanation:</p><p>The key requirements are: &nbsp;</p><p>1. Maximize scalability to handle increasing FTP connections. &nbsp;</p><p>2. Ensure files are always archived in Amazon S3. &nbsp;</p><p>3. Update the central system reliably with metadata. &nbsp;</p><p>4. Cannot modify the handheld devices, so the FTP process must remain unchanged. &nbsp;</p><p>Let’s analyze the options: &nbsp;</p><p> Option A: Auto Scaling Group with ALB &nbsp;</p><p>- ❌ FTP is stateful and does not work well with load balancers (ALB is for HTTP/HTTPS, not FTP). &nbsp;</p><p>- ❌ Rebooting instances (current issue) won’t be fixed—this just spreads the problem across multiple instances. &nbsp;</p><p>- ❌ No improvement in reliability for file processing or metadata updates. &nbsp;</p><p> Option B: AWS Transfer Family + EFS &nbsp;</p><p>- ❌ EFS adds unnecessary complexity—files still need processing by the EC2 instance, which is the bottleneck. &nbsp;</p><p>- ❌ Does not solve the core issue (scalability and reliability of file processing). &nbsp;</p><p>- ❌ EC2 instance remains a single point of failure for metadata updates. &nbsp;</p><p> Option C: AWS Transfer Family + S3 + Lambda &nbsp;</p><p>- ✅ AWS Transfer Family replaces the EC2 FTP server, providing a fully managed, scalable FTP solution. &nbsp;</p><p>- ✅ Files go directly to S3, eliminating the EC2 bottleneck. &nbsp;</p><p>- ✅ S3 event notifications trigger Lambda, which reliably adds metadata and updates the system (no lost files). &nbsp;</p><p>- ✅ No changes needed to handheld devices (still use FTP). &nbsp;</p><p>- ✅ Fully serverless and scalable—no EC2 instance to manage or reboot. &nbsp;</p><p> Option D: Update Handheld Devices to Use S3 Directly &nbsp;</p><p>- ❌ Violates the requirement (handheld devices cannot be modified). &nbsp;</p><p>- ❌ Not a feasible solution since the question explicitly rules out changes to the devices. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- Scalable FTP: AWS Transfer Family handles FTP connections without EC2 limitations. &nbsp;</p><p>- Reliable processing: S3 + Lambda ensures no files are lost and metadata is always added. &nbsp;</p><p>- No changes to handheld devices—maintains compatibility. &nbsp;</p><p>- Fully managed solution—eliminates EC2 maintenance (no more reboots). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "9322f4b10a484ee083c8c147199c7cb9",
            "questionNumber": 146,
            "type": "single",
            "content": "<p>Question #146</p><p>A company is running an application in the AWS Cloud. The application runs on containers in an Amazon Elastic Container Service (Amazon ECS) cluster. The ECS tasks use the Fargate launch type. The application's data is relational and is stored in Amazon Aurora MySQL. To meet regulatory requirements, the application must be able to recover to a separate AWS Region in the event of an application failure. In case of a failure, no data can be lost.</p><p><br></p><p>Which solution will meet these requirements with the LEAST amount of operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Provision an Aurora Replica in a different Region."
                },
                {
                    "label": "B",
                    "content": "Set up AWS DataSync for continuous replication of the data to a different Region."
                },
                {
                    "label": "C",
                    "content": "Set up AWS Database Migration Service (AWS DMS) to perform a continuous replication of the data to a different Region."
                },
                {
                    "label": "D",
                    "content": "Use Amazon Data Lifecycle Manager (Amazon DLM) to schedule a snapshot every 5 minutes."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Provision an Aurora Replica in a different Region.</p><p>This solution meets the requirements with the least amount of operational overhead for the following reasons:</p><p>Zero data loss requirement: Aurora cross-region replicas provide continuous, asynchronous replication with typical lag times of less than a second. This ensures that virtually no data is lost in the event of a primary region failure.</p><p>Recovery to a separate AWS Region: By provisioning an Aurora Replica in a different region, the company creates a standby database that can be promoted to become the primary database in case of a regional outage or application failure.</p><p>Least operational overhead: Aurora Replicas are fully managed by AWS, requiring minimal setup and ongoing maintenance. The replication process is handled automatically by the Aurora service.</p><p>The other options have significant drawbacks:</p><p>B. AWS DataSync is primarily designed for file-based data transfer between storage systems, not for continuous database replication. It would require additional components and custom scripting to implement a database disaster recovery solution, increasing operational overhead.</p><p>C. AWS Database Migration Service (AWS DMS) can perform continuous replication, but it requires more configuration and monitoring compared to native Aurora replication. It would involve setting up and maintaining replication instances, tasks, and endpoints, resulting in higher operational overhead.</p><p>D. Amazon Data Lifecycle Manager (Amazon DLM) is designed for managing EBS snapshots and AMIs, not database replication. Using snapshots every 5 minutes would not meet the zero data loss requirement, as any data written between snapshots could be lost. Additionally, restoring from snapshots would result in longer recovery times.</p><p>Aurora's built-in cross-region replication capability provides the most straightforward and efficient solution for meeting the company's disaster recovery requirements with minimal operational overhead.</p><p>Sources</p><p>Comparing Amazon Aurora Replicas, Aurora cross-Region Replicas, and Aurora global databases - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/aurora-replication-options/introduction.html）</p><p>Disaster recovery options in the cloud - Disaster Recovery of Workloads on AWS: Recovery in the Cloud （https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html）</p><p>AWS Backup vs global table/database for cost-effective RTO,RPO requirements | AWS re:Post （https://repost.aws/questions/QUWf0GuonKSvW8U2N4o8huHw/aws-backup-vs-global-table-database-for-cost-effective-rto-rpo-requirements）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7597ebd2dd3f41e8869777dc5776a166",
            "questionNumber": 147,
            "type": "single",
            "content": "<p>Question #147</p><p>A financial services company receives a regular data feed from its credit card servicing partner. Approximately 5,000 records are sent every 15 minutes in plaintext, delivered over HTTPS directly into an Amazon S3 bucket with server-side encryption. This feed contains sensitive credit card primary account number (PAN) data. The company needs to automatically mask the PAN before sending the data to another S3 bucket for additional internal processing. The company also needs to remove and merge specific fields, and then transform the record into JSON format. Additionally, extra feeds are likely to be added in the future, so any design needs to be easily expandable.</p><p><br></p><p>Which solutions will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Invoke another Lambda function when new messages arrive in the SQS queue to process the records, writing the results to a temporary location in Amazon S3. Invoke a final Lambda function once the SQS queue is empty to transform the records into JSON format and send the results to another S3 bucket for internal processing."
                },
                {
                    "label": "B",
                    "content": "Invoke an AWS Lambda function on file delivery that extracts each record and writes it to an Amazon SQS queue. Configure an AWS Fargate container application to automatically scale to a single instance when the SQS queue contains messages. Have the application process each record, and transform the record into JSON format. When the queue is empty, send the results to another S3 bucket for internal processing and scale down the AWS Fargate instance."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Glue crawler and custom classifier based on the data feed formats and build a table definition to match. Invoke an AWS Lambda function on file delivery to start an AWS Glue ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, have the ETL job send the results to another S3 bucket for internal processing."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Glue crawler and custom classifier based upon the data feed formats and build a table definition to match. Perform an Amazon Athena query on file delivery to start an Amazon EMR ETL job to transform the entire record according to the processing and transformation requirements. Define the output format as JSON. Once complete, send the results to another S3 bucket for internal processing and scale down the EMR cluster."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Option C is correct. It will process the data in batch mode using Glue ETL job which can handle large amount of data and can be scheduled to run periodically. This solution is also easily expandable for future feeds.The requirements are: &nbsp;</p><p>1. Automatically mask PAN data (sensitive credit card info). &nbsp;</p><p>2. Remove, merge, and transform records into JSON. &nbsp;</p><p>3. Handle ~5,000 records every 15 minutes (scalable). &nbsp;</p><p>4. Easily expandable for future feeds. &nbsp;</p><p> Analysis of Options:</p><p> Option A: Lambda + SQS + Lambda + Lambda &nbsp;</p><p>- ❌ Overly complex with multiple Lambda functions and manual orchestration. &nbsp;</p><p>- ❌ No built-in data transformation logic (masking, merging fields, JSON conversion). &nbsp;</p><p>- ❌ Error-prone—if Lambda fails mid-process, records may be lost or duplicated. &nbsp;</p><p> Option B: Lambda + SQS + Fargate &nbsp;</p><p>- ❌ Fargate adds unnecessary overhead—Lambda is better for lightweight transformations. &nbsp;</p><p>- ❌ Manual scaling of Fargate is inefficient for batch processing. &nbsp;</p><p>- ❌ No built-in data masking/transformation—custom code required. &nbsp;</p><p> Option C: AWS Glue (Crawler + ETL Job) + Lambda &nbsp;</p><p>- ✅ AWS Glue is purpose-built for ETL (masking PAN, merging fields, converting to JSON). &nbsp;</p><p>- ✅ Serverless and scalable—handles 5,000 records every 15 minutes easily. &nbsp;</p><p>- ✅ Glue Crawler auto-discovers schema, making it expandable for future feeds. &nbsp;</p><p>- ✅ Lambda triggers Glue ETL on new file arrival (fully automated). &nbsp;</p><p>- ✅ Outputs directly to S3 in JSON format. &nbsp;</p><p> Option D: Glue + Athena + EMR &nbsp;</p><p>- ❌ Overkill for 5,000 records—EMR is for big data (petabyte-scale). &nbsp;</p><p>- ❌ Athena is for querying, not ETL—adds unnecessary steps. &nbsp;</p><p>- ❌ EMR requires cluster management, increasing cost and complexity. &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- AWS Glue provides out-of-the-box ETL (masking, transformations, JSON conversion). &nbsp;</p><p>- Fully serverless—no infrastructure to manage. &nbsp;</p><p>- Scalable and cost-effective for batch processing. &nbsp;</p><p>- Easily expandable—just add new crawlers/ETL jobs for future feeds. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "68625a4ba3aa40a6ab9c8eee4b9490e4",
            "questionNumber": 148,
            "type": "single",
            "content": "Question #148<p>A company wants to use AWS to create a business continuity solution in case the company's main on-premises application fails. The application runs on physical servers that also run other applications. The on-premises application that the company is planning to migrate uses a MySQL database as a data store. All the company's on-premises applications use operating systems that are compatible with Amazon EC2.</p><p>Which solution will achieve the company's goal with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Set up replication for all servers. Launch test instances for regular drills. Cut over to the test instances to fail over the workload in the case of a failure event."
                },
                {
                    "label": "B",
                    "content": "Install the AWS Replication Agent on the source servers, including the MySQL servers. Initialize AWS Elastic Disaster Recovery in the target AWS Region. Define the launch settings. Frequently perform failover and fallback from the most recent point in time."
                },
                {
                    "label": "C",
                    "content": "Create AWS Database Migration Service (AWS DMS) replication servers and a target Amazon Aurora MySQL DB cluster to host the database. Create a DMS replication task to copy the existing data to the target DB cluster. Create a local AWS Schema Conversion Tool (AWS SCT) change data capture (CDC) task to keep the data synchronized. Install the rest of the software on EC2 instances by starting with a compatible base AMI."
                },
                {
                    "label": "D",
                    "content": "Deploy an AWS Storage Gateway Volume Gateway on premises. Mount volumes on all on-premises servers. Install the application and the MySQL database on the new volumes. Take regular snapshots. Install all the software on EC2 Instances by starting with a compatible base AMI. Launch a Volume Gateway on an EC2 instance. Restore the volumes from the latest snapshot. Mount the new volumes on the EC2 instances in the case of a failure event."
                }
            ],
            "correctAnswer": "B",
            "explanation": "Tricky one. This is not an on premise migration use case which prompts for answer C. Its a current situation of on premise application which the <p>company wants to continue its state in the requirement of using AWS as DR solution. </p><p>https://docs.aws.amazon.com/images/drs/latest/userguide/images/drs-failback-arc.png </p><p>https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html</p><p><br></p><p>1. Use AWS Database Migration Service (DMS) to replicate the on-premises MySQL database to Amazon RDS for MySQL. This provides a managed database solution with minimal operational overhead.</p><p>2. Set up AWS Application Discovery Service to analyze the on-premises application and its dependencies. This will help in planning the migration and identifying any potential issues.</p><p>3. Use AWS Server Migration Service (SMS) to replicate the application servers to Amazon EC2 instances. This service can handle the migration of multiple servers simultaneously and supports incremental replication.</p><p>4. Configure Amazon Route 53 for DNS failover. This will allow automatic redirection of traffic to the AWS environment in case of a failure in the on-premises infrastructure.</p><p>5. Implement AWS CloudWatch for monitoring both the on-premises and AWS environments. Set up alarms to notify of any issues or failures.</p><p>6. Use AWS Systems Manager to manage and configure the EC2 instances, reducing operational overhead for server management.</p><p>This solution offers several benefits:</p><p>- Minimal operational overhead: Amazon RDS manages the database, while EC2 instances can be easily managed using Systems Manager.</p><p>- Automated failover: Route 53 can automatically redirect traffic in case of a failure.</p><p>- Scalability: The AWS environment can be easily scaled up or down as needed.</p><p>- Continuous replication: DMS and SMS ensure that the AWS environment stays up-to-date with the on-premises systems.</p><p>Best practices to consider:</p><p><br></p><p>- Regularly test the failover process to ensure business continuity.</p><p>- Implement proper security measures, such as using security groups and network ACLs to control access to AWS resources.</p><p>- Use AWS Identity and Access Management (IAM) to manage permissions and access to AWS services.</p><p>- Encrypt data at rest and in transit using AWS Key Management Service (KMS) and SSL/TLS.</p><p><br></p><p>[1] [Amazon RDS for MySQL Resources – AWS] (https://aws.amazon.com/rds/mysql/resources/)</p><p>[3] [Amazon RDS for MySQL Resources – AWS] (https://aws.amazon.com/rds/mysql/resources/)</p><p>[6] [Conclusion - Optimizing MySQL Running on Amazon EC2 Using Amazon EBS] (https://docs.aws.amazon.com/whitepapers/latest/optimizing-mysql-on-ec2-using-amazon-ebs/conclusion.html)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "77b13aaabcff4a05af7d60adfe4bbca5",
            "questionNumber": 149,
            "type": "single",
            "content": "<p>Question #149</p><p>A company is subject to regulatory audits of its financial information. External auditors who use a single AWS account need access to the company's AWS account. A solutions architect must provide the auditors with secure, read-only access to the company's AWS account. The solution must comply with AWS security best practices.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "In the company&#39;s AWS account, create resource policies for all resources in the account to grant access to the auditors&#39; AWS account. Assign a unique external ID to the resource policy."
                },
                {
                    "label": "B",
                    "content": "In the company&#39;s AWS account, create an IAM role that trusts the auditors&#39; AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role&#39;s trust policy."
                },
                {
                    "label": "C",
                    "content": "In the company&#39;s AWS account, create an IAM user. Attach the required IAM policies to the IAM user. Create API access keys for the IAM user. Share the access keys with the auditors."
                },
                {
                    "label": "D",
                    "content": "In the company&#39;s AWS account, create an IAM group that has the required permissions. Create an IAM user in the company&#39;s account for each auditor. Add the IAM users to the IAM group."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. In the company's AWS account, create an IAM role that trusts the auditors' AWS account. Create an IAM policy that has the required permissions. Attach the policy to the role. Assign a unique external ID to the role's trust policy.</p><p>This solution meets the requirements and follows AWS security best practices for the following reasons:</p><p>Cross-account IAM roles are the recommended AWS best practice for providing secure access to external entities. This approach allows the auditors to assume a role in the company's account without needing to create and manage separate IAM users.</p><p>By creating an IAM policy with read-only permissions and attaching it to the role, the company can precisely control what resources and actions the auditors can access, following the principle of least privilege.</p><p>The trust relationship between the company's role and the auditors' AWS account establishes a secure delegation mechanism that allows users in the auditors' account to assume the role.</p><p>Including a unique external ID in the role's trust policy adds an additional layer of security by preventing the \"confused deputy\" problem. The external ID acts as a shared secret between the company and the auditors, ensuring that only authorized entities can assume the role.</p><p>Option A (creating resource policies for all resources) is not the recommended approach because:</p><p>It would be extremely time-consuming and error-prone to create and maintain individual resource policies for all resources in the account</p><p>Not all AWS services support resource-based policies</p><p>Managing access at the resource level would be much more complex than using a role-based approach</p><p>It would be difficult to ensure consistent permissions across all resources</p><p>The IAM role approach provides a centralized, secure, and manageable solution that aligns with AWS security best practices for cross-account access.</p><p>Sources</p><p>Access to AWS accounts owned by third parties - AWS Identity and Access Management （https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html）</p><p>SEC03-BP09 Share resources securely with a third party - AWS Well-Architected Framework (2023-04-10) （https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/sec_permissions_share_securely_third_party.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f88e2ae3930f4f41bbb354d46541716f",
            "questionNumber": 150,
            "type": "single",
            "content": "<p>Question #150</p><p>A company has a latency-sensitive trading platform that uses Amazon DynamoDB as a storage backend. The company configured the DynamoDB table to use on-demand capacity mode. A solutions architect needs to design a solution to improve the performance of the trading platform. The new solution must ensure high availability for the trading platform.</p><p><br></p><p>Which solution will meet these requirements with the LEAST latency?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a two-node DynamoDB Accelerator (DAX) cluster. Configure an application to read and write data by using DAX."
                },
                {
                    "label": "B",
                    "content": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table."
                },
                {
                    "label": "C",
                    "content": "Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data directly from the DynamoDB table and to write data by using DAX."
                },
                {
                    "label": "D",
                    "content": "Create a single-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create a three-node DynamoDB Accelerator (DAX) cluster. Configure an application to read data by using DAX and to write data directly to the DynamoDB table.</p><p>This solution will meet the requirements with the least latency while ensuring high availability for the following reasons:</p><p>High Availability: A three-node DAX cluster provides better fault tolerance than a two-node cluster. With three nodes distributed across different Availability Zones, the cluster can continue to operate even if one node fails, ensuring the high availability requirement is met for the trading platform.</p><p>Read Latency: DAX significantly reduces read latency for DynamoDB by caching frequently accessed data. For a trading platform that likely performs many read operations to retrieve market data, prices, and order information, this caching layer can reduce read latency from milliseconds to microseconds.</p><p>Write Consistency: By writing directly to the DynamoDB table rather than through DAX, the solution ensures that all write operations are immediately consistent. This is crucial for a trading platform where data accuracy is critical (e.g., order placements, trade executions).</p><p>Overall System Latency: While writing directly to DynamoDB might seem counterintuitive for reducing latency, it actually provides the best balance of low latency and data consistency. DAX is primarily designed as a read-through cache, and its write-through capabilities can introduce additional complexity and potential consistency issues.</p><p>Option A (two-node DAX cluster with both read and write operations through DAX) has two significant drawbacks:</p><p>Lower availability: A two-node cluster doesn't provide the same level of fault tolerance as a three-node cluster. If one node fails, you're left with a single node, creating a single point of failure.</p><p>Potential consistency issues: Writing through DAX can introduce eventual consistency challenges that could be problematic for a latency-sensitive trading platform where accurate, up-to-date data is critical.</p><p>For a trading platform where both low latency and high availability are essential requirements, a three-node DAX cluster with reads through DAX and writes directly to DynamoDB provides the optimal solution.</p><p>Sources</p><p>Amazon DynamoDB Accelerator (DAX) （https://aws.amazon.com/cn/dynamodbaccelerator/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "61341cb40b83470493a346e2c4c015d6",
            "questionNumber": 151,
            "type": "multiple",
            "content": "<p>Question #151</p><p>A company has migrated an application from on premises to AWS. The application frontend is a static website that runs on two Amazon EC2 instances behind an Application Load Balancer (ALB). The application backend is a Python application that runs on three EC2 instances behind another ALB. The EC2 instances are large, general purpose On-Demand Instances that were sized to meet the on-premises specifications for peak usage of the application. </p><p><br></p><p>The application averages hundreds of thousands of requests each month. However, the application is used mainly during lunchtime and receives minimal traffic during the rest of the day. </p><p><br></p><p>A solutions architect needs to optimize the infrastructure cost of the application without negatively affecting the application availability. </p><p><br></p><p>Which combination of steps will meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Change all the EC2 instances to compute optimized instances that have the same number of cores as the existing EC2 instances."
                },
                {
                    "label": "B",
                    "content": "Move the application frontend to a static website that is hosted on Amazon S3."
                },
                {
                    "label": "C",
                    "content": "Deploy the application frontend by using AWS Elastic Beanstalk. Use the same instance type for the nodes."
                },
                {
                    "label": "D",
                    "content": "Change all the backend EC2 instances to Spot Instances."
                },
                {
                    "label": "E",
                    "content": "Deploy the backend Python application to general purpose burstable EC2 instances that have the same number of cores as the existing EC2 instances."
                }
            ],
            "correctAnswer": "BE",
            "explanation": "<p>The correct answers are B and E. &nbsp;</p><p> Explanation: &nbsp;</p><p>The requirements are: &nbsp;</p><p>1. Optimize costs (current setup is over-provisioned for peak usage). &nbsp;</p><p>2. Maintain availability (no negative impact during lunchtime traffic spikes). &nbsp;</p><p>3. Application has bursty traffic (high usage at lunchtime, minimal otherwise). &nbsp;</p><p> Analysis of Options: &nbsp;</p><p> Option A: Change to Compute Optimized Instances &nbsp;</p><p>- ❌ Compute optimized instances are more expensive and unnecessary for a Python backend (not CPU-bound). &nbsp;</p><p>- ❌ Does not address cost optimization—just changes instance type without reducing capacity. &nbsp;</p><p> Option B: Move Frontend to Amazon S3 (Static Website Hosting) &nbsp;</p><p>- ✅ Eliminates EC2 costs for the frontend (S3 is cheaper and scales infinitely). &nbsp;</p><p>- ✅ Static websites are highly available (no servers to manage). &nbsp;</p><p>- ✅ Reduces cost without affecting performance (frontend is static content). &nbsp;</p><p> Option C: Deploy Frontend with Elastic Beanstalk &nbsp;</p><p>- ❌ Still uses EC2 instances, so no cost savings over current setup. &nbsp;</p><p>- ❌ Overkill for a static website (S3 is simpler and cheaper). &nbsp;</p><p> Option D: Use Spot Instances for Backend &nbsp;</p><p>- ❌ Spot Instances can be interrupted, risking availability during lunchtime peaks. &nbsp;</p><p>- ❌ Not ideal for bursty workloads where consistent uptime is critical. &nbsp;</p><p> Option E: Use Burstable Instances (e.g., T3/T4g) for Backend &nbsp;</p><p>- ✅ Burstable instances (T-series) are cost-effective for variable workloads. &nbsp;</p><p>- ✅ CPU credits handle lunchtime spikes, then scale down during low traffic. &nbsp;</p><p>- ✅ Same vCPU count ensures no performance degradation. &nbsp;</p><p> Why B + E? &nbsp;</p><p>- Frontend (Option B): Moving to S3 removes unnecessary EC2 costs. &nbsp;</p><p>- Backend (Option E): Burstable instances optimize costs while handling traffic spikes. &nbsp;</p><p>Avoid: &nbsp;</p><p>- Compute optimized (A) → Overkill for Python backend. &nbsp;</p><p>- Spot Instances (D) → Risk interruptions during peak usage. &nbsp;</p><p>- Elastic Beanstalk (C) → No cost benefit for static content. &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "282d5e4c481347fcb52b0e04206ee397",
            "questionNumber": 152,
            "type": "single",
            "content": "Question #152<p>A company is running an event ticketing platform on AWS and wants to optimize the platform's cost-effectiveness. The platform is deployed on Amazon Elastic Kubernetes Service (Amazon EKS) with Amazon EC2 and is backed by an Amazon RDS for MySQL DB instance. The company is developing new application features to run on Amazon EKS with AWS Fargate. <br><br>The platform experiences infrequent high peaks in demand. The surges in demand depend on event dates.</p><p><br></p><p>Which solution will provide the MOST cost-effective setup for the platform?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Purchase Standard Reserved Instances for the EC2 instances that the EKS cluster uses in its baseline load. Scale the cluster with Spot Instances to handle peaks. Purchase 1-year All Upfront Reserved Instances for the database to meet predicted peak load for the year."
                },
                {
                    "label": "B",
                    "content": "Purchase Compute Savings Plans for the predicted medium load of the EKS cluster. Scale the cluster with On-Demand Capacity Reservations based on event dates for peaks. Purchase 1-year No Upfront Reserved Instances for the database to meet the predicted base load. Temporarily scale out database read replicas during peaks."
                },
                {
                    "label": "C",
                    "content": "Purchase EC2 Instance Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks."
                },
                {
                    "label": "D",
                    "content": "Purchase Compute Savings Plans for the predicted base load of the EKS cluster. Scale the cluster with Spot Instances to handle peaks."
                }
            ],
            "correctAnswer": "B",
            "explanation": "Option B is correct. Compute saving plan will also cover Fargate. A spot instance is not reliable for a production server, and the company is developing new application features to run on AWS Fargate, which means we must plan the future cost improvement including AWS Fargate.",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "3ba0b2823f4b4b10a20281dd04159fbc",
            "questionNumber": 153,
            "type": "multiple",
            "content": "<p>Question #153</p><p>A company has deployed an application on AWS Elastic Beanstalk. The application uses Amazon Aurora for the database layer. An Amazon CloudFront distribution serves web requests and includes the Elastic Beanstalk domain name as the origin server. The distribution is configured with an alternate domain name that visitors use when they access the application. </p><p><br></p><p>Each week, the company takes the application out of service for routine maintenance. During the time that the application is unavailable, the company wants visitors to receive an informational message instead of a CloudFront error message. </p><p><br></p><p>A solutions architect creates an Amazon S3 bucket as the first step in the process. </p><p><br></p><p>Which combination of steps should the solutions architect take next to meet the requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Upload static informational content to the S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Create a new CloudFront distribution. Set the S3 bucket as the origin."
                },
                {
                    "label": "C",
                    "content": "Set the S3 bucket as a second origin in the original CloudFront distribution. Confi gure the distribution and the S3 bucket to use an origin"
                },
                {
                    "label": "D",
                    "content": "During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete."
                },
                {
                    "label": "E",
                    "content": "During the weekly maintenance, create a cache behavior for the S3 origin on the new distribution. Set the path pattern to \\ Set the precedence to 0. Delete the cache behavior when the maintenance is complete."
                },
                {
                    "label": "F",
                    "content": "During the weekly maintenance, confi gure Elastic Beanstalk to serve traffi c from the S3 bucket."
                }
            ],
            "correctAnswer": "ACD",
            "explanation": "<p>The correct answers are A, C, and D. &nbsp;</p><p> Explanation: &nbsp;</p><p>The requirements are: &nbsp;</p><p>1. Show a maintenance page during weekly downtimes (instead of CloudFront errors). &nbsp;</p><p>2. Use an existing S3 bucket (already created) to host the static maintenance page. &nbsp;</p><p>3. Avoid creating a new CloudFront distribution (use the existing one). &nbsp;</p><p> Step-by-Step Solution: &nbsp;</p><p>1. Upload static maintenance content to S3 (Option A). &nbsp;</p><p> &nbsp; - The maintenance page (HTML) must be stored in S3. &nbsp;</p><p>2. Add the S3 bucket as a secondary origin in the existing CloudFront distribution (Option C). &nbsp;</p><p> &nbsp; - Configure an Origin Access Identity (OAI) to securely serve S3 content. &nbsp;</p><p> &nbsp; - Avoids creating a new distribution (Option B is unnecessary). &nbsp;</p><p>3. During maintenance, edit the default cache behavior to point to the S3 origin (Option D). &nbsp;</p><p> &nbsp; - Temporarily switch traffic from Elastic Beanstalk to S3. &nbsp;</p><p> &nbsp; - Revert after maintenance completes. &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- Option B (New CloudFront distribution) → Unnecessary complexity. &nbsp;</p><p>- Option E (Cache behavior on a new distribution) → Overcomplicates the solution. &nbsp;</p><p>- Option F (Elastic Beanstalk serving from S3) → Not how Beanstalk works; CloudFront must handle the switch. &nbsp;</p><p> Summary of Correct Steps: &nbsp;</p><p>1. A: Upload maintenance page to S3. &nbsp;</p><p>2. C: Add S3 as a secondary origin in the existing CloudFront distribution (with OAI). &nbsp;</p><p>3. D: Temporarily switch CloudFront’s default behavior to S3 during maintenance. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "df6fee64b8e74ae2affaffc879e5117a",
            "questionNumber": 154,
            "type": "single",
            "content": "<p>Question #154</p><p>A company gives users the ability to upload images from a custom application. The upload process invokes an AWS Lambda function that processes and stores the image in an Amazon S3 bucket. The application invokes the Lambda function by using a specific function version ARN. </p><p><br></p><p>The Lambda function accepts image processing parameters by using environment variables. The company often adjusts the environment variables of the Lambda function to achieve optimal image processing output. The company tests different parameters and publishes a new function version with the updated environment variables after validating results. This update process also requires frequent changes to the custom application to invoke the new function version ARN. These changes cause interruptions for users. </p><p><br></p><p>A solutions architect needs to simplify this process to minimize disruption to users. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Directly modify the environment variables of the published Lambda function version. Use the $LATEST version to test image processing parameters."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon DynamoDB table to store the image processing parameters. Modify the Lambda function to retrieve the image processing parameters from the DynamoDB table."
                },
                {
                    "label": "C",
                    "content": "Directly code the image processing parameters within the Lambda function and remove the environment variables. Publish a new function version when the company updates the parameters."
                },
                {
                    "label": "D",
                    "content": "Create a Lambda function alias. Modify the client application to use the function alias ARN. Reconfigure the Lambda alias to point to new versions of the function when the company finishes testing."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The correct answer is D. &nbsp;</p><p> Explanation: &nbsp;</p><p>The problem is: &nbsp;</p><p>- The company frequently updates Lambda environment variables for image processing. &nbsp;</p><p>- Each update requires publishing a new version and updating the application’s ARN reference, causing disruptions. &nbsp;</p><p>- Goal: Minimize user interruptions while keeping the ability to test and deploy new parameters. &nbsp;</p><p> Analysis of Options: &nbsp;</p><p> Option A: Modify Published Version + Use `$LATEST` &nbsp;</p><p>- ❌ Modifying a published version is not allowed (immutable). &nbsp;</p><p>- ❌ `$LATEST` is unstable—changes directly impact users without testing. &nbsp;</p><p> Option B: DynamoDB for Parameters &nbsp;</p><p>- ✅ Avoids Lambda version updates, but: &nbsp;</p><p>- ❌ Adds complexity (DynamoDB table + IAM permissions). &nbsp;</p><p>- ❌ Not the simplest solution (still requires Lambda code changes). &nbsp;</p><p> Option C: Hardcode Parameters in Lambda &nbsp;</p><p>- ❌ Worse than current setup—requires redeploying code for every parameter change. &nbsp;</p><p>- ❌ No separation of config from code (bad practice). &nbsp;</p><p> Option D: Lambda Alias &nbsp;</p><p>- ✅ Alias acts as a stable pointer (e.g., `PROD` or `STAGING`). &nbsp;</p><p>- ✅ Application invokes the alias ARN (never needs updates). &nbsp;</p><p>- ✅ Alias can be updated to point to new versions after testing. &nbsp;</p><p>- ✅ Zero downtime—users always hit the same ARN, while backend switches versions seamlessly. &nbsp;</p><p> Why Option D is Best? &nbsp;</p><p>- Eliminates application ARN changes (alias stays constant). &nbsp;</p><p>- Allows safe testing (test in `$LATEST` or new versions, then update alias). &nbsp;</p><p>- Least operational overhead—no DynamoDB, no hardcoding, no immutable version issues. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "0b7cd02f5be644828466cf5df5b503e0",
            "questionNumber": 155,
            "type": "single",
            "content": "<p>Question #155</p><p>A global media company is planning a multi-Region deployment of an application. Amazon DynamoDB global tables will back the deployment to keep the user experience consistent across the two continents where users are concentrated. Each deployment will have a public Application Load Balancer (ALB). The company manages public DNS internally. The company wants to make the application available through an apex domain. </p><p><br></p><p>Which solution will meet these requirements with the LEAST effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate public DNS to Amazon Route 53. Create CNAME records for the apex domain to point to the ALB. Use a geolocation routing policy to route traffic based on user location."
                },
                {
                    "label": "B",
                    "content": "Place a Network Load Balancer (NLB) in front of the ALB. Migrate public DNS to Amazon Route 53. Create a CNAME record for the apex domain to point to the NLB&rsquo;s static IP address. Use a geolocation routing policy to route traffic based on user location."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator&rsquo;s static IP address to create a record in public DNS for the apex domain."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon API Gateway API that is backed by AWS Lambda in one of the AWS Regions. Configure a Lambda function to route traffic to application deployments by using the round robin method. Create CNAME records for the apex domain to point to the API&#39;s URL."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an AWS Global Accelerator accelerator with multiple endpoint groups that target endpoints in appropriate AWS Regions. Use the accelerator's static IP address to create a record in public DNS for the apex domain.</p><p>This solution meets all the requirements with the least effort for the following reasons:</p><p>Multi-Region deployment support: AWS Global Accelerator is specifically designed to route traffic to multiple AWS Regions, making it ideal for the company's global deployment across two continents.</p><p>Apex domain compatibility: Global Accelerator provides static IP addresses that can be used with an apex domain (also known as a zone apex or naked domain). This is important because apex domains cannot use CNAME records, which would typically be needed for services like Application Load Balancers.</p><p>Integration with Application Load Balancers: Global Accelerator can use the company's public ALBs as endpoints in different regions, directing users to the closest available endpoint.</p><p>Consistent user experience: Global Accelerator routes traffic over the AWS global network rather than the public internet, reducing latency and providing a more consistent experience for users across different continents, which complements the DynamoDB global tables approach.</p><p>Minimal effort: This solution requires minimal configuration - simply creating a Global Accelerator, adding the ALBs as endpoints in appropriate endpoint groups, and then creating a simple A record in the company's existing DNS system pointing to the static IP addresses.</p><p>Global Accelerator also provides additional benefits that align with the company's needs:</p><p>Automatic health checking and failover between regions</p><p>Traffic dials to control the percentage of traffic going to each region</p><p>Improved availability through anycast static IP addresses</p><p>Since the company already manages their public DNS internally, they can easily create the necessary A records for the apex domain pointing to Global Accelerator's static IP addresses, completing the solution with minimal effort.</p><p>Sources</p><p>Routing strategies for global tables - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/dynamodb-global-tables/routing-strategies.html）</p><p>REL10-BP02 Select the appropriate locations for your multi-location deployment - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2023-10-03/framework/rel_fault_isolation_select_location.html）</p><p>REL10-BP02 Select the appropriate locations for your multi-location deployment - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/2024-06-27/framework/rel_fault_isolation_select_location.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7ed82bccf99041bb918f6ad431c87f9b",
            "questionNumber": 156,
            "type": "single",
            "content": "<p>Question #156</p><p>A company is developing a new serverless API by using Amazon API Gateway and AWS Lambda. The company integrated the Lambda functions with API Gateway to use several shared libraries and custom classes. </p><p><br></p><p>A solutions architect needs to simplify the deployment of the solution and optimize for code reuse. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy the shared libraries and custom classes into a Docker image. Store the image in an S3 bucket. Create a Lambda layer that uses the Docker image as the source. Deploy the API&#39;s Lambda functions as Zip packages. Configure the packages to use the Lambda layer."
                },
                {
                    "label": "B",
                    "content": "Deploy the shared libraries and custom classes to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Create a Lambda layer that uses the Docker image as the source. Deploy the API&#39;s Lambda functions as Zip packages. Configure the packages to use the Lambda layer."
                },
                {
                    "label": "C",
                    "content": "Deploy the shared libraries and custom classes to a Docker container in Amazon Elastic Container Service (Amazon ECS) by using the AWS Fargate launch type. Deploy the API&#39;s Lambda functions as Zip packages. Configure the packages to use the deployed container as a Lambda layer."
                },
                {
                    "label": "D",
                    "content": "Deploy the shared libraries, custom classes, and code for the API&#39;s Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API&#39;s Lambda functions to use the Docker image as the deployment package."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>D. Deploy the shared libraries, custom classes, and code for the API's Lambda functions to a Docker image. Upload the image to Amazon Elastic Container Registry (Amazon ECR). Configure the API's Lambda functions to use the Docker image as the deployment package.</p><p>This solution meets the requirements for simplifying deployment and optimizing for code reuse for the following reasons:</p><p>Code reuse optimization: By packaging all shared libraries, custom classes, and function code into a single Docker image, the company ensures that all components are consistently available to all Lambda functions. This eliminates the need to manage multiple packages or layers separately.</p><p>Simplified deployment: Using a Docker image stored in Amazon ECR creates a single deployment artifact that contains everything needed for the API's Lambda functions. This simplifies the deployment process as there's only one artifact to build, version, and deploy.</p><p>Consistency: The Docker image approach ensures that all Lambda functions are using exactly the same versions of shared libraries and custom classes, reducing the risk of compatibility issues.</p><p>Scalability: As the application grows and more Lambda functions are added, the Docker image approach continues to be efficient since new functions can simply reference the same image without additional configuration.</p><p>Option A (using Lambda layers with Docker images stored in S3) is incorrect because:</p><p>Lambda layers don't use Docker images as their source</p><p>S3 is not the appropriate storage location for Docker images (ECR is the AWS service designed for this purpose)</p><p>This approach would unnecessarily complicate the deployment process by mixing container and non-container technologies</p><p>The Docker image approach with ECR provides a cleaner, more consistent solution that better meets the requirements for simplified deployment and code reuse.</p><p>Sources</p><p>Deploy .NET Lambda functions with container images - AWS Lambda （https://docs.aws.amazon.com/lambda/latest/dg/csharp-image.html）</p><p>Lambda Package Exceeds 60MB: Solutions for Large Dependencies? | AWS re:Post （https://repost.aws/questions/QUoXSNWrkdSf61jVqG7IwBxg/lambda-package-exceeds-60mb-solutions-for-large-dependencies）</p><p>Simplifying operations - Implementing Microservices on AWS（https://docs.aws.amazon.com/whitepapers/latest/microservices-on-aws/simplyfing-operations.html） </p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2f1525b1fbd74e53bdc49af6665b9fd9",
            "questionNumber": 157,
            "type": "single",
            "content": "<p>Question #157</p><p>A manufacturing company is building an inspection solution for its factory. The company has IP cameras at the end of each assembly line. The company has used Amazon SageMaker to train a machine learning (ML) model to identify common defects from still images. </p><p><br></p><p>The company wants to provide local feedback to factory workers when a defect is detected. The company must be able to provide this feedback even if the factory’s internet connectivity is down. The company has a local Linux server that hosts an API that provides local feedback to the workers. </p><p><br></p><p>How should the company deploy the ML model to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up an Amazon Kinesis video stream from each IP camera to AWS. Use Amazon EC2 instances to take still images of the streams. Upload the images to an Amazon S3 bucket. Deploy a SageMaker endpoint with the ML model. Invoke an AWS Lambda function to call the inference endpoint when new images are uploaded. Configure the Lambda function to call the local API when a defect is detected."
                },
                {
                    "label": "B",
                    "content": "Deploy AWS IoT Greengrass on the local server. Deploy the ML model to the Greengrass server. Create a Greengrass component to take still images from the cameras and run inference. Configure the component to call the local API when a defect is detected."
                },
                {
                    "label": "C",
                    "content": "Order an AWS Snowball device. Deploy a SageMaker endpoint the ML model and an Amazon EC2 instance on the Snowball device. Take still images from the cameras. Run inference from the EC2 instance. Configure the instance to call the local API when a defect is detected."
                },
                {
                    "label": "D",
                    "content": "Deploy Amazon Monitron devices on each IP camera. Deploy an Amazon Monitron Gateway on premises. Deploy the ML model to the Amazon Monitron devices. Use Amazon Monitron health state alarms to call the local API from an AWS Lambda function when a defect is detected."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p>The key requirements are: &nbsp;</p><p>1. Local defect detection (must work without internet). &nbsp;</p><p>2. Real-time feedback to factory workers via a local API. &nbsp;</p><p>3. Deploy ML model on-premises (since cloud connectivity is unreliable). &nbsp;</p><p> Analysis of Options: &nbsp;</p><p> Option A: Kinesis + S3 + SageMaker Endpoint + Lambda &nbsp;</p><p>- ❌ Depends on internet connectivity (SageMaker endpoint is cloud-based). &nbsp;</p><p>- ❌ No offline capability—fails if the factory loses internet. &nbsp;</p><p>- ❌ Latency issues (uploading images to S3 and waiting for Lambda). &nbsp;</p><p> Option B: AWS IoT Greengrass on Local Server &nbsp;</p><p>- ✅ Greengrass runs ML models locally (no internet needed). &nbsp;</p><p>- ✅ Takes still images from cameras and runs inference on-premises. &nbsp;</p><p>- ✅ Calls local API directly for real-time feedback. &nbsp;</p><p>- ✅ Works offline (syncs with AWS when connectivity is restored). &nbsp;</p><p> Option C: AWS Snowball &nbsp;</p><p>- ❌ Snowball is for data transfer, not persistent edge computing. &nbsp;</p><p>- ❌ Overkill—Greengrass is simpler and designed for this use case. &nbsp;</p><p>- ❌ No real-time processing (Snowball is not a runtime environment). &nbsp;</p><p> Option D: Amazon Monitron &nbsp;</p><p>- ❌ Monitron is for vibration/sensor-based monitoring, not image processing. &nbsp;</p><p>- ❌ Cannot deploy custom ML models (only supports Monitron’s predefined ML). &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- Greengrass is purpose-built for edge ML deployments. &nbsp;</p><p>- Works offline (critical for factory environments). &nbsp;</p><p>- Lightweight and scalable (runs on existing Linux server). &nbsp;</p><p>- Direct integration with local APIs (no cloud dependency). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "8704969ab1e544ea90474c6f1ce0650b",
            "questionNumber": 158,
            "type": "single",
            "content": "<p>Question #158</p><p>A solutions architect must create a business case for migration of a company's on-premises data center to the AWS Cloud. The solutions architect will use a configuration management database (CMDB) export of all the company's servers to create the case. </p><p><br></p><p>Which solution will meet these requirements MOST cost-effectively?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Well-Architected Tool to import the CMDB data to perform an analysis and generate recommendations."
                },
                {
                    "label": "B",
                    "content": "Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export."
                },
                {
                    "label": "C",
                    "content": "Implement resource matching rules. Use the CMDB export and the AWS Price List Bulk API to query CMDB data against AWS services in bulk."
                },
                {
                    "label": "D",
                    "content": "Use AWS Application Discovery Service to import the CMDB data to perform an analysis."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Use Migration Evaluator to perform an analysis. Use the data import template to upload the data from the CMDB export.</p><p>This solution will meet the requirements most cost-effectively for the following reasons:</p><p>AWS Migration Evaluator (formerly known as TSO Logic) is specifically designed for creating business cases for cloud migration. It's purpose-built to analyze on-premises infrastructure data and provide detailed cost comparisons and recommendations for AWS migration.</p><p>Migration Evaluator has a built-in data import template that's designed to work with CMDB exports, making it straightforward to upload and analyze the company's server inventory data without additional processing or development work.</p><p>The service is offered at no additional cost as part of AWS's migration assessment services, making it the most cost-effective option for this specific use case.</p><p>Migration Evaluator provides comprehensive TCO (Total Cost of Ownership) analysis, right-sizing recommendations, and detailed migration business cases that can be presented to stakeholders.</p><p>The other options have significant limitations:</p><p>A. AWS Well-Architected Tool is designed to review architectural best practices against the AWS Well-Architected Framework. It's not designed to import CMDB data or create migration business cases.</p><p>C. Using the AWS Price List Bulk API with custom resource matching rules would require significant development effort to build a custom solution. This would be more time-consuming and costly than using a purpose-built tool like Migration Evaluator.</p><p>D. AWS Application Discovery Service is primarily designed for discovering applications and their dependencies for migration planning. While it can import some CMDB data, it's more focused on dependency mapping than creating business cases and cost analyses.</p><p>Migration Evaluator is the most appropriate tool for this specific requirement as it's designed exactly for the purpose of creating migration business cases using existing infrastructure data like CMDB exports.</p><p>Sources</p><p>Tools for migrating to the AWS Cloud - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/migration-tools/introduction.html）</p><p>Create a data-driven Migration Business Case using AWS Cloud Value Framework | AWS Cloud Operations & Migrations Blog （https://aws.amazon.com/cn/blogs/mt/create-a-data-driven-migration-business-case-using-aws-cloud-value-framework/）</p><p>Choosing AWS migration services and tools - Choosing AWS migration services and tools （https://docs.aws.amazon.com/decision-guides/latest/migration-on-aws-how-to-choose/migration-on-aws-how-to-choose.html）</p><p>Seeking Advice on Utilizing Migration Evaluator for AWS Migration Business Cases | AWS re:Post （https://repost.aws/questions/QU11RsGVWsQpOYHehhrbHpPA/seeking-advice-on-utilizing-migration-evaluator-for-aws-migration-business-cases）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ac71b133e4ed4b2785094eb56d49a93d",
            "questionNumber": 159,
            "type": "single",
            "content": "<p>Question #159</p><p>A company has a website that runs on Amazon EC2 instances behind an Application Load Balancer (ALB). The instances are in an Auto Scaling group. The ALB is associated with an AWS WAF web ACL. </p><p><br></p><p>The website often encounters attacks in the application layer. The attacks produce sudden and significant increases in traffic on the application server. The access logs show that each attack originates from different IP addresses. A solutions architect needs to implement a solution to mitigate these attacks. </p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon CloudWatch alarm that monitors server access. Set a threshold based on access by IP address. Configure an alarm action that adds the IP address to the web ACL&rsquo;s deny list."
                },
                {
                    "label": "B",
                    "content": "Deploy AWS Shield Advanced in addition to AWS WAF. Add the ALB as a protected resource."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon CloudWatch alarm that monitors user IP addresses. Set a threshold based on access by IP address. Configure the alarm to invoke an AWS Lambda function to add a deny rule in the application server&rsquo;s subnet route table for any IP addresses that activate the alarm."
                },
                {
                    "label": "D",
                    "content": "Inspect access logs to find a pattern of IP addresses that launched the attacks. Use an Amazon Route 53 geolocation routing policy to deny traffic from the countries that host those IP addresses."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p>The problem is: &nbsp;</p><p>- Application-layer attacks (e.g., HTTP floods, SQL injection, etc.) causing sudden traffic spikes. &nbsp;</p><p>- Attacks come from different IPs, making IP-based blocking ineffective. &nbsp;</p><p>- Goal: Mitigate attacks with minimal operational overhead. &nbsp;</p><p> Analysis of Options: &nbsp;</p><p> Option A: CloudWatch Alarm + WAF IP Deny List &nbsp;</p><p>- ❌ Reactive, not proactive—attacks must already be happening before blocking. &nbsp;</p><p>- ❌ High operational overhead—manual tuning of thresholds and rules. &nbsp;</p><p>- ❌ IP-based blocking fails (attackers use many IPs). &nbsp;</p><p> Option B: AWS Shield Advanced + AWS WAF &nbsp;</p><p>- ✅ Shield Advanced provides DDoS protection (automatically detects & mitigates attacks). &nbsp;</p><p>- ✅ Works with WAF to block application-layer attacks (e.g., HTTP floods). &nbsp;</p><p>- ✅ Minimal operational overhead—AWS manages detection and mitigation. &nbsp;</p><p> Option C: CloudWatch Alarm + Lambda + Route Table Deny Rules &nbsp;</p><p>- ❌ Subnet route table blocks are too broad (affects all traffic, not just HTTP). &nbsp;</p><p>- ❌ Complex and slow (Lambda + route table updates introduce latency). &nbsp;</p><p>- ❌ Does not scale for large attacks (IP-based blocking is ineffective). &nbsp;</p><p> Option D: Route 53 Geolocation Blocking &nbsp;</p><p>- ❌ Geolocation blocking is imprecise (attackers can use proxies/VPNs). &nbsp;</p><p>- ❌ Manual effort required (log analysis + policy updates). &nbsp;</p><p>- ❌ False positives (legitimate users from blocked regions are denied). &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- Shield Advanced automatically mitigates DDoS attacks (network & application layer). &nbsp;</p><p>- WAF + Shield Advanced together block sophisticated HTTP attacks. &nbsp;</p><p>- No manual intervention needed—AWS handles detection and mitigation. &nbsp;</p><p>- Scalable—works even if attackers rotate IPs. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "56125928142649bba428a9db2de157f2",
            "questionNumber": 160,
            "type": "multiple",
            "content": "<p>Question #160</p><p>A company has a critical application in which the data tier is deployed in a single AWS Region. The data tier uses an Amazon DynamoDB table and an Amazon Aurora MySQL DB cluster. The current Aurora MySQL engine version supports a global database. The application tier is already deployed in two Regions. </p><p><br></p><p>Company policy states that critical applications must have application tier components and data tier components deployed across two Regions. The RTO and RPO must be no more than a few minutes each. A solutions architect must recommend a solution to make the data tier compliant with company policy.<br><br>Which combination of steps will meet these requirements? (Choose two.) </p>",
            "options": [
                {
                    "label": "A",
                    "content": "Add another Region to the Aurora MySQL DB cluster"
                },
                {
                    "label": "B",
                    "content": "Add another Region to each table in the Aurora MySQL DB cluster"
                },
                {
                    "label": "C",
                    "content": "Set up scheduled cross-Region backups for the DynamoDB table and the Aurora MySQL DB cluster"
                },
                {
                    "label": "D",
                    "content": "Convert the existing DynamoDB table to a global table by adding another Region to its configuration"
                },
                {
                    "label": "E",
                    "content": "Use Amazon Route 53 Application Recovery Controller to automate database backup and recovery to the secondary Region"
                }
            ],
            "correctAnswer": "AD",
            "explanation": "<p>Based on the information provided, the correct answers are:</p><p>A. Add another Region to the Aurora MySQL DB cluster</p><p>D. Convert the existing DynamoDB table to a global table by adding another Region to its configuration</p><p>These two steps together will make the data tier compliant with the company policy for the following reasons:</p><p>The company policy requires critical applications to have data tier components deployed across two regions, with RTO and RPO of no more than a few minutes each.</p><p>Adding another Region to the Aurora MySQL DB cluster (option A) creates an Aurora Global Database, which:</p><p>Replicates data from the primary region to the secondary region with typical lag of less than 1 second</p><p>Provides an RPO of approximately 1 second</p><p>Allows for failover to the secondary region with an RTO of less than 1 minute</p><p>Is supported by the current Aurora MySQL engine version (as stated in the question)</p><p>Converting the existing DynamoDB table to a global table by adding another Region (option D):</p><p>Enables multi-region replication for DynamoDB with typically sub-second latency</p><p>Provides automatic multi-master replication</p><p>Allows for reads and writes in both regions</p><p>Achieves RPO and RTO of seconds, well within the \"few minutes\" requirement</p><p>Together, these solutions address both database technologies in the data tier (Aurora MySQL and DynamoDB) and ensure they are deployed across two regions with the required RTO and RPO metrics.</p><p>Option E (using Amazon Route 53 Application Recovery Controller) is not correct because:</p><p>It doesn't handle data replication itself</p><p>Backup and recovery processes would not meet the \"few minutes\" RTO requirement</p><p>It's more suited for managing traffic routing and failover rather than being the primary mechanism for data replication</p><p>The combination of Aurora Global Database and DynamoDB Global Tables provides the most direct and effective solution to meet the company's policy requirements with minimal operational complexity.</p><p>Sources</p><p>AWS Backup vs global table/database for cost-effective RTO,RPO requirements | AWS re:Post （https://repost.aws/questions/QUWf0GuonKSvW8U2N4o8huHw/aws-backup-vs-global-table-database-for-cost-effective-rto-rpo-requirements）</p><p>MySQL PostgreSQL Relational Database – Amazon Aurora Global Database – AWS （https://aws.amazon.com/cn/rds/aurora/global-database/）</p><p>Use Amazon Aurora Global Database to build resilient multi-Region applications | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/use-amazon-aurora-global-database-to-build-resilient-multi-region-applications/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "dda9e97fbf4748e7abd90ad8f7145f80",
            "questionNumber": 161,
            "type": "single",
            "content": "<p>Question #161</p><p>A telecommunications company is running an application on AWS. The company has set up an AWS Direct Connect connection between the company's on-premises data center and AWS. The company deployed the application on Amazon EC2 instances in multiple Availability Zones behind an internal Application Load Balancer (ALB). The company's clients connect from the on-premises network by using HTTPS. The TLS terminates in the ALB. The company has multiple target groups and uses path-based routing to forward requests based on the URL path. </p><p><br></p><p>The company is planning to deploy an on-premises firewall appliance with an allow list that is based on IP address. A solutions architect must develop a solution to allow traffic flow to AWS from the on-premises network so that the clients can continue to access the application. </p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the existing ALB to use static IP addresses. Assign IP addresses in multiple Availability Zones to the ALB. Add the ALB IP addresses to the firewall appliance."
                },
                {
                    "label": "B",
                    "content": "Create a Network Load Balancer (NLB). Associate the NLB with one static IP addresses in multiple Availability Zones. Create an ALB-type target group for the NLB and add the existing ALAdd the NLB IP addresses to the firewall appliance. Update the clients to connect to the NLB.&nbsp;"
                },
                {
                    "label": "C",
                    "content": "Create a Network Load Balancer (NLB). Associate the LNB with one static IP addresses in multiple Availability Zones. Add the existing target groups to the NLB. Update the clients to connect to the NLB. Delete the ALB Add the NLB IP addresses to the firewall appliance.&nbsp;"
                },
                {
                    "label": "D",
                    "content": "Create a Gateway Load Balancer (GWLB). Assign static IP addresses to the GWLB in multiple Availability Zones. Create an ALB-type target group for the GWLB and add the existing ALB. Add the GWLB IP addresses to the firewall appliance. Update the clients to connect to the GWLB"
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. On-premises clients must connect to an internal ALB over Direct Connect. &nbsp;</p><p>2. A firewall appliance will enforce an IP allow list, so the ALB must have static IPs. &nbsp;</p><p>3. Path-based routing must continue to work (ALB functionality). &nbsp;</p><p> Problem: &nbsp;</p><p>- ALBs do not have static IPs (they use dynamic DNS). &nbsp;</p><p>- NLBs provide static IPs, but NLBs cannot do path-based routing (ALB feature). &nbsp;</p><p> Solution: &nbsp;</p><p>- Place an NLB in front of the ALB to provide static IPs for the firewall allow list. &nbsp;</p><p>- Clients connect to the NLB, which forwards traffic to the ALB (retaining path-based routing). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>✅ NLB provides static IPs (required for firewall allow list). &nbsp;</p><p>✅ ALB remains behind NLB, preserving path-based routing. &nbsp;</p><p>✅ No loss of functionality—ALB still handles TLS termination and routing. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: ALBs cannot have static IPs (only NLBs/GWLBs can). &nbsp;</p><p>❌ C: NLB cannot replace ALB (loses path-based routing). &nbsp;</p><p>❌ D: GWLB is for third-party appliances, not for this use case. &nbsp;</p><p> Implementation Steps (Option B): &nbsp;</p><p>1. Create an NLB with static IPs in each AZ. &nbsp;</p><p>2. Point NLB to ALB (ALB becomes a target). &nbsp;</p><p>3. Add NLB IPs to firewall allow list. &nbsp;</p><p>4. Update clients to connect to NLB (instead of ALB). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "a6a91069e3024f6aba258cdb6ff9b9cd",
            "questionNumber": 162,
            "type": "single",
            "content": "<p>Question #162</p><p>A company runs an application on a fleet of Amazon EC2 instances that are in private subnets behind an internet-facing Application Load Balancer (ALB). The ALB is the origin for an Amazon CloudFront distribution. An AWS WAF web ACL that contains various AWS managed rules is associated with the CloudFront distribution.</p><p><br></p><p>The company needs a solution that will prevent internet traffic from directly accessing the ALB.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new web ACL that contains the same rules that the existing web ACL contains. Associate the new web ACL with the ALB."
                },
                {
                    "label": "B",
                    "content": "Associate the existing web ACL with the ALB."
                },
                {
                    "label": "C",
                    "content": "Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only."
                },
                {
                    "label": "D",
                    "content": "Add a security group rule to the ALB to allow only the various CloudFront IP address ranges."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Add a security group rule to the ALB to allow traffic from the AWS managed prefix list for CloudFront only.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>Effectiveness: By configuring the ALB's security group to only allow traffic from CloudFront's managed prefix list, you create a network-level restriction that prevents any direct internet access to the ALB. This ensures that all traffic must flow through CloudFront first.</p><p>Minimal operational overhead:</p><p>AWS automatically maintains and updates the CloudFront managed prefix list, so you don't need to manually track or update IP ranges when they change</p><p>This is a one-time configuration that requires minimal ongoing maintenance</p><p>No additional services or costs are involved</p><p>Simplicity: This approach uses native AWS networking features (security groups) without requiring additional services or complex configurations.</p><p>The other options have significant drawbacks:</p><p>A. Creating a new web ACL with the same rules would add unnecessary complexity and cost. Web ACLs are primarily for filtering malicious traffic patterns, not for restricting access to specific sources.</p><p>B. Associating the existing web ACL with the ALB wouldn't prevent direct internet access to the ALB; it would only apply the same filtering rules at both the CloudFront and ALB levels.</p><p>D. Adding security group rules for individual CloudFront IP address ranges would work initially but would create significant operational overhead, as you would need to continuously monitor and update these rules whenever AWS adds or changes CloudFront IP ranges.</p><p>Using the AWS managed prefix list for CloudFront provides the most efficient and low-maintenance solution to ensure that only traffic coming through CloudFront can reach the ALB.</p><p>Sources</p><p>Restrict Application Load Balancer traffic through only CloudFront | AWS re:Post （https://repost.aws/knowledge-center/waf-restrict-alb-allow-cloudfront）</p><p>WAF on ALB or CloudFront ? | AWS re:Post （https://repost.aws/questions/QUTY5hPVxgS6Caa3eZHX7-nQ/waf-on-alb-or-cloudfront）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "278f8404a52849efbc9ab355d4fca371",
            "questionNumber": 163,
            "type": "single",
            "content": "<p>Question #163</p><p>A company is running an application that uses an Amazon ElastiCache for Redis cluster as a caching layer. A recent security audit revealed that the company has configured encryption at rest for ElastiCache. However, the company did not configure ElastiCache to use encryption in transit. Additionally, users can access the cache without authentication.</p><p><br></p><p>A solutions architect must make changes to require user authentication and to ensure that the company is using end-to-end encryption.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AUTH token. Store the token in AWS System Manager Parameter Store, as an encrypted parameter. Create a new cluster with AUTH, and configure encryption in transit. Update the application to retrieve the AUTH token from Parameter Store when necessary and to use the AUTH token for authentication."
                },
                {
                    "label": "B",
                    "content": "Create an AUTH token. Store the token in AWS Secrets Manager. Configure the existing cluster to use the AUTH token, and configure encryption in transit. Update the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication."
                },
                {
                    "label": "C",
                    "content": "Create an SSL certificate. Store the certificate in AWS Secrets Manager. Create a new cluster, and configure encryption in transit. Update the application to retrieve the SSL certificate from Secrets Manager when necessary and to use the certificate for authentication."
                },
                {
                    "label": "D",
                    "content": "Create an SSL certificate. Store the certificate in AWS Systems Manager Parameter Store, as an encrypted advanced parameter. Update the existing cluster to configure encryption in transit. Update the application to retrieve the SSL certificate from Parameter Store when necessary and to use the certificate for authentication."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Enable encryption in transit for ElastiCache for Redis. &nbsp;</p><p>2. Require authentication (AUTH token) for Redis access. &nbsp;</p><p>3. Secure storage of credentials (AUTH token). &nbsp;</p><p> Why Option B is Correct? &nbsp;</p><p>✅ AUTH token enables authentication (required by the audit). &nbsp;</p><p>✅ Secrets Manager securely stores the token (encrypted, managed rotation). &nbsp;</p><p>✅ Encryption in transit must be configured (cannot be enabled on an existing cluster—must create a new one or modify the existing one). &nbsp;</p><p>✅ Application retrieves token securely from Secrets Manager. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: While it uses Parameter Store, encryption in transit cannot be enabled on an existing cluster without creating a new one. &nbsp;</p><p>❌ C & D: SSL certificates are not used for Redis authentication (AUTH tokens are required). &nbsp;</p><p> Steps to Implement (Option B): &nbsp;</p><p>1. Create an AUTH token (complex password). &nbsp;</p><p>2. Store it in Secrets Manager (secure, encrypted). &nbsp;</p><p>3. Enable encryption in transit (requires modifying the cluster or creating a new one). &nbsp;</p><p>4. Update the application to fetch the AUTH token from Secrets Manager. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "177e12903d2548e8b49197a82d68d15b",
            "questionNumber": 164,
            "type": "single",
            "content": "<p>Question #164</p><p>A company is running a compute workload by using Amazon EC2 Spot Instances that are in an Auto Scaling group. The launch template uses two placement groups and a single instance type.</p><p><br></p><p>Recently, a monitoring system reported Auto Scaling instance launch failures that correlated with longer wait times for system users. The company needs to improve the overall reliability of the workload.</p><p><br></p><p>Which solution will meet this requirement?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Replace the launch template with a launch configuration to use an Auto Scaling group that uses attribute-based instance type selection."
                },
                {
                    "label": "B",
                    "content": "Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version."
                },
                {
                    "label": "C",
                    "content": "Update the launch template Auto Scaling group to increase the number of placement groups."
                },
                {
                    "label": "D",
                    "content": "Update the launch template to use a larger instance type."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create a new launch template version that uses attribute-based instance type selection. Configure the Auto Scaling group to use the new launch template version.</p><p>This solution will improve the overall reliability of the workload for the following reasons:</p><p>Root cause identification: The current configuration uses a single instance type in the launch template, which is likely causing the launch failures. When specific instance types are not available in the Spot market, the Auto Scaling group cannot launch new instances, leading to the reported failures and longer wait times.</p><p>Attribute-based instance type selection: By implementing attribute-based instance type selection in the launch template, the Auto Scaling group can dynamically choose from a variety of instance types that meet the specified attributes (like vCPU, memory, etc.) rather than being restricted to a single instance type. This significantly increases the pool of available Spot Instances.</p><p>Spot Instance best practices: Using multiple instance types is a recommended best practice for Spot Instance workloads as it increases the chances of finding available capacity in the Spot market, reducing the likelihood of launch failures.</p><p>Maintaining launch templates: Creating a new version of the existing launch template is more efficient than replacing it entirely, as it preserves other configuration settings while updating only what needs to change.</p><p>The other options are less effective:</p><p>A. Replacing the launch template with a launch configuration is not recommended as launch configurations are considered legacy and offer fewer features than launch templates. Additionally, this would require recreating all configuration settings rather than just updating what needs to change.</p><p>C. Increasing the number of placement groups wouldn't address the fundamental issue of Spot Instance availability. Placement groups control how instances are physically placed relative to each other but don't affect the availability of specific instance types in the Spot market.</p><p>D. Updating to a larger instance type would still restrict the Auto Scaling group to a single instance type, which doesn't solve the underlying problem of limited Spot Instance availability. It might even worsen the situation if the larger instance type has lower availability in the Spot market.</p><p>Sources</p><p>Capacity Rebalancing in Auto Scaling to replace at-risk Spot Instances - Amazon EC2 Auto Scaling （https://docs.aws.amazon.com/autoscaling/ec2/userguide/ec2-auto-scaling-capacity-rebalancing.html）</p><p>Why is my Auto Scaling group not launching new instances when the load increases? | AWS re:Post （https://repost.aws/questions/QUQsrSasV_Tyyqw_lB8W9Lfw/why-is-my-auto-scaling-group-not-launching-new-instances-when-the-load-increases）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "03345ea92a6942698b431f24cbaf56c3",
            "questionNumber": 165,
            "type": "single",
            "content": "Question #165<p>A company is migrating a document processing workload to AWS. The company has updated many applications to natively use the Amazon S3 API to store, retrieve, and modify documents that a processing server generates at a rate of approximately 5 documents every second. After the document processing is finished, customers can download the documents directly from Amazon S3.</p><p><br></p><p>During the migration, the company discovered that it could not immediately update the processing server that generates many documents to support the S3 API. The server runs on Linux and requires fast local access to the files that the server generates and modifies. When the server finishes processing, the files must be available to the public for download within 30 minutes.</p><p><br></p><p>Which solution will meet these requirements with the LEAST amount of effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate the application to an AWS Lambda function. Use the AWS SDK for Java to generate, modify, and access the files that the company stores directly in Amazon S3."
                },
                {
                    "label": "B",
                    "content": "Set up an Amazon S3 File Gateway and configure a file share that is linked to the document store. Mount the file share on an Amazon EC2 instance by using NFS. When changes occur in Amazon S3, initiate a RefreshCache API call to update the S3 File Gateway."
                },
                {
                    "label": "C",
                    "content": "Configure Amazon FSx for Lustre with an import and export policy. Link the new file system to an S3 bucket. Install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS."
                },
                {
                    "label": "D",
                    "content": "Configure AWS DataSync to connect to an Amazon EC2 instance. Configure a task to synchronize the generated files to and from Amazon S3."
                }
            ],
            "correctAnswer": "B",
            "explanation": "Reference Answer: B<p>Explanation:</p><p>Problem:</p><p>The company needs a solution to migrate a document processing workload to AWS while meeting the following constraints:</p><ol><li>Local Access: The server requires fast local access to the files it generates and modifies.</li><li>Public Availability: The processed files must be available for public download within 30 minutes.</li><li>Minimal Effort: The solution must require the least amount of effort for implementation.</li></ol><p>Options Analysis:</p><ul><li><p>A: Migrating the application to AWS Lambda requires significant changes to the existing application to integrate with the AWS SDK and S3 API. This would require extensive rework, violating the \"least amount of effort\" requirement.</p></li><li><p>B: Using an Amazon S3 File Gateway allows the existing Linux server to access files through NFS, minimizing changes to the application. It supports a seamless transition by linking a local file share to Amazon S3, and the RefreshCache API ensures updates in S3 are visible locally. This is the simplest solution requiring minimal effort.</p></li><li><p>C: Configuring Amazon FSx for Lustre requires setting up a new Lustre file system and linking it to the EC2 instance. This involves significant setup and configuration, which adds complexity.</p></li><li><p>D: Using AWS DataSync introduces an additional synchronization layer, requiring the setup of tasks and schedules. While effective, it is more complex than using an S3 File Gateway for this use case.</p></li></ul><p>Key Benefits of Option B:</p><ul><li>Minimal Application Changes: It integrates with the existing workflow, requiring minimal updates.</li><li>Fast Access: The NFS-based file share provides fast local access.</li><li>Easy S3 Synchronization: The RefreshCache API simplifies synchronization with Amazon S3.</li></ul><p>Thus, Option B is the correct answer.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "29f2a14b229f48258fced5e50abfbc4d",
            "questionNumber": 166,
            "type": "single",
            "content": "<p>Question #166</p><p>A delivery company is running a serverless solution in the AWS Cloud. The solution manages user data, delivery information, and past purchase details. The solution consists of several microservices. The central user service stores sensitive data in an Amazon DynamoDB table. Several of the other microservices store a copy of parts of the sensitive data in different storage services.</p><p><br></p><p>The company needs the ability to delete user information upon request. As soon as the central user service deletes a user, every other microservice must also delete its copy of the data immediately.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Activate DynamoDB Streams on the DynamoDB table. Create an AWS Lambda trigger for the DynamoDB stream that will post events about user deletion in an Amazon Simple Queue Service (Amazon SQS) queue. Configure each microservice to poll the queue and delete the user from the DynamoDB table."
                },
                {
                    "label": "B",
                    "content": "Set up DynamoDB event notifications on the DynamoDB table. Create an Amazon Simple Notification Service (Amazon SNS) topic as a target for the DynamoDB event notification. Configure each microservice to subscribe to the SNS topic and to delete the user from the DynamoDB table."
                },
                {
                    "label": "C",
                    "content": "Configure the central user service to post an event on a custom Amazon EventBridge event bus when the company deletes a user. Create an EventBridge rule for each microservice to match the user deletion event pattern and invoke logic in the microservice to delete the user from the DynamoDB table."
                },
                {
                    "label": "D",
                    "content": "Configure the central user service to post a message on an Amazon Simple Queue Service (Amazon SQS) queue when the company deletes a user. Configure each microservice to create an event filter on the SQS queue and to delete the user from the DynamoDB table."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Immediate deletion of user data across all microservices when the central user service deletes a user. &nbsp;</p><p>2. Event-driven architecture to ensure all services react in real-time. &nbsp;</p><p>3. Scalable and decoupled solution (no direct dependencies between microservices). &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>✅ EventBridge provides a centralized event bus for real-time event distribution. &nbsp;</p><p>✅ Each microservice subscribes to user deletion events via EventBridge rules. &nbsp;</p><p>✅ Decoupled and scalable—microservices independently process events. &nbsp;</p><p>✅ Immediate propagation of deletion events (no polling delays). &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: DynamoDB Streams + SQS + Polling &nbsp;</p><p>- Polling introduces delays (not immediate). &nbsp;</p><p>- Complex to manage (each service must poll SQS). &nbsp;</p><p>❌ B: DynamoDB Notifications + SNS &nbsp;</p><p>- SNS cannot filter events (all services receive all events, increasing overhead). &nbsp;</p><p>- Services must implement logic to ignore irrelevant events. &nbsp;</p><p>❌ D: SQS Queue + Event Filters &nbsp;</p><p>- SQS does not natively support event filtering (services must process all messages). &nbsp;</p><p>- Polling is required, leading to latency. &nbsp;</p><p> Implementation Steps (Option C): &nbsp;</p><p>1. Central user service publishes a \"UserDeleted\" event to EventBridge. &nbsp;</p><p>2. EventBridge rules route the event to each microservice. &nbsp;</p><p>3. Each microservice triggers its deletion logic upon receiving the event. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "e98ce29914344ef4918f55d93523cceb",
            "questionNumber": 167,
            "type": "single",
            "content": "<p>Question #167</p><p>A company is running a web application in a VPC. The web application runs on a group of Amazon EC2 instances behind an Application Load Balancer (ALB). The ALB is using AWS WAF.</p><p><br></p><p>An external customer needs to connect to the web application. The company must provide IP addresses to all external customers.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Replace the ALB with a Network Load Balancer (NLB). Assign an Elastic IP address to the NLB."
                },
                {
                    "label": "B",
                    "content": "Allocate an Elastic IP address. Assign the Elastic IP address to the ALB and provide the Elastic IP address to the customer."
                },
                {
                    "label": "C",
                    "content": "Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator&#39;s endpoint. Provide the accelerator&#39;s IP addresses to the customer."
                },
                {
                    "label": "D",
                    "content": "Configure an Amazon CloudFront distribution. Set the ALB as the origin. Ping the distribution&#39;s DNS name to determine the distribution&#39;s public IP address. Provide the IP address to the customer."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an AWS Global Accelerator standard accelerator. Specify the ALB as the accelerator's endpoint. Provide the accelerator's IP addresses to the customer.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>Fixed IP addresses: AWS Global Accelerator provides two static anycast IP addresses that remain fixed for the life of the accelerator. This directly addresses the requirement to provide IP addresses to external customers.</p><p>Preservation of existing architecture: This solution allows the company to keep their current setup with the Application Load Balancer and AWS WAF intact, without requiring any architectural changes.</p><p>Minimal operational overhead: Global Accelerator is a fully managed service that requires minimal setup and maintenance. Once configured, it automatically routes traffic to the ALB without additional management.</p><p>Additional benefits: Global Accelerator also provides improved availability and performance by routing traffic through the AWS global network and automatically directing users to the nearest AWS edge location.</p><p>The other options have significant drawbacks:</p><p>A. Replacing the ALB with an NLB would require reconfiguring the entire load balancing setup and would lose the AWS WAF integration, as WAF doesn't directly integrate with NLBs. This would create significant operational overhead.</p><p>B. This option is not technically feasible. You cannot directly assign Elastic IP addresses to Application Load Balancers. ALBs use DNS names, not fixed IP addresses.</p><p>D. CloudFront distributions don't have fixed IP addresses that can be provided to customers. The IP addresses associated with CloudFront distributions can change over time. Additionally, \"pinging\" the distribution to determine IP addresses is not a reliable method and goes against AWS best practices.</p><p>AWS Global Accelerator is specifically designed for use cases where fixed IP addresses are required while maintaining the benefits of AWS networking services like ALB and WAF, making it the optimal solution with the least operational overhead.</p><p>Sources</p><p>WAF on ALB or CloudFront ? | AWS re:Post （https://repost.aws/questions/QUTY5hPVxgS6Caa3eZHX7-nQ/waf-on-alb-or-cloudfront）</p><p>Integrations for your Application Load Balancer - Elastic Load Balancing （https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-integrations.html）</p><p>Can an Application Load Balancer have a static IP | AWS re:Post （https://repost.aws/questions/QU-VlS2bqXTJil3WEwJkfHiA/can-an-application-load-balancer-have-a-static-ip）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "60a4bb927cf948ec8db80bd2fdcaac25",
            "questionNumber": 168,
            "type": "multiple",
            "content": "<p>Question #168</p><p>A company has a few AWS accounts for development and wants to move its production application to AWS. The company needs to enforce Amazon Elastic Block Store (Amazon EBS) encryption at rest for current production accounts and future production accounts only. The company needs a solution that includes built-in blueprints and guardrails.</p><p>Which combination of steps will meet these requirements? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS CloudFormation StackSets to deploy AWS Config rules on production accounts."
                },
                {
                    "label": "B",
                    "content": "Create a new AWS Control Tower landing zone in an existing developer account. Create OUs for accounts. Add production and development accounts to production and development OUs, respectively."
                },
                {
                    "label": "C",
                    "content": "Create a new AWS Control Tower landing zone in the company&rsquo;s management account. Add production and development accounts to production and development OUs, respectively."
                },
                {
                    "label": "D",
                    "content": "Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance."
                },
                {
                    "label": "E",
                    "content": "Create a guardrail from the management account to detect EBS encryption."
                },
                {
                    "label": "F",
                    "content": "Create a guardrail for the production OU to detect EBS encryption."
                }
            ],
            "correctAnswer": "CDF",
            "explanation": "<p>Based on the information provided, the correct combination of steps to meet the requirements are:</p><p>C. Create a new AWS Control Tower landing zone in the company's management account. Add production and development accounts to production and development OUs, respectively.</p><p>D. Invite existing accounts to join the organization in AWS Organizations. Create SCPs to ensure compliance.</p><p>F. Create a guardrail for the production OU to detect EBS encryption.</p><p>This combination of steps will meet the requirements for the following reasons:</p><p>AWS Control Tower provides built-in blueprints and guardrails, which is specifically mentioned as a requirement. By creating a new AWS Control Tower landing zone (step C), the company establishes a well-architected multi-account environment with pre-configured governance.</p><p>The company already has existing development accounts that need to be incorporated into this structure. By inviting these existing accounts to join the organization in AWS Organizations (step D), they can be properly organized and managed within the Control Tower environment.</p><p>Service Control Policies (SCPs) are powerful policy tools that can be used to ensure compliance across the organization. Creating SCPs (part of step D) allows the company to enforce security policies like EBS encryption at the organization level.</p><p>Creating a guardrail specifically for the production OU to detect EBS encryption (step F) ensures that the encryption requirement is enforced only on production accounts (current and future), not on development accounts. This meets the requirement of enforcing EBS encryption at rest for \"production accounts only.\"</p><p>This approach provides:</p><p>A structured environment with separate OUs for production and development</p><p>Built-in blueprints and guardrails through AWS Control Tower</p><p>The ability to enforce EBS encryption specifically on production accounts</p><p>A scalable solution that will automatically apply to future production accounts added to the production OU</p><p>A way to incorporate existing development accounts into the new structure</p><p>The combination of AWS Control Tower's landing zone, AWS Organizations' account structure and SCPs, and OU-specific guardrails creates a comprehensive solution that meets all the stated requirements with the appropriate level of control and flexibility.</p><p>Sources</p><p>Enforcing Encryption of Data at Rest - Encrypting File Data with Amazon Elastic File System （https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html）</p><p>Amazon Elastic Compute Cloud (Amazon EC2) controls - AWS Control Tower （https://docs.aws.amazon.com/controltower/latest/controlreference/ec2-rules.html）</p><p>Amazon Elastic File System controls - AWS Control Tower （https://docs.aws.amazon.com/controltower/latest/controlreference/efs-rules.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f34812a444ce41e28ec93a98c3626f2c",
            "questionNumber": 169,
            "type": "single",
            "content": "Question #169<p>A company is running a critical stateful web application on two Linux Amazon EC2 instances behind an Application Load Balancer (ALB) with an Amazon RDS for MySQL database. The company hosts the DNS records for the application in Amazon Route 53. A solutions architect must recommend a solution to improve the resiliency of the application. </p><p><br></p><p>The solution must meet the following objectives:</p><p>- Application tier: RPO of 2 minutes. RTO of 30 minutes</p><p>- Database tier: RPO of 5 minutes. RTO of 30 minutes</p><p><br></p><p>The company does not want to make significant changes to the existing application architecture. The company must ensure optimal latency after a failover.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure the EC2 instances to use AWS Elastic Disaster Recovery. Create a cross-Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint."
                },
                {
                    "label": "B",
                    "content": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Configure RDS automated backups. Configure backup replication to a second AWS Region. Create an ALB in the second Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs. Update DNS records to point to the Global Accelerator endpoint."
                },
                {
                    "label": "C",
                    "content": "Create a backup plan in AWS Backup for the EC2 instances and RDS DB instance. Configure backup replication to a second AWS Region.Create an ALB in the second Region. Configure an Amazon CloudFront distribution in front of the ALB. Update DNS records to point to CloudFront."
                },
                {
                    "label": "D",
                    "content": "Configure the EC2 instances to use Amazon Data Lifecycle Manager (Amazon DLM) to take snapshots of the EBS volumes. Create a cross Region read replica for the RDS DB instance. Create an ALB in a second AWS Region. Create an AWS Global Accelerator endpoint, and associate the endpoint with the ALBs."
                }
            ],
            "correctAnswer": "A",
            "explanation": "The correct answer is A. AWS Elastic Disaster Recovery (DRS) is designed to recover from disasters by providing a cost-effective, fully managed, and scalable solution for disaster recovery. By configuring EC2 instances to use DRS, the company can meet the RPO and RTO objectives for the application tier. Creating a cross-Region read replica for the RDS DB instance ensures that data is replicated to a secondary region, which helps meet the RPO and RTO for the database tier. Having an ALB in a second AWS Region, along with a Global Accelerator endpoint, provides a resilient architecture that can redirect traffic to the healthy region in case of a disaster, thus ensuring optimal latency after failover.",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "85ac9c8f3ac1417d85257831add3c978",
            "questionNumber": 170,
            "type": "multiple",
            "content": "<p>Question #170</p><p>A solutions architect wants to cost-optimize and appropriately size Amazon EC2 instances in a single AWS account. The solutions architect wants to ensure that the instances are optimized based on CPU, memory, and network metrics.</p><p>Which combination of steps should the solutions architect take to meet these requirements? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Purchase AWS Business Support or AWS Enterprise Support for the account."
                },
                {
                    "label": "B",
                    "content": "Turn on AWS Trusted Advisor and review any &ldquo;Low Utilization Amazon EC2 Instances&rdquo; recommendations."
                },
                {
                    "label": "C",
                    "content": "Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances."
                },
                {
                    "label": "D",
                    "content": "Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations."
                },
                {
                    "label": "E",
                    "content": "Create an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest."
                }
            ],
            "correctAnswer": "CD",
            "explanation": "<p>Based on the information provided, the correct combination of steps for the solutions architect to cost-optimize and appropriately size EC2 instances based on CPU, memory, and network metrics are:</p><p>C. Install the Amazon CloudWatch agent and configure memory metric collection on the EC2 instances.</p><p>D. Configure AWS Compute Optimizer in the AWS account to receive findings and optimization recommendations.</p><p>This combination will meet the requirements for the following reasons:</p><p>Installing the Amazon CloudWatch agent and configuring memory metric collection (option C) is essential because:</p><p>EC2 instances do not provide memory utilization metrics by default</p><p>Memory is one of the specific metrics mentioned in the requirements</p><p>The CloudWatch agent allows for detailed memory utilization data collection</p><p>This data is crucial for accurate rightsizing recommendations</p><p>Configuring AWS Compute Optimizer (option D) is the most comprehensive solution because:</p><p>It uses machine learning to analyze utilization patterns across CPU, memory, and network</p><p>It provides specific recommendations for instance types based on these metrics</p><p>It can identify both under-provisioned and over-provisioned resources</p><p>It offers detailed rightsizing recommendations with projected impact on performance and cost</p><p>The other options are less suitable for the specific requirements:</p><p>A. Purchasing AWS Business Support or Enterprise Support would provide access to more Trusted Advisor checks, but this alone doesn't provide the comprehensive CPU, memory, and network metric analysis needed.</p><p>B. Trusted Advisor's \"Low Utilization Amazon EC2 Instances\" check only looks at basic CPU and network metrics, not memory metrics, which are specifically required.</p><p>E. Creating an EC2 Instance Savings Plan would help reduce costs for committed usage but doesn't help with appropriately sizing instances based on utilization metrics.</p><p>The combination of CloudWatch agent for memory metrics and AWS Compute Optimizer provides the most comprehensive solution for analyzing all required metrics (CPU, memory, and network) and receiving specific optimization recommendations to properly size EC2 instances.</p><p>Sources</p><p>Amazon EC2 Cost and Capacity Optimization | Amazon Web Services （https://aws.amazon.com/cn/ec2/cost-and-capacity/）</p><p>My boss asking me on cost optimization in AWS | AWS re:Post （https://repost.aws/questions/QU3MV37BYRR6ePI6WmUgOW0g/my-boss-asking-me-on-cost-optimization-in-aws）</p><p>Identifying Opportunities to Right Size - Right Sizing: Provisioning Instances to Match Workloads （https://docs.aws.amazon.com/whitepapers/latest/cost-optimization-right-sizing/identifying-opportunities-to-right-size.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "642d3c63696b471ca5c269df7a71f432",
            "questionNumber": 171,
            "type": "single",
            "content": "<p>Question #171</p><p>A company uses an AWS CodeCommit repository. The company must store a backup copy of the data that is in the repository in a second AWS Region.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure AWS Elastic Disaster Recovery to replicate the CodeCommit repository data to the second Region."
                },
                {
                    "label": "B",
                    "content": "Use AWS Backup to back up the CodeCommit repository on an hourly schedule. Create a cross-Region copy in the second Region."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region."
                },
                {
                    "label": "D",
                    "content": "Create an AWS Step Functions workflow on an hourly schedule to take a snapshot of the CodeCommit repository. Configure the workflow to copy the snapshot to an S3 bucket in the second Region."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an Amazon EventBridge rule to invoke AWS CodeBuild when the company pushes code to the repository. Use CodeBuild to clone the repository. Create a .zip file of the content. Copy the file to an S3 bucket in the second Region.</p><p>This solution will meet the requirements for the following reasons:</p><p>Appropriate service selection: AWS CodeCommit repositories are not directly supported by AWS Backup or AWS Elastic Disaster Recovery. The EventBridge and CodeBuild approach is specifically designed to work with code repositories.</p><p>Automation and triggering: The EventBridge rule will automatically trigger the backup process whenever code is pushed to the repository, ensuring that the backup is always current and reflects the latest changes.</p><p>Cross-region capability: CodeBuild can be configured to copy the zipped repository content to an S3 bucket in the second region, fulfilling the cross-region backup requirement.</p><p>Efficiency: This approach is serverless and only runs when needed (when code is pushed), making it cost-effective.</p><p>Complete backup: The solution captures the entire repository content by cloning it, ensuring a complete backup.</p><p>The other options have significant limitations:</p><p>A. AWS Elastic Disaster Recovery is designed for EC2 instances and EBS volumes, not for code repositories like CodeCommit. It's not an appropriate service for this use case.</p><p>B. AWS Backup does not natively support CodeCommit repositories. It's designed for services like EBS, RDS, DynamoDB, and other storage services, but not for code repositories.</p><p>D. While AWS Step Functions could theoretically orchestrate a backup workflow, CodeCommit doesn't have a native \"snapshot\" feature that Step Functions could leverage. This approach would still require additional services to actually perform the backup, making it more complex than necessary.</p><p>The EventBridge and CodeBuild approach (option C) is a well-established pattern for backing up CodeCommit repositories to another region and represents the most appropriate solution for the stated requirements.</p><p>Sources</p><p>DR and Fault Tolerance | AWS re:Post （https://repost.aws/questions/QUJMujNJwQSpKQIjhmUtEbTQ/dr-and-fault-tolerance）</p><p>Backup - Ransomware Risk Management on AWS Using the NIST Cyber Security Framework (CSF) （https://docs.aws.amazon.com/whitepapers/latest/ransomware-risk-management-on-aws-using-nist-csf/backup.html）</p><p>Automate event-driven backups from CodeCommit to Amazon S3 using CodeBuild and CloudWatch Events - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/automate-event-driven-backups-from-codecommit-to-amazon-s3-using-codebuild-and-cloudwatch-events.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "e18205cb277e4729accbccd3aa1591ba",
            "questionNumber": 172,
            "type": "single",
            "content": "<p>Question #172</p><p>A company has multiple business units that each have separate accounts on AWS. Each business unit manages its own network with several VPCs that have CIDR ranges that overlap. The company’s marketing team has created a new internal application and wants to make the application accessible to all the other business units. The solution must use private IP addresses only.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Instruct each business unit to add a unique secondary CIDR range to the business unit&#39;s VPC. Peer the VPCs and use a private NAT gateway in the secondary range to route traffic to the marketing team."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon EC2 instance to serve as a virtual appliance in the marketing account&#39;s VPC. Create an AWS Site-to-Site VPN connection between the marketing team and each business unit&#39;s VPC. Perform NAT where necessary."
                },
                {
                    "label": "C",
                    "content": "Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses."
                },
                {
                    "label": "D",
                    "content": "Create a Network Load Balancer (NLB) in front of the marketing application in a private subnet. Create an API Gateway API. Use the Amazon API Gateway private integration to connect the API to the NLB. Activate IAM authorization for the API. Grant access to the accounts of the other business units."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>C. Create an AWS PrivateLink endpoint service to share the marketing application. Grant permission to specific AWS accounts to connect to the service. Create interface VPC endpoints in other accounts to access the application by using private IP addresses.</p><p>This solution will meet the requirements with the least operational overhead for the following reasons:</p><p>Handling overlapping CIDR ranges: AWS PrivateLink is specifically designed to work across VPCs with overlapping IP address ranges. It creates a private connection between services without exposing them to the public internet or requiring IP address management.</p><p>Private IP addresses only: PrivateLink uses interface VPC endpoints that operate entirely on private IP addresses, meeting the requirement to use only private IP addresses.</p><p>Minimal operational overhead:</p><p>No need to manage complex NAT configurations</p><p>No need to maintain VPN tunnels</p><p>No need to manage routing tables for overlapping CIDRs</p><p>Scales automatically as usage increases</p><p>Security and access control: PrivateLink allows the marketing team to grant specific permissions to each business unit's AWS account, providing fine-grained access control.</p><p>Simplicity: Each business unit simply creates an interface VPC endpoint in their account to connect to the marketing application, without having to worry about IP address conflicts.</p><p>Option B (using EC2 as a virtual appliance with Site-to-Site VPN and NAT) would require significantly more operational overhead:</p><p>Setting up and maintaining multiple VPN connections</p><p>Configuring and managing complex NAT rules to handle the overlapping CIDR ranges</p><p>Ensuring high availability of the EC2 instance serving as the virtual appliance</p><p>Managing routing tables and troubleshooting connectivity issues</p><p>Scaling the solution as traffic increases</p><p>AWS PrivateLink is purpose-built for this exact scenario - sharing services across multiple AWS accounts with overlapping IP ranges while maintaining private connectivity. It provides a much more streamlined and manageable solution compared to the VPN approach.</p><p>Sources</p><p>Use Amazon RDS Proxy and AWS PrivateLink to access Amazon RDS databases across AWS Organizations at American Family Insurance Group | AWS Database Blog （https://aws.amazon.com/cn/blogs/database/use-amazon-rds-proxy-and-aws-privatelink-to-access-amazon-rds-databases-across-aws-organizations-at-american-family-insurance-group/）</p><p>Public IPs and NLB from VPN | AWS re:Post （https://repost.aws/questions/QUYKMhQhj_SSKSXYjDl-xLBQ/public-ips-and-nlb-from-vpn）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "865d49b1acfa4d97b28b608411e9217c",
            "questionNumber": 173,
            "type": "single",
            "content": "<p>Question #173</p><p>A company needs to audit the security posture of a newly acquired AWS account. The company’s data security team requires a notification only when an Amazon S3 bucket becomes publicly exposed. The company has already established an Amazon Simple Notification Service (Amazon SNS) topic that has the data security team's email address subscribed.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an S3 event notification on all S3 buckets for the isPublic event. Select the SNS topic as the target for the event notifications."
                },
                {
                    "label": "B",
                    "content": "Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type &ldquo;Access Analyzer Finding&rdquo; with a filter for &ldquo;isPublic: true.&rdquo; Select the SNS topic as the EventBridge rule target."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon EventBridge rule for the event type &ldquo;Bucket-Level API Call via CloudTrail&rdquo; with a filter for &ldquo;PutBucketPolicy.&rdquo; Select the SNS topic as the EventBridge rule target."
                },
                {
                    "label": "D",
                    "content": "Activate AWS Config and add the cloudtrail-s3-dataevents-enabled rule. Create an Amazon EventBridge rule for the event type &ldquo;Config Rules Re-evaluation Status&rdquo; with a filter for &ldquo;NON_COMPLIANT.&rdquo; Select the SNS topic as the EventBridge rule target."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Create an analyzer in AWS Identity and Access Management Access Analyzer. Create an Amazon EventBridge rule for the event type \"Access Analyzer Finding\" with a filter for \"isPublic: true.\" Select the SNS topic as the EventBridge rule target.</p><p>This solution will meet the requirements for the following reasons:</p><p>Comprehensive detection: IAM Access Analyzer is specifically designed to analyze resource policies and identify when resources like S3 buckets are accessible to external entities, including public access. It continuously monitors for policy changes that could expose S3 buckets.</p><p>Accurate identification: Access Analyzer can precisely identify when a bucket becomes publicly accessible through various means (bucket policies, ACLs, etc.), reducing false positives and ensuring the data security team only receives relevant notifications.</p><p>Targeted notifications: By creating an EventBridge rule with a filter for \"isPublic: true,\" the solution ensures that notifications are only sent when buckets are specifically identified as publicly exposed, meeting the requirement for notification only when a bucket becomes publicly exposed.</p><p>Integration with existing infrastructure: The solution leverages the company's already established SNS topic, making implementation straightforward.</p><p>Security best practice: This approach aligns with AWS security best practices for monitoring resource access.</p><p>Option A (using S3 event notifications) is not correct because:</p><p>S3 event notifications don't have a specific \"isPublic\" event type that can reliably detect all scenarios where a bucket becomes publicly accessible.</p><p>S3 event notifications are primarily designed for tracking object-level operations (uploads, downloads, etc.) rather than changes to bucket access policies or ACLs that would make a bucket public.</p><p>This approach would require configuring notifications on each individual bucket, which becomes cumbersome as new buckets are created and doesn't provide a centralized view of security posture.</p><p>IAM Access Analyzer with EventBridge provides a more comprehensive, accurate, and maintainable solution for detecting and notifying when S3 buckets become publicly accessible, making it the appropriate choice for the company's security auditing requirements.</p><p>Sources</p><p>Automatically scan for public Amazon S3 buckets and block public access | AWS Storage Blog （https://aws.amazon.com/cn/blogs/storage/automatically-scan-for-public-amazon-s3-buckets-and-block-public-access/）</p><p>SEC03-BP07 Analyze public and cross-account access - Security Pillar （https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/sec_permissions_analyze_cross_account.html）</p><p>SEC03-BP07 Analyze public and cross-account access - AWS Well-Architected Framework （https://docs.aws.amazon.com/wellarchitected/latest/framework/sec_permissions_analyze_cross_account.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7dc5f8a669564ff992c4b8102c4c1a6e",
            "questionNumber": 174,
            "type": "single",
            "content": "<p>Question #174</p><p>A solutions architect needs to assess a newly acquired company’s portfolio of applications and databases. The solutions architect must create a business case to migrate the portfolio to AWS. The newly acquired company runs applications in an on-premises data center. The data center is not well documented. The solutions architect cannot immediately determine how many applications and databases exist. Traffic for the applications is variable. Some applications are batch processes that run at the end of each month.</p><p><br></p><p>The solutions architect must gain a better understanding of the portfolio before a migration to AWS can begin.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Server Migration Service (AWS SMS) and AWS Database Migration Service (AWS DMS) to evaluate migration. Use AWS Service Catalog to understand application and database dependencies."
                },
                {
                    "label": "B",
                    "content": "Use AWS Application Migration Service. Run agents on the on-premises infrastructure. Manage the agents by using AWS Migration Hub. Use AWS Storage Gateway to assess local storage needs and database dependencies."
                },
                {
                    "label": "C",
                    "content": "Use Migration Evaluator to generate a list of servers. Build a report for a business case. Use AWS Migration Hub to view the portfolio. Use AWS Application Discovery Service to gain an understanding of application dependencies."
                },
                {
                    "label": "D",
                    "content": "Use AWS Control Tower in the destination account to generate an application portfolio. Use AWS Server Migration Service (AWS SMS) to generate deeper reports and a business case. Use a landing zone for core accounts and resources."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Assess an undocumented on-premises portfolio (applications, databases, dependencies). &nbsp;</p><p>2. Understand traffic patterns (variable usage, batch processes). &nbsp;</p><p>3. Build a business case for migration (cost, effort, dependencies). &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>✅ Migration Evaluator (formerly TSO Logic) provides server inventory, cost estimates, and business case reports. &nbsp;</p><p>✅ AWS Application Discovery Service identifies application dependencies (critical for migration planning). &nbsp;</p><p>✅ Migration Hub consolidates discovery data into a single view of the portfolio. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: AWS SMS + DMS + Service Catalog &nbsp;</p><p>- SMS/DMS are for migration execution, not assessment. &nbsp;</p><p>- Service Catalog is for managing approved AWS services, not discovery. &nbsp;</p><p>❌ B: AWS Application Migration Service + Storage Gateway &nbsp;</p><p>- Application Migration Service (MGN) is for lift-and-shift, not assessment. &nbsp;</p><p>- Storage Gateway is for hybrid storage, not dependency mapping. &nbsp;</p><p>❌ D: AWS Control Tower + SMS + Landing Zone &nbsp;</p><p>- Control Tower is for multi-account governance, not discovery. &nbsp;</p><p>- SMS is not a discovery tool. &nbsp;</p><p> Implementation Steps (Option C): &nbsp;</p><p>1. Deploy Migration Evaluator to analyze on-premises servers (CPU, memory, usage). &nbsp;</p><p>2. Use Application Discovery Service (agent-based or agentless) to map dependencies. &nbsp;</p><p>3. View consolidated data in Migration Hub to prioritize migration waves. &nbsp;</p><p>4. Generate a business case report (TCO, ROI, migration strategy). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "171e725394314b6780c021116757e403",
            "questionNumber": 175,
            "type": "single",
            "content": "<p>Question #175</p><p>A company has an application that runs as a ReplicaSet of multiple pods in an Amazon Elastic Kubernetes Service (Amazon EKS) cluster. The EKS cluster has nodes in multiple Availability Zones. The application generates many small files that must be accessible across all running instances of the application. The company needs to back up the files and retain the backups for 1 year.</p><p><br></p><p>Which solution will meet these requirements while providing the FASTEST storage performance?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon Elastic File System (Amazon EFS) file system and a mount target for each subnet that contains nodes in the EKS cluster. Configure the ReplicaSet to mount the file system. Direct the application to store files in the file system. Configure AWS Backup to back up and retain copies of the data for 1 year."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon Elastic Block Store (Amazon EBS) volume. Enable the EBS Multi-Attach feature. Configure the ReplicaSet to mount the EBS volume. Direct the application to store files in the EBS volume. Configure AWS Backup to back up and retain copies of the data for 1 year."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon S3 bucket. Configure the ReplicaSet to mount the S3 bucket. Direct the application to store files in the S3 bucket. Configure S3 Versioning to retain copies of the data. Configure an S3 Lifecycle policy to delete objects after 1 year."
                },
                {
                    "label": "D",
                    "content": "Configure the ReplicaSet to use the storage available on each of the running application pods to store the files locally. Use a third-party tool to back up the EKS cluster for 1 year."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>The correct answer is A. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Shared storage accessible across all pods in an EKS ReplicaSet. &nbsp;</p><p>2. High performance for many small files. &nbsp;</p><p>3. Backup retention for 1 year. &nbsp;</p><p> Why Option A is Best? &nbsp;</p><p>✅ Amazon EFS provides shared, low-latency storage for Kubernetes pods. &nbsp;</p><p>✅ Multi-AZ availability (mount targets in each subnet). &nbsp;</p><p>✅ AWS Backup integrates with EFS for automated, long-term retention. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ B: EBS Multi-Attach &nbsp;</p><p>- Limited to a single Availability Zone (violates multi-AZ resilience). &nbsp;</p><p>- Not ideal for many small files (better for block storage). &nbsp;</p><p>❌ C: Amazon S3 &nbsp;</p><p>- S3 is object storage, not file storage (cannot be directly mounted as a filesystem in EKS without performance tradeoffs). &nbsp;</p><p>- Higher latency for small file operations. &nbsp;</p><p>❌ D: Local pod storage &nbsp;</p><p>- Not shared (files are pod-specific, lost if pod terminates). &nbsp;</p><p>- No built-in backup solution (third-party tools add complexity). &nbsp;</p><p> Implementation Steps (Option A): &nbsp;</p><p>1. Create an EFS file system with mount targets in each EKS node subnet. &nbsp;</p><p>2. Configure the ReplicaSet to mount EFS as a PersistentVolume (PV). &nbsp;</p><p>3. Set up AWS Backup for EFS with a 1-year retention policy. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "62b042c87eaa429d9cce58756acd8da8",
            "questionNumber": 176,
            "type": "single",
            "content": "<p>Question #176</p><p>A company runs a customer service center that accepts calls and automatically sends all customers a managed, interactive, two-way experience survey by text message. The applications that support the customer service center run on machines that the company hosts in an on-premises data center. The hardware that the company uses is old, and the company is experiencing downtime with the system. The company wants to migrate the system to AWS to improve reliability.</p><p><br></p><p>Which solution will meet these requirements with the LEAST ongoing operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers."
                },
                {
                    "label": "B",
                    "content": "Use Amazon Connect to replace the old call center hardware. Use Amazon Simple Notification Service (Amazon SNS) to send text message surveys to customers."
                },
                {
                    "label": "C",
                    "content": "Migrate the call center software to Amazon EC2 instances that are in an Auto Scaling group. Use the EC2 instances to send text message surveys to customers."
                },
                {
                    "label": "D",
                    "content": "Use Amazon Pinpoint to replace the old call center hardware and to send text message surveys to customers."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>A. Use Amazon Connect to replace the old call center hardware. Use Amazon Pinpoint to send text message surveys to customers.</p><p>This solution will meet the requirements with the least ongoing operational overhead for the following reasons:</p><p>Amazon Connect is specifically designed to replace traditional call center hardware with a cloud-based solution. It provides:</p><p>A fully managed contact center service that eliminates the need to manage physical hardware</p><p>Automatic scaling to handle call volume fluctuations</p><p>Built-in reliability and redundancy across AWS availability zones</p><p>Simple setup and configuration without specialized telephony knowledge</p><p>Integration capabilities with other AWS services</p><p>Amazon Pinpoint is purpose-built for sending interactive, two-way text message surveys:</p><p>It specializes in customer engagement through multiple channels, including SMS</p><p>It supports two-way messaging, which is specifically required for the interactive surveys</p><p>It provides analytics and campaign management features for tracking customer responses</p><p>It's a fully managed service that requires minimal operational overhead</p><p>Together, these services create a complete solution that addresses both the call center functionality and the text message survey requirements while minimizing operational overhead.</p><p>Option D (using Amazon Pinpoint to replace both the call center hardware and send text message surveys) is incorrect because:</p><p>Amazon Pinpoint is not designed to replace call center hardware or handle inbound voice calls</p><p>It lacks the call routing, queuing, and agent management capabilities needed for a customer service center</p><p>Using Pinpoint for call center functionality would require significant custom development, increasing operational overhead</p><p>By using Amazon Connect for the call center functionality and Amazon Pinpoint for the text message surveys, the company gets the best of both worlds - specialized services that are fully managed by AWS, requiring minimal operational overhead while providing the reliability improvements they're seeking.</p><p>Sources</p><p>Guidance for Enhancing Customer Engagement in Travel & Hospitality (T&H) Using AWS AI/ML （https://aws.amazon.com/cn/solutions/guidance/enhancing-customer-engagement-in-travel-and-hospitality-using-aws-ai-ml/）</p><p>Amazon Connect - Amazon Connect Data Lake Best Practices （https://docs.aws.amazon.com/connect/）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "57c9d9fd78ba4076a792cc5afc3e5847",
            "questionNumber": 177,
            "type": "single",
            "content": "<p>Question #177</p><p>A company is building a call center by using Amazon Connect. The company’s operations team is defining a disaster recovery (DR) strategy across AWS Regions. The contact center has dozens of contact flows, hundreds of users, and dozens of claimed phone numbers.</p><p><br></p><p>Which solution will provide DR with the LOWEST RTO?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS Lambda function to check the availability of the Amazon Connect instance and to send a notification to the operations team in case of unavailability. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. After notification, instruct the operations team to use the AWS Management Console to provision a new Amazon Connect instance in a second Region. Deploy the contact flows, users, and claimed phone numbers by using an AWS CloudFormation template."
                },
                {
                    "label": "B",
                    "content": "Provision a new Amazon Connect instance with all existing users in a second Region. Create an AWS Lambda function to check the availability of the Amazon Connect instance. Create an Amazon EventBridge rule to invoke the Lambda function every 5 minutes. In the event of an issue, configure the Lambda function to deploy an AWS CloudFormation template that provisions contact flows and claimed numbers in the second Region."
                },
                {
                    "label": "C",
                    "content": "Provision a new Amazon Connect instance with all existing contact flows and claimed phone numbers in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions all users. Configure the alarm to invoke the Lambda function."
                },
                {
                    "label": "D",
                    "content": "Provision a new Amazon Connect instance with all existing users and contact flows in a second Region. Create an Amazon Route 53 health check for the URL of the Amazon Connect instance. Create an Amazon CloudWatch alarm for failed health checks. Create an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. Configure the alarm to invoke the Lambda function."
                }
            ],
            "correctAnswer": "D",
            "explanation": "<p>The correct answer is D. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Disaster Recovery (DR) for Amazon Connect (contact flows, users, phone numbers). &nbsp;</p><p>2. Lowest RTO (minimize downtime during failover). &nbsp;</p><p>3. Automated failover (avoid manual steps). &nbsp;</p><p> Why Option D is Best? &nbsp;</p><p>✅ Pre-provisioned Amazon Connect instance (users + contact flows already deployed in secondary Region). &nbsp;</p><p>✅ Automated phone number provisioning via Lambda + CloudFormation (claimed numbers cannot be pre-provisioned). &nbsp;</p><p>✅ Route 53 health checks + CloudWatch alarm trigger failover automatically. &nbsp;</p><p>✅ Lowest RTO (only phone numbers need provisioning during failover). &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: Manual Failover &nbsp;</p><p>- High RTO (operations team must manually deploy CloudFormation). &nbsp;</p><p>- No pre-provisioning (slow recovery). &nbsp;</p><p>❌ B: Partial Pre-Provisioning &nbsp;</p><p>- Contact flows + phone numbers are not pre-provisioned (slower RTO than Option D). &nbsp;</p><p>❌ C: Missing Pre-Provisioned Users &nbsp;</p><p>- Users must be provisioned during failover (increases RTO). &nbsp;</p><p> Key Notes: &nbsp;</p><p>- Claimed phone numbers cannot be pre-provisioned (must be claimed during failover). &nbsp;</p><p>- Users + contact flows can be pre-provisioned (reduces RTO). &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f08a19f0a3c04e3c904584c8cbcda55c",
            "questionNumber": 178,
            "type": "single",
            "content": "<p>Question #178</p><p>A company runs an application on AWS. The company curates data from several different sources. The company uses proprietary algorithms to perform data transformations and aggregations. After the company performs ETL processes, the company stores the results in Amazon Redshift tables. The company sells this data to other companies. The company downloads the data as files from the Amazon Redshift tables and transmits the files to several data customers by using FTP. The number of data customers has grown significantly. Management of the data customers has become difficult.</p><p><br></p><p>The company will use AWS Data Exchange to create a data product that the company can use to share data with customers. The company wants to confirm the identities of the customers before the company shares data. The customers also need access to the most recent data when the company publishes the data.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Data Exchange for APIs to share data with customers. Configure subscription verification. In the AWS account of the company that produces the data, create an Amazon API Gateway Data API service integration with Amazon Redshift. Require the data customers to subscribe to the data product."
                },
                {
                    "label": "B",
                    "content": "In the AWS account of the company that produces the data, create an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster. Configure subscription verification. Require the data customers to subscribe to the data product."
                },
                {
                    "label": "C",
                    "content": "Download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically. Use AWS Data Exchange for S3 to share data with customers. Configure subscription verification. Require the data customers to subscribe to the data product."
                },
                {
                    "label": "D",
                    "content": "Publish the Amazon Redshift data to an Open Data on AWS Data Exchange. Require the customers to subscribe to the data product in AWS Data Exchange. In the AWS account of the company that produces the data, attach IAM resource-based policies to the Amazon Redshift tables to allow access only to verified AWS accounts."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. &nbsp;</p><p> Explanation: &nbsp;</p><p> Requirements: &nbsp;</p><p>1. Share data securely with customers via AWS Data Exchange. &nbsp;</p><p>2. Verify customer identities before granting access. &nbsp;</p><p>3. Ensure customers get the latest data (real-time or near-real-time updates). &nbsp;</p><p>4. Minimize operational overhead (avoid manual processes). &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>✅ AWS Data Exchange + Redshift datashare provides direct, real-time access to Redshift tables. &nbsp;</p><p>✅ Subscription verification ensures only approved customers access data. &nbsp;</p><p>✅ No manual ETL or file transfers (eliminates FTP and S3 downloads). &nbsp;</p><p>✅ Least operational overhead—fully managed by AWS. &nbsp;</p><p> Why Other Options Fail? &nbsp;</p><p>❌ A: Data Exchange for APIs + API Gateway &nbsp;</p><p>- Overkill for structured data (Redshift datashare is simpler). &nbsp;</p><p>- Higher operational overhead (API development/maintenance). &nbsp;</p><p>❌ C: Data Exchange for S3 + Periodic Downloads &nbsp;</p><p>- Not real-time (data must be exported from Redshift to S3 first). &nbsp;</p><p>- Adds ETL steps (increases operational overhead). &nbsp;</p><p>❌ D: Open Data on AWS + IAM Policies &nbsp;</p><p>- Open Data is for public datasets, not private customer data. &nbsp;</p><p>- IAM policies on Redshift are not scalable for many customers. &nbsp;</p><p> Implementation Steps (Option B): &nbsp;</p><p>1. Create a datashare in AWS Data Exchange linked to the Redshift cluster. &nbsp;</p><p>2. Enable subscription verification to authenticate customers. &nbsp;</p><p>3. Customers subscribe to the data product in AWS Data Exchange. &nbsp;</p><p>4. Data is automatically updated in Redshift and accessible to subscribers. &nbsp;</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "dc13b2b42c4d4deabd4b12b5f3de6bc8",
            "questionNumber": 179,
            "type": "single",
            "content": "<p>Question #179</p><p>A solutions architect is designing a solution to process events. The solution must have the ability to scale in and out based on the number of events that the solution receives. If a processing error occurs, the event must move into a separate queue for review.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Send event details to an Amazon Simple Notification Service (Amazon SNS) topic. Configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Add an on-failure destination to the function. Set an Amazon Simple Queue Service (Amazon SQS) queue as the target."
                },
                {
                    "label": "B",
                    "content": "Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messages to a dead-letter queue."
                },
                {
                    "label": "C",
                    "content": "Write events to an Amazon DynamoDB table. Configure a DynamoDB stream for the table. Configure the stream to invoke an AWS Lambda function. Configure the Lambda function to process the events."
                },
                {
                    "label": "D",
                    "content": "Publish events to an Amazon EventBridge event bus. Create and run an application on an Amazon EC2 instance with an Auto Scaling group that is behind an Application Load Balancer (ALB). Set the ALB as the event bus target. Configure the event bus to retry events. Write messages to a dead-letter queue if the application cannot process the messages."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the information provided, the correct answer is:</p><p>B. Publish events to an Amazon Simple Queue Service (Amazon SQS) queue. Create an Amazon EC2 Auto Scaling group. Configure the Auto Scaling group to scale in and out based on the ApproximateAgeOfOldestMessage metric of the queue. Configure the application to write failed messages to a dead-letter queue.</p><p>This solution meets the requirements for the following reasons:</p><p>Scaling capability: The EC2 Auto Scaling group can be configured to scale in and out based on the ApproximateAgeOfOldestMessage metric of the SQS queue. This ensures that as the number of events increases, more EC2 instances are added to process them, and as the queue empties, instances are removed to save costs.</p><p>Error handling: The solution explicitly states that the application will be configured to write failed messages to a dead-letter queue (DLQ). This satisfies the requirement that if a processing error occurs, the event must move into a separate queue for review.</p><p>Decoupling: Using SQS as the event queue provides a buffer between event producers and consumers, allowing for reliable event processing even during traffic spikes or when processing instances fail.</p><p>Control over processing: This approach gives more control over how events are processed, including retry logic, batch processing, and custom error handling.</p><p>Option A (using SNS with Lambda and an on-failure destination) would also provide scaling and error handling capabilities. However, there are a few reasons why option B might be considered more appropriate for certain scenarios:</p><p>Long-running processes: If the event processing requires more time than Lambda's maximum execution duration (currently 15 minutes), EC2 instances would be necessary.</p><p>Custom processing requirements: If the processing requires specific software or libraries that are difficult to package with Lambda, EC2 instances provide more flexibility.</p><p>Cost considerations: For high-volume, consistent workloads, EC2 instances might be more cost-effective than Lambda functions.</p><p>Batch processing: EC2 instances can be configured to process multiple messages in a batch, which might be more efficient for certain workloads.</p><p>Both solutions could work, but option B provides a more traditional and flexible approach to event processing with explicit scaling based on queue metrics and clear error handling through a dead-letter queue.</p><p>Sources</p><p>Scaling and event processing with SNS and SQS | AWS re:Post （https://ap-southeast-2.console.aws.amazon.com/console/home?nc2=h_ct&region=ap-southeast-2&src=header-signin#）</p><p>Implementing AWS Lambda error handling patterns | AWS Compute Blog （https://aws.amazon.com/cn/blogs/compute/implementing-aws-lambda-error-handling-patterns/）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "82f0a9c6e8654b4bb9f3e5f400f6df6f",
            "questionNumber": 180,
            "type": "single",
            "content": "<p>Question #180</p><p>A company runs a processing engine in the AWS Cloud. The engine processes environmental data from logistics centers to calculate a sustainability index. The company has millions of devices in logistics centers that are spread across Europe. The devices send information to the processing engine through a RESTful API.</p><p><br></p><p>The API experiences unpredictable bursts of traffic. The company must implement a solution to process all data that the devices send to the processing engine. Data loss is unacceptable.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Application Load Balancer (ALB) for the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create a listener and a target group for the ALB. Add the SQS queue as the target. Use a container that runs in Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type to process messages in the queue."
                },
                {
                    "label": "B",
                    "content": "Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue."
                },
                {
                    "label": "C",
                    "content": "Create an Amazon API Gateway REST API that implements the RESTful API. Create a fleet of Amazon EC2 instances in an Auto Scaling group. Create an API Gateway Auto Scaling group proxy integration. Use the EC2 instances to process incoming data."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon CloudFront distribution for the RESTful API. Create a data stream in Amazon Kinesis Data Streams. Set the data stream as the origin for the distribution. Create an AWS Lambda function to consume and process data in the data stream."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Answer: &nbsp;</p><p>B. Create an Amazon API Gateway HTTP API that implements the RESTful API. Create an Amazon Simple Queue Service (Amazon SQS) queue. Create an API Gateway service integration with the SQS queue. Create an AWS Lambda function to process messages in the SQS queue. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Unpredictable Traffic Bursts: &nbsp;</p><p> &nbsp; - Millions of devices send data with spiky, unpredictable loads. &nbsp;</p><p>2. Zero Data Loss: &nbsp;</p><p> &nbsp; - Must durably queue all incoming requests. &nbsp;</p><p>3. RESTful API Endpoint: &nbsp;</p><p> &nbsp; - Devices communicate via HTTP(S). &nbsp;</p><p> Why Option B is Best? &nbsp;</p><p>- API Gateway HTTP API: &nbsp;</p><p> &nbsp;- Low-latency RESTful interface (cheaper/faster than REST API). &nbsp;</p><p> &nbsp;- Direct SQS Integration: &nbsp;</p><p> &nbsp; &nbsp;- Posts device data directly to SQS (no intermediate compute). &nbsp;</p><p> &nbsp; &nbsp;- SQS guarantees no data loss (messages persist until processed). &nbsp;</p><p>- Lambda for Processing: &nbsp;</p><p> &nbsp;- Auto-scales with queue depth (handles bursts seamlessly). &nbsp;</p><p> &nbsp;- Serverless: No EC2 management overhead. &nbsp;</p><p> Architecture Flow: &nbsp;</p><p>1. Devices → API Gateway HTTP API → SQS Queue (buffers bursts). &nbsp;</p><p>2. SQS → Lambda (processes messages, calculates sustainability index). &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (ALB → SQS): &nbsp;</p><p> &nbsp;- ALB can’t natively integrate with SQS (requires Lambda proxy). &nbsp;</p><p> &nbsp;- ECS Fargate is overkill (Lambda is simpler for message processing). &nbsp;</p><p>- C (API Gateway → EC2): &nbsp;</p><p> &nbsp;- EC2 scaling lags behind sudden traffic spikes (risk of data loss). &nbsp;</p><p>- D (CloudFront → Kinesis): &nbsp;</p><p> &nbsp;- Kinesis is expensive for simple queuing (SQS is cheaper). &nbsp;</p><p> &nbsp;- CloudFront doesn’t integrate with Kinesis directly. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create SQS Queue: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws sqs create-queue --queue-name device-data</p><p> &nbsp; ``` &nbsp;</p><p>2. Configure API Gateway HTTP API: &nbsp;</p><p> &nbsp; - Add SQS integration (POST to `https://sqs.region.amazonaws.com/account-id/queue-name`). &nbsp;</p><p>3. Deploy Lambda Consumer: &nbsp;</p><p> &nbsp; ```python</p><p> &nbsp; def lambda_handler(event, context):</p><p> &nbsp; &nbsp; &nbsp; for record in event['Records']:</p><p> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; process_data(record['body'])</p><p> &nbsp; ``` &nbsp;</p><p> Cost Efficiency: &nbsp;</p><p>| Service &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Cost Advantage &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;</p><p>|-------------------|--------------------------------------------| &nbsp;</p><p>| API Gateway &nbsp; | $1.00/million requests (HTTP API pricing). | &nbsp;</p><p>| SQS &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | $0.40/million requests. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp;</p><p>| Lambda &nbsp; &nbsp; &nbsp; &nbsp;| $0.20/million invocations. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp;</p><p> Conclusion: &nbsp;</p><p>B is the only solution that: &nbsp;</p><p>- Handles spiky traffic (SQS buffer). &nbsp;</p><p>- Prevents data loss (SQS durability). &nbsp;</p><p>- Scales cost-effectively (serverless). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "aec99a38c5644afb8661bf9ed4e470c5",
            "questionNumber": 181,
            "type": "single",
            "content": "<p>uestion #181</p><p>A company is designing its network configuration in the AWS Cloud. The company uses AWS Organizations to manage a multi-account setup. The company has three OUs. Each OU contains more than 100 AWS accounts. Each account has a single VPC, and all the VPCs in each OU are in the same AWS Region.</p><p><br></p><p>The CIDR ranges for all the AWS accounts do not overlap. The company needs to implement a solution in which VPCs in the same OU can communicate with each other but cannot communicate with VPCs in other OUs.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an AWS CloudFormation stack set that establishes VPC peering between accounts in each OU. Provision the stack set in each OU."
                },
                {
                    "label": "B",
                    "content": "In each OU, create a dedicated networking account that has a single VPC. Share this VPC with all the other accounts in the OU by using AWS Resource Access Manager (AWS RAM). Create a VPC peering connection between the networking account and each account in the OU."
                },
                {
                    "label": "C",
                    "content": "Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM). Create transit gateway VPC attachments for each VPC."
                },
                {
                    "label": "D",
                    "content": "In each OU, create a dedicated networking account that has a single VPC. Establish a VPN connection between the networking account and the other accounts in the OU. Use third-party routing software to route transitive traffic between the VPCs."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the scenario with three OUs each containing more than 100 AWS accounts with non-overlapping CIDR ranges, the solution that will meet the requirements with the LEAST operational overhead is:</p><p>C. Provision a transit gateway in an account in each OU. Share the transit gateway across the organization by using AWS Resource Access Manager (AWS RAM). Create transit gateway VPC attachments for each VPC.</p><p>This is the correct choice because:</p><p>Transit Gateway acts as a network transit hub that can connect thousands of VPCs, significantly reducing the number of connections required compared to VPC peering.</p><p>With VPC peering, the number of connections grows exponentially as the number of VPCs increases. For 100+ accounts per OU, this would mean thousands of individual peering connections to manage within each OU, creating substantial operational overhead.</p><p>Using AWS Resource Access Manager (AWS RAM) to share the transit gateway across accounts within each OU simplifies management by centralizing the networking configuration.</p><p>Transit Gateway route tables provide granular control over routing between VPCs, making it easy to allow communication within an OU while preventing communication between OUs.</p><p>This approach is more scalable and future-proof, allowing for easy addition of new accounts or VPCs as the organization grows without requiring reconfiguration of existing connections.</p><p>Option A (VPC peering with CloudFormation stack sets) would create significant operational overhead because:</p><p>It would require managing n(n-1)/2 peering connections within each OU (where n is the number of VPCs)</p><p>For 100+ accounts per OU, this means thousands of peering connections per OU</p><p>Each new account added would require new peering connections with all existing accounts</p><p>VPC peering doesn't support transitive routing, making the configuration more complex</p><p>The Transit Gateway approach with AWS RAM provides a more centralized, manageable, and scalable solution with significantly less operational overhead for this large multi-account environment.</p><p>Sources</p><p>Communicating across VPCs and AWS Regions - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/secure-outbound-network-traffic/vpc-region-communication.html）</p><p>Transit gateway attachment configuration | AWS re:Post （https://repost.aws/questions/QUvX0QyO3pRQWkBQspwcgVKg/transit-gateway-attachment-configuration）</p><p>how to Connect VPCs with Overlapping CIDRs Across Accounts | AWS re:Post （https://repost.aws/questions/QU0z4ZmjEESRuDr1K1_ZYjYw/how-to-connect-vpcs-with-overlapping-cidrs-across-accounts）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2d4a3649bbce4261a1161b217d409ab4",
            "questionNumber": 182,
            "type": "single",
            "content": "<p>Question #182</p><p>A company is migrating an application to AWS. It wants to use fully managed services as much as possible during the migration. The company needs to store large important documents within the application with the following requirements:</p><p><br></p><p>1. The data must be highly durable and available</p><p>2. The data must always be encrypted at rest and in transit</p><p>3. The encryption key must be managed by the company and rotated periodically</p><p><br></p><p>Which of the following solutions should the solutions architect recommend?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy the storage gateway to AWS in file gateway mode. Use Amazon EBS volume encryption using an AWS KMS key to encrypt the storage gateway volumes."
                },
                {
                    "label": "B",
                    "content": "Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption."
                },
                {
                    "label": "C",
                    "content": "Amazon DynamoDB with SSL to connect to DynamoDB. Use an AWS KMS key to encrypt DynamoDB objects at rest."
                },
                {
                    "label": "D",
                    "content": "Deploy instances with Amazon EBS volumes attached to store this data. Use EBS volume encryption using an AWS KMS key to encrypt the data."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the requirements for storing large important documents with high durability, availability, encryption at rest and in transit, and customer-managed encryption keys that can be rotated periodically, the answer is:</p><p>B. Use Amazon S3 with a bucket policy to enforce HTTPS for connections to the bucket and to enforce server-side encryption and AWS KMS for object encryption.</p><p>This is the correct choice because:</p><p>Amazon S3 provides industry-leading durability of 99.999999999% (11 9's), meeting the requirement for high durability. It also offers high availability across multiple Availability Zones.</p><p>S3 bucket policies can be configured to enforce HTTPS (TLS) for all connections, ensuring data is encrypted in transit.</p><p>S3 supports server-side encryption with AWS KMS keys (SSE-KMS), allowing the company to manage their own encryption keys.</p><p>AWS KMS provides key rotation capabilities, allowing the company to periodically rotate their encryption keys while maintaining access to their encrypted data.</p><p>S3 is a fully managed service, aligning with the company's requirement to use fully managed services as much as possible.</p><p>S3 is optimized for storing large objects like documents, making it ideal for this use case.</p><p>Option A (Storage Gateway in file gateway mode) is not the best choice because it's primarily designed for hybrid cloud storage scenarios and adds unnecessary complexity when the goal is to use fully managed services.</p><p>Option C (DynamoDB with SSL and KMS) is not optimal because DynamoDB is designed for structured data and key-value pairs, not for storing large documents. While it supports encryption features, it's not the most appropriate service for this specific use case.</p><p>Amazon S3 with the specified security configurations provides the best combination of durability, availability, encryption capabilities, and management simplicity for storing large important documents.</p><p>Sources</p><p>Serverless data storage options - AWS Serverless Multi-Tier Architectures with Amazon API Gateway and AWS Lambda （https://docs.aws.amazon.com/whitepapers/latest/serverless-multi-tier-architectures-api-gateway-lambda/serverless-data-storage-options.html）</p><p>Storage - AWS Well-Architected Framework (2023-04-10) （https://docs.aws.amazon.com/wellarchitected/2023-04-10/framework/perf-storage.html）</p><p>DynamoDB encryption at rest - Amazon DynamoDB （https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/EncryptionAtRest.html）</p><p>Choosing AWS services for data protection - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/backup-recovery/services.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "2cc2270340bc41d292b9f3815b9da2b9",
            "questionNumber": 183,
            "type": "single",
            "content": "<p>Question #183 <br>A company’s public API runs as tasks on Amazon Elastic Container Service (Amazon ECS). The tasks run on AWS Fargate behind an Application Load Balancer (ALB) and are configured with Service Auto Scaling for the tasks based on CPU utilization. This service has been running well for several months.</p><p><br></p><p>Recently, API performance slowed down and made the application unusable. The company discovered that a significant number of SQL injection attacks had occurred against the API and that the API service had scaled to its maximum amount.</p><p><br></p><p>A solutions architect needs to implement a solution that prevents SQL injection attacks from reaching the ECS API service. The solution must allow legitimate traffic through and must maximize operational efficiency.</p><p><br></p><p>Which solution meets these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks."
                },
                {
                    "label": "B",
                    "content": "Create a new AWS WAF Bot Control implementation. Add a rule in the AWS WAF Bot Control managed rule group to monitor traffic and allow only legitimate traffic to the ALB in front of the ECS tasks."
                },
                {
                    "label": "C",
                    "content": "Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other traffic that does not match those rules. Attach the web ACL to the ALB in front of the ECS tasks."
                },
                {
                    "label": "D",
                    "content": "Create a new AWS WAF web ACL. Create a new empty IP set in AWS WAF. Add a new rule to the web ACL to block requests that originate from IP addresses in the new IP set. Create an AWS Lambda function that scrapes the API logs for IP addresses that send SQL injection attacks, and add those IP addresses to the IP set. Attach the web ACL to the ALB in front of the ECS tasks."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p><br></p><p> Answer: &nbsp;</p><p>C. Create a new AWS WAF web ACL. Add a new rule that blocks requests that match the SQL database rule group. Set the web ACL to allow all other traffic that does not match those rules. Attach the web ACL to the ALB in front of the ECS tasks. &nbsp;</p><p> Explanation: &nbsp;</p><p> Key Requirements: &nbsp;</p><p>1. Prevent SQL Injection Attacks: &nbsp;</p><p> &nbsp; - Block malicious SQL queries before they reach ECS tasks. &nbsp;</p><p>2. Allow Legitimate Traffic: &nbsp;</p><p> &nbsp; - Ensure valid API requests pass through. &nbsp;</p><p>3. Operational Efficiency: &nbsp;</p><p> &nbsp; - Use managed services (minimize custom code/maintenance). &nbsp;</p><p> Why Option C is Best? &nbsp;</p><p>- AWS WAF SQL Database Rule Group: &nbsp;</p><p> &nbsp;- Predefined rules to block common SQLi patterns (e.g., `' OR 1=1 --`). &nbsp;</p><p> &nbsp;- No manual tuning required (AWS manages rule updates). &nbsp;</p><p>- Web ACL Configuration: &nbsp;</p><p> &nbsp;- Block SQLi requests + allow all other traffic (default action). &nbsp;</p><p> &nbsp;- Attach to ALB (frontend protection). &nbsp;</p><p>- Zero Impact on Scaling: &nbsp;</p><p> &nbsp;- Malicious requests are blocked before consuming ECS resources. &nbsp;</p><p> Implementation Steps: &nbsp;</p><p>1. Create Web ACL: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws wafv2 create-web-acl \\</p><p> &nbsp; &nbsp; --name SQLi-Protection \\</p><p> &nbsp; &nbsp; --scope REGIONAL \\</p><p> &nbsp; &nbsp; --default-action Allow={} \\</p><p> &nbsp; &nbsp; --visibility-config SampledRequestsEnabled=true,CloudWatchMetricsEnabled=true</p><p> &nbsp; ``` &nbsp;</p><p>2. Add SQLi Rule: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws wafv2 create-rule-group \\</p><p> &nbsp; &nbsp; --name SQLi-Rules \\</p><p> &nbsp; &nbsp; --scope REGIONAL \\</p><p> &nbsp; &nbsp; --capacity 100 \\</p><p> &nbsp; &nbsp; --rules '[</p><p> &nbsp; &nbsp; &nbsp; {</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Name\": \"SQLi-Block\",</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Priority\": 1,</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Statement\": { \"ManagedRuleGroupStatement\": { \"VendorName\": \"AWS\", \"Name\": \"AWSManagedRulesSQLiRuleSet\" } },</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"Action\": { \"Block\": {} },</p><p> &nbsp; &nbsp; &nbsp; &nbsp; \"VisibilityConfig\": { \"SampledRequestsEnabled\": true, \"CloudWatchMetricsEnabled\": true }</p><p> &nbsp; &nbsp; &nbsp; }</p><p> &nbsp; &nbsp; ]'</p><p> &nbsp; ``` &nbsp;</p><p>3. Attach to ALB: &nbsp;</p><p> &nbsp; ```bash</p><p> &nbsp; aws wafv2 associate-web-acl \\</p><p> &nbsp; &nbsp; --web-acl-arn arn:aws:wafv2:us-east-1:123456789012:regional/webacl/SQLi-Protection \\</p><p> &nbsp; &nbsp; --resource-arn arn:aws:elasticloadbalancing:us-east-1:123456789012:loadbalancer/app/my-alb/1234567890123456</p><p> &nbsp; ``` &nbsp;</p><p> Why Not Other Options? &nbsp;</p><p>- A (Monitor Only): &nbsp;</p><p> &nbsp;- Doesn’t block attacks (only logs them). &nbsp;</p><p>- B (Bot Control): &nbsp;</p><p> &nbsp;- Bot Control targets scrapers, not SQLi (different threat). &nbsp;</p><p>- D (IP Blocking): &nbsp;</p><p> &nbsp;- Reactive: Requires manual Lambda to update IPs (inefficient). &nbsp;</p><p> &nbsp;- Easily bypassed (attackers change IPs). &nbsp;</p><p> Cost Efficiency: &nbsp;</p><p>- AWS WAF: $5/web ACL/month + $1/rule/month (SQLi rule group is free). &nbsp;</p><p>- No EC2/Lambda costs (unlike Option D). &nbsp;</p><p> Conclusion: &nbsp;</p><p>C is the only solution that: &nbsp;</p><p>- Blocks SQLi proactively (managed rules). &nbsp;</p><p>- Preserves scaling (no resource waste on attacks). &nbsp;</p><p>- Requires no maintenance (fully managed). &nbsp;</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "20ebee8090584af3bf0d60a0c2946bf4",
            "questionNumber": 184,
            "type": "single",
            "content": "<p>Question #184</p><p>An environmental company is deploying sensors in major cities throughout a country to measure air quality. The sensors connect to AWS IoT Core to ingest timeseries data readings. The company stores the data in Amazon DynamoDB.</p><p><br></p><p>For business continuity, the company must have the ability to ingest and store data in two AWS Regions.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon Route 53 alias failover routing policy with values for AWS IoT Core data endpoints in both Regions. Migrate data to Amazon Aurora global tables."
                },
                {
                    "label": "B",
                    "content": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Migrate the data to Amazon MemoryDB for Redis and configure cross-Region replication."
                },
                {
                    "label": "C",
                    "content": "Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations. Update the DynamoDB table to a global table."
                },
                {
                    "label": "D",
                    "content": "Create an Amazon Route 53 latency-based routing policy. Use AWS IoT Core data endpoints in both Regions as values. Configure DynamoDB streams and cross-Region data replication."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirements for deploying IoT sensors that connect to AWS IoT Core and store data in DynamoDB across two AWS Regions for business continuity, the answer is:</p><p>C. Create a domain configuration for AWS IoT Core in each Region. Create an Amazon Route 53 health check that evaluates domain configuration health. Create a failover routing policy with values for the domain name from the AWS IoT Core domain configurations. Update the DynamoDB table to a global table.</p><p>This is the correct choice because:</p><p>Domain configurations for AWS IoT Core allow you to create custom endpoints in each region, which is necessary for multi-region IoT deployments. This gives the sensors a consistent endpoint to connect to while allowing traffic to be routed to different regions.</p><p>Route 53 health checks can monitor the health of the IoT Core endpoints in each region, enabling automatic failover when issues are detected in the primary region.</p><p>The failover routing policy with the domain names from the IoT Core domain configurations ensures that IoT devices will connect to the healthy endpoint, maintaining data ingestion capabilities even if one region experiences an outage.</p><p>DynamoDB global tables provide multi-region replication, ensuring that data is automatically replicated across both regions. This is critical for business continuity as it ensures data availability in both regions without requiring custom replication logic.</p><p>Option A is incorrect because it mentions migrating data to Amazon Aurora global tables, which is not appropriate for this use case. The requirement specifically mentions storing data in DynamoDB, not Aurora. Additionally, simply creating a Route 53 alias failover policy without proper domain configurations for IoT Core would not provide the necessary infrastructure for multi-region IoT data ingestion.</p><p>The solution in option C provides a comprehensive approach to ensuring business continuity for both data ingestion (through IoT Core with domain configurations and Route 53 failover) and data storage (through DynamoDB global tables), meeting all the stated requirements.</p><p>Sources</p><p>How to implement a disaster recovery solution for IoT platforms on AWS | The Internet of Things on AWS – Official Blog （https://aws.amazon.com/cn/blogs/iot/how-to-implement-a-disaster-recovery-solution-for-iot-platforms-on-aws/）</p><p>Migrating connected device fleets to AWS with Custom Domains for AWS IoT Core | The Internet of Things on AWS – Official Blog （https://aws.amazon.com/cn/blogs/iot/migrating-devices-aws-iot-custom-domains/）</p><p>Community | Building a Multi-region Serverless IoT system （https://community.aws/content/2fjFMxLIwHkcuCy8ui7ya2KRbPt/building-a-multi-region-serverless-iot-system）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d8908983af934a21800ac47cd2eb6697",
            "questionNumber": 185,
            "type": "single",
            "content": "<p>Question #185</p><p>A company uses AWS Organizations for a multi-account setup in the AWS Cloud. The company's finance team has a data processing application that uses AWS Lambda and Amazon DynamoDB. The company's marketing team wants to access the data that is stored in the DynamoDB table. </p><p><br></p><p>The DynamoDB table contains confidential data. The marketing team can have access to only specific attributes of data in the DynamoDB table. The finance team and the marketing team have separate AWS accounts.</p><p><br></p><p>What should a solutions architect do to provide the marketing team with the appropriate access to the DynamoDB table?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an SCP to grant the marketing team&#39;s AWS account access to the specific attributes of the DynamoDB table. Attach the SCP to the OU of the finance team."
                },
                {
                    "label": "B",
                    "content": "Create an IAM role in the finance team&#39;s account by using IAM policy conditions for specific DynamoDB attributes (fine-grained access control). Establish trust with the marketing team&#39;s account. In the marketing team&#39;s account, create an IAM role that has permissions to assume the IAM role in the finance team&#39;s account."
                },
                {
                    "label": "C",
                    "content": "Create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). Attach the policy to the DynamoDB table. In the marketing team&#39;s account, create an IAM role that has permissions to access the DynamoDB table in the finance team&#39;s account."
                },
                {
                    "label": "D",
                    "content": "Create an IAM role in the finance team&#39;s account to access the DynamoDB table. Use an IAM permissions boundary to limit the access to the specific attributes. In the marketing team&#39;s account, create an IAM role that has permissions to assume the IAM role in the finance team&#39;s account."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. </p><p> Why Option B is Correct:</p><p>1. Fine-Grained Access Control with IAM Policy Conditions: &nbsp;</p><p> &nbsp; - The finance team's DynamoDB table contains confidential data, and the marketing team should only access specific attributes. &nbsp;</p><p> &nbsp; - An IAM role in the finance team's account can be configured with a policy that restricts access to only the required DynamoDB attributes using [fine-grained access control](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifying-conditions.html).</p><p>2. Cross-Account Access via Role Assumption: &nbsp;</p><p> &nbsp; - The marketing team's account needs to assume the IAM role in the finance team's account. &nbsp;</p><p> &nbsp; - The finance team's IAM role must have a trust policy allowing the marketing team's account to assume it. &nbsp;</p><p> &nbsp; - The marketing team's IAM role must have permissions to assume the role in the finance team's account (`sts:AssumeRole`).</p><p>3. Secure and Scalable Solution: &nbsp;</p><p> &nbsp; - This approach follows AWS best practices for cross-account access without exposing credentials. &nbsp;</p><p> &nbsp; - It ensures least privilege by restricting access to only the required DynamoDB attributes.</p><p> Why Other Options Are Incorrect:</p><p>- Option A: &nbsp;</p><p> &nbsp;- Service Control Policies (SCPs) are used to set guardrails (permission boundaries) for accounts in AWS Organizations. &nbsp;</p><p> &nbsp;- They cannot grant fine-grained DynamoDB attribute access. &nbsp;</p><p> &nbsp;- SCPs deny or allow services at a high level but do not manage DynamoDB attribute-level permissions.</p><p>- Option C: &nbsp;</p><p> &nbsp;- Resource-based policies (attached to DynamoDB) are not supported for DynamoDB (unlike S3 or SQS). &nbsp;</p><p> &nbsp;- DynamoDB access must be managed via IAM policies attached to roles/users, not directly on the table.</p><p>- Option D: &nbsp;</p><p> &nbsp;- Permissions boundaries define the maximum permissions an IAM entity can have but do not enforce fine-grained DynamoDB attribute filtering. &nbsp;</p><p> &nbsp;- The marketing team would still need an IAM policy with conditions to restrict DynamoDB attribute access.</p><p> Conclusion: &nbsp;</p><p>Option B is the correct solution because it uses fine-grained IAM policies with conditions to restrict DynamoDB access and enables secure cross-account role assumption.</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "4c0ec3899ed64230ab1ff8133a0ddaa4",
            "questionNumber": 186,
            "type": "multiple",
            "content": "<p>Question #186</p><p>A solutions architect is creating an application that stores objects in an Amazon S3 bucket. The solutions architect must deploy the application in two AWS Regions that will be used simultaneously. The objects in the two S3 buckets must remain synchronized with each other.</p><p>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an S3 Multi-Region Access Point. Change the application to refer to the Multi-Region Access Point."
                },
                {
                    "label": "B",
                    "content": "Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets."
                },
                {
                    "label": "C",
                    "content": "Modify the application to store objects in each S3 bucket."
                },
                {
                    "label": "D",
                    "content": "Create an S3 Lifecycle rule for each S3 bucket to copy objects from one S3 bucket to the other S3 bucket."
                },
                {
                    "label": "E",
                    "content": "Enable S3 Versioning for each S3 bucket."
                },
                {
                    "label": "F",
                    "content": "Configure an event notification for each S3 bucket to invoke an AWS Lambda function to copy objects from one S3 bucket to the other S3 bucket."
                }
            ],
            "correctAnswer": "ABE",
            "explanation": "<p>Based on the requirement to create an application that stores objects in S3 buckets across two AWS Regions that must remain synchronized with the least operational overhead, the correct combination of steps is:</p><p>A. Create an S3 Multi-Region Access Point. Change the application to refer to the Multi-Region Access Point. </p><p>B. Configure two-way S3 Cross-Region Replication (CRR) between the two S3 buckets. </p><p>E. Enable S3 Versioning for each S3 bucket.</p><p>Here's why these three steps together provide the optimal solution with the least operational overhead:</p><p>S3 Multi-Region Access Point (Option A):</p><p>Provides a single global endpoint for the application to use when accessing objects</p><p>Automatically routes requests to the appropriate region based on network conditions and availability</p><p>Simplifies application code as it only needs to reference one endpoint instead of managing multiple regional endpoints</p><p>Reduces operational overhead by eliminating the need for custom routing logic</p><p>Two-way S3 Cross-Region Replication (Option B):</p><p>Automatically synchronizes objects between the two S3 buckets in different regions</p><p>Ensures that objects created or updated in either region are replicated to the other region</p><p>Provides a fully managed replication service with no custom code required</p><p>Maintains consistency between buckets with minimal operational effort</p><p>Enable S3 Versioning (Option E):</p><p>Required prerequisite for both S3 Cross-Region Replication and Multi-Region Access Points</p><p>Ensures that all versions of objects are maintained and properly replicated</p><p>Provides additional data protection benefits</p><p>The other options would increase operational overhead:</p><p>Option C (Modify the application to store objects in each S3 bucket) would require custom application logic to write to multiple buckets, handle failures, and ensure consistency, creating significant operational overhead.</p><p>Option D (Create S3 Lifecycle rules to copy objects) is not designed for real-time synchronization and would be less efficient than using the purpose-built CRR feature.</p><p>The combination of Multi-Region Access Points, two-way Cross-Region Replication, and S3 Versioning provides a fully managed solution that keeps the buckets synchronized with minimal operational overhead while allowing the application to use both regions simultaneously.</p><p>Sources</p><p>Object Storage Features – Amazon S3 （https://aws.amazon.com/cn/s3/features/replication/）</p><p>Amazon S3 Multi-Region Access Points failover controls - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/MrapFailover.html）</p><p>Data protection in Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/data-protection.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "026d76e040f743378bde3f4c55912744",
            "questionNumber": 187,
            "type": "multiple",
            "content": "<p>Question #187</p><p>A company has an IoT platform that runs in an on-premises environment. The platform consists of a server that connects to IoT devices by using the MQTT protocol. The platform collects telemetry data from the devices at least once every 5 minutes. The platform also stores device metadata in a MongoDB cluster.</p><p>An application that is installed on an on-premises machine runs periodic jobs to aggregate and transform the telemetry and device metadata. The application creates reports that users view by using another web application that runs on the same on-premises machine. The periodic jobs take 120-600 seconds to run. However, the web application is always running.</p><p>The company is moving the platform to AWS and must reduce the operational overhead of the stack.</p><p>Which combination of steps will meet these requirements with the LEAST operational overhead? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Lambda functions to connect to the IoT devices."
                },
                {
                    "label": "B",
                    "content": "Configure the IoT devices to publish to AWS IoT Core."
                },
                {
                    "label": "C",
                    "content": "Write the metadata to a self-managed MongoDB database on an Amazon EC2 instance."
                },
                {
                    "label": "D",
                    "content": "Write the metadata to Amazon DocumentDB (with MongoDB compatibility)."
                },
                {
                    "label": "E",
                    "content": "Use AWS Step Functions state machines with AWS Lambda tasks to prepare the reports and to write the reports to Amazon S3. Use Amazon CloudFront with an S3 origin to serve the reports."
                },
                {
                    "label": "F",
                    "content": "Use an Amazon Elastic Kubernetes Service (Amazon EKS) cluster with Amazon EC2 instances to prepare the reports. Use an ingress controller in the EKS cluster to serve the reports."
                }
            ],
            "correctAnswer": "BDE",
            "explanation": "<p><br></p><p>The correct combination of steps to meet the requirements with the least operational overhead is: &nbsp;</p><p> B, D, E &nbsp;</p><p> Detailed Explanation: &nbsp;</p><p> 1. (B) Configure the IoT devices to publish to AWS IoT Core &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS IoT Core is a fully managed MQTT broker that eliminates the need to manage an on-premises server. &nbsp;</p><p> &nbsp;- IoT devices can directly publish telemetry data to AWS IoT Core, reducing infrastructure management. &nbsp;</p><p> &nbsp;- Supports MQTT, HTTPS, and other protocols natively. &nbsp;</p><p> 2. (D) Write the metadata to Amazon DocumentDB (with MongoDB compatibility) &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- Amazon DocumentDB is a fully managed MongoDB-compatible database. &nbsp;</p><p> &nbsp;- Eliminates the operational burden of managing a self-hosted MongoDB cluster (Option C). &nbsp;</p><p> &nbsp;- Provides high availability, scalability, and automatic backups. &nbsp;</p><p> 3. (E) Use AWS Step Functions with AWS Lambda to prepare reports and store them in Amazon S3, then serve via CloudFront &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS Step Functions orchestrates serverless workflows, making it easy to manage periodic jobs (120-600 sec runtime). &nbsp;</p><p> &nbsp;- AWS Lambda runs the transformation logic without managing servers. &nbsp;</p><p> &nbsp;- Amazon S3 + CloudFront provides a scalable, low-cost way to serve reports globally. &nbsp;</p><p> &nbsp;- Eliminates the need for a constantly running web server (unlike Option F, which requires managing EKS). &nbsp;</p><p>---</p><p> Why Other Options Are Not the Best Fit: &nbsp;</p><p>- A (Lambda to connect to IoT devices): &nbsp;</p><p> &nbsp;- Lambda is not designed for persistent MQTT connections. &nbsp;</p><p> &nbsp;- AWS IoT Core (Option B) is the correct managed solution. &nbsp;</p><p>- C (Self-managed MongoDB on EC2): &nbsp;</p><p> &nbsp;- Requires manual management of backups, scaling, and patching. &nbsp;</p><p> &nbsp;- Amazon DocumentDB (Option D) is fully managed and a better choice. &nbsp;</p><p>- F (EKS with EC2 instances): &nbsp;</p><p> &nbsp;- Overly complex for report generation and serving. &nbsp;</p><p> &nbsp;- Higher operational overhead than serverless (Step Functions + Lambda + S3). &nbsp;</p><p>---</p><p> Final Answer: &nbsp;</p><p>✅ B, D, E (AWS IoT Core + Amazon DocumentDB + Step Functions/Lambda/S3/CloudFront) &nbsp;</p><p>This combination provides a fully managed, serverless architecture with the least operational overhead.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "7ca432e874dd45e8899f9869181e454c",
            "questionNumber": 188,
            "type": "single",
            "content": "<p>Question #188</p><p>A global manufacturing company plans to migrate the majority of its applications to AWS. However, the company is concerned about applications that need to remain within a specific country or in the company's central on-premises data center because of data regulatory requirements or requirements for latency of single-digit milliseconds. The company also is concerned about the applications that it hosts in some of its factory sites, where limited network infrastructure exists.</p><p><br></p><p>The company wants a consistent developer experience so that its developers can build applications once and deploy on-premises, in the cloud, or in a hybrid architecture. The developers must be able to use the same tools, APIs, and services that are familiar to them.</p><p><br></p><p>Which solution will provide a consistent hybrid experience to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Migrate all applications to the closest AWS Region that is compliant. Set up an AWS Direct Connect connection between the central on-premises data center and AWS. Deploy a Direct Connect gateway."
                },
                {
                    "label": "B",
                    "content": "Use AWS Snowball Edge Storage Optimized devices for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Retain the devices on premises. Deploy AWS Wavelength to host the workloads in the factory sites."
                },
                {
                    "label": "C",
                    "content": "Install AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds. Use AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites."
                },
                {
                    "label": "D",
                    "content": "Migrate the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds to an AWS Local Zone. Deploy AWS Wavelength to host the workloads in the factory sites."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>The correct answer is C. Here's the detailed explanation:</p><p> Why Option C is Correct:</p><p>1. AWS Outposts for Low-Latency & Regulatory Requirements:</p><p> &nbsp; - Problem: Some applications must stay on-premises due to data residency laws or single-digit millisecond latency needs.</p><p> &nbsp; - Solution: AWS Outposts brings AWS infrastructure (compute, storage, database, etc.) to the on-premises data center.</p><p> &nbsp; &nbsp; - Provides consistent AWS experience (same APIs, tools, services as in the cloud).</p><p> &nbsp; &nbsp; - Ensures low-latency access for local applications.</p><p> &nbsp; &nbsp; - Helps meet regulatory compliance by keeping data in a specific location.</p><p>2. AWS Snowball Edge Compute Optimized for Factory Sites:</p><p> &nbsp; - Problem: Factory sites have limited network infrastructure but still need to run workloads.</p><p> &nbsp; - Solution: Snowball Edge Compute Optimized provides AWS compute & storage in a rugged, portable device.</p><p> &nbsp; &nbsp; - Can run Lambda, EC2, and EBS locally without continuous internet.</p><p> &nbsp; &nbsp; - Syncs data back to AWS when connectivity is available.</p><p> &nbsp; &nbsp; - Ideal for remote or disconnected environments.</p><p>3. Consistent Developer Experience:</p><p> &nbsp; - Both Outposts and Snowball Edge integrate with AWS services (e.g., EKS, ECS, RDS).</p><p> &nbsp; - Developers use the same AWS tools (CLI, SDKs, CloudFormation) whether deploying on-premises or in AWS Regions.</p><p>---</p><p> Why Other Options Are Incorrect:</p><p>- A (Migrate to closest AWS Region + Direct Connect):</p><p> &nbsp;- Fails for applications needing on-premises hosting (due to regulations or latency).</p><p> &nbsp;- Direct Connect improves connectivity but does not solve data residency or ultra-low latency requirements.</p><p>- B (Snowball Edge Storage Optimized + Wavelength):</p><p> &nbsp;- Snowball Edge Storage Optimized is for data transfer/storage, not running persistent workloads.</p><p> &nbsp;- AWS Wavelength embeds AWS in 5G networks (for mobile apps), not factory sites with limited networking.</p><p>- D (Local Zone + Wavelength):</p><p> &nbsp;- AWS Local Zones extend AWS Regions to metro areas but still rely on cloud connectivity (not fully on-premises).</p><p> &nbsp;- Wavelength is for 5G mobile apps, not factory environments.</p><p>---</p><p> Key Takeaways:</p><p>✅ For on-premises/low-latency needs → AWS Outposts &nbsp;</p><p>✅ For remote factories → Snowball Edge Compute Optimized &nbsp;</p><p>✅ Consistent AWS experience across hybrid environments &nbsp;</p><p>Final Answer: C (AWS Outposts + Snowball Edge Compute Optimized)</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "088a8769a68f477f8c77d23f236098b5",
            "questionNumber": 189,
            "type": "multiple",
            "content": "<p>Question #189</p><p>A company is updating an application that customers use to make online orders. The number of attacks on the application by bad actors has increased recently.</p><p>The company will host the updated application on an Amazon Elastic Container Service (Amazon ECS) cluster. The company will use Amazon DynamoDB to store application data. A public Application Load Balancer (ALB) will provide end users with access to the application. The company must prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack.</p><p>Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Amazon CloudFront distribution with the ALB as the origin. Add a custom header and random value on the CloudFront domain. Configure the ALB to conditionally forward traffic if the header and value match."
                },
                {
                    "label": "B",
                    "content": "Deploy the application in two AWS Regions. Configure Amazon Route 53 to route to both Regions with equal weight."
                },
                {
                    "label": "C",
                    "content": "Configure auto scaling for Amazon ECS tasks. Create a DynamoDB Accelerator (DAX) cluster."
                },
                {
                    "label": "D",
                    "content": "Configure Amazon ElastiCache to reduce overhead on DynamoDB."
                },
                {
                    "label": "E",
                    "content": "Deploy an AWS WAF web ACL that includes an appropriate rule group. Associate the web ACL with the Amazon CloudFront distribution."
                }
            ],
            "correctAnswer": "AE",
            "explanation": "<p>The correct combination of steps to prevent attacks and ensure business continuity in the most cost-effective way is: &nbsp;</p><p> A & E &nbsp;</p><p> Detailed Explanation: &nbsp;</p><p> 1. (A) Use Amazon CloudFront with a Custom Header for Security &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- Prevents direct access to the ALB by forcing traffic through CloudFront (reduces DDoS risks). &nbsp;</p><p> &nbsp;- Custom header validation ensures only CloudFront can forward traffic to the ALB, blocking malicious requests. &nbsp;</p><p> &nbsp;- Cost-effective: CloudFront provides built-in DDoS protection (AWS Shield Standard) and caching to reduce backend load. &nbsp;</p><p> 2. (E) Deploy AWS WAF with a Rule Group to Block Attacks &nbsp;</p><p>- Why? &nbsp;</p><p> &nbsp;- AWS WAF protects against common web exploits (SQLi, XSS, bots, etc.). &nbsp;</p><p> &nbsp;- Rule groups (e.g., AWS Managed Rules or custom rules) filter malicious traffic before it reaches the ALB. &nbsp;</p><p> &nbsp;- Cost-effective: Pay-as-you-go pricing (only for rules and requests processed). &nbsp;</p><p>---</p><p> Why Other Options Are Not the Best Fit: &nbsp;</p><p>- B (Multi-Region Deployment with Route 53): &nbsp;</p><p> &nbsp;- Overkill for most attacks—adds complexity and high cost (duplicate infrastructure in 2 regions). &nbsp;</p><p> &nbsp;- Business continuity ≠ multi-region unless the app requires extreme HA (not specified here). &nbsp;</p><p>- C (ECS Auto Scaling + DAX): &nbsp;</p><p> &nbsp;- Auto Scaling helps with load handling but does not prevent attacks. &nbsp;</p><p> &nbsp;- DAX improves DynamoDB performance but does not mitigate security risks. &nbsp;</p><p>- D (Amazon ElastiCache): &nbsp;</p><p> &nbsp;- Reduces DynamoDB load but does not protect against attacks. &nbsp;</p><p> &nbsp;- Not a security measure—better for performance optimization. &nbsp;</p><p>---</p><p> Final Answer: &nbsp;</p><p>✅ A & E (CloudFront with custom header + AWS WAF) &nbsp;</p><p>This combination: &nbsp;</p><p>✔ Blocks attacks (DDoS, bots, SQLi, etc.) &nbsp;</p><p>✔ Ensures business continuity (CloudFront absorbs traffic spikes) &nbsp;</p><p>✔ Minimizes cost (no unnecessary multi-region or caching overhead)</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "5b1d835599f944baa5e04d977d323b36",
            "questionNumber": 190,
            "type": "single",
            "content": "<p>Question #190</p><p>A company runs a web application on AWS. The web application delivers static content from an Amazon S3 bucket that is behind an Amazon CloudFront distribution. The application serves dynamic content by using an Application Load Balancer (ALB) that distributes requests to a fleet of Amazon EC2 instances in Auto Scaling groups. The application uses a domain name set up in Amazon Route 53.</p><p><br></p><p>Some users reported occasional issues when the users attempted to access the website during peak hours. An operations team found that the ALB sometimes returned HTTP 503 Service Unavailable errors. The company wants to display a custom error message page when these errors occur. The page should be displayed immediately for this error code.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up a Route 53 failover routing policy. Configure a health check to determine the status of the ALB endpoint and to fail over to the failover S3 bucket endpoint."
                },
                {
                    "label": "B",
                    "content": "Create a second CloudFront distribution and an S3 static website to host the custom error page. Set up a Route 53 failover routing policy. Use an active-passive configuration between the two distributions."
                },
                {
                    "label": "C",
                    "content": "Create a CloudFront origin group that has two origins. Set the ALB endpoint as the primary origin. For the secondary origin, set an S3 bucket that is configured to host a static website. Set up origin failover for the CloudFront distribution. Update the S3 static website to incorporate the custom error page."
                },
                {
                    "label": "D",
                    "content": "Create a CloudFront function that validates each HTTP response code that the ALB returns. Create an S3 static website in an S3 bucket. Upload the custom error page to the S3 bucket as a failover. Update the function to read the S3 bucket and to serve the error page to the end users."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p><br></p><p>The correct answer is C.</p><p> Why Option C is Correct:</p><p>1. CloudFront Origin Groups with Automatic Failover:</p><p> &nbsp; - Problem: The ALB occasionally returns HTTP 503 errors during peak traffic, and the company wants to immediately display a custom error page.</p><p> &nbsp; - Solution: &nbsp;</p><p> &nbsp; &nbsp; - Primary Origin: ALB (for dynamic content). &nbsp;</p><p> &nbsp; &nbsp; - Secondary Origin: S3 bucket (hosting a static error page). &nbsp;</p><p> &nbsp; &nbsp; - CloudFront automatically switches to the S3 origin when the ALB returns a 503 error, serving the custom error page without delays or manual intervention.</p><p>2. Least Operational Overhead:</p><p> &nbsp; - No additional Route 53 configurations (unlike Options A & B, which require failover policies). &nbsp;</p><p> &nbsp; - No custom scripting (unlike Option D, which requires CloudFront Functions). &nbsp;</p><p> &nbsp; - Fully managed by AWS—CloudFront handles failover automatically.</p><p>3. Immediate Error Handling:</p><p> &nbsp; - CloudFront caches the error page, ensuring quick delivery when the ALB fails. &nbsp;</p><p> &nbsp; - No dependency on health checks or DNS propagation delays (unlike Option A).</p><p>---</p><p> Why Other Options Are Not the Best Fit:</p><p>- A (Route 53 Failover + Health Checks): &nbsp;</p><p> &nbsp;- Slow failover (depends on health check intervals, typically 30+ seconds). &nbsp;</p><p> &nbsp;- DNS propagation delays can cause temporary outages. &nbsp;</p><p> &nbsp;- Higher operational overhead (managing health checks and failover policies). &nbsp;</p><p>- B (Second CloudFront Distribution + Route 53 Failover): &nbsp;</p><p> &nbsp;- Overly complex—requires managing two CloudFront distributions. &nbsp;</p><p> &nbsp;- Delayed failover due to DNS-based routing. &nbsp;</p><p> &nbsp;- More expensive (two active distributions). &nbsp;</p><p>- D (CloudFront Function + S3 Fallback): &nbsp;</p><p> &nbsp;- Custom code required (increases maintenance). &nbsp;</p><p> &nbsp;- Slower response (function must process each error before redirecting). &nbsp;</p><p> &nbsp;- Not fully automated (unlike native CloudFront failover). &nbsp;</p><p>---</p><p> Key Takeaways:</p><p>✅ CloudFront Origin Groups provide automatic, instant failover to S3 when the ALB fails. &nbsp;</p><p>✅ No custom code or DNS changes needed—minimal operational overhead. &nbsp;</p><p>✅ Cost-effective (uses existing S3 + CloudFront infrastructure). &nbsp;</p><p>Final Answer: C (CloudFront origin group with ALB + S3 failover)</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "6c48cd1ac5fc46039f1951e3307a4c6f",
            "questionNumber": 191,
            "type": "single",
            "content": "<p>Question #191</p><p>A company is planning to migrate an application to AWS. The application runs as a Docker container and uses an NFS version 4 file share.</p><p><br></p><p>A solutions architect must design a secure and scalable containerized solution that does not require provisioning or management of the underlying infrastructure.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon Elastic File System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition."
                },
                {
                    "label": "B",
                    "content": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon FSx for Lustre for shared storage. Reference the FSx for Lustre file system ID, container mount point, and FSx for Lustre authorization IAM role in the ECS task definition."
                },
                {
                    "label": "C",
                    "content": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic File System (Amazon EFS) for shared storage. Mount the EFS file system on the ECS container instances. Add the EFS authorization IAM role to the EC2 instance profile."
                },
                {
                    "label": "D",
                    "content": "Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Amazon EC2 launch type and auto scaling turned on. Use Amazon Elastic Block Store (Amazon EBS) volumes with Multi-Attach enabled for shared storage. Attach the EBS volumes to ECS container instances. Add the EBS authorization IAM role to an EC2 instance profile."
                }
            ],
            "correctAnswer": "A",
            "explanation": "<p>Based on the requirements to migrate a Docker container application that uses NFS version 4 file share to a secure and scalable containerized solution on AWS without provisioning or managing the underlying infrastructure, the answer is:</p><p>A. Deploy the application containers by using Amazon Elastic Container Service (Amazon ECS) with the Fargate launch type. Use Amazon ElasticFile System (Amazon EFS) for shared storage. Reference the EFS file system ID, container mount point, and EFS authorization IAM role in the ECS task definition.</p><p>This is the correct choice because:</p><p>Amazon ECS with Fargate launch type provides a fully managed, serverless container orchestration service that eliminates the need to provision or manage the underlying infrastructure, directly meeting the requirement for no infrastructure management.</p><p>Amazon EFS is a fully managed NFS file system that natively supports NFSv4, making it a direct replacement for the application's existing NFS version 4 file share requirement.</p><p>EFS integrates seamlessly with ECS Fargate tasks through task definitions, allowing containers to mount the file system and access shared data.</p><p>The solution is secure as it uses IAM roles for authorization to the EFS file system, implementing the principle of least privilege.</p><p>Both ECS Fargate and EFS are designed to scale automatically based on demand, meeting the scalability requirement.</p><p>Option B suggests using Amazon FSx for Lustre instead of EFS. While FSx for Lustre is a high-performance file system, it's primarily designed for compute-intensive workloads like high-performance computing, machine learning, and media processing. It would be overprovisioned for a standard application that simply requires NFS v4 compatibility. Additionally, FSx for Lustre is optimized for high throughput and IOPS rather than general-purpose file sharing, making it less suitable for a direct migration of a standard NFS workload.</p><p>Amazon EFS is purpose-built for general NFS workloads and provides a more straightforward, cost-effective solution for applications that require NFS compatibility without specialized performance requirements.</p><p>Sources</p><p>Chat examples - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/best-practices-code-generation/examples-chat.html）</p><p>Exploring AWS Container Services | AWS re:Post （https://repost.aws/articles/AR4s12umYqSdGKOpsimKGO7A/exploring-aws-container-services）</p><p>Run stateful workloads with persistent data storage by using Amazon EFS on Amazon EKS with AWS Fargate - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/run-stateful-workloads-with-persistent-data-storage-by-using-amazon-efs-on-amazon-eks-with-aws-fargate.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "f3d7275472944385808cb691a76c6f9d",
            "questionNumber": 192,
            "type": "single",
            "content": "<p>Question #192</p><p>A company is running an application in the AWS Cloud. The core business logic is running on a set of Amazon EC2 instances in an Auto Scaling group. An Application Load Balancer (ALB) distributes traffic to the EC2 instances. Amazon Route 53 record api.example.com is pointing to the ALB.</p><p><br></p><p>The company's development team makes major updates to the business logic. The company has a rule that when changes are deployed, only 10% of customers can receive the new logic during a testing window. A customer must use the same version of the business logic during the testing window.</p><p><br></p><p>How should the company deploy the updates to meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a second ALB, and deploy the new logic to a set of EC2 instances in a new Auto Scaling group. Configure the ALB to distribute traffic to the EC2 instances. Update the Route 53 record to use weighted routing, and point the record to both of the ALBs."
                },
                {
                    "label": "B",
                    "content": "Create a second target group that is referenced by the ALB. Deploy the new logic to EC2 instances in this new target group. Update the ALB listener rule to use weighted target groups. Configure ALB target group stickiness."
                },
                {
                    "label": "C",
                    "content": "Create a new launch configuration for the Auto Scaling group. Specify the launch configuration to use the AutoScalingRollingUpdate policy, and set the MaxBatchSize option to 10. Replace the launch configuration on the Auto Scaling group. Deploy the changes."
                },
                {
                    "label": "D",
                    "content": "Create a second Auto Scaling group that is referenced by the ALB. Deploy the new logic on a set of EC2 instances in this new Auto Scaling group. Change the ALB routing algorithm to least outstanding requests (LOR). Configure ALB session stickiness."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>The correct answer is B. Here’s the detailed explanation:</p><p> Why Option B is Correct:</p><p>1. Weighted Target Groups for Controlled Rollout (10% Traffic): &nbsp;</p><p> &nbsp; - The ALB supports weighted routing between target groups, allowing precise traffic splitting (90% old version, 10% new version). &nbsp;</p><p> &nbsp; - This meets the requirement that only 10% of customers receive the new logic during testing.</p><p>2. ALB Stickiness Ensures Consistent User Experience: &nbsp;</p><p> &nbsp; - Target group stickiness ensures that once a user is routed to a specific version (old or new), they continue to use the same version during the testing window. &nbsp;</p><p> &nbsp; - This prevents users from switching between versions unpredictably.</p><p>3. Minimal Operational Overhead: &nbsp;</p><p> &nbsp; - Uses the existing ALB (no need for a second ALB or Route 53 changes). &nbsp;</p><p> &nbsp; - No need to modify Auto Scaling groups or launch configurations (unlike Options A, C, and D). &nbsp;</p><p>---</p><p> Why Other Options Are Incorrect:</p><p>- A (Second ALB + Route 53 Weighted Routing): &nbsp;</p><p> &nbsp;- Overly complex—requires managing two ALBs and Route 53 weighted records. &nbsp;</p><p> &nbsp;- No built-in session stickiness in Route 53 (users might switch versions mid-session). &nbsp;</p><p>- C (Auto Scaling Rolling Update with MaxBatchSize=10): &nbsp;</p><p> &nbsp;- Does not guarantee 10% traffic—only controls how many instances update at once. &nbsp;</p><p> &nbsp;- No session stickiness—users may bounce between old and new versions. &nbsp;</p><p>- D (Second Auto Scaling Group + LOR Routing): &nbsp;</p><p> &nbsp;- Least Outstanding Requests (LOR) does not enforce 10% traffic split. &nbsp;</p><p> &nbsp;- Session stickiness alone isn’t enough—traffic distribution must be explicitly controlled. &nbsp;</p><p>---</p><p> Key Takeaways:</p><p>✅ Weighted target groups in ALB enable precise traffic splitting (90/10). &nbsp;</p><p>✅ ALB stickiness ensures users stay on the same version during testing. &nbsp;</p><p>✅ Simpler and more cost-effective than multi-ALB or Route 53 solutions. &nbsp;</p><p>Final Answer: B (ALB weighted target groups + stickiness)</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "d60b59c1bab5428aa1ff81ef8f9eb74a",
            "questionNumber": 193,
            "type": "single",
            "content": "Question #193<p>A large education company recently introduced Amazon Workspaces to provide access to internal applications across multiple universities. The company is storing user profiles on an Amazon FSx for Windows File Server file system. The file system is configured with a DNS alias and is connected to a self-managed Active Directory. As more users begin to use the Workspaces, login time increases to unacceptable levels. </p><p><br></p><p>An investigation reveals a degradation in performance of the file system. The company created the file system on HDD storage with a throughput of 16 MBps. A solutions architect must improve the performance of the file system during a defined maintenance window.</p><p><br></p><p><br></p><p>What should the solutions architect do to meet these requirements with the LEAST administrative effort?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Use AWS Backup to create a point-in-time backup of the file system. Restore the backup to a new FSx for Windows File Server file system. Select SSD as the storage type. Select 32 MBps as the throughput capacity. When the backup and restore process is completed, adjust the DNS alias accordingly. Delete the original file system."
                },
                {
                    "label": "B",
                    "content": "Disconnect users from the file system. In the Amazon FSx console, update the throughput capacity to 32 MBps. Update the storage type to SSD. Reconnect users to the file system."
                },
                {
                    "label": "C",
                    "content": "Deploy an AWS DataSync agent onto a new Amazon EC2 instance. Create a task. Configure the existing file system as the source location. Configure a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput as the target location. Schedule the task. When the task is completed, adjust the DNS alias accordingly. Delete the original file system."
                },
                {
                    "label": "D",
                    "content": "Enable shadow copies on the existing file system by using a Windows PowerShell command. Schedule the shadow copy job to create a point-in-time backup of the file system. Choose to restore previous versions. Create a new FSx for Windows File Server file system with SSD storage and 32 MBps of throughput. When the copy job is completed, adjust the DNS alias. Delete the original file system."
                }
            ],
            "correctAnswer": "B",
            "explanation": "https://docs.aws.amazon.com/fsx/latest/WindowsGuide/managing-storage-type.html<p><br></p><p>SSD to HDD is impossible, but HDD to SSD is okay =&gt; B is feasible. </p><p>B is lesseffort since B just disconnects users from the file system for a while,and then updates the FSx. While A needsa new </p><p>FSx, backup, restore, clean up then switch, more steps to do than A</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "67c09617a6424ec1a888ceb569281cdd",
            "questionNumber": 194,
            "type": "single",
            "content": "<p>Question #194</p><p>A company hosts an application on AWS. The application reads and writes objects that are stored in a single Amazon S3 bucket. The company must modify the application to deploy the application in two AWS Regions.</p><p><br></p><p>Which solution will meet these requirements with the LEAST operational overhead?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Set up an Amazon CloudFront distribution with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the CloudFront distribution. Use AWS Global Accelerator to access the data in the S3 bucket."
                },
                {
                    "label": "B",
                    "content": "Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions."
                },
                {
                    "label": "C",
                    "content": "Create a new S3 bucket in a second Region. Deploy the application in the second Region. Configure the application to use the new S3 bucket. Set up S3 Cross-Region Replication (CRR) from the original S3 bucket to the new S3 bucket."
                },
                {
                    "label": "D",
                    "content": "Set up an S3 gateway endpoint with the S3 bucket as an origin. Deploy the application to a second Region. Modify the application to use the new S3 gateway endpoint. Use S3 Intelligent-Tiering on the S3 bucket."
                }
            ],
            "correctAnswer": "B",
            "explanation": "<p>Based on the requirement to deploy an application that reads and writes objects to S3 across two AWS Regions with the least operational overhead, the answer is:</p><p>B. Create a new S3 bucket in a second Region. Set up bidirectional S3 Cross-Region Replication (CRR) between the original S3 bucket and the new S3 bucket. Configure an S3 Multi-Region Access Point that uses both S3 buckets. Deploy a modified application to both Regions.</p><p>This is the correct choice because:</p><p>S3 Multi-Region Access Points are specifically designed for multi-region access to S3 data, providing a single endpoint that automatically routes requests to the appropriate region based on network conditions and availability.</p><p>Bidirectional S3 Cross-Region Replication ensures that objects written to either bucket are automatically replicated to the other bucket, maintaining data consistency across regions without requiring application-level replication logic.</p><p>This solution is purpose-built for S3 multi-region access and requires minimal configuration changes to the application - it simply needs to use the Multi-Region Access Point endpoint instead of a specific bucket endpoint.</p><p>The entire solution uses native S3 features that are designed to work together, reducing integration complexity and operational overhead.</p><p>Option A (CloudFront distribution with Global Accelerator) has higher operational overhead because:</p><p>It requires setting up and managing multiple services (CloudFront and Global Accelerator) that aren't specifically designed for bidirectional S3 access across regions.</p><p>CloudFront is primarily a content delivery network optimized for read operations, not for write operations that the application requires.</p><p>Global Accelerator improves network performance but doesn't handle data replication between regions, which would require additional custom logic.</p><p>This approach would require more complex configuration and ongoing management of multiple services that weren't designed specifically for this use case.</p><p>The S3 Multi-Region Access Point with bidirectional Cross-Region Replication provides a more streamlined, purpose-built solution with significantly less operational overhead for deploying an application that needs to read and write S3 objects across two regions.</p><p>Sources</p><p>Cloudfront OAC S3: Public Reads & Only Signed Secure Writes | AWS re:Post （https://repost.aws/questions/QUY3prSJX-QMS-0UtwtmbeMQ/cloudfront-oac-s3-public-reads-only-signed-secure-writes）</p><p>Replicating objects within and across Regions - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "616a28d5616c44149d06e312e8c7d2d3",
            "questionNumber": 195,
            "type": "single",
            "content": "<p>Question #195</p><p>An online gaming company needs to rehost its gaming platform on AWS. The company's gaming application requires high-performance computing (HPC) processing and has a leaderboard that changes frequently. An Ubuntu instance that is optimized for compute generation hosts a Node.js application for game display. Game state is tracked in an on-premises Redis instance.</p><p><br></p><p>The company needs a migration strategy that optimizes application performance.</p><p><br></p><p>Which solution will meet these requirements?</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create an Auto Scaling group of m5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard."
                },
                {
                    "label": "B",
                    "content": "Create an Auto Scaling group of c5.large Amazon EC2 Spot Instances behind an Application Load Balancer. Use an Amazon OpenSearch Service cluster to maintain the leaderboard."
                },
                {
                    "label": "C",
                    "content": "Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard."
                },
                {
                    "label": "D",
                    "content": "Create an Auto Scaling group of m5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon DynamoDB table to maintain the leaderboard."
                }
            ],
            "correctAnswer": "C",
            "explanation": "<p>Based on the requirements for migrating an online gaming platform that needs high performance computing (HPC) and has a frequently changing leaderboard currently using Redis, the answer is:</p><p>C. Create an Auto Scaling group of c5.large Amazon EC2 On-Demand Instances behind an Application Load Balancer. Use an Amazon ElastiCache for Redis cluster to maintain the leaderboard.</p><p>This is the correct choice because:</p><p>Compute-optimized instances (c5.large) are specifically designed for compute-intensive workloads like high performance computing (HPC) that the gaming platform requires. The \"c\" family of instances offers higher CPU performance compared to the general-purpose \"m\" family, making them better suited for the computational demands of gaming applications.</p><p>On-Demand Instances provide consistent, uninterrupted performance which is critical for gaming applications where any interruption would negatively impact the user experience. Unlike Spot Instances which can be terminated with little notice, On-Demand Instances ensure stability for the gaming service.</p><p>Amazon ElastiCache for Redis is the ideal choice for maintaining the frequently changing leaderboard as it provides:</p><p>Sub-millisecond latency for real-time updates</p><p>In-memory data storage for high-speed access</p><p>Built-in sorting and ranking capabilities perfect for leaderboards</p><p>Compatibility with the existing Redis implementation, simplifying migration</p><p>The Application Load Balancer with an Auto Scaling group ensures the application can handle varying loads while maintaining high availability.</p><p>Option A is less suitable because:</p><p>m5.large instances are general-purpose rather than compute-optimized, making them less efficient for HPC workloads</p><p>Spot Instances can be interrupted with minimal notice, which would disrupt the gaming experience</p><p>While Spot Instances are more cost-effective, they sacrifice the reliability and consistent performance needed for a gaming platform</p><p>The solution in option C provides the optimal combination of compute power, reliability, and performance required for the gaming platform while maintaining compatibility with the existing Redis-based leaderboard system.</p><p>Sources</p><p>Migrate Redis workloads to Redis Enterprise Cloud on AWS - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/migrate-redis-workloads-to-redis-enterprise-cloud-on-aws.html）</p><p>Amazon EC2 C6i Instances - Amazon Web Services （https://aws.amazon.com/cn/ec2/instance-types/c6i/）</p><p>Best practices for Amazon EC2 Spot - Amazon Elastic Compute Cloud （https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/spot-best-practices.html）</p><p>About the migration strategies - AWS Prescriptive Guidance （https://docs.aws.amazon.com/prescriptive-guidance/latest/large-migration-guide/migration-strategies.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "0d972295b7954972ad108709620011c7",
            "questionNumber": 196,
            "type": "multiple",
            "content": "<p>Question #196</p><p>A solutions architect is designing an application to accept timesheet entries from employees on their mobile devices. Timesheets will be submitted weekly, with most of the submissions occurring on Friday. The data must be stored in a format that allows payroll administrators to run monthly reports. The infrastructure must be highly available and scale to match the rate of incoming data and reporting requests.</p><p>Which combination of steps meets these requirements while minimizing operational overhead? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Deploy the application to Amazon EC2 On-Demand Instances with load balancing across multiple Availability Zones. Use scheduled Amazon EC2 Auto Scaling to add capacity before the high volume of submissions on Fridays."
                },
                {
                    "label": "B",
                    "content": "Deploy the application in a container using Amazon Elastic Container Service (Amazon ECS) with load balancing across multiple Availability Zones. Use scheduled Service Auto Scaling to add capacity before the high volume of submissions on Fridays."
                },
                {
                    "label": "C",
                    "content": "Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration."
                },
                {
                    "label": "D",
                    "content": "Store the timesheet submission data in Amazon Redshift. Use Amazon QuickSight to generate the reports using Amazon Redshift as the data source."
                },
                {
                    "label": "E",
                    "content": "Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source."
                }
            ],
            "correctAnswer": "CE",
            "explanation": "<p>Based on the requirements for a timesheet application that needs to handle weekly submissions (mostly on Fridays), store data for monthly reports, and be highly available and scalable while minimizing operational overhead, the correct combination of steps is:</p><p>C. Deploy the application front end to an Amazon S3 bucket served by Amazon CloudFront. Deploy the application backend using Amazon API Gateway with an AWS Lambda proxy integration.</p><p>E. Store the timesheet submission data in Amazon S3. Use Amazon Athena and Amazon QuickSight to generate the reports using Amazon S3 as the data source.</p><p>Here's why this combination is optimal:</p><p>For the application architecture (Option C):</p><p>Using S3 and CloudFront for the front end provides a highly available, globally distributed solution that can handle any number of mobile users accessing the application simultaneously.</p><p>CloudFront's caching capabilities ensure low latency access to the application's static assets.</p><p>API Gateway with Lambda provides a serverless backend that automatically scales to handle the Friday submission spikes without any manual intervention.</p><p>This serverless architecture eliminates the need to provision, manage, or patch servers, significantly reducing operational overhead.</p><p>Lambda functions can be configured to process timesheet submissions and store the data in a structured format.</p><p>For the data storage and reporting solution (Option E):</p><p>Amazon S3 provides virtually unlimited, highly durable storage for timesheet data at a low cost.</p><p>S3 can easily handle the weekly influx of submissions, especially the Friday peaks.</p><p>Amazon Athena allows payroll administrators to run SQL queries directly against the data stored in S3 without having to set up and manage a database server.</p><p>Amazon QuickSight integrates with Athena to create interactive dashboards and visualizations for the monthly reports.</p><p>This combination eliminates the need to manage database infrastructure, reducing operational overhead while providing powerful reporting capabilities.</p><p>Together, these solutions create a complete serverless architecture that:</p><p>Automatically scales to match incoming data submission rates</p><p>Provides high availability for both the application and data</p><p>Minimizes operational overhead by eliminating server management</p><p>Offers powerful reporting capabilities for payroll administrators</p><p>Can handle the weekly submission pattern with Friday peaks efficiently</p><p>This combination leverages fully managed AWS services that work together seamlessly while requiring minimal operational effort to maintain.</p><p>Sources</p><p>Scaling Learning: How Geniusee Migrated a WordPress E-Learning Solution to AWS Serverless | AWS Partner Network (APN) Blog （https://aws.amazon.com/cn/blogs/apn/scaling-learning-how-geniusee-migrated-a-wordpress-e-learning-solution-to-aws-serverless/）</p><p>[DL.ADS.6] Use cell-based architectures for granular deployment and release - DevOps Guidance （https://docs.aws.amazon.com/wellarchitected/latest/devops-guidance/dl.ads.6-utilize-cell-based-architectures-for-granular-deployment-and-release.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "1e0f7a204bad4ef3a712027ba37cdbb7",
            "questionNumber": 197,
            "type": "multiple",
            "content": "<p>Question #197</p><p>A company is storing sensitive data in an Amazon S3 bucket. The company must log all activities for objects in the S3 bucket and must keep the logs for 5 years. The company's security team must also receive an email notification every time there is an attempt to delete data in the S3 bucket.</p><p>Which combination of steps will meet these requirements MOST cost-effectively? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Configure AWS CloudTrail to log S3 data events."
                },
                {
                    "label": "B",
                    "content": "Configure S3 server access logging for the S3 bucket."
                },
                {
                    "label": "C",
                    "content": "Configure Amazon S3 to send object deletion events to Amazon Simple Email Service (Amazon SES)."
                },
                {
                    "label": "D",
                    "content": "Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic."
                },
                {
                    "label": "E",
                    "content": "Configure Amazon S3 to send the logs to Amazon Timestream with data storage tiering."
                },
                {
                    "label": "F",
                    "content": "Configure a new S3 bucket to store the logs with an S3 Lifecycle policy."
                }
            ],
            "correctAnswer": "ADF",
            "explanation": "<p>Based on the requirements to log all activities for objects in an S3 bucket for 5 years and receive email notifications for deletion attempts in the most cost-effective way, the correct combination of steps is:</p><p>A. Configure AWS CloudTrail to log S3 data events. </p><p>D. Configure Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic. </p><p>F. Configure a new S3 bucket to store the logs with an S3 Lifecycle policy.</p><p>Here's why this combination is the most cost-effective solution:</p><p>CloudTrail for logging (Option A):</p><p>CloudTrail data events provide comprehensive logging of all object-level activities in S3, including GET, PUT, DELETE operations</p><p>While there is a cost for CloudTrail data events, they provide the detailed information needed for security auditing and compliance</p><p>CloudTrail logs include important metadata like who performed the action, when it occurred, and from where</p><p>EventBridge with SNS for notifications (Option D):</p><p>This combination allows S3 to send object deletion events to EventBridge, which then triggers an SNS notification</p><p>SNS can be configured to send email notifications to the security team</p><p>This approach is more flexible and cost-effective than using Amazon SES directly, as it allows for future expansion to other notification methods or integrations</p><p>S3 with Lifecycle policies for log storage (Option F):</p><p>Storing logs in another S3 bucket with lifecycle policies is the most cost-effective option for 5-year retention</p><p>Lifecycle policies can automatically transition older logs to lower-cost storage tiers like S3 Glacier or S3 Glacier Deep Archive</p><p>This approach is significantly more cost-effective than using Timestream for long-term storage</p><p>The rejected options are less cost-effective:</p><p>Option B (S3 server access logging): While free, it provides less detailed information than CloudTrail and doesn't integrate as well with other AWS services for analysis</p><p>Option C (S3 to SES): Direct integration with SES is less flexible than using EventBridge with SNS, which can support multiple notification targets and filtering capabilities</p><p>Option E (Timestream): While Timestream is excellent for time-series data analysis, it's much more expensive than S3 for long-term storage of logs that aren't frequently queried</p><p>The selected combination provides the right balance of comprehensive logging, effective notifications, and cost-efficient long-term storage to meet the company's requirements for monitoring and securing their sensitive S3 data.</p><p>Sources</p><p>Amazon S3 CloudTrail events - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudtrail-logging-s3-info.html）</p><p>Logging options for Amazon S3 - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/logging-with-S3.html）</p><p>Configuring S3 Lifecycle event notifications - Amazon Simple Storage Service （https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-configure-notification.html）</p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "817ac242c51a4dfebe7b8ab2a3e1a855",
            "questionNumber": 198,
            "type": "multiple",
            "content": "<p>Question #198</p><p>A company is building a hybrid environment that includes servers in an on-premises data center and in the AWS Cloud. The company has deployed Amazon EC2 instances in three VPCs. Each VPC is in a different AWS Region. The company has established an AWS Direct Connect connection to the data center from the Region that is closest to the data center.</p><p>The company needs the servers in the on-premises data center to have access to the EC2 instances in all three VPCs. The servers in the on-premises data center also must have access to AWS public services.</p><p>Which combination of steps will meet these requirements with the LEAST cost? (Choose two.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Create a Direct Connect gateway in the Region that is closest to the data center. Attach the Direct Connect connection to the Direct Connect gateway. Use the Direct Connect gateway to connect the VPCs in the other two Regions."
                },
                {
                    "label": "B",
                    "content": "Set up additional Direct Connect connections from the on-premises data center to the other two Regions."
                },
                {
                    "label": "C",
                    "content": "Create a private VIF. Establish an AWS Site-to-Site VPN connection over the private VIF to the VPCs in the other two Regions."
                },
                {
                    "label": "D",
                    "content": "Create a public VIF. Establish an AWS Site-to-Site VPN connection over the public VIF to the VPCs in the other two Regions."
                },
                {
                    "label": "E",
                    "content": "Use VPC peering to establish a connection between the VPCs across the Regions. Create a private VIF with the existing Direct Connect connection to connect to the peered VPCs."
                }
            ],
            "correctAnswer": "AD",
            "explanation": "<p>The correct answers are A and D. </p><p> Why Option A is Correct:</p><p>1. Direct Connect Gateway for Multi-Region VPC Access:</p><p> &nbsp; - Creates a hub that connects your Direct Connect connection to multiple VPCs across different regions</p><p> &nbsp; - Allows the single existing Direct Connect connection to access all three VPCs</p><p> &nbsp; - Most cost-effective solution as it doesn't require additional Direct Connect connections</p><p>2. Simplified Architecture:</p><p> &nbsp; - Single connection management point</p><p> &nbsp; - No need for additional physical connections</p><p> Why Option D is Correct (Not C or E):</p><p>1. Access to AWS Public Services:</p><p> &nbsp; - A public VIF is specifically designed to access AWS public services (S3, DynamoDB, etc.)</p><p> &nbsp; - Required to meet the \"access to AWS public services\" requirement in the question</p><p> &nbsp; - More efficient than routing public service traffic through VPN (Option C)</p><p>2. Complete Solution:</p><p> &nbsp; - Option A handles private VPC access across regions</p><p> &nbsp; - Option D handles public AWS services access</p><p> &nbsp; - Together they satisfy all requirements</p><p> Why Other Options Are Incorrect:</p><p>- B: Additional Direct Connect connections are expensive and unnecessary</p><p>- C: Private VIF + VPN is redundant when you already have Direct Connect and doesn't efficiently handle public services</p><p>- E: VPC peering doesn't help with public services access and complicates the architecture</p><p> Key Advantages of A+D Solution:</p><p>1. Cost Optimization:</p><p> &nbsp; - Uses existing Direct Connect connection</p><p> &nbsp; - No additional physical circuits needed</p><p> &nbsp; - Public VIF is included at no extra cost with Direct Connect</p><p>2. Complete Connectivity:</p><p> &nbsp; - Private VPC access via Direct Connect Gateway</p><p> &nbsp; - Public services access via public VIF</p><p> &nbsp; - All requirements met with minimal components</p><p>3. Operational Simplicity:</p><p> &nbsp; - Single Direct Connect connection to manage</p><p> &nbsp; - Clear separation of private and public traffic</p><p> &nbsp; - AWS-managed services reduce overhead</p><p>Final Answer: A & D (Direct Connect Gateway + Public VIF) is the most cost-effective solution that meets all requirements.</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "ae3c517316384cb792f2090a5c06ccd5",
            "questionNumber": 199,
            "type": "multiple",
            "content": "<p>Question #199</p><p>A company is using an organization in AWS Organizations to manage hundreds of AWS accounts. A solutions architect is working on a solution to provide baseline protection for the Open Web Application Security Project (OWASP) top 10 web application vulnerabilities. The solutions architect is using AWS WAF for all existing and new Amazon CloudFront distributions that are deployed within the organization.</p><p>Which combination of steps should the solutions architect take to provide the baseline protection? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "Enable AWS Config in all accounts"
                },
                {
                    "label": "B",
                    "content": "Enable Amazon GuardDuty in all accounts"
                },
                {
                    "label": "C",
                    "content": "Enable all features for the organization"
                },
                {
                    "label": "D",
                    "content": "Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions"
                },
                {
                    "label": "E",
                    "content": "Use AWS Shield Advanced to deploy AWS WAF rules in all accounts for all CloudFront distributions"
                },
                {
                    "label": "F",
                    "content": "Use AWS Security Hub to deploy AWS WAF rules in all accounts for all CloudFront distributions"
                }
            ],
            "correctAnswer": "ACD",
            "explanation": "<p>Based on the requirement to provide baseline protection against OWASP top 10 web application vulnerabilities using AWS WAF for all CloudFront distributions across an AWS organization with hundreds of accounts, the correct combination of steps is:</p><p>A. Enable AWS Config in all accounts C. Enable all features for the organization D. Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions</p><p>Here's why this combination is correct:</p><p>Enable AWS Config in all accounts (Option A):</p><p>AWS Config provides continuous monitoring and recording of AWS resource configurations</p><p>It's essential for tracking compliance with security policies and detecting configuration drift</p><p>AWS Config helps ensure that WAF protections remain in place and aren't accidentally removed or modified</p><p>It provides visibility into the security posture across all accounts in the organization</p><p>Enable all features for the organization (Option C):</p><p>This is a prerequisite for using AWS Firewall Manager across the organization</p><p>Enabling all features in AWS Organizations allows for centralized management of security services</p><p>It enables service-linked roles needed for cross-account management</p><p>It allows policies to be applied consistently across all accounts in the organization</p><p>Use AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions (Option D):</p><p>Firewall Manager is specifically designed for centralized management of AWS WAF rules across multiple accounts</p><p>It allows for consistent application of WAF rules to protect against OWASP top 10 vulnerabilities</p><p>It can automatically apply protection to new CloudFront distributions as they're created</p><p>It provides a single point of management for hundreds of accounts, significantly reducing operational overhead</p><p>Option B (Enable Amazon GuardDuty in all accounts) is not part of the correct answer because:</p><p>While GuardDuty is a valuable security service, it focuses on threat detection rather than web application protection</p><p>GuardDuty doesn't directly help protect against OWASP top 10 vulnerabilities in web applications</p><p>It's complementary to WAF but not essential for the specific requirement of providing baseline WAF protection</p><p>This combination of steps creates a comprehensive approach to deploying and maintaining baseline WAF protection across the organization:</p><p>AWS Config provides monitoring and compliance checking</p><p>Enabling all organization features provides the necessary foundation for centralized management</p><p>AWS Firewall Manager handles the actual deployment and management of WAF rules across all accounts</p><p>Together, these steps ensure that all CloudFront distributions, both existing and new, have consistent protection against OWASP top 10 vulnerabilities throughout the organization.</p><p>Sources</p><p>Amazon CloudFront simplifies web application delivery and security with new user-friendly interface | AWS News Blog （https://aws.amazon.com/cn/blogs/aws/amazon-cloudfront-simplifies-web-application-delivery-and-security-with-new-user-friendly-interface/）</p><p>How to enforce a security baseline for an AWS WAF ACL across your organization using AWS Firewall Manager | AWS Security Blog （https://aws.amazon.com/cn/blogs/security/how-to-enforce-a-security-baseline-for-an-aws-waf-acl-across-your-organization-using-aws-firewall-manager/）</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        },
        {
            "id": "9ee0b122938e4dc5baa36228cbb4bd5a",
            "questionNumber": 200,
            "type": "multiple",
            "content": "Question #200<p>A solutions architect has implemented a SAML 2.0 federated identity solution with their company's on-premises identity provider (IdP) to authenticate users' access to the AWS environment. When the solutions architect tests authentication through the federated identity web portal, access to the AWS environment is granted. However, when test users attempt to authenticate through the federated identity web portal, they are not able to access the AWS environment.</p><p>Which items should the solutions architect check to ensure identity federation is properly configured? (Choose three.)</p>",
            "options": [
                {
                    "label": "A",
                    "content": "The IAM user&#39;s permissions policy has allowed the use of SAML federation for that user."
                },
                {
                    "label": "B",
                    "content": "The IAM roles created for the federated users&#39; or federated groups&#39; trust policy have set the SAML provider as the principal."
                },
                {
                    "label": "C",
                    "content": "Test users are not in the AWSFederatedUsers group in the company&#39;s IdP."
                },
                {
                    "label": "D",
                    "content": "The web portal calls the AWS STS AssumeRoleWithSAML API with the ARN of the SAML provider, the ARN of the IAM role, and the SAML assertion from IdP."
                },
                {
                    "label": "E",
                    "content": "The on-premises IdP&#39;s DNS hostname is reachable from the AWS environment VPCs."
                }
            ],
            "correctAnswer": "BCE",
            "explanation": "Based on the scenario described, here are the three items the solutions architect should check to ensure identity federation is properly configured:<p><br></p><p>1. The IAM roles created for the federated users' or federated groups' trust policy have set the SAML provider as the principal.</p><p>   This is crucial for allowing the SAML-authenticated users to assume the appropriate IAM roles. The trust policy should specify the SAML provider as the principal who can assume the role.</p><p><br></p><p>2. Test users are not in the AWSFederatedUsers group in the company's IdP.</p><p>   If the test users are not part of the correct group in the identity provider, they may not receive the necessary SAML assertions to access AWS resources. Ensure that the test users are added to the appropriate group in the company's IdP.</p><p><br></p><p>3. The on-premises IdP's DNS hostname is reachable from the AWS environment VPCs.</p><p>   For the federation to work correctly, there needs to be proper network connectivity between the on-premises IdP and the AWS environment. Ensuring that the IdP's DNS hostname is reachable from the AWS VPCs is important for the SAML authentication process.</p><p><br></p><p>It's important to note that the option suggesting \"The IAM user's permissions policy has allowed the use of SAML federation for that user\" is not correct in this context. SAML federation doesn't rely on IAM user permissions, but rather on the trust relationship between the SAML provider and the IAM roles.</p><p><br></p><p>To further improve the federation setup and troubleshoot any issues, the solutions architect should also consider the following best practices:</p><p><br></p><p>- Review the SAML metadata document from the IdP to ensure it's correctly configured and uploaded to AWS.</p><p>- Check that the SAML assertions from the IdP contain the correct attributes and values expected by AWS.</p><p>- Verify that the IAM roles have the appropriate permissions policies attached to grant the necessary access to AWS resources.</p><p>- Use AWS CloudTrail logs to investigate any authentication or authorization failures.</p><p>- Test the SAML configuration using the AWS CLI or SDK to isolate any issues specific to console access.</p><p><br></p><p>Remember to always follow the principle of least privilege when configuring IAM roles and policies, and regularly review and audit the federation setup to maintain security best practices.</p><p>Sources</p><p>[1] [SCIM profile and SAML 2.0 implementation - AWS IAM Identity Center] (https://docs.aws.amazon.com/singlesignon/latest/userguide/scim-profile-saml.html)</p><p>[3] [SAML 2.0 federation - AWS Identity and Access Management] (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html)</p><p>[6] [Create a SAML identity provider in IAM - AWS Identity and Access Management] (https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_create_saml.html)</p><p><br></p>",
            "subQuestions": null,
            "caseId": null,
            "caseOrder": null,
            "case": null,
            "caseContent": null,
            "bookmarked": false,
            "hasNote": false
        }
    ],
    "totalQuestions": 529,
    "hasNextPage": true,
    "page": 2,
    "pageSize": 100
}
